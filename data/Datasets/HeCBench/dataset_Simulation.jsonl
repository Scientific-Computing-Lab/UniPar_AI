{"kernel_name": "bspline-vgh", "parallel_api": "cuda", "code": {"main.cu": "#include <chrono>\n#include <cmath>\n#include <cstdio>\n#include <cstdlib>\n#include <cuda.h>\n#include \"kernel.h\"\n\n#define max(a,b) ((a<b)?b:a)\n#define min(a,b) ((a<b)?a:b)\n\nconst int NSIZE_round = NSIZE%16 ? NSIZE+16-NSIZE%16: NSIZE;\nconst size_t SSIZE = (size_t)NSIZE_round*48*48*48;  \n\n\nvoid  eval_abc(const float *Af, float tx, float *a) {\n  a[0] = ( ( Af[0]  * tx + Af[1] ) * tx + Af[2] ) * tx + Af[3];\n  a[1] = ( ( Af[4]  * tx + Af[5] ) * tx + Af[6] ) * tx + Af[7];\n  a[2] = ( ( Af[8]  * tx + Af[9] ) * tx + Af[10] ) * tx + Af[11];\n  a[3] = ( ( Af[12] * tx + Af[13] ) * tx + Af[14] ) * tx + Af[15];\n}\n\nint main(int argc, char ** argv) {\n\n  float *Af = (float*) malloc (sizeof(float)*16);\n  float *dAf = (float*) malloc (sizeof(float)*16);\n  float *d2Af = (float*) malloc (sizeof(float)*16);\n\n  Af[0]=-0.166667;\n  Af[1]=0.500000;\n  Af[2]=-0.500000;\n  Af[3]=0.166667;\n  Af[4]=0.500000;\n  Af[5]=-1.000000;\n  Af[6]=0.000000;\n  Af[7]=0.666667;\n  Af[8]=-0.500000;\n  Af[9]=0.500000;\n  Af[10]=0.500000;\n  Af[11]=0.166667;\n  Af[12]=0.166667;\n  Af[13]=0.000000;\n  Af[14]=0.000000;\n  Af[15]=0.000000;\n  dAf[0]=0.000000; d2Af[0]=0.000000;\n  dAf[1]=-0.500000; d2Af[1]=0.000000;\n  dAf[2]=1.000000; d2Af[2]=-1.000000;\n  dAf[3]=-0.500000; d2Af[3]=1.000000;\n  dAf[4]=0.000000; d2Af[4]=0.000000;\n  dAf[5]=1.500000; d2Af[5]=0.000000;\n  dAf[6]=-2.000000; d2Af[6]=3.000000;\n  dAf[7]=0.000000; d2Af[7]=-2.000000;\n  dAf[8]=0.000000; d2Af[8]=0.000000;\n  dAf[9]=-1.500000; d2Af[9]=0.000000;\n  dAf[10]=1.000000; d2Af[10]=-3.00000;\n  dAf[11]=0.500000; d2Af[11]=1.000000;\n  dAf[12]=0.000000; d2Af[12]=0.000000;\n  dAf[13]=0.500000; d2Af[13]=0.000000;\n  dAf[14]=0.000000; d2Af[14]=1.000000;\n  dAf[15]=0.000000; d2Af[15]=0.000000;\n\n  float x=0.822387;  \n  float y=0.989919;  \n  float z=0.104573;\n\n  float* walkers_vals = (float*) malloc(sizeof(float)*WSIZE*NSIZE);\n  float* walkers_grads = (float*) malloc(sizeof(float)*WSIZE*MSIZE);\n  float* walkers_hess = (float*) malloc(sizeof(float)*WSIZE*OSIZE);\n  float* walkers_x = (float*) malloc(sizeof(float)*WSIZE);\n  float* walkers_y = (float*) malloc(sizeof(float)*WSIZE);\n  float* walkers_z = (float*) malloc(sizeof(float)*WSIZE);\n\n  for (int i=0; i<WSIZE; i++) {\n    walkers_x[i] = x + i*1.0/WSIZE;\n    walkers_y[i] = y + i*1.0/WSIZE;\n    walkers_z[i] = z + i*1.0/WSIZE;\n  }\n\n  float* spline_coefs = (float*) malloc (sizeof(float)*SSIZE);\n  for(size_t i=0;i<SSIZE;i++)\n    spline_coefs[i]=sqrt(0.22+i*1.0)*sin(i*1.0);\n\n  int spline_num_splines = NSIZE;\n  int spline_x_grid_start = 0; \n  int spline_y_grid_start = 0; \n  int spline_z_grid_start = 0; \n  int spline_x_grid_num = 45; \n  int spline_y_grid_num = 45; \n  int spline_z_grid_num = 45; \n  int spline_x_stride=NSIZE_round*48*48;\n  int spline_y_stride=NSIZE_round*48;\n  int spline_z_stride=NSIZE_round;\n  int spline_x_grid_delta_inv=45;\n  int spline_y_grid_delta_inv=45;\n  int spline_z_grid_delta_inv=45;\n\n  float* d_walkers_vals;\n  cudaMalloc((void**)&d_walkers_vals, sizeof(float)*WSIZE*NSIZE);\n  cudaMemcpy(d_walkers_vals, walkers_vals, sizeof(float)*WSIZE*NSIZE, cudaMemcpyHostToDevice);\n\n  float* d_walkers_grads;\n  cudaMalloc((void**)&d_walkers_grads, sizeof(float)*WSIZE*MSIZE);\n  cudaMemcpy(d_walkers_grads, walkers_grads, sizeof(float)*WSIZE*MSIZE, cudaMemcpyHostToDevice);\n\n  float* d_walkers_hess;\n  cudaMalloc((void**)&d_walkers_hess, sizeof(float)*WSIZE*OSIZE);\n  cudaMemcpy(d_walkers_hess, walkers_hess, sizeof(float)*WSIZE*OSIZE, cudaMemcpyHostToDevice);\n\n  float* d_spline_coefs;\n  cudaMalloc((void**)&d_spline_coefs, sizeof(float)*SSIZE);\n  cudaMemcpy(d_spline_coefs, spline_coefs, sizeof(float)*SSIZE, cudaMemcpyHostToDevice);\n\n  float* d_a;\n  cudaMalloc((void**)&d_a, sizeof(float)*4);\n  float* d_b;\n  cudaMalloc((void**)&d_b, sizeof(float)*4);\n  float* d_c;\n  cudaMalloc((void**)&d_c, sizeof(float)*4);\n  float* d_da;\n  cudaMalloc((void**)&d_da, sizeof(float)*4);\n  float* d_db;\n  cudaMalloc((void**)&d_db, sizeof(float)*4);\n  float* d_dc;\n  cudaMalloc((void**)&d_dc, sizeof(float)*4);\n  float* d_d2a;\n  cudaMalloc((void**)&d_d2a, sizeof(float)*4);\n  float* d_d2b;\n  cudaMalloc((void**)&d_d2b, sizeof(float)*4);\n  float* d_d2c;\n  cudaMalloc((void**)&d_d2c, sizeof(float)*4);\n\n  double total_time = 0.0;\n\n  for(int i=0; i<WSIZE; i++) {\n    float x = walkers_x[i], y = walkers_y[i], z = walkers_z[i];\n\n    float ux = x*spline_x_grid_delta_inv;\n    float uy = y*spline_y_grid_delta_inv;\n    float uz = z*spline_z_grid_delta_inv;\n    float ipartx, iparty, ipartz, tx, ty, tz;\n    float a[4], b[4], c[4], da[4], db[4], dc[4], d2a[4], d2b[4], d2c[4];\n    intptr_t xs = spline_x_stride;\n    intptr_t ys = spline_y_stride;\n    intptr_t zs = spline_z_stride;\n\n    x -= spline_x_grid_start;\n    y -= spline_y_grid_start;\n    z -= spline_z_grid_start;\n    ipartx = (int) ux; tx = ux-ipartx; int ix = min(max(0,(int) ipartx),spline_x_grid_num-1);\n    iparty = (int) uy; ty = uy-iparty; int iy = min(max(0,(int) iparty),spline_y_grid_num-1);\n    ipartz = (int) uz; tz = uz-ipartz; int iz = min(max(0,(int) ipartz),spline_z_grid_num-1);\n\n    eval_abc(Af,tx,&a[0]);\n    cudaMemcpy(d_a, a, sizeof(float)*4, cudaMemcpyHostToDevice);\n\n    eval_abc(Af,ty,&b[0]);\n    cudaMemcpy(d_b, b, sizeof(float)*4, cudaMemcpyHostToDevice);\n\n    eval_abc(Af,tz,&c[0]);\n    cudaMemcpy(d_c, c, sizeof(float)*4, cudaMemcpyHostToDevice);\n\n    eval_abc(dAf,tx,&da[0]);\n    cudaMemcpy(d_da, da, sizeof(float)*4, cudaMemcpyHostToDevice);\n\n    eval_abc(dAf,ty,&db[0]);\n    cudaMemcpy(d_db, db, sizeof(float)*4, cudaMemcpyHostToDevice);\n\n    eval_abc(dAf,tz,&dc[0]);\n    cudaMemcpy(d_dc, dc, sizeof(float)*4, cudaMemcpyHostToDevice);\n\n    eval_abc(d2Af,tx,&d2a[0]);\n    cudaMemcpy(d_d2a, d2a, sizeof(float)*4, cudaMemcpyHostToDevice);\n\n    eval_abc(d2Af,ty,&d2b[0]);\n    cudaMemcpy(d_d2b, d2b, sizeof(float)*4, cudaMemcpyHostToDevice);\n\n    eval_abc(d2Af,tz,&d2c[0]);              \n    cudaMemcpy(d_d2c, d2c, sizeof(float)*4, cudaMemcpyHostToDevice);\n\n    dim3 global_size((spline_num_splines+255)/256);\n    dim3 local_size(256);\n\n    cudaDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    bspline<<<global_size, local_size>>>(\n        d_spline_coefs,\n        xs, ys, zs, \n        d_walkers_vals,\n        d_walkers_grads,\n        d_walkers_hess,\n        d_a,\n        d_b,\n        d_c,\n        d_da,\n        d_db,\n        d_dc,\n        d_d2a,\n        d_d2b,\n        d_d2c,\n        spline_x_grid_delta_inv,\n        spline_y_grid_delta_inv,\n        spline_z_grid_delta_inv,\n        spline_num_splines,\n        i, ix, iy, iz\t);\n\n    cudaDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n  }\n  printf(\"Total kernel execution time %lf (s)\\n\", total_time * 1e-9);\n\n  cudaMemcpy(walkers_vals, d_walkers_vals, sizeof(float)*WSIZE*NSIZE, cudaMemcpyDeviceToHost);\n  cudaMemcpy(walkers_grads, d_walkers_grads, sizeof(float)*WSIZE*MSIZE, cudaMemcpyDeviceToHost);\n  cudaMemcpy(walkers_hess, d_walkers_hess, sizeof(float)*WSIZE*OSIZE, cudaMemcpyDeviceToHost);\n\n  \n\n  float resVal = 0.f;\n  float resGrad = 0.f;\n  float resHess = 0.f;\n\n  for( int i = 0; i < NSIZE; i++ ) resVal = resVal + walkers_vals[i];\n  for( int i = 0; i < MSIZE; i++ ) resGrad = resGrad + walkers_grads[i];\n  for( int i = 0; i < OSIZE; i++ ) resHess = resHess + walkers_hess[i];\n  printf(\"walkers[0]->collect([resVal resGrad resHess]) = [%e %e %e]\\n\",\n         resVal,resGrad, resHess);\n\n  free(Af);\n  free(dAf);\n  free(d2Af);\n  free(walkers_vals);\n  free(walkers_grads);\n  free(walkers_hess);\n  free(walkers_x);\n  free(walkers_y);\n  free(walkers_z);\n  free(spline_coefs);\n\n  cudaFree(d_walkers_vals);\n  cudaFree(d_walkers_grads);\n  cudaFree(d_walkers_hess);\n  cudaFree(d_spline_coefs);\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_c);\n  cudaFree(d_da);\n  cudaFree(d_db);\n  cudaFree(d_dc);\n  cudaFree(d_d2a);\n  cudaFree(d_d2b);\n  cudaFree(d_d2c);\n  return 0;\n}\n"}}
{"kernel_name": "bspline-vgh", "parallel_api": "hip", "code": {"main.cu": "#include <chrono>\n#include <cmath>\n#include <cstdio>\n#include <cstdlib>\n#include <hip/hip_runtime.h>\n#include \"kernel.h\"\n\n#define max(a,b) ((a<b)?b:a)\n#define min(a,b) ((a<b)?a:b)\n\nconst int NSIZE_round = NSIZE%16 ? NSIZE+16-NSIZE%16: NSIZE;\nconst size_t SSIZE = (size_t)NSIZE_round*48*48*48;  \n\n\nvoid  eval_abc(const float *Af, float tx, float *a) {\n  a[0] = ( ( Af[0]  * tx + Af[1] ) * tx + Af[2] ) * tx + Af[3];\n  a[1] = ( ( Af[4]  * tx + Af[5] ) * tx + Af[6] ) * tx + Af[7];\n  a[2] = ( ( Af[8]  * tx + Af[9] ) * tx + Af[10] ) * tx + Af[11];\n  a[3] = ( ( Af[12] * tx + Af[13] ) * tx + Af[14] ) * tx + Af[15];\n}\n\nint main(int argc, char ** argv) {\n\n  float *Af = (float*) malloc (sizeof(float)*16);\n  float *dAf = (float*) malloc (sizeof(float)*16);\n  float *d2Af = (float*) malloc (sizeof(float)*16);\n\n  Af[0]=-0.166667;\n  Af[1]=0.500000;\n  Af[2]=-0.500000;\n  Af[3]=0.166667;\n  Af[4]=0.500000;\n  Af[5]=-1.000000;\n  Af[6]=0.000000;\n  Af[7]=0.666667;\n  Af[8]=-0.500000;\n  Af[9]=0.500000;\n  Af[10]=0.500000;\n  Af[11]=0.166667;\n  Af[12]=0.166667;\n  Af[13]=0.000000;\n  Af[14]=0.000000;\n  Af[15]=0.000000;\n  dAf[0]=0.000000; d2Af[0]=0.000000;\n  dAf[1]=-0.500000; d2Af[1]=0.000000;\n  dAf[2]=1.000000; d2Af[2]=-1.000000;\n  dAf[3]=-0.500000; d2Af[3]=1.000000;\n  dAf[4]=0.000000; d2Af[4]=0.000000;\n  dAf[5]=1.500000; d2Af[5]=0.000000;\n  dAf[6]=-2.000000; d2Af[6]=3.000000;\n  dAf[7]=0.000000; d2Af[7]=-2.000000;\n  dAf[8]=0.000000; d2Af[8]=0.000000;\n  dAf[9]=-1.500000; d2Af[9]=0.000000;\n  dAf[10]=1.000000; d2Af[10]=-3.00000;\n  dAf[11]=0.500000; d2Af[11]=1.000000;\n  dAf[12]=0.000000; d2Af[12]=0.000000;\n  dAf[13]=0.500000; d2Af[13]=0.000000;\n  dAf[14]=0.000000; d2Af[14]=1.000000;\n  dAf[15]=0.000000; d2Af[15]=0.000000;\n\n  float x=0.822387;  \n  float y=0.989919;  \n  float z=0.104573;\n\n  float* walkers_vals = (float*) malloc(sizeof(float)*WSIZE*NSIZE);\n  float* walkers_grads = (float*) malloc(sizeof(float)*WSIZE*MSIZE);\n  float* walkers_hess = (float*) malloc(sizeof(float)*WSIZE*OSIZE);\n  float* walkers_x = (float*) malloc(sizeof(float)*WSIZE);\n  float* walkers_y = (float*) malloc(sizeof(float)*WSIZE);\n  float* walkers_z = (float*) malloc(sizeof(float)*WSIZE);\n\n  for (int i=0; i<WSIZE; i++) {\n    walkers_x[i] = x + i*1.0/WSIZE;\n    walkers_y[i] = y + i*1.0/WSIZE;\n    walkers_z[i] = z + i*1.0/WSIZE;\n  }\n\n  float* spline_coefs = (float*) malloc (sizeof(float)*SSIZE);\n  for(size_t i=0;i<SSIZE;i++)\n    spline_coefs[i]=sqrt(0.22+i*1.0)*sin(i*1.0);\n\n  int spline_num_splines = NSIZE;\n  int spline_x_grid_start = 0; \n  int spline_y_grid_start = 0; \n  int spline_z_grid_start = 0; \n  int spline_x_grid_num = 45; \n  int spline_y_grid_num = 45; \n  int spline_z_grid_num = 45; \n  int spline_x_stride=NSIZE_round*48*48;\n  int spline_y_stride=NSIZE_round*48;\n  int spline_z_stride=NSIZE_round;\n  int spline_x_grid_delta_inv=45;\n  int spline_y_grid_delta_inv=45;\n  int spline_z_grid_delta_inv=45;\n\n  float* d_walkers_vals;\n  hipMalloc((void**)&d_walkers_vals, sizeof(float)*WSIZE*NSIZE);\n  hipMemcpy(d_walkers_vals, walkers_vals, sizeof(float)*WSIZE*NSIZE, hipMemcpyHostToDevice);\n\n  float* d_walkers_grads;\n  hipMalloc((void**)&d_walkers_grads, sizeof(float)*WSIZE*MSIZE);\n  hipMemcpy(d_walkers_grads, walkers_grads, sizeof(float)*WSIZE*MSIZE, hipMemcpyHostToDevice);\n\n  float* d_walkers_hess;\n  hipMalloc((void**)&d_walkers_hess, sizeof(float)*WSIZE*OSIZE);\n  hipMemcpy(d_walkers_hess, walkers_hess, sizeof(float)*WSIZE*OSIZE, hipMemcpyHostToDevice);\n\n  float* d_spline_coefs;\n  hipMalloc((void**)&d_spline_coefs, sizeof(float)*SSIZE);\n  hipMemcpy(d_spline_coefs, spline_coefs, sizeof(float)*SSIZE, hipMemcpyHostToDevice);\n\n  float* d_a;\n  hipMalloc((void**)&d_a, sizeof(float)*4);\n  float* d_b;\n  hipMalloc((void**)&d_b, sizeof(float)*4);\n  float* d_c;\n  hipMalloc((void**)&d_c, sizeof(float)*4);\n  float* d_da;\n  hipMalloc((void**)&d_da, sizeof(float)*4);\n  float* d_db;\n  hipMalloc((void**)&d_db, sizeof(float)*4);\n  float* d_dc;\n  hipMalloc((void**)&d_dc, sizeof(float)*4);\n  float* d_d2a;\n  hipMalloc((void**)&d_d2a, sizeof(float)*4);\n  float* d_d2b;\n  hipMalloc((void**)&d_d2b, sizeof(float)*4);\n  float* d_d2c;\n  hipMalloc((void**)&d_d2c, sizeof(float)*4);\n\n  double total_time = 0.0;\n\n  for(int i=0; i<WSIZE; i++) {\n    float x = walkers_x[i], y = walkers_y[i], z = walkers_z[i];\n\n    float ux = x*spline_x_grid_delta_inv;\n    float uy = y*spline_y_grid_delta_inv;\n    float uz = z*spline_z_grid_delta_inv;\n    float ipartx, iparty, ipartz, tx, ty, tz;\n    float a[4], b[4], c[4], da[4], db[4], dc[4], d2a[4], d2b[4], d2c[4];\n    intptr_t xs = spline_x_stride;\n    intptr_t ys = spline_y_stride;\n    intptr_t zs = spline_z_stride;\n\n    x -= spline_x_grid_start;\n    y -= spline_y_grid_start;\n    z -= spline_z_grid_start;\n    ipartx = (int) ux; tx = ux-ipartx; int ix = min(max(0,(int) ipartx),spline_x_grid_num-1);\n    iparty = (int) uy; ty = uy-iparty; int iy = min(max(0,(int) iparty),spline_y_grid_num-1);\n    ipartz = (int) uz; tz = uz-ipartz; int iz = min(max(0,(int) ipartz),spline_z_grid_num-1);\n\n    eval_abc(Af,tx,&a[0]);\n    hipMemcpy(d_a, a, sizeof(float)*4, hipMemcpyHostToDevice);\n\n    eval_abc(Af,ty,&b[0]);\n    hipMemcpy(d_b, b, sizeof(float)*4, hipMemcpyHostToDevice);\n\n    eval_abc(Af,tz,&c[0]);\n    hipMemcpy(d_c, c, sizeof(float)*4, hipMemcpyHostToDevice);\n\n    eval_abc(dAf,tx,&da[0]);\n    hipMemcpy(d_da, da, sizeof(float)*4, hipMemcpyHostToDevice);\n\n    eval_abc(dAf,ty,&db[0]);\n    hipMemcpy(d_db, db, sizeof(float)*4, hipMemcpyHostToDevice);\n\n    eval_abc(dAf,tz,&dc[0]);\n    hipMemcpy(d_dc, dc, sizeof(float)*4, hipMemcpyHostToDevice);\n\n    eval_abc(d2Af,tx,&d2a[0]);\n    hipMemcpy(d_d2a, d2a, sizeof(float)*4, hipMemcpyHostToDevice);\n\n    eval_abc(d2Af,ty,&d2b[0]);\n    hipMemcpy(d_d2b, d2b, sizeof(float)*4, hipMemcpyHostToDevice);\n\n    eval_abc(d2Af,tz,&d2c[0]);              \n    hipMemcpy(d_d2c, d2c, sizeof(float)*4, hipMemcpyHostToDevice);\n\n    dim3 global_size((spline_num_splines+255)/256);\n    dim3 local_size(256);\n\n    hipDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    hipLaunchKernelGGL(bspline, global_size, local_size, 0, 0, \n        d_spline_coefs,\n        xs, ys, zs, \n        d_walkers_vals,\n        d_walkers_grads,\n        d_walkers_hess,\n        d_a,\n        d_b,\n        d_c,\n        d_da,\n        d_db,\n        d_dc,\n        d_d2a,\n        d_d2b,\n        d_d2c,\n        spline_x_grid_delta_inv,\n        spline_y_grid_delta_inv,\n        spline_z_grid_delta_inv,\n        spline_num_splines,\n        i, ix, iy, iz\t);\n\n    hipDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n  }\n  printf(\"Total kernel execution time %lf (s)\\n\", total_time * 1e-9);\n\n  hipMemcpy(walkers_vals, d_walkers_vals, sizeof(float)*WSIZE*NSIZE, hipMemcpyDeviceToHost);\n  hipMemcpy(walkers_grads, d_walkers_grads, sizeof(float)*WSIZE*MSIZE, hipMemcpyDeviceToHost);\n  hipMemcpy(walkers_hess, d_walkers_hess, sizeof(float)*WSIZE*OSIZE, hipMemcpyDeviceToHost);\n\n  \n\n  float resVal = 0.f;\n  float resGrad = 0.f;\n  float resHess = 0.f;\n\n  for( int i = 0; i < NSIZE; i++ ) resVal = resVal + walkers_vals[i];\n  for( int i = 0; i < MSIZE; i++ ) resGrad = resGrad + walkers_grads[i];\n  for( int i = 0; i < OSIZE; i++ ) resHess = resHess + walkers_hess[i];\n  printf(\"walkers[0]->collect([resVal resGrad resHess]) = [%e %e %e]\\n\",\n         resVal,resGrad, resHess);\n\n  free(Af);\n  free(dAf);\n  free(d2Af);\n  free(walkers_vals);\n  free(walkers_grads);\n  free(walkers_hess);\n  free(walkers_x);\n  free(walkers_y);\n  free(walkers_z);\n  free(spline_coefs);\n\n  hipFree(d_walkers_vals);\n  hipFree(d_walkers_grads);\n  hipFree(d_walkers_hess);\n  hipFree(d_spline_coefs);\n  hipFree(d_a);\n  hipFree(d_b);\n  hipFree(d_c);\n  hipFree(d_da);\n  hipFree(d_db);\n  hipFree(d_dc);\n  hipFree(d_d2a);\n  hipFree(d_d2b);\n  hipFree(d_d2c);\n  return 0;\n}\n"}}
{"kernel_name": "bspline-vgh", "parallel_api": "omp", "code": {"main.cpp": "#include <chrono>\n#include <cmath>\n#include <cstdio>\n#include <cstdlib>\n#include <cstdint>\n#include <omp.h>\n\n#define max(a,b) ((a<b)?b:a)\n#define min(a,b) ((a<b)?a:b)\n\nconst int WSIZE = 12000;          \n\nconst int NSIZE = 2003;           \n\nconst int MSIZE = NSIZE*3+3;      \n\nconst int OSIZE = NSIZE*9+9;      \n\n\nconst int NSIZE_round = NSIZE%16 ? NSIZE+16-NSIZE%16: NSIZE;\nconst size_t SSIZE = (size_t)NSIZE_round*48*48*48;  \n\n\nvoid eval_abc(const float *Af, float tx, float *a) {\n\n  a[0] = ( ( Af[0]  * tx + Af[1] ) * tx + Af[2] ) * tx + Af[3];\n  a[1] = ( ( Af[4]  * tx + Af[5] ) * tx + Af[6] ) * tx + Af[7];\n  a[2] = ( ( Af[8]  * tx + Af[9] ) * tx + Af[10] ) * tx + Af[11];\n  a[3] = ( ( Af[12] * tx + Af[13] ) * tx + Af[14] ) * tx + Af[15];\n}\n\n#pragma omp declare target\n\n#pragma omp declare simd \n\nstatic inline void eval_UBspline_3d_s_vgh (\n    const float * __restrict coefs_init,\n    const intptr_t xs,\n    const intptr_t ys,\n    const intptr_t zs,\n    float * __restrict vals,\n    float * __restrict grads,\n    float * __restrict hess,\n    const float *   a, const float *   b, const float *   c,\n    const float *  da, const float *  db, const float *  dc,\n    const float * d2a, const float * d2b, const float * d2c,\n    const float dxInv, const float dyInv, const float dzInv)\n{\n  float h[9];\n  float v0 = 0.0f;\n  for (int i = 0; i < 9; ++i) h[i] = 0.0f;\n\n  for (int i=0; i<4; i++)\n    for (int j=0; j<4; j++) {\n      float pre20 = d2a[i]*  b[j];\n      float pre10 =  da[i]*  b[j];\n      float pre00 =   a[i]*  b[j];\n      float pre11 =  da[i]* db[j];\n      float pre01 =   a[i]* db[j];\n      float pre02 =   a[i]*d2b[j];\n\n      const float *coefs = coefs_init + i*xs + j*ys;\n\n      float sum0 =   c[0] * coefs[0] +   c[1] * coefs[zs] +   c[2] * coefs[zs*2] +   c[3] * coefs[zs*3];\n      float sum1 =  dc[0] * coefs[0] +  dc[1] * coefs[zs] +  dc[2] * coefs[zs*2] +  dc[3] * coefs[zs*3];\n      float sum2 = d2c[0] * coefs[0] + d2c[1] * coefs[zs] + d2c[2] * coefs[zs*2] + d2c[3] * coefs[zs*3];\n\n      h[0]  += pre20 * sum0;\n      h[1]  += pre11 * sum0;\n      h[2]  += pre10 * sum1;\n      h[4]  += pre02 * sum0;\n      h[5]  += pre01 * sum1;\n      h[8]  += pre00 * sum2;\n      h[3]  += pre10 * sum0;\n      h[6]  += pre01 * sum0;\n      h[7]  += pre00 * sum1;\n      v0    += pre00 * sum0;\n    }\n  vals[0] = v0;\n  grads[0]  = h[3] * dxInv;\n  grads[1]  = h[6] * dyInv;\n  grads[2]  = h[7] * dzInv;\n\n  hess [0] = h[0]*dxInv*dxInv;\n  hess [1] = h[1]*dxInv*dyInv;\n  hess [2] = h[2]*dxInv*dzInv;\n  hess [3] = h[1]*dxInv*dyInv; \n\n  hess [4] = h[4]*dyInv*dyInv;\n  hess [5] = h[5]*dyInv*dzInv;\n  hess [6] = h[2]*dxInv*dzInv; \n\n  hess [7] = h[5]*dyInv*dzInv; \n\n  hess [8] = h[8]*dzInv*dzInv;\n}\n#pragma omp end declare target\n\nint main(int argc, char ** argv) {\n\n  float *Af = (float*) malloc (sizeof(float)*16);\n  float *dAf = (float*) malloc (sizeof(float)*16);\n  float *d2Af = (float*) malloc (sizeof(float)*16);\n\n  Af[0]=-0.166667;\n  Af[1]=0.500000;\n  Af[2]=-0.500000;\n  Af[3]=0.166667;\n  Af[4]=0.500000;\n  Af[5]=-1.000000;\n  Af[6]=0.000000;\n  Af[7]=0.666667;\n  Af[8]=-0.500000;\n  Af[9]=0.500000;\n  Af[10]=0.500000;\n  Af[11]=0.166667;\n  Af[12]=0.166667;\n  Af[13]=0.000000;\n  Af[14]=0.000000;\n  Af[15]=0.000000;\n  dAf[0]=0.000000; d2Af[0]=0.000000;\n  dAf[1]=-0.500000; d2Af[1]=0.000000;\n  dAf[2]=1.000000; d2Af[2]=-1.000000;\n  dAf[3]=-0.500000; d2Af[3]=1.000000;\n  dAf[4]=0.000000; d2Af[4]=0.000000;\n  dAf[5]=1.500000; d2Af[5]=0.000000;\n  dAf[6]=-2.000000; d2Af[6]=3.000000;\n  dAf[7]=0.000000; d2Af[7]=-2.000000;\n  dAf[8]=0.000000; d2Af[8]=0.000000;\n  dAf[9]=-1.500000; d2Af[9]=0.000000;\n  dAf[10]=1.000000; d2Af[10]=-3.00000;\n  dAf[11]=0.500000; d2Af[11]=1.000000;\n  dAf[12]=0.000000; d2Af[12]=0.000000;\n  dAf[13]=0.500000; d2Af[13]=0.000000;\n  dAf[14]=0.000000; d2Af[14]=1.000000;\n  dAf[15]=0.000000; d2Af[15]=0.000000;\n\n  float x=0.822387;  \n  float y=0.989919;  \n  float z=0.104573;\n\n  float* walkers_vals = (float*) malloc(sizeof(float)*WSIZE*NSIZE);\n  float* walkers_grads = (float*) malloc(sizeof(float)*WSIZE*MSIZE);\n  float* walkers_hess = (float*) malloc(sizeof(float)*WSIZE*OSIZE);\n  float* walkers_x = (float*) malloc(sizeof(float)*WSIZE);\n  float* walkers_y = (float*) malloc(sizeof(float)*WSIZE);\n  float* walkers_z = (float*) malloc(sizeof(float)*WSIZE);\n\n  for (int i=0; i<WSIZE; i++) {\n    walkers_x[i] = x + i*1.0/WSIZE;\n    walkers_y[i] = y + i*1.0/WSIZE;\n    walkers_z[i] = z + i*1.0/WSIZE;\n  }\n\n  float* spline_coefs = (float*) malloc (sizeof(float)*SSIZE);\n  for(size_t i=0;i<SSIZE;i++)\n    spline_coefs[i]=sqrt(0.22+i*1.0)*sin(i*1.0);\n\n  int spline_num_splines = NSIZE;\n  int spline_x_grid_start = 0; \n  int spline_y_grid_start = 0; \n  int spline_z_grid_start = 0; \n  int spline_x_grid_num = 45; \n  int spline_y_grid_num = 45; \n  int spline_z_grid_num = 45; \n  int spline_x_stride=NSIZE_round*48*48;\n  int spline_y_stride=NSIZE_round*48;\n  int spline_z_stride=NSIZE_round;\n  int spline_x_grid_delta_inv=45;\n  int spline_y_grid_delta_inv=45;\n  int spline_z_grid_delta_inv=45;\n\n  float a[4], b[4], c[4], da[4], db[4], dc[4], d2a[4], d2b[4], d2c[4]; \n\n  #pragma omp target data map(from: walkers_vals[0:WSIZE*NSIZE],\\\n                                    walkers_grads[0:WSIZE*MSIZE], \\\n                                    walkers_hess[0:WSIZE*OSIZE])\\\n                          map(to: spline_coefs[0:SSIZE]) \\\n                          map(alloc: a[0:4], b[0:4], c[0:4], \\\n                                     da[0:4], db[0:4], dc[0:4], \\\n                                     d2a[0:4], d2b[0:4], d2c[0:4])\n  {\n    double total_time = 0.0;\n\n    for(int i=0; i<WSIZE; i++) {\n      float x = walkers_x[i], y = walkers_y[i], z = walkers_z[i];\n\n      float *vals  = &walkers_vals[i*NSIZE];\n      float *grads = &walkers_grads[i*MSIZE];\n      float *hess  = &walkers_hess[i*OSIZE];\n      float ux = x*spline_x_grid_delta_inv;\n      float uy = y*spline_y_grid_delta_inv;\n      float uz = z*spline_z_grid_delta_inv;\n      float ipartx, iparty, ipartz, tx, ty, tz;\n      intptr_t xs = spline_x_stride;\n      intptr_t ys = spline_y_stride;\n      intptr_t zs = spline_z_stride;\n\n      x -= spline_x_grid_start;\n      y -= spline_y_grid_start;\n      z -= spline_z_grid_start;\n      ipartx = (int) ux; tx = ux-ipartx;    int ix = min(max(0,(int) ipartx),spline_x_grid_num-1);\n      iparty = (int) uy; ty = uy-iparty;    int iy = min(max(0,(int) iparty),spline_y_grid_num-1);\n      ipartz = (int) uz; tz = uz-ipartz;    int iz = min(max(0,(int) ipartz),spline_z_grid_num-1);\n\n      eval_abc(Af,tx,&a[0]);\n      eval_abc(Af,ty,&b[0]);\n      eval_abc(Af,tz,&c[0]);\n      eval_abc(dAf,tx,&da[0]);\n      eval_abc(dAf,ty,&db[0]);\n      eval_abc(dAf,tz,&dc[0]);\n      eval_abc(d2Af,tx,&d2a[0]);\n      eval_abc(d2Af,ty,&d2b[0]);\n      eval_abc(d2Af,tz,&d2c[0]);              \n\n      #pragma omp target update to(a[0:4])\n      #pragma omp target update to(b[0:4])\n      #pragma omp target update to(c[0:4])\n      #pragma omp target update to(da[0:4])\n      #pragma omp target update to(db[0:4])\n      #pragma omp target update to(dc[0:4])\n      #pragma omp target update to(d2a[0:4])\n      #pragma omp target update to(d2b[0:4])\n      #pragma omp target update to(d2c[0:4])\n\n      auto start = std::chrono::steady_clock::now();\n\n      #pragma omp target teams distribute parallel for thread_limit(256)\n      for (int n = 0; n < spline_num_splines; n++) {\n        eval_UBspline_3d_s_vgh ( spline_coefs + ix*xs + iy*ys + iz*zs + n,\n            xs, ys, zs, vals+n, grads+n*3, hess+n*9,\n            a, b, c, da, db, dc, d2a, d2b, d2c,\n            spline_x_grid_delta_inv,\n            spline_y_grid_delta_inv,\n            spline_z_grid_delta_inv );\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      total_time += time;\n    }\n\n    printf(\"Total kernel execution time %lf (s)\\n\", total_time * 1e-9);\n  }\n\n  \n\n  float resVal = 0.0;\n  float resGrad = 0.0;\n  float resHess = 0.0;\n\n  for( int i = 0; i < NSIZE; i++ ) resVal = resVal + walkers_vals[i];\n  for( int i = 0; i < MSIZE; i++ ) resGrad = resGrad + walkers_grads[i];\n  for( int i = 0; i < OSIZE; i++ ) resHess = resHess + walkers_hess[i];\n  printf(\"walkers[0]->collect([resVal resGrad resHess]) = [%e %e %e]\\n\",\n         resVal,resGrad, resHess);  \n\n  free(Af);\n  free(dAf);\n  free(d2Af);\n  free(walkers_vals);\n  free(walkers_grads);\n  free(walkers_hess);\n  free(walkers_x);\n  free(walkers_y);\n  free(walkers_z);\n  free(spline_coefs);\n\n  return 0;\n}\n"}}
{"kernel_name": "bspline-vgh", "parallel_api": "serial", "code": {"main.cpp": "#include <chrono>\n#include <cmath>\n#include <cstdio>\n#include <cstdlib>\n#include <cstdint>\n\n#define max(a,b) ((a<b)?b:a)\n#define min(a,b) ((a<b)?a:b)\n\nconst int WSIZE = 12000;          \n\nconst int NSIZE = 2003;           \n\nconst int MSIZE = NSIZE*3+3;      \n\nconst int OSIZE = NSIZE*9+9;      \n\n\nconst int NSIZE_round = NSIZE%16 ? NSIZE+16-NSIZE%16: NSIZE;\nconst size_t SSIZE = (size_t)NSIZE_round*48*48*48;  \n\n\nvoid eval_abc(const float *Af, float tx, float *a) {\n\n  a[0] = ( ( Af[0]  * tx + Af[1] ) * tx + Af[2] ) * tx + Af[3];\n  a[1] = ( ( Af[4]  * tx + Af[5] ) * tx + Af[6] ) * tx + Af[7];\n  a[2] = ( ( Af[8]  * tx + Af[9] ) * tx + Af[10] ) * tx + Af[11];\n  a[3] = ( ( Af[12] * tx + Af[13] ) * tx + Af[14] ) * tx + Af[15];\n}\n\n\n\nstatic inline void eval_UBspline_3d_s_vgh (\n    const float * __restrict coefs_init,\n    const intptr_t xs,\n    const intptr_t ys,\n    const intptr_t zs,\n    float * __restrict vals,\n    float * __restrict grads,\n    float * __restrict hess,\n    const float *   a, const float *   b, const float *   c,\n    const float *  da, const float *  db, const float *  dc,\n    const float * d2a, const float * d2b, const float * d2c,\n    const float dxInv, const float dyInv, const float dzInv)\n{\n  float h[9];\n  float v0 = 0.0f;\n  for (int i = 0; i < 9; ++i) h[i] = 0.0f;\n\n  for (int i=0; i<4; i++)\n    for (int j=0; j<4; j++) {\n      float pre20 = d2a[i]*  b[j];\n      float pre10 =  da[i]*  b[j];\n      float pre00 =   a[i]*  b[j];\n      float pre11 =  da[i]* db[j];\n      float pre01 =   a[i]* db[j];\n      float pre02 =   a[i]*d2b[j];\n\n      const float *coefs = coefs_init + i*xs + j*ys;\n\n      float sum0 =   c[0] * coefs[0] +   c[1] * coefs[zs] +   c[2] * coefs[zs*2] +   c[3] * coefs[zs*3];\n      float sum1 =  dc[0] * coefs[0] +  dc[1] * coefs[zs] +  dc[2] * coefs[zs*2] +  dc[3] * coefs[zs*3];\n      float sum2 = d2c[0] * coefs[0] + d2c[1] * coefs[zs] + d2c[2] * coefs[zs*2] + d2c[3] * coefs[zs*3];\n\n      h[0]  += pre20 * sum0;\n      h[1]  += pre11 * sum0;\n      h[2]  += pre10 * sum1;\n      h[4]  += pre02 * sum0;\n      h[5]  += pre01 * sum1;\n      h[8]  += pre00 * sum2;\n      h[3]  += pre10 * sum0;\n      h[6]  += pre01 * sum0;\n      h[7]  += pre00 * sum1;\n      v0    += pre00 * sum0;\n    }\n  vals[0] = v0;\n  grads[0]  = h[3] * dxInv;\n  grads[1]  = h[6] * dyInv;\n  grads[2]  = h[7] * dzInv;\n\n  hess [0] = h[0]*dxInv*dxInv;\n  hess [1] = h[1]*dxInv*dyInv;\n  hess [2] = h[2]*dxInv*dzInv;\n  hess [3] = h[1]*dxInv*dyInv; \n\n  hess [4] = h[4]*dyInv*dyInv;\n  hess [5] = h[5]*dyInv*dzInv;\n  hess [6] = h[2]*dxInv*dzInv; \n\n  hess [7] = h[5]*dyInv*dzInv; \n\n  hess [8] = h[8]*dzInv*dzInv;\n}\n\nint main(int argc, char ** argv) {\n\n  float *Af = (float*) malloc (sizeof(float)*16);\n  float *dAf = (float*) malloc (sizeof(float)*16);\n  float *d2Af = (float*) malloc (sizeof(float)*16);\n\n  Af[0]=-0.166667;\n  Af[1]=0.500000;\n  Af[2]=-0.500000;\n  Af[3]=0.166667;\n  Af[4]=0.500000;\n  Af[5]=-1.000000;\n  Af[6]=0.000000;\n  Af[7]=0.666667;\n  Af[8]=-0.500000;\n  Af[9]=0.500000;\n  Af[10]=0.500000;\n  Af[11]=0.166667;\n  Af[12]=0.166667;\n  Af[13]=0.000000;\n  Af[14]=0.000000;\n  Af[15]=0.000000;\n  dAf[0]=0.000000; d2Af[0]=0.000000;\n  dAf[1]=-0.500000; d2Af[1]=0.000000;\n  dAf[2]=1.000000; d2Af[2]=-1.000000;\n  dAf[3]=-0.500000; d2Af[3]=1.000000;\n  dAf[4]=0.000000; d2Af[4]=0.000000;\n  dAf[5]=1.500000; d2Af[5]=0.000000;\n  dAf[6]=-2.000000; d2Af[6]=3.000000;\n  dAf[7]=0.000000; d2Af[7]=-2.000000;\n  dAf[8]=0.000000; d2Af[8]=0.000000;\n  dAf[9]=-1.500000; d2Af[9]=0.000000;\n  dAf[10]=1.000000; d2Af[10]=-3.00000;\n  dAf[11]=0.500000; d2Af[11]=1.000000;\n  dAf[12]=0.000000; d2Af[12]=0.000000;\n  dAf[13]=0.500000; d2Af[13]=0.000000;\n  dAf[14]=0.000000; d2Af[14]=1.000000;\n  dAf[15]=0.000000; d2Af[15]=0.000000;\n\n  float x=0.822387;  \n  float y=0.989919;  \n  float z=0.104573;\n\n  float* walkers_vals = (float*) malloc(sizeof(float)*WSIZE*NSIZE);\n  float* walkers_grads = (float*) malloc(sizeof(float)*WSIZE*MSIZE);\n  float* walkers_hess = (float*) malloc(sizeof(float)*WSIZE*OSIZE);\n  float* walkers_x = (float*) malloc(sizeof(float)*WSIZE);\n  float* walkers_y = (float*) malloc(sizeof(float)*WSIZE);\n  float* walkers_z = (float*) malloc(sizeof(float)*WSIZE);\n\n  for (int i=0; i<WSIZE; i++) {\n    walkers_x[i] = x + i*1.0/WSIZE;\n    walkers_y[i] = y + i*1.0/WSIZE;\n    walkers_z[i] = z + i*1.0/WSIZE;\n  }\n\n  float* spline_coefs = (float*) malloc (sizeof(float)*SSIZE);\n  for(size_t i=0;i<SSIZE;i++)\n    spline_coefs[i]=sqrt(0.22+i*1.0)*sin(i*1.0);\n\n  int spline_num_splines = NSIZE;\n  int spline_x_grid_start = 0; \n  int spline_y_grid_start = 0; \n  int spline_z_grid_start = 0; \n  int spline_x_grid_num = 45; \n  int spline_y_grid_num = 45; \n  int spline_z_grid_num = 45; \n  int spline_x_stride=NSIZE_round*48*48;\n  int spline_y_stride=NSIZE_round*48;\n  int spline_z_stride=NSIZE_round;\n  int spline_x_grid_delta_inv=45;\n  int spline_y_grid_delta_inv=45;\n  int spline_z_grid_delta_inv=45;\n\n  float a[4], b[4], c[4], da[4], db[4], dc[4], d2a[4], d2b[4], d2c[4]; \n\n    {\n    double total_time = 0.0;\n\n    for(int i=0; i<WSIZE; i++) {\n      float x = walkers_x[i], y = walkers_y[i], z = walkers_z[i];\n\n      float *vals  = &walkers_vals[i*NSIZE];\n      float *grads = &walkers_grads[i*MSIZE];\n      float *hess  = &walkers_hess[i*OSIZE];\n      float ux = x*spline_x_grid_delta_inv;\n      float uy = y*spline_y_grid_delta_inv;\n      float uz = z*spline_z_grid_delta_inv;\n      float ipartx, iparty, ipartz, tx, ty, tz;\n      intptr_t xs = spline_x_stride;\n      intptr_t ys = spline_y_stride;\n      intptr_t zs = spline_z_stride;\n\n      x -= spline_x_grid_start;\n      y -= spline_y_grid_start;\n      z -= spline_z_grid_start;\n      ipartx = (int) ux; tx = ux-ipartx;    int ix = min(max(0,(int) ipartx),spline_x_grid_num-1);\n      iparty = (int) uy; ty = uy-iparty;    int iy = min(max(0,(int) iparty),spline_y_grid_num-1);\n      ipartz = (int) uz; tz = uz-ipartz;    int iz = min(max(0,(int) ipartz),spline_z_grid_num-1);\n\n      eval_abc(Af,tx,&a[0]);\n      eval_abc(Af,ty,&b[0]);\n      eval_abc(Af,tz,&c[0]);\n      eval_abc(dAf,tx,&da[0]);\n      eval_abc(dAf,ty,&db[0]);\n      eval_abc(dAf,tz,&dc[0]);\n      eval_abc(d2Af,tx,&d2a[0]);\n      eval_abc(d2Af,ty,&d2b[0]);\n      eval_abc(d2Af,tz,&d2c[0]);              \n\n                                                      \n      auto start = std::chrono::steady_clock::now();\n\n            for (int n = 0; n < spline_num_splines; n++) {\n        eval_UBspline_3d_s_vgh ( spline_coefs + ix*xs + iy*ys + iz*zs + n,\n            xs, ys, zs, vals+n, grads+n*3, hess+n*9,\n            a, b, c, da, db, dc, d2a, d2b, d2c,\n            spline_x_grid_delta_inv,\n            spline_y_grid_delta_inv,\n            spline_z_grid_delta_inv );\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      total_time += time;\n    }\n\n    printf(\"Total kernel execution time %lf (s)\\n\", total_time * 1e-9);\n  }\n\n  \n\n  float resVal = 0.0;\n  float resGrad = 0.0;\n  float resHess = 0.0;\n\n  for( int i = 0; i < NSIZE; i++ ) resVal = resVal + walkers_vals[i];\n  for( int i = 0; i < MSIZE; i++ ) resGrad = resGrad + walkers_grads[i];\n  for( int i = 0; i < OSIZE; i++ ) resHess = resHess + walkers_hess[i];\n  printf(\"walkers[0]->collect([resVal resGrad resHess]) = [%e %e %e]\\n\",\n         resVal,resGrad, resHess);  \n\n  free(Af);\n  free(dAf);\n  free(d2Af);\n  free(walkers_vals);\n  free(walkers_grads);\n  free(walkers_hess);\n  free(walkers_x);\n  free(walkers_y);\n  free(walkers_z);\n  free(spline_coefs);\n\n  return 0;\n}"}}
{"kernel_name": "bspline-vgh", "parallel_api": "sycl", "code": {"main.cpp": "#include <chrono>\n#include <cmath>\n#include <cstdio>\n#include <cstdlib>\n#include <sycl/sycl.hpp>\n\n#define max(a,b) ((a<b)?b:a)\n#define min(a,b) ((a<b)?a:b)\n\nconst int WSIZE = 12000;          \n\nconst int NSIZE = 2003;           \n\nconst int MSIZE = NSIZE*3+3;      \n\nconst int OSIZE = NSIZE*9+9;      \n\n\nconst int NSIZE_round = NSIZE%16 ? NSIZE+16-NSIZE%16: NSIZE;\nconst size_t SSIZE = (size_t)NSIZE_round*48*48*48;  \n\n\nvoid eval_abc(const float *Af, float tx, float *a) {\n\n  a[0] = ( ( Af[0]  * tx + Af[1] ) * tx + Af[2] ) * tx + Af[3];\n  a[1] = ( ( Af[4]  * tx + Af[5] ) * tx + Af[6] ) * tx + Af[7];\n  a[2] = ( ( Af[8]  * tx + Af[9] ) * tx + Af[10] ) * tx + Af[11];\n  a[3] = ( ( Af[12] * tx + Af[13] ) * tx + Af[14] ) * tx + Af[15];\n}\n\nstatic inline void eval_UBspline_3d_s_vgh (\n    const float * __restrict coefs_init,\n    const intptr_t xs,\n    const intptr_t ys,\n    const intptr_t zs,\n    float * __restrict vals,\n    float * __restrict grads,\n    float * __restrict hess,\n    const float *   a, const float *   b, const float *   c,\n    const float *  da, const float *  db, const float *  dc,\n    const float * d2a, const float * d2b, const float * d2c,\n    const float dxInv, const float dyInv, const float dzInv)\n{\n  float h[9];\n  float v0 = 0.0f;\n  for (int i = 0; i < 9; ++i) h[i] = 0.0f;\n\n  for (int i=0; i<4; i++)\n    for (int j=0; j<4; j++) {\n      float pre20 = d2a[i]*  b[j];\n      float pre10 =  da[i]*  b[j];\n      float pre00 =   a[i]*  b[j];\n      float pre11 =  da[i]* db[j];\n      float pre01 =   a[i]* db[j];\n      float pre02 =   a[i]*d2b[j];\n\n      const float *coefs = coefs_init + i*xs + j*ys;\n\n      float sum0 =   c[0] * coefs[0] +   c[1] * coefs[zs] +   c[2] * coefs[zs*2] +   c[3] * coefs[zs*3];\n      float sum1 =  dc[0] * coefs[0] +  dc[1] * coefs[zs] +  dc[2] * coefs[zs*2] +  dc[3] * coefs[zs*3];\n      float sum2 = d2c[0] * coefs[0] + d2c[1] * coefs[zs] + d2c[2] * coefs[zs*2] + d2c[3] * coefs[zs*3];\n\n      h[0]  += pre20 * sum0;\n      h[1]  += pre11 * sum0;\n      h[2]  += pre10 * sum1;\n      h[4]  += pre02 * sum0;\n      h[5]  += pre01 * sum1;\n      h[8]  += pre00 * sum2;\n      h[3]  += pre10 * sum0;\n      h[6]  += pre01 * sum0;\n      h[7]  += pre00 * sum1;\n      v0    += pre00 * sum0;\n    }\n  vals[0] = v0;\n  grads[0]  = h[3] * dxInv;\n  grads[1]  = h[6] * dyInv;\n  grads[2]  = h[7] * dzInv;\n\n  hess [0] = h[0]*dxInv*dxInv;\n  hess [1] = h[1]*dxInv*dyInv;\n  hess [2] = h[2]*dxInv*dzInv;\n  hess [3] = h[1]*dxInv*dyInv; \n\n  hess [4] = h[4]*dyInv*dyInv;\n  hess [5] = h[5]*dyInv*dzInv;\n  hess [6] = h[2]*dxInv*dzInv; \n\n  hess [7] = h[5]*dyInv*dzInv; \n\n  hess [8] = h[8]*dzInv*dzInv;\n}\n\nint main(int argc, char ** argv) {\n\n  float *Af = (float*) malloc (sizeof(float)*16);\n  float *dAf = (float*) malloc (sizeof(float)*16);\n  float *d2Af = (float*) malloc (sizeof(float)*16);\n\n  Af[0]=-0.166667;\n  Af[1]=0.500000;\n  Af[2]=-0.500000;\n  Af[3]=0.166667;\n  Af[4]=0.500000;\n  Af[5]=-1.000000;\n  Af[6]=0.000000;\n  Af[7]=0.666667;\n  Af[8]=-0.500000;\n  Af[9]=0.500000;\n  Af[10]=0.500000;\n  Af[11]=0.166667;\n  Af[12]=0.166667;\n  Af[13]=0.000000;\n  Af[14]=0.000000;\n  Af[15]=0.000000;\n  dAf[0]=0.000000; d2Af[0]=0.000000;\n  dAf[1]=-0.500000; d2Af[1]=0.000000;\n  dAf[2]=1.000000; d2Af[2]=-1.000000;\n  dAf[3]=-0.500000; d2Af[3]=1.000000;\n  dAf[4]=0.000000; d2Af[4]=0.000000;\n  dAf[5]=1.500000; d2Af[5]=0.000000;\n  dAf[6]=-2.000000; d2Af[6]=3.000000;\n  dAf[7]=0.000000; d2Af[7]=-2.000000;\n  dAf[8]=0.000000; d2Af[8]=0.000000;\n  dAf[9]=-1.500000; d2Af[9]=0.000000;\n  dAf[10]=1.000000; d2Af[10]=-3.00000;\n  dAf[11]=0.500000; d2Af[11]=1.000000;\n  dAf[12]=0.000000; d2Af[12]=0.000000;\n  dAf[13]=0.500000; d2Af[13]=0.000000;\n  dAf[14]=0.000000; d2Af[14]=1.000000;\n  dAf[15]=0.000000; d2Af[15]=0.000000;\n\n  float x=0.822387;  \n  float y=0.989919;  \n  float z=0.104573;\n\n  float* walkers_vals = (float*) malloc(sizeof(float)*WSIZE*NSIZE);\n  float* walkers_grads = (float*) malloc(sizeof(float)*WSIZE*MSIZE);\n  float* walkers_hess = (float*) malloc(sizeof(float)*WSIZE*OSIZE);\n  float* walkers_x = (float*) malloc(sizeof(float)*WSIZE);\n  float* walkers_y = (float*) malloc(sizeof(float)*WSIZE);\n  float* walkers_z = (float*) malloc(sizeof(float)*WSIZE);\n\n  for (int i=0; i<WSIZE; i++) {\n    walkers_x[i] = x + i*1.0/WSIZE;\n    walkers_y[i] = y + i*1.0/WSIZE;\n    walkers_z[i] = z + i*1.0/WSIZE;\n  }\n\n  float* spline_coefs = (float*) malloc (sizeof(float)*SSIZE);\n  for(size_t i=0;i<SSIZE;i++)\n    spline_coefs[i]=std::sqrt(0.22+i*1.0)*std::sin(i*1.0);\n\n  int spline_num_splines = NSIZE;\n  int spline_x_grid_start = 0; \n  int spline_y_grid_start = 0; \n  int spline_z_grid_start = 0; \n  int spline_x_grid_num = 45; \n  int spline_y_grid_num = 45; \n  int spline_z_grid_num = 45; \n  int spline_x_stride=NSIZE_round*48*48;\n  int spline_y_stride=NSIZE_round*48;\n  int spline_z_stride=NSIZE_round;\n  int spline_x_grid_delta_inv=45;\n  int spline_y_grid_delta_inv=45;\n  int spline_z_grid_delta_inv=45;\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  float *d_walkers_vals = sycl::malloc_device<float>(WSIZE*NSIZE, q);\n  q.memcpy(d_walkers_vals, walkers_vals, WSIZE*NSIZE*sizeof(float));\n\n  float *d_walkers_grads = sycl::malloc_device<float>(WSIZE*MSIZE, q);\n  q.memcpy(d_walkers_grads, walkers_grads, WSIZE*MSIZE*sizeof(float));\n\n  float *d_walkers_hess = sycl::malloc_device<float>(WSIZE*OSIZE, q);\n  q.memcpy(d_walkers_hess, walkers_hess, WSIZE*OSIZE*sizeof(float));\n\n  float *d_spline_coefs = sycl::malloc_device<float>(SSIZE, q);\n  q.memcpy(d_spline_coefs, spline_coefs, SSIZE*sizeof(float));\n\n  float *d_a = sycl::malloc_device<float>(4, q);\n  float *d_b = sycl::malloc_device<float>(4, q);\n  float *d_c = sycl::malloc_device<float>(4, q);\n  float *d_da = sycl::malloc_device<float>(4, q);\n  float *d_db = sycl::malloc_device<float>(4, q);\n  float *d_dc = sycl::malloc_device<float>(4, q);\n  float *d_d2a = sycl::malloc_device<float>(4, q);\n  float *d_d2b = sycl::malloc_device<float>(4, q);\n  float *d_d2c = sycl::malloc_device<float>(4, q);\n\n  double total_time = 0.0;\n\n  for(int i=0; i<WSIZE; i++) {\n    float x = walkers_x[i], y = walkers_y[i], z = walkers_z[i];\n\n    float ux = x*spline_x_grid_delta_inv;\n    float uy = y*spline_y_grid_delta_inv;\n    float uz = z*spline_z_grid_delta_inv;\n    float ipartx, iparty, ipartz, tx, ty, tz;\n    float a[4], b[4], c[4], da[4], db[4], dc[4], d2a[4], d2b[4], d2c[4];\n    intptr_t xs = spline_x_stride;\n    intptr_t ys = spline_y_stride;\n    intptr_t zs = spline_z_stride;\n\n    x -= spline_x_grid_start;\n    y -= spline_y_grid_start;\n    z -= spline_z_grid_start;\n    ipartx = (int) ux; tx = ux-ipartx; int ix = min(max(0,(int) ipartx),spline_x_grid_num-1);\n    iparty = (int) uy; ty = uy-iparty; int iy = min(max(0,(int) iparty),spline_y_grid_num-1);\n    ipartz = (int) uz; tz = uz-ipartz; int iz = min(max(0,(int) ipartz),spline_z_grid_num-1);\n\n    eval_abc(Af,tx,&a[0]);\n    q.memcpy(d_a, a, sizeof(float)*4);\n\n    eval_abc(Af,ty,&b[0]);\n    q.memcpy(d_b, b, sizeof(float)*4);\n\n    eval_abc(Af,tz,&c[0]);\n    q.memcpy(d_c, c, sizeof(float)*4);\n\n    eval_abc(dAf,tx,&da[0]);\n    q.memcpy(d_da, da, sizeof(float)*4);\n\n    eval_abc(dAf,ty,&db[0]);\n    q.memcpy(d_db, db, sizeof(float)*4);\n\n    eval_abc(dAf,tz,&dc[0]);\n    q.memcpy(d_dc, dc, sizeof(float)*4);\n\n    eval_abc(d2Af,tx,&d2a[0]);\n    q.memcpy(d_d2a, d2a, sizeof(float)*4);\n\n    eval_abc(d2Af,ty,&d2b[0]);\n    q.memcpy(d_d2b, d2b, sizeof(float)*4);\n\n    eval_abc(d2Af,tz,&d2c[0]);              \n    q.memcpy(d_d2c, d2c, sizeof(float)*4);\n\n    sycl::range<1> gws ((spline_num_splines+255)/256*256);\n    sycl::range<1> lws (256);\n\n    q.wait();\n    auto start = std::chrono::steady_clock::now();\n\n    q.submit([&] (sycl::handler &h) {\n      h.parallel_for<class vgh_spline>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        const int n = item.get_global_id(0);\n        if (n < spline_num_splines)\n          eval_UBspline_3d_s_vgh ( \n            d_spline_coefs+ix*xs+iy*ys+iz*zs+n,\n            xs, ys, zs, \n            d_walkers_vals+i*NSIZE+n,\n            d_walkers_grads+i*MSIZE+n*3,\n            d_walkers_hess+i*OSIZE+n*9,\n            d_a,\n            d_b,\n            d_c,\n            d_da,\n            d_db,\n            d_dc,\n            d_d2a,\n            d_d2b,\n            d_d2c,\n            spline_x_grid_delta_inv,\n            spline_y_grid_delta_inv,\n            spline_z_grid_delta_inv );\n      });\n    }).wait();\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n  }\n  printf(\"Total kernel execution time %lf (s)\\n\", total_time * 1e-9);\n\n  q.memcpy(walkers_vals, d_walkers_vals, sizeof(float)*WSIZE*NSIZE);\n  q.memcpy(walkers_grads, d_walkers_grads, sizeof(float)*WSIZE*MSIZE);\n  q.memcpy(walkers_hess, d_walkers_hess, sizeof(float)*WSIZE*OSIZE);\n  q.wait();\n\n  \n\n  float resVal = 0.0;\n  float resGrad = 0.0;\n  float resHess = 0.0;\n\n  for( int i = 0; i < NSIZE; i++ ) resVal = resVal + walkers_vals[i];\n  for( int i = 0; i < MSIZE; i++ ) resGrad = resGrad + walkers_grads[i];\n  for( int i = 0; i < OSIZE; i++ ) resHess = resHess + walkers_hess[i];\n  printf(\"walkers[0]->collect([resVal resGrad resHess]) = [%e %e %e]\\n\",\n         resVal,resGrad, resHess);\n\n  free(Af);\n  free(dAf);\n  free(d2Af);\n  free(walkers_vals);\n  free(walkers_grads);\n  free(walkers_hess);\n  free(walkers_x);\n  free(walkers_y);\n  free(walkers_z);\n  free(spline_coefs);\n\n  sycl::free(d_walkers_vals, q);\n  sycl::free(d_walkers_grads, q);\n  sycl::free(d_walkers_hess, q);\n  sycl::free(d_spline_coefs, q);\n  sycl::free(d_a, q);\n  sycl::free(d_b, q);\n  sycl::free(d_c, q);\n  sycl::free(d_da, q);\n  sycl::free(d_db, q);\n  sycl::free(d_dc, q);\n  sycl::free(d_d2a, q);\n  sycl::free(d_d2b, q);\n  sycl::free(d_d2c, q);\n  return 0;\n}\n"}}
{"kernel_name": "burger", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n#include <cuda.h>\n#include \"kernels.h\"\n\nint main(int argc, char* argv[])\n{\n  if (argc != 4) {\n    printf(\"Usage: %s <dim_x> <dim_y> <nt>\\n\", argv[0]);\n    printf(\"dim_x: number of grid points in the x axis\\n\");\n    printf(\"dim_y: number of grid points in the y axis\\n\");\n    printf(\"nt: number of time steps\\n\");\n    exit(-1);\n  }\n\n  \n\n  const int x_points = atoi(argv[1]);\n  const int y_points = atoi(argv[2]);\n  const int num_itrs = atoi(argv[3]);\n  const double x_len = 2.0;\n  const double y_len = 2.0;\n  const double del_x = x_len/(x_points-1);\n  const double del_y = y_len/(y_points-1);\n\n  const int grid_elems = x_points * y_points;\n  const int grid_size = sizeof(double) * grid_elems;\n\n  double *x = (double*) malloc (sizeof(double) * x_points);\n  double *y = (double*) malloc (sizeof(double) * y_points);\n  double *u = (double*) malloc (grid_size);\n  double *v = (double*) malloc (grid_size);\n  double *u_new = (double*) malloc (grid_size);\n  double *v_new = (double*) malloc (grid_size);\n\n  \n\n  double *du = (double*) malloc (grid_size);\n  double *dv = (double*) malloc (grid_size);\n\n  \n\n  const double nu = 0.01;\n  const double sigma = 0.0009;\n  const double del_t = sigma * del_x * del_y / nu;      \n\n\n  printf(\"2D Burger's equation\\n\");\n  printf(\"Grid dimension: x = %d y = %d\\n\", x_points, y_points);\n  printf(\"Number of time steps: %d\\n\", num_itrs);\n\n  for(int i = 0; i < x_points; i++) x[i] = i * del_x;\n  for(int i = 0; i < y_points; i++) y[i] = i * del_y;\n\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      u[idx(i,j)] = 1.0;\n      v[idx(i,j)] = 1.0;\n      u_new[idx(i,j)] = 1.0;\n      v_new[idx(i,j)] = 1.0;\n\n      if(x[j] > 0.5 && x[j] < 1.0 && y[i] > 0.5 && y[i] < 1.0){\n        u[idx(i,j)] = 2.0;\n        v[idx(i,j)] = 2.0;\n        u_new[idx(i,j)] = 2.0;\n        v_new[idx(i,j)] = 2.0;\n      }\n    }\n  }\n\n  double *d_u_new;\n  cudaMalloc((void**)&d_u_new, grid_size);\n\n  double *d_v_new;\n  cudaMalloc((void**)&d_v_new, grid_size);\n\n  double *d_u;\n  cudaMalloc((void**)&d_u, grid_size);\n\n  double *d_v;\n  cudaMalloc((void**)&d_v, grid_size);\n\n  cudaMemcpy(d_u_new, u_new, grid_size, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_v_new, v_new, grid_size, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_u, u, grid_size, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_v, v, grid_size, cudaMemcpyHostToDevice);\n\n  \n\n  dim3 grid ((x_points-2+15)/16, (y_points-2+15)/16);\n  dim3 block (16, 16);\n  dim3 grid2 ((x_points+255)/256);\n  dim3 block2 (256);\n  dim3 grid3 ((y_points+255)/256);\n  dim3 block3 (256);\n  dim3 grid4 ((grid_elems+255)/256);\n  dim3 block4 (256);\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for(int itr = 0; itr < num_itrs; itr++){\n\n    core<<<grid, block>>>(d_u_new, d_v_new, d_u, d_v, x_points, y_points, nu, del_t, del_x, del_y);\n\n    \n\n    bound_h<<<grid2, block2>>>(d_u_new, d_v_new, x_points, y_points);\n\n    bound_v<<<grid3, block3>>>(d_u_new, d_v_new, x_points, y_points);\n\n    \n\n    update<<<grid4, block4>>>(d_u, d_v, d_u_new, d_v_new, grid_elems);\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Total kernel execution time %f (s)\\n\", time * 1e-9f);\n\n  cudaMemcpy(du, d_u, grid_size, cudaMemcpyDeviceToHost);\n  cudaMemcpy(dv, d_v, grid_size, cudaMemcpyDeviceToHost);\n\n  printf(\"Serial computing for verification...\\n\");\n\n  \n\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      u[idx(i,j)] = 1.0;\n      v[idx(i,j)] = 1.0;\n      u_new[idx(i,j)] = 1.0;\n      v_new[idx(i,j)] = 1.0;\n\n      if(x[j] > 0.5 && x[j] < 1.0 && y[i] > 0.5 && y[i] < 1.0){\n        u[idx(i,j)] = 2.0;\n        v[idx(i,j)] = 2.0;\n        u_new[idx(i,j)] = 2.0;\n        v_new[idx(i,j)] = 2.0;\n      }\n    }\n  }\n\n  for(int itr = 0; itr < num_itrs; itr++){\n\n    for(int i = 1; i < y_points-1; i++){\n      for(int j = 1; j < x_points-1; j++){\n        u_new[idx(i,j)] = u[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (u[idx(i,j+1)] + u[idx(i,j-1)] - 2 * u[idx(i,j)]) + \n          (nu*del_t/(del_y*del_y)) * (u[idx(i+1,j)] + u[idx(i-1,j)] - 2 * u[idx(i,j)]) - \n          (del_t/del_x)*u[idx(i,j)] * (u[idx(i,j)] - u[idx(i,j-1)]) - \n          (del_t/del_y)*v[idx(i,j)] * (u[idx(i,j)] - u[idx(i-1,j)]);\n\n        v_new[idx(i,j)] = v[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (v[idx(i,j+1)] + v[idx(i,j-1)] - 2 * v[idx(i,j)]) + \n          (nu*del_t/(del_y*del_y)) * (v[idx(i+1,j)] + v[idx(i-1,j)] - 2 * v[idx(i,j)]) -\n          (del_t/del_x)*u[idx(i,j)] * (v[idx(i,j)] - v[idx(i,j-1)]) - \n          (del_t/del_y)*v[idx(i,j)] * (v[idx(i,j)] - v[idx(i-1,j)]);\n      }\n    }\n\n    \n\n    for(int i = 0; i < x_points; i++){\n      u_new[idx(0,i)] = 1.0;\n      v_new[idx(0,i)] = 1.0;\n      u_new[idx(y_points-1,i)] = 1.0;\n      v_new[idx(y_points-1,i)] = 1.0;\n    }\n\n    for(int j = 0; j < y_points; j++){\n      u_new[idx(j,0)] = 1.0;\n      v_new[idx(j,0)] = 1.0;\n      u_new[idx(j,x_points-1)] = 1.0;\n      v_new[idx(j,x_points-1)] = 1.0;\n    }\n\n    \n\n    for(int i = 0; i < y_points; i++){\n      for(int j = 0; j < x_points; j++){\n        u[idx(i,j)] = u_new[idx(i,j)];\n        v[idx(i,j)] = v_new[idx(i,j)];\n      }\n    }\n  }\n\n  bool ok = true;\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      if (fabs(du[idx(i,j)] - u[idx(i,j)]) > 1e-6 || \n          fabs(dv[idx(i,j)] - v[idx(i,j)]) > 1e-6) ok = false;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  free(x);\n  free(y);\n  free(u);\n  free(v);\n  free(du);\n  free(dv);\n  free(u_new);\n  free(v_new);\n  cudaFree(d_u);\n  cudaFree(d_v);\n  cudaFree(d_u_new);\n  cudaFree(d_v_new);\n\n  return 0;\n}\n"}}
{"kernel_name": "burger", "parallel_api": "hip", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n#include \"kernels.h\"\n\nint main(int argc, char* argv[])\n{\n  if (argc != 4) {\n    printf(\"Usage: %s <dim_x> <dim_y> <nt>\\n\", argv[0]);\n    printf(\"dim_x: number of grid points in the x axis\\n\");\n    printf(\"dim_y: number of grid points in the y axis\\n\");\n    printf(\"nt: number of time steps\\n\");\n    exit(-1);\n  }\n\n  \n\n  const int x_points = atoi(argv[1]);\n  const int y_points = atoi(argv[2]);\n  const int num_itrs = atoi(argv[3]);\n  const double x_len = 2.0;\n  const double y_len = 2.0;\n  const double del_x = x_len/(x_points-1);\n  const double del_y = y_len/(y_points-1);\n\n  const int grid_elems = x_points * y_points;\n  const int grid_size = sizeof(double) * grid_elems;\n\n  double *x = (double*) malloc (sizeof(double) * x_points);\n  double *y = (double*) malloc (sizeof(double) * y_points);\n  double *u = (double*) malloc (grid_size);\n  double *v = (double*) malloc (grid_size);\n  double *u_new = (double*) malloc (grid_size);\n  double *v_new = (double*) malloc (grid_size);\n\n  \n\n  double *du = (double*) malloc (grid_size);\n  double *dv = (double*) malloc (grid_size);\n\n  \n\n  const double nu = 0.01;\n  const double sigma = 0.0009;\n  const double del_t = sigma * del_x * del_y / nu;      \n\n\n  printf(\"2D Burger's equation\\n\");\n  printf(\"Grid dimension: x = %d y = %d\\n\", x_points, y_points);\n  printf(\"Number of time steps: %d\\n\", num_itrs);\n\n  for(int i = 0; i < x_points; i++) x[i] = i * del_x;\n  for(int i = 0; i < y_points; i++) y[i] = i * del_y;\n\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      u[idx(i,j)] = 1.0;\n      v[idx(i,j)] = 1.0;\n      u_new[idx(i,j)] = 1.0;\n      v_new[idx(i,j)] = 1.0;\n\n      if(x[j] > 0.5 && x[j] < 1.0 && y[i] > 0.5 && y[i] < 1.0){\n        u[idx(i,j)] = 2.0;\n        v[idx(i,j)] = 2.0;\n        u_new[idx(i,j)] = 2.0;\n        v_new[idx(i,j)] = 2.0;\n      }\n    }\n  }\n\n  double *d_u_new;\n  hipMalloc((void**)&d_u_new, grid_size);\n\n  double *d_v_new;\n  hipMalloc((void**)&d_v_new, grid_size);\n\n  double *d_u;\n  hipMalloc((void**)&d_u, grid_size);\n\n  double *d_v;\n  hipMalloc((void**)&d_v, grid_size);\n\n  hipMemcpy(d_u_new, u_new, grid_size, hipMemcpyHostToDevice);\n  hipMemcpy(d_v_new, v_new, grid_size, hipMemcpyHostToDevice);\n  hipMemcpy(d_u, u, grid_size, hipMemcpyHostToDevice);\n  hipMemcpy(d_v, v, grid_size, hipMemcpyHostToDevice);\n\n  \n\n  dim3 grid ((x_points-2+15)/16, (y_points-2+15)/16);\n  dim3 block (16, 16);\n  dim3 grid2 ((x_points+255)/256);\n  dim3 block2 (256);\n  dim3 grid3 ((y_points+255)/256);\n  dim3 block3 (256);\n  dim3 grid4 ((grid_elems+255)/256);\n  dim3 block4 (256);\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for(int itr = 0; itr < num_itrs; itr++){\n\n    hipLaunchKernelGGL(core, dim3(grid), dim3(block), 0, 0, d_u_new, d_v_new, d_u, d_v, x_points, y_points, nu, del_t, del_x, del_y);\n\n    \n\n    hipLaunchKernelGGL(bound_h, dim3(grid2), dim3(block2), 0, 0, d_u_new, d_v_new, x_points, y_points);\n\n    hipLaunchKernelGGL(bound_v, dim3(grid3), dim3(block3), 0, 0, d_u_new, d_v_new, x_points, y_points);\n\n    \n\n    hipLaunchKernelGGL(update, dim3(grid4), dim3(block4), 0, 0, d_u, d_v, d_u_new, d_v_new, grid_elems);\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Total kernel execution time %f (s)\\n\", time * 1e-9f);\n\n  hipMemcpy(du, d_u, grid_size, hipMemcpyDeviceToHost);\n  hipMemcpy(dv, d_v, grid_size, hipMemcpyDeviceToHost);\n\n  printf(\"Serial computing for verification...\\n\");\n\n  \n\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      u[idx(i,j)] = 1.0;\n      v[idx(i,j)] = 1.0;\n      u_new[idx(i,j)] = 1.0;\n      v_new[idx(i,j)] = 1.0;\n\n      if(x[j] > 0.5 && x[j] < 1.0 && y[i] > 0.5 && y[i] < 1.0){\n        u[idx(i,j)] = 2.0;\n        v[idx(i,j)] = 2.0;\n        u_new[idx(i,j)] = 2.0;\n        v_new[idx(i,j)] = 2.0;\n      }\n    }\n  }\n\n  for(int itr = 0; itr < num_itrs; itr++){\n\n    for(int i = 1; i < y_points-1; i++){\n      for(int j = 1; j < x_points-1; j++){\n        u_new[idx(i,j)] = u[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (u[idx(i,j+1)] + u[idx(i,j-1)] - 2 * u[idx(i,j)]) + \n          (nu*del_t/(del_y*del_y)) * (u[idx(i+1,j)] + u[idx(i-1,j)] - 2 * u[idx(i,j)]) - \n          (del_t/del_x)*u[idx(i,j)] * (u[idx(i,j)] - u[idx(i,j-1)]) - \n          (del_t/del_y)*v[idx(i,j)] * (u[idx(i,j)] - u[idx(i-1,j)]);\n\n        v_new[idx(i,j)] = v[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (v[idx(i,j+1)] + v[idx(i,j-1)] - 2 * v[idx(i,j)]) + \n          (nu*del_t/(del_y*del_y)) * (v[idx(i+1,j)] + v[idx(i-1,j)] - 2 * v[idx(i,j)]) -\n          (del_t/del_x)*u[idx(i,j)] * (v[idx(i,j)] - v[idx(i,j-1)]) - \n          (del_t/del_y)*v[idx(i,j)] * (v[idx(i,j)] - v[idx(i-1,j)]);\n      }\n    }\n\n    \n\n    for(int i = 0; i < x_points; i++){\n      u_new[idx(0,i)] = 1.0;\n      v_new[idx(0,i)] = 1.0;\n      u_new[idx(y_points-1,i)] = 1.0;\n      v_new[idx(y_points-1,i)] = 1.0;\n    }\n\n    for(int j = 0; j < y_points; j++){\n      u_new[idx(j,0)] = 1.0;\n      v_new[idx(j,0)] = 1.0;\n      u_new[idx(j,x_points-1)] = 1.0;\n      v_new[idx(j,x_points-1)] = 1.0;\n    }\n\n    \n\n    for(int i = 0; i < y_points; i++){\n      for(int j = 0; j < x_points; j++){\n        u[idx(i,j)] = u_new[idx(i,j)];\n        v[idx(i,j)] = v_new[idx(i,j)];\n      }\n    }\n  }\n\n  bool ok = true;\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      if (fabs(du[idx(i,j)] - u[idx(i,j)]) > 1e-6 || \n          fabs(dv[idx(i,j)] - v[idx(i,j)]) > 1e-6) ok = false;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  free(x);\n  free(y);\n  free(u);\n  free(v);\n  free(du);\n  free(dv);\n  free(u_new);\n  free(v_new);\n  hipFree(d_u);\n  hipFree(d_v);\n  hipFree(d_u_new);\n  hipFree(d_v_new);\n\n  return 0;\n}\n"}}
{"kernel_name": "burger", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n#include <omp.h>\n\n#define idx(i,j)   (i)*y_points+(j)\n\nint main(int argc, char* argv[])\n{\n  if (argc != 4) {\n    printf(\"Usage: %s <dim_x> <dim_y> <nt>\\n\", argv[0]);\n    printf(\"dim_x: number of grid points in the x axis\\n\");\n    printf(\"dim_y: number of grid points in the y axis\\n\");\n    printf(\"nt: number of time steps\\n\");\n    exit(-1);\n  }\n\n  \n\n  const int x_points = atoi(argv[1]);\n  const int y_points = atoi(argv[2]);\n  const int num_itrs = atoi(argv[3]);\n  const double x_len = 2.0;\n  const double y_len = 2.0;\n  const double del_x = x_len/(x_points-1);\n  const double del_y = y_len/(y_points-1);\n\n  const int grid_size = sizeof(double) * x_points * y_points;\n\n  double *x = (double*) malloc (sizeof(double) * x_points);\n  double *y = (double*) malloc (sizeof(double) * y_points);\n  double *u = (double*) malloc (grid_size);\n  double *v = (double*) malloc (grid_size);\n  double *u_new = (double*) malloc (grid_size);\n  double *v_new = (double*) malloc (grid_size);\n\n  \n\n  double *d_u = (double*) malloc (grid_size);\n  double *d_v = (double*) malloc (grid_size);\n\n  \n\n  const double nu = 0.01;\n  const double sigma = 0.0009;\n  const double del_t = sigma * del_x * del_y / nu;      \n\n\n  printf(\"2D Burger's equation\\n\");\n  printf(\"Grid dimension: x = %d y = %d\\n\", x_points, y_points);\n  printf(\"Number of time steps: %d\\n\", num_itrs);\n\n  for(int i = 0; i < x_points; i++) x[i] = i * del_x;\n  for(int i = 0; i < y_points; i++) y[i] = i * del_y;\n\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      u[idx(i,j)] = 1.0;\n      v[idx(i,j)] = 1.0;\n      u_new[idx(i,j)] = 1.0;\n      v_new[idx(i,j)] = 1.0;\n\n      if(x[j] > 0.5 && x[j] < 1.0 && y[i] > 0.5 && y[i] < 1.0){\n        u[idx(i,j)] = 2.0;\n        v[idx(i,j)] = 2.0;\n        u_new[idx(i,j)] = 2.0;\n        v_new[idx(i,j)] = 2.0;\n      }\n    }\n  }\n\n#pragma omp target data map (to: u_new[0:x_points*y_points], v_new[0:x_points*y_points]) \\\n                        map (tofrom: u[0:x_points*y_points], v[0:x_points*y_points])\n{\n  auto start = std::chrono::steady_clock::now();\n\n  for(int itr = 0; itr < num_itrs; itr++){\n\n    #pragma omp target teams distribute parallel for collapse(2) thread_limit(256) nowait\n    for(int i = 1; i < y_points-1; i++){\n      for(int j = 1; j < x_points-1; j++){\n        u_new[idx(i,j)] = u[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (u[idx(i,j+1)] + u[idx(i,j-1)] - 2 * u[idx(i,j)]) + \n                                        (nu*del_t/(del_y*del_y)) * (u[idx(i+1,j)] + u[idx(i-1,j)] - 2 * u[idx(i,j)]) - \n                                                (del_t/del_x)*u[idx(i,j)] * (u[idx(i,j)] - u[idx(i,j-1)]) - \n                                                (del_t/del_y)*v[idx(i,j)] * (u[idx(i,j)] - u[idx(i-1,j)]);\n\n        v_new[idx(i,j)] = v[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (v[idx(i,j+1)] + v[idx(i,j-1)] - 2 * v[idx(i,j)]) + \n                                        (nu*del_t/(del_y*del_y)) * (v[idx(i+1,j)] + v[idx(i-1,j)] - 2 * v[idx(i,j)]) -\n                                                  (del_t/del_x)*u[idx(i,j)] * (v[idx(i,j)] - v[idx(i,j-1)]) - \n                                                  (del_t/del_y)*v[idx(i,j)] * (v[idx(i,j)] - v[idx(i-1,j)]);\n      }\n    }\n\n    \n\n    #pragma omp target teams distribute parallel for thread_limit(256) nowait\n    for(int i = 0; i < x_points; i++){\n      u_new[idx(0,i)] = 1.0;\n      v_new[idx(0,i)] = 1.0;\n      u_new[idx(y_points-1,i)] = 1.0;\n      v_new[idx(y_points-1,i)] = 1.0;\n    }\n\n    #pragma omp target teams distribute parallel for thread_limit(256) nowait\n    for(int j = 0; j < y_points; j++){\n      u_new[idx(j,0)] = 1.0;\n      v_new[idx(j,0)] = 1.0;\n      u_new[idx(j,x_points-1)] = 1.0;\n      v_new[idx(j,x_points-1)] = 1.0;\n    }\n\n    \n\n    #pragma omp target teams distribute parallel for collapse(2) thread_limit(256)\n    for(int i = 0; i < y_points; i++){\n      for(int j = 0; j < x_points; j++){\n        u[idx(i,j)] = u_new[idx(i,j)];\n        v[idx(i,j)] = v_new[idx(i,j)];\n      }\n    }\n  }\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Total kernel execution time %f (s)\\n\", time * 1e-9f);\n}\n\n  memcpy(d_u, u, grid_size);\n  memcpy(d_v, v, grid_size);\n\n  printf(\"Serial computing for verification...\\n\");\n\n  \n\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      u[idx(i,j)] = 1.0;\n      v[idx(i,j)] = 1.0;\n      u_new[idx(i,j)] = 1.0;\n      v_new[idx(i,j)] = 1.0;\n\n      if(x[j] > 0.5 && x[j] < 1.0 && y[i] > 0.5 && y[i] < 1.0){\n        u[idx(i,j)] = 2.0;\n        v[idx(i,j)] = 2.0;\n        u_new[idx(i,j)] = 2.0;\n        v_new[idx(i,j)] = 2.0;\n      }\n    }\n  }\n\n  for(int itr = 0; itr < num_itrs; itr++){\n\n    for(int i = 1; i < y_points-1; i++){\n      for(int j = 1; j < x_points-1; j++){\n        u_new[idx(i,j)] = u[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (u[idx(i,j+1)] + u[idx(i,j-1)] - 2 * u[idx(i,j)]) + \n                              (nu*del_t/(del_y*del_y)) * (u[idx(i+1,j)] + u[idx(i-1,j)] - 2 * u[idx(i,j)]) - \n                                 (del_t/del_x)*u[idx(i,j)] * (u[idx(i,j)] - u[idx(i,j-1)]) - \n                                 (del_t/del_y)*v[idx(i,j)] * (u[idx(i,j)] - u[idx(i-1,j)]);\n\n        v_new[idx(i,j)] = v[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (v[idx(i,j+1)] + v[idx(i,j-1)] - 2 * v[idx(i,j)]) + \n                              (nu*del_t/(del_y*del_y)) * (v[idx(i+1,j)] + v[idx(i-1,j)] - 2 * v[idx(i,j)]) -\n                                 (del_t/del_x)*u[idx(i,j)] * (v[idx(i,j)] - v[idx(i,j-1)]) - \n                                 (del_t/del_y)*v[idx(i,j)] * (v[idx(i,j)] - v[idx(i-1,j)]);\n      }\n    }\n\n    \n\n    for(int i = 0; i < x_points; i++){\n      u_new[idx(0,i)] = 1.0;\n      v_new[idx(0,i)] = 1.0;\n      u_new[idx(y_points-1,i)] = 1.0;\n      v_new[idx(y_points-1,i)] = 1.0;\n    }\n\n    for(int j = 0; j < y_points; j++){\n      u_new[idx(j,0)] = 1.0;\n      v_new[idx(j,0)] = 1.0;\n      u_new[idx(j,x_points-1)] = 1.0;\n      v_new[idx(j,x_points-1)] = 1.0;\n    }\n\n    \n\n    for(int i = 0; i < y_points; i++){\n      for(int j = 0; j < x_points; j++){\n        u[idx(i,j)] = u_new[idx(i,j)];\n        v[idx(i,j)] = v_new[idx(i,j)];\n      }\n    }\n  }\n\n  bool ok = true;\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      if (fabs(d_u[idx(i,j)] - u[idx(i,j)]) > 1e-6 || \n          fabs(d_v[idx(i,j)] - v[idx(i,j)]) > 1e-6) ok = false;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  free(x);\n  free(y);\n  free(u);\n  free(v);\n  free(d_u);\n  free(d_v);\n  free(u_new);\n  free(v_new);\n\n  return 0;\n}\n"}}
{"kernel_name": "burger", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n\n#define idx(i,j)   (i)*y_points+(j)\n\nint main(int argc, char* argv[])\n{\n  if (argc != 4) {\n    printf(\"Usage: %s <dim_x> <dim_y> <nt>\\n\", argv[0]);\n    printf(\"dim_x: number of grid points in the x axis\\n\");\n    printf(\"dim_y: number of grid points in the y axis\\n\");\n    printf(\"nt: number of time steps\\n\");\n    exit(-1);\n  }\n\n  \n\n  const int x_points = atoi(argv[1]);\n  const int y_points = atoi(argv[2]);\n  const int num_itrs = atoi(argv[3]);\n  const double x_len = 2.0;\n  const double y_len = 2.0;\n  const double del_x = x_len/(x_points-1);\n  const double del_y = y_len/(y_points-1);\n\n  const int grid_size = sizeof(double) * x_points * y_points;\n\n  double *x = (double*) malloc (sizeof(double) * x_points);\n  double *y = (double*) malloc (sizeof(double) * y_points);\n  double *u = (double*) malloc (grid_size);\n  double *v = (double*) malloc (grid_size);\n  double *u_new = (double*) malloc (grid_size);\n  double *v_new = (double*) malloc (grid_size);\n\n  \n\n  double *d_u = (double*) malloc (grid_size);\n  double *d_v = (double*) malloc (grid_size);\n\n  \n\n  const double nu = 0.01;\n  const double sigma = 0.0009;\n  const double del_t = sigma * del_x * del_y / nu;      \n\n\n  printf(\"2D Burger's equation\\n\");\n  printf(\"Grid dimension: x = %d y = %d\\n\", x_points, y_points);\n  printf(\"Number of time steps: %d\\n\", num_itrs);\n\n  for(int i = 0; i < x_points; i++) x[i] = i * del_x;\n  for(int i = 0; i < y_points; i++) y[i] = i * del_y;\n\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      u[idx(i,j)] = 1.0;\n      v[idx(i,j)] = 1.0;\n      u_new[idx(i,j)] = 1.0;\n      v_new[idx(i,j)] = 1.0;\n\n      if(x[j] > 0.5 && x[j] < 1.0 && y[i] > 0.5 && y[i] < 1.0){\n        u[idx(i,j)] = 2.0;\n        v[idx(i,j)] = 2.0;\n        u_new[idx(i,j)] = 2.0;\n        v_new[idx(i,j)] = 2.0;\n      }\n    }\n  }\n\n{\n  auto start = std::chrono::steady_clock::now();\n\n  for(int itr = 0; itr < num_itrs; itr++){\n\n        for(int i = 1; i < y_points-1; i++){\n      for(int j = 1; j < x_points-1; j++){\n        u_new[idx(i,j)] = u[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (u[idx(i,j+1)] + u[idx(i,j-1)] - 2 * u[idx(i,j)]) + \n                                        (nu*del_t/(del_y*del_y)) * (u[idx(i+1,j)] + u[idx(i-1,j)] - 2 * u[idx(i,j)]) - \n                                                (del_t/del_x)*u[idx(i,j)] * (u[idx(i,j)] - u[idx(i,j-1)]) - \n                                                (del_t/del_y)*v[idx(i,j)] * (u[idx(i,j)] - u[idx(i-1,j)]);\n\n        v_new[idx(i,j)] = v[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (v[idx(i,j+1)] + v[idx(i,j-1)] - 2 * v[idx(i,j)]) + \n                                        (nu*del_t/(del_y*del_y)) * (v[idx(i+1,j)] + v[idx(i-1,j)] - 2 * v[idx(i,j)]) -\n                                                  (del_t/del_x)*u[idx(i,j)] * (v[idx(i,j)] - v[idx(i,j-1)]) - \n                                                  (del_t/del_y)*v[idx(i,j)] * (v[idx(i,j)] - v[idx(i-1,j)]);\n      }\n    }\n\n    \n\n        for(int i = 0; i < x_points; i++){\n      u_new[idx(0,i)] = 1.0;\n      v_new[idx(0,i)] = 1.0;\n      u_new[idx(y_points-1,i)] = 1.0;\n      v_new[idx(y_points-1,i)] = 1.0;\n    }\n\n        for(int j = 0; j < y_points; j++){\n      u_new[idx(j,0)] = 1.0;\n      v_new[idx(j,0)] = 1.0;\n      u_new[idx(j,x_points-1)] = 1.0;\n      v_new[idx(j,x_points-1)] = 1.0;\n    }\n\n    \n\n        for(int i = 0; i < y_points; i++){\n      for(int j = 0; j < x_points; j++){\n        u[idx(i,j)] = u_new[idx(i,j)];\n        v[idx(i,j)] = v_new[idx(i,j)];\n      }\n    }\n  }\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Total kernel execution time %f (s)\\n\", time * 1e-9f);\n}\n\n  memcpy(d_u, u, grid_size);\n  memcpy(d_v, v, grid_size);\n\n  printf(\"Serial computing for verification...\\n\");\n\n  \n\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      u[idx(i,j)] = 1.0;\n      v[idx(i,j)] = 1.0;\n      u_new[idx(i,j)] = 1.0;\n      v_new[idx(i,j)] = 1.0;\n\n      if(x[j] > 0.5 && x[j] < 1.0 && y[i] > 0.5 && y[i] < 1.0){\n        u[idx(i,j)] = 2.0;\n        v[idx(i,j)] = 2.0;\n        u_new[idx(i,j)] = 2.0;\n        v_new[idx(i,j)] = 2.0;\n      }\n    }\n  }\n\n  for(int itr = 0; itr < num_itrs; itr++){\n\n    for(int i = 1; i < y_points-1; i++){\n      for(int j = 1; j < x_points-1; j++){\n        u_new[idx(i,j)] = u[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (u[idx(i,j+1)] + u[idx(i,j-1)] - 2 * u[idx(i,j)]) + \n                              (nu*del_t/(del_y*del_y)) * (u[idx(i+1,j)] + u[idx(i-1,j)] - 2 * u[idx(i,j)]) - \n                                 (del_t/del_x)*u[idx(i,j)] * (u[idx(i,j)] - u[idx(i,j-1)]) - \n                                 (del_t/del_y)*v[idx(i,j)] * (u[idx(i,j)] - u[idx(i-1,j)]);\n\n        v_new[idx(i,j)] = v[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (v[idx(i,j+1)] + v[idx(i,j-1)] - 2 * v[idx(i,j)]) + \n                              (nu*del_t/(del_y*del_y)) * (v[idx(i+1,j)] + v[idx(i-1,j)] - 2 * v[idx(i,j)]) -\n                                 (del_t/del_x)*u[idx(i,j)] * (v[idx(i,j)] - v[idx(i,j-1)]) - \n                                 (del_t/del_y)*v[idx(i,j)] * (v[idx(i,j)] - v[idx(i-1,j)]);\n      }\n    }\n\n    \n\n    for(int i = 0; i < x_points; i++){\n      u_new[idx(0,i)] = 1.0;\n      v_new[idx(0,i)] = 1.0;\n      u_new[idx(y_points-1,i)] = 1.0;\n      v_new[idx(y_points-1,i)] = 1.0;\n    }\n\n    for(int j = 0; j < y_points; j++){\n      u_new[idx(j,0)] = 1.0;\n      v_new[idx(j,0)] = 1.0;\n      u_new[idx(j,x_points-1)] = 1.0;\n      v_new[idx(j,x_points-1)] = 1.0;\n    }\n\n    \n\n    for(int i = 0; i < y_points; i++){\n      for(int j = 0; j < x_points; j++){\n        u[idx(i,j)] = u_new[idx(i,j)];\n        v[idx(i,j)] = v_new[idx(i,j)];\n      }\n    }\n  }\n\n  bool ok = true;\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      if (fabs(d_u[idx(i,j)] - u[idx(i,j)]) > 1e-6 || \n          fabs(d_v[idx(i,j)] - v[idx(i,j)]) > 1e-6) ok = false;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  free(x);\n  free(y);\n  free(u);\n  free(v);\n  free(d_u);\n  free(d_v);\n  free(u_new);\n  free(v_new);\n\n  return 0;\n}"}}
{"kernel_name": "burger", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n\n#define idx(i,j)   (i)*y_points+(j)\n\nint main(int argc, char* argv[])\n{\n  if (argc != 4) {\n    printf(\"Usage: %s <dim_x> <dim_y> <nt>\\n\", argv[0]);\n    printf(\"dim_x: number of grid points in the x axis\\n\");\n    printf(\"dim_y: number of grid points in the y axis\\n\");\n    printf(\"nt: number of time steps\\n\");\n    exit(-1);\n  }\n\n  \n\n  const int x_points = atoi(argv[1]);\n  const int y_points = atoi(argv[2]);\n  const int num_itrs = atoi(argv[3]);\n  const double x_len = 2.0;\n  const double y_len = 2.0;\n  const double del_x = x_len/(x_points-1);\n  const double del_y = y_len/(y_points-1);\n\n  const int grid_elems = x_points * y_points;\n  const int grid_size = sizeof(double) * grid_elems;\n\n  double *x = (double*) malloc (sizeof(double) * x_points);\n  double *y = (double*) malloc (sizeof(double) * y_points);\n  double *u = (double*) malloc (grid_size);\n  double *v = (double*) malloc (grid_size);\n  double *u_new = (double*) malloc (grid_size);\n  double *v_new = (double*) malloc (grid_size);\n\n  \n\n  double *du = (double*) malloc (grid_size);\n  double *dv = (double*) malloc (grid_size);\n\n  \n\n  const double nu = 0.01;\n  const double sigma = 0.0009;\n  const double del_t = sigma * del_x * del_y / nu;      \n\n\n  printf(\"2D Burger's equation\\n\");\n  printf(\"Grid dimension: x = %d y = %d\\n\", x_points, y_points);\n  printf(\"Number of time steps: %d\\n\", num_itrs);\n\n  for(int i = 0; i < x_points; i++) x[i] = i * del_x;\n  for(int i = 0; i < y_points; i++) y[i] = i * del_y;\n\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      u[idx(i,j)] = 1.0;\n      v[idx(i,j)] = 1.0;\n      u_new[idx(i,j)] = 1.0;\n      v_new[idx(i,j)] = 1.0;\n\n      if(x[j] > 0.5 && x[j] < 1.0 && y[i] > 0.5 && y[i] < 1.0){\n        u[idx(i,j)] = 2.0;\n        v[idx(i,j)] = 2.0;\n        u_new[idx(i,j)] = 2.0;\n        v_new[idx(i,j)] = 2.0;\n      }\n    }\n  }\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  double *d_u_new = sycl::malloc_device<double>(grid_elems, q);\n  q.memcpy(d_u_new, u_new, grid_size); \n\n  double *d_v_new = sycl::malloc_device<double>(grid_elems, q);\n  q.memcpy(d_v_new, v_new, grid_size); \n\n  double *d_u = sycl::malloc_device<double>(grid_elems, q);\n  q.memcpy(d_u, u, grid_size); \n\n  double *d_v = sycl::malloc_device<double>(grid_elems, q);\n  q.memcpy(d_v, v, grid_size); \n\n  \n\n  sycl::range<2> gws ((y_points-2+15)/16*16, (x_points-2+15)/16*16);\n  sycl::range<2> lws (16, 16);\n  sycl::range<1> gws2 ((x_points+255)/256*256);\n  sycl::range<1> lws2 (256);\n  sycl::range<1> gws3 ((y_points+255)/256*256);\n  sycl::range<1> lws3 (256);\n  sycl::range<1> gws4 ((grid_elems+255)/256*256);\n  sycl::range<1> lws4 (256);\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  for(int itr = 0; itr < num_itrs; itr++) {\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class core>(\n        sycl::nd_range<2>(gws, lws), [=] (sycl::nd_item<2> item) {\n        int i = item.get_global_id(0) + 1;\n        int j = item.get_global_id(1) + 1;\n        if (j < x_points - 1 && i < y_points - 1) {\n          d_u_new[idx(i,j)] = d_u[idx(i,j)] + \n            (nu*del_t/(del_x*del_x)) * (d_u[idx(i,j+1)] + d_u[idx(i,j-1)] - 2 * d_u[idx(i,j)]) + \n            (nu*del_t/(del_y*del_y)) * (d_u[idx(i+1,j)] + d_u[idx(i-1,j)] - 2 * d_u[idx(i,j)]) - \n            (del_t/del_x)*d_u[idx(i,j)] * (d_u[idx(i,j)] - d_u[idx(i,j-1)]) - \n            (del_t/del_y)*d_v[idx(i,j)] * (d_u[idx(i,j)] - d_u[idx(i-1,j)]);\n\n          d_v_new[idx(i,j)] = d_v[idx(i,j)] +\n            (nu*del_t/(del_x*del_x)) * (d_v[idx(i,j+1)] + d_v[idx(i,j-1)] - 2 * d_v[idx(i,j)]) + \n            (nu*del_t/(del_y*del_y)) * (d_v[idx(i+1,j)] + d_v[idx(i-1,j)] - 2 * d_v[idx(i,j)]) -\n            (del_t/del_x)*d_u[idx(i,j)] * (d_v[idx(i,j)] - d_v[idx(i,j-1)]) - \n            (del_t/del_y)*d_v[idx(i,j)] * (d_v[idx(i,j)] - d_v[idx(i-1,j)]);\n        }\n      });\n    });\n\n    \n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class bound_h>(\n        sycl::nd_range<1>(gws2, lws2), [=] (sycl::nd_item<1> item) {\n        int i = item.get_global_id(0);\n        if (i < x_points) {\n          d_u_new[idx(0,i)] = 1.0;\n          d_v_new[idx(0,i)] = 1.0;\n          d_u_new[idx(y_points-1,i)] = 1.0;\n          d_v_new[idx(y_points-1,i)] = 1.0;\n        }\n      });\n    });\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class bound_v>(\n        sycl::nd_range<1>(gws3, lws3), [=] (sycl::nd_item<1> item) {\n        int j = item.get_global_id(0);\n        if (j < y_points) {\n          d_u_new[idx(j,0)] = 1.0;\n          d_v_new[idx(j,0)] = 1.0;\n          d_u_new[idx(j,x_points-1)] = 1.0;\n          d_v_new[idx(j,x_points-1)] = 1.0;\n        }\n      });\n    });\n\n    \n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class update>(\n        sycl::nd_range<1>(gws4, lws4), [=] (sycl::nd_item<1> item) {\n        int i = item.get_global_id(0);\n        if (i < grid_elems) {\n          d_u[i] = d_u_new[i];\n          d_v[i] = d_v_new[i];\n        }\n      });\n    });\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Total kernel execution time %f (s)\\n\", time * 1e-9f);\n\n  q.memcpy(du, d_u, grid_size);\n  q.memcpy(dv, d_v, grid_size);\n  q.wait();\n\n  printf(\"Serial computing for verification...\\n\");\n\n  \n\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      u[idx(i,j)] = 1.0;\n      v[idx(i,j)] = 1.0;\n      u_new[idx(i,j)] = 1.0;\n      v_new[idx(i,j)] = 1.0;\n\n      if(x[j] > 0.5 && x[j] < 1.0 && y[i] > 0.5 && y[i] < 1.0){\n        u[idx(i,j)] = 2.0;\n        v[idx(i,j)] = 2.0;\n        u_new[idx(i,j)] = 2.0;\n        v_new[idx(i,j)] = 2.0;\n      }\n    }\n  }\n\n  for(int itr = 0; itr < num_itrs; itr++){\n\n    for(int i = 1; i < y_points-1; i++){\n      for(int j = 1; j < x_points-1; j++){\n        u_new[idx(i,j)] = u[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (u[idx(i,j+1)] + u[idx(i,j-1)] - 2 * u[idx(i,j)]) + \n                              (nu*del_t/(del_y*del_y)) * (u[idx(i+1,j)] + u[idx(i-1,j)] - 2 * u[idx(i,j)]) - \n                                 (del_t/del_x)*u[idx(i,j)] * (u[idx(i,j)] - u[idx(i,j-1)]) - \n                                 (del_t/del_y)*v[idx(i,j)] * (u[idx(i,j)] - u[idx(i-1,j)]);\n\n        v_new[idx(i,j)] = v[idx(i,j)] + (nu*del_t/(del_x*del_x)) * (v[idx(i,j+1)] + v[idx(i,j-1)] - 2 * v[idx(i,j)]) + \n                              (nu*del_t/(del_y*del_y)) * (v[idx(i+1,j)] + v[idx(i-1,j)] - 2 * v[idx(i,j)]) -\n                                 (del_t/del_x)*u[idx(i,j)] * (v[idx(i,j)] - v[idx(i,j-1)]) - \n                                 (del_t/del_y)*v[idx(i,j)] * (v[idx(i,j)] - v[idx(i-1,j)]);\n      }\n    }\n\n    \n\n    for(int i = 0; i < x_points; i++){\n      u_new[idx(0,i)] = 1.0;\n      v_new[idx(0,i)] = 1.0;\n      u_new[idx(y_points-1,i)] = 1.0;\n      v_new[idx(y_points-1,i)] = 1.0;\n    }\n\n    for(int j = 0; j < y_points; j++){\n      u_new[idx(j,0)] = 1.0;\n      v_new[idx(j,0)] = 1.0;\n      u_new[idx(j,x_points-1)] = 1.0;\n      v_new[idx(j,x_points-1)] = 1.0;\n    }\n\n    \n\n    for(int i = 0; i < y_points; i++){\n      for(int j = 0; j < x_points; j++){\n        u[idx(i,j)] = u_new[idx(i,j)];\n        v[idx(i,j)] = v_new[idx(i,j)];\n      }\n    }\n  }\n\n  bool ok = true;\n  for(int i = 0; i < y_points; i++){\n    for(int j = 0; j < x_points; j++){\n      if (fabs(du[idx(i,j)] - u[idx(i,j)]) > 1e-6 || \n          fabs(dv[idx(i,j)] - v[idx(i,j)]) > 1e-6) ok = false;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  free(x);\n  free(y);\n  free(u);\n  free(v);\n  free(du);\n  free(dv);\n  free(u_new);\n  free(v_new);\n  sycl::free(d_u, q);\n  sycl::free(d_v, q);\n  sycl::free(d_u_new, q);\n  sycl::free(d_v_new, q);\n\n  return 0;\n}\n"}}
{"kernel_name": "che", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <iostream>\n#include <random>\n#include <fstream>\n#include <cuda.h>\n#include \"kernels.h\"\n\nusing namespace std;\n\nvoid initialization(double c[][DATAYSIZE][DATAXSIZE])\n{\n  srand(2);\n  for (unsigned int idz = 0.0; idz < DATAZSIZE; idz++) {\n    for (unsigned int idy = 0.0; idy < DATAYSIZE; idy++) {\n      for (unsigned int idx = 0.0; idx < DATAXSIZE; idx++) {\n        double f = (double)rand() / RAND_MAX;\n        c[idz][idy][idx] = -1.0 + 2.0*f;\n      }\n    }\n  }\n}\n\ndouble integral(const double c[][DATAYSIZE][DATAXSIZE], int nx, int ny, int nz)\n{\n  double summation = 0.0;  \n\n  for (int k = 0; k < nz; k++)\n    for(int j = 0; j < ny; j++)\n      for(int i = 0; i < nx; i++)\n        summation = summation + c[k][j][i];\n\n  return summation;\n}\n\nint main(int argc, char *argv[])\n{\n  const double dx = 1.0;\n  const double dy = 1.0;\n  const double dz = 1.0;\n  const double dt = 0.01;\n  const double e_AA = -(2.0/9.0);\n  const double e_BB = -(2.0/9.0);\n  const double e_AB = (2.0/9.0);\n  const int t_f = atoi(argv[1]);    \n\n#ifndef DEBUG\n  const int t_freq = t_f; \n#else\n  const int t_freq = 10;\n#endif\n  const double gamma = 0.5;\n  const double D = 1.0;\n\n  string name_c = \"./out/integral_c.txt\";\n  ofstream ofile_c (name_c);\n\n  string name_mu = \"./out/integral_mu.txt\";\n  ofstream ofile_mu (name_mu);\n\n  string name_f = \"./out/integral_f.txt\";\n  ofstream ofile_f (name_f);\n\n  typedef double nRarray[DATAYSIZE][DATAXSIZE];\n\n  \n\n  const int nx = DATAXSIZE;\n  const int ny = DATAYSIZE;\n  const int nz = DATAZSIZE;\n  const int vol = nx * ny * nz;\n  const size_t vol_bytes = vol * sizeof(double);\n\n  \n\n  nRarray *c_host; \n\n  nRarray *mu_host;\n  nRarray *f_host;\n  nRarray *d_cold; \n\n  nRarray *d_cnew;\n  nRarray *d_muold;\n  nRarray *d_fold;\n\n  if ((c_host = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"c_host malloc failed\\n\"); \n    return 1;\n  }\n  if ((mu_host = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"mu_host malloc failed\\n\"); \n    return 1;\n  }\n  if ((f_host = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"f_host malloc failed\\n\"); \n    return 1;\n  }\n\n  cudaMalloc((void **) &d_cold, vol_bytes);\n  cudaMalloc((void **) &d_cnew, vol_bytes);\n  cudaMalloc((void **) &d_muold, vol_bytes);\n  cudaMalloc((void **) &d_fold, vol_bytes);\n\n  initialization(c_host);\n\n  double integral_c = 0.0;\n  double integral_mu = 0.0;\n  double integral_f = 0.0;\n\n  cudaMemcpy(d_cold, c_host, vol_bytes, cudaMemcpyHostToDevice);\n\n  const dim3 blockSize(BLKXSIZE, BLKYSIZE, BLKZSIZE);\n  const dim3 gridSize((DATAXSIZE+BLKXSIZE-1)/BLKXSIZE, \n                      (DATAYSIZE+BLKYSIZE-1)/BLKYSIZE,\n                      (DATAZSIZE+BLKZSIZE-1)/BLKZSIZE);\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int t = 0; t < t_f; t++) {\n\n    chemicalPotential<<<gridSize, blockSize>>>(d_cold,d_muold,dx,dy,dz,gamma,e_AA,e_BB,e_AB);\n    localFreeEnergyFunctional<<<gridSize, blockSize>>>(d_cold,d_fold,dx,dy,dz,gamma,e_AA,e_BB,e_AB);\n    cahnHilliard<<<gridSize, blockSize>>>(d_cnew,d_cold,d_muold,D,dt,dx,dy,dz);\n\n    if (t > 0 && t % (t_freq - 1) == 0) {\n      cudaMemcpy(c_host, d_cnew, vol_bytes, cudaMemcpyDeviceToHost);\n\n      cudaMemcpy(mu_host, d_muold, vol_bytes, cudaMemcpyDeviceToHost);\n\n      cudaMemcpy(f_host, d_fold, vol_bytes, cudaMemcpyDeviceToHost);\n\n      integral_c = integral(c_host,nx,ny,nz);\n\n      ofile_c << t << \",\" << integral_c << endl;\n\n      integral_mu = integral(mu_host,nx,ny,nz);\n\n      ofile_mu << t << \",\" << integral_mu << endl;\n\n      integral_f = integral(f_host,nx,ny,nz);\n\n      ofile_f << t << \",\" << integral_f << endl;\n    }\n\n    Swap<<<gridSize, blockSize>>>(d_cnew, d_cold);\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Kernel exeuction time on the GPU (%d iterations) = %.3f (s)\\n\", t_f, time * 1e-9f);\n\n  free(c_host);\n  free(mu_host);\n  free(f_host);\n  cudaFree(d_cold);\n  cudaFree(d_cnew);\n  cudaFree(d_muold);\n  cudaFree(d_fold);\n  return 0;\n}\n"}}
{"kernel_name": "che", "parallel_api": "hip", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <iostream>\n#include <random>\n#include <fstream>\n#include <hip/hip_runtime.h>\n#include \"kernels.h\"\n\nusing namespace std;\n\nvoid initialization(double c[][DATAYSIZE][DATAXSIZE])\n{\n  srand(2);\n  for (unsigned int idz = 0.0; idz < DATAZSIZE; idz++) {\n    for (unsigned int idy = 0.0; idy < DATAYSIZE; idy++) {\n      for (unsigned int idx = 0.0; idx < DATAXSIZE; idx++) {\n        double f = (double)rand() / RAND_MAX;\n        c[idz][idy][idx] = -1.0 + 2.0*f;\n      }\n    }\n  }\n}\n\ndouble integral(const double c[][DATAYSIZE][DATAXSIZE], int nx, int ny, int nz)\n{\n  double summation = 0.0;  \n\n  for (int k = 0; k < nz; k++)\n    for(int j = 0; j < ny; j++)\n      for(int i = 0; i < nx; i++)\n        summation = summation + c[k][j][i];\n\n  return summation;\n}\n\nint main(int argc, char *argv[])\n{\n  const double dx = 1.0;\n  const double dy = 1.0;\n  const double dz = 1.0;\n  const double dt = 0.01;\n  const double e_AA = -(2.0/9.0);\n  const double e_BB = -(2.0/9.0);\n  const double e_AB = (2.0/9.0);\n  const int t_f = atoi(argv[1]);    \n\n#ifndef DEBUG\n  const int t_freq = t_f; \n#else\n  const int t_freq = 10;\n#endif\n  const double gamma = 0.5;\n  const double D = 1.0;\n\n  string name_c = \"./out/integral_c.txt\";\n  ofstream ofile_c (name_c);\n\n  string name_mu = \"./out/integral_mu.txt\";\n  ofstream ofile_mu (name_mu);\n\n  string name_f = \"./out/integral_f.txt\";\n  ofstream ofile_f (name_f);\n\n  typedef double nRarray[DATAYSIZE][DATAXSIZE];\n\n  \n\n  const int nx = DATAXSIZE;\n  const int ny = DATAYSIZE;\n  const int nz = DATAZSIZE;\n  const int vol = nx * ny * nz;\n  const size_t vol_bytes = vol * sizeof(double);\n\n  \n\n  nRarray *c_host; \n\n  nRarray *mu_host;\n  nRarray *f_host;\n  nRarray *d_cold; \n\n  nRarray *d_cnew;\n  nRarray *d_muold;\n  nRarray *d_fold;\n\n  if ((c_host = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"c_host malloc failed\\n\"); \n    return 1;\n  }\n  if ((mu_host = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"mu_host malloc failed\\n\"); \n    return 1;\n  }\n  if ((f_host = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"f_host malloc failed\\n\"); \n    return 1;\n  }\n\n  hipMalloc((void **) &d_cold, vol_bytes);\n  hipMalloc((void **) &d_cnew, vol_bytes);\n  hipMalloc((void **) &d_muold, vol_bytes);\n  hipMalloc((void **) &d_fold, vol_bytes);\n\n  initialization(c_host);\n\n  double integral_c = 0.0;\n  double integral_mu = 0.0;\n  double integral_f = 0.0;\n\n  hipMemcpy(d_cold, c_host, vol_bytes, hipMemcpyHostToDevice);\n\n  const dim3 blockSize(BLKXSIZE, BLKYSIZE, BLKZSIZE);\n  const dim3 gridSize((DATAXSIZE+BLKXSIZE-1)/BLKXSIZE, \n                      (DATAYSIZE+BLKYSIZE-1)/BLKYSIZE,\n                      (DATAZSIZE+BLKZSIZE-1)/BLKZSIZE);\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int t = 0; t < t_f; t++) {\n\n    chemicalPotential<<<gridSize, blockSize>>>(d_cold,d_muold,dx,dy,dz,gamma,e_AA,e_BB,e_AB);\n    localFreeEnergyFunctional<<<gridSize, blockSize>>>(d_cold,d_fold,dx,dy,dz,gamma,e_AA,e_BB,e_AB);\n    cahnHilliard<<<gridSize, blockSize>>>(d_cnew,d_cold,d_muold,D,dt,dx,dy,dz);\n\n    if (t > 0 && t % (t_freq - 1) == 0) {\n      hipMemcpy(c_host, d_cnew, vol_bytes, hipMemcpyDeviceToHost);\n\n      hipMemcpy(mu_host, d_muold, vol_bytes, hipMemcpyDeviceToHost);\n\n      hipMemcpy(f_host, d_fold, vol_bytes, hipMemcpyDeviceToHost);\n\n      integral_c = integral(c_host,nx,ny,nz);\n\n      ofile_c << t << \",\" << integral_c << endl;\n\n      integral_mu = integral(mu_host,nx,ny,nz);\n\n      ofile_mu << t << \",\" << integral_mu << endl;\n\n      integral_f = integral(f_host,nx,ny,nz);\n\n      ofile_f << t << \",\" << integral_f << endl;\n    }\n\n    Swap<<<gridSize, blockSize>>>(d_cnew, d_cold);\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Kernel exeuction time on the GPU (%d iterations) = %.3f (s)\\n\", t_f, time * 1e-9f);\n\n  free(c_host);\n  free(mu_host);\n  free(f_host);\n  hipFree(d_cold);\n  hipFree(d_cnew);\n  hipFree(d_muold);\n  hipFree(d_fold);\n  return 0;\n}\n"}}
{"kernel_name": "che", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <iostream>\n#include <random>\n#include <fstream>\n#include <omp.h>\n\n\n\n#define DATAXSIZE 256\n#define DATAYSIZE 256\n#define DATAZSIZE 256\n\n\n\n#define BLKXSIZE 16\n#define BLKYSIZE 4\n#define BLKZSIZE 4\n\nusing namespace std;\n\n#pragma omp declare target\ndouble Laplacian(const double c[][DATAYSIZE][DATAXSIZE],\n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int xp, xn, yp, yn, zp, zn;\n\n  int nx = (int)DATAXSIZE - 1;\n  int ny = (int)DATAYSIZE - 1;\n  int nz = (int)DATAZSIZE - 1;\n\n  xp = x+1;\n  xn = x-1;\n  yp = y+1;\n  yn = y-1;\n  zp = z+1;\n  zn = z-1;\n\n  if (xp > nx) xp = 0;\n  if (yp > ny) yp = 0;\n  if (zp > nz) zp = 0;\n  if (xn < 0)  xn = nx;\n  if (yn < 0)  yn = ny;\n  if (zn < 0)  zn = nz;\n\n  double cxx = (c[z][y][xp] + c[z][y][xn] - 2.0*c[z][y][x]) / (dx*dx);\n  double cyy = (c[z][yp][x] + c[z][yn][x] - 2.0*c[z][y][x]) / (dy*dy);\n  double czz = (c[zp][y][x] + c[zn][y][x] - 2.0*c[z][y][x]) / (dz*dz);\n\n  return cxx + cyy + czz;\n}\n\ndouble GradientX(const double phi[][DATAYSIZE][DATAXSIZE], \n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int nx = (int)DATAXSIZE - 1;\n  int xp = x+1;\n  int xn = x-1;\n\n  if (xp > nx) xp = 0;\n  if (xn < 0)  xn = nx;\n\n  return (phi[z][y][xp] - phi[z][y][xn]) / (2.0*dx);\n}\n\ndouble GradientY(const double phi[][DATAYSIZE][DATAXSIZE], \n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int ny = (int)DATAYSIZE - 1;\n  int yp = y+1;\n  int yn = y-1;\n\n  if (yp > ny) yp = 0;\n  if (yn < 0)  yn = ny;\n\n  return (phi[z][yp][x] - phi[z][yn][x]) / (2.0*dy);\n}\n\ndouble GradientZ(const double phi[][DATAYSIZE][DATAXSIZE],\n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int nz = (int)DATAZSIZE - 1;\n  int zp = z+1;\n  int zn = z-1;\n\n  if (zp > nz) zp = 0;\n  if (zn < 0)  zn = nz;\n\n  return (phi[zp][y][x] - phi[zn][y][x]) / (2.0*dz);\n}\n\ndouble freeEnergy(double c, double e_AA, double e_BB, double e_AB)\n{\n  return (((9.0 / 4.0) * ((c*c+2.0*c+1.0)*e_AA+(c*c-2.0*c+1.0)*e_BB+\n          2.0*(1.0-c*c)*e_AB)) + ((3.0/2.0) * c * c) + ((3.0/12.0) * c * c * c * c));\n}\n\n#pragma omp end declare target\n\nvoid chemicalPotential(\n    const double c[][DATAYSIZE][DATAXSIZE], \n    double mu[][DATAYSIZE][DATAXSIZE], \n    double dx,\n    double dy,\n    double dz,\n    double gamma,\n    double e_AA,\n    double e_BB,\n    double e_AB)\n{\n  #pragma omp target teams distribute parallel for collapse(3)\n  for (int idz = 0; idz < DATAZSIZE; idz++) {\n    for (int idy = 0; idy < DATAYSIZE; idy++) {\n      for (int idx = 0; idx < DATAXSIZE; idx++) {\n\n        mu[idz][idy][idx] = 4.5 * ( ( c[idz][idy][idx] + 1.0 ) * e_AA + \n            ( c[idz][idy][idx] - 1 ) * e_BB - 2.0 * c[idz][idy][idx] * e_AB ) + \n          3.0 * c[idz][idy][idx] + c[idz][idy][idx] * c[idz][idy][idx] * c[idz][idy][idx] - \n          gamma * Laplacian(c,dx,dy,dz,idx,idy,idz);\n      }\n    }\n  }\n}\n\nvoid localFreeEnergyFunctional(\n    const double c[][DATAYSIZE][DATAXSIZE],\n    double f[][DATAYSIZE][DATAXSIZE], \n    double dx,\n    double dy,\n    double dz,\n    double gamma,\n    double e_AA,\n    double e_BB,\n    double e_AB)\n{\n  #pragma omp target teams distribute parallel for collapse(3)\n  for (int idz = 0; idz < DATAZSIZE; idz++) {\n    for (int idy = 0; idy < DATAYSIZE; idy++) {\n      for (int idx = 0; idx < DATAXSIZE; idx++) {\n\n        f[idz][idy][idx] = freeEnergy(c[idz][idy][idx],e_AA,e_BB,e_AB) + (gamma / 2.0) * (\n            GradientX(c,dx,dy,dz,idx,idy,idz) * GradientX(c,dx,dy,dz,idx,idy,idz) + \n            GradientY(c,dx,dy,dz,idx,idy,idz) * GradientY(c,dx,dy,dz,idx,idy,idz) + \n            GradientZ(c,dx,dy,dz,idx,idy,idz) * GradientZ(c,dx,dy,dz,idx,idy,idz));\n      }\n    }\n  }\n}\n\nvoid cahnHilliard(\n    double cnew[][DATAYSIZE][DATAXSIZE], \n    const double cold[][DATAYSIZE][DATAXSIZE], \n    const double mu[][DATAYSIZE][DATAXSIZE],\n    double D,\n    double dt,\n    double dx,\n    double dy,\n    double dz)\n{\n  #pragma omp target teams distribute parallel for collapse(3)\n  for (int idz = 0; idz < DATAZSIZE; idz++) {\n    for (int idy = 0; idy < DATAYSIZE; idy++) {\n      for (int idx = 0; idx < DATAXSIZE; idx++) {\n        cnew[idz][idy][idx] = cold[idz][idy][idx] + dt * D * Laplacian(mu,dx,dy,dz,idx,idy,idz);\n      }\n    }\n  }\n}\n\nvoid Swap(double cnew[][DATAYSIZE][DATAXSIZE], double cold[][DATAYSIZE][DATAXSIZE])\n{\n  #pragma omp target teams distribute parallel for collapse(3)\n  for (int idz = 0; idz < DATAZSIZE; idz++) {\n    for (int idy = 0; idy < DATAYSIZE; idy++) {\n      for (int idx = 0; idx < DATAXSIZE; idx++) {\n        double tmp = cnew[idz][idy][idx];\n        cnew[idz][idy][idx] = cold[idz][idy][idx];\n        cold[idz][idy][idx] = tmp;\n      }\n    }\n  }\n\n}\n\nvoid initialization(double c[][DATAYSIZE][DATAXSIZE])\n{\n  srand(2);\n  for (unsigned int idz = 0.0; idz < DATAZSIZE; idz++) {\n    for (unsigned int idy = 0.0; idy < DATAYSIZE; idy++) {\n      for (unsigned int idx = 0.0; idx < DATAXSIZE; idx++) {\n        double f = (double)rand() / RAND_MAX;\n        c[idz][idy][idx] = -1.0 + 2.0*f;\n      }\n    }\n  }\n}\n\ndouble integral(const double c[][DATAYSIZE][DATAXSIZE], int nx, int ny, int nz)\n{\n  double summation = 0.0;  \n\n  for (int k = 0; k < nz; k++)\n    for(int j = 0; j < ny; j++)\n      for(int i = 0; i < nx; i++)\n        summation = summation + c[k][j][i];\n\n  return summation;\n}\n\nint main(int argc, char *argv[])\n{\n  const double dx = 1.0;\n  const double dy = 1.0;\n  const double dz = 1.0;\n  const double dt = 0.01;\n  const double e_AA = -(2.0/9.0);\n  const double e_BB = -(2.0/9.0);\n  const double e_AB = (2.0/9.0);\n  const int t_f = atoi(argv[1]);    \n\n#ifndef DEBUG\n  const int t_freq = t_f; \n#else\n  const int t_freq = 10;\n#endif\n  const double gamma = 0.5;\n  const double D = 1.0;\n\n  string name_c = \"./out/integral_c.txt\";\n  ofstream ofile_c (name_c);\n\n  string name_mu = \"./out/integral_mu.txt\";\n  ofstream ofile_mu (name_mu);\n\n  string name_f = \"./out/integral_f.txt\";\n  ofstream ofile_f (name_f);\n\n  typedef double nRarray[DATAYSIZE][DATAXSIZE];\n\n  \n\n  const int nx = DATAXSIZE;\n  const int ny = DATAYSIZE;\n  const int nz = DATAZSIZE;\n  const int vol = nx * ny * nz;\n  const size_t vol_bytes = vol * sizeof(double);\n\n  \n\n  nRarray *cold; \n\n  nRarray *cnew; \n\n  nRarray *muold;\n  nRarray *fold;\n\n  if ((cold = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"cold host malloc failed\\n\"); \n    return 1;\n  }\n  if ((cnew = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"cnew host malloc failed\\n\"); \n    return 1;\n  }\n  if ((muold = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"muold host malloc failed\\n\"); \n    return 1;\n  }\n  if ((fold = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"fold host malloc failed\\n\"); \n    return 1;\n  }\n\n  initialization(cold);\n\n  \n\n  double *co  = (double*) cold;\n  double *mu = (double*) muold;\n  double *f  = (double*) fold;\n  double *cn  = (double*) cnew;\n\n#pragma omp target data map(to: co[0:vol]) \\\n                        map(alloc: cn[0:vol], mu[0:vol], f[0:vol])\n\n{\n  auto start = std::chrono::steady_clock::now();\n  double integral_c = 0.0;\n  double integral_mu = 0.0;\n  double integral_f = 0.0;\n\n  for (int t = 0; t < t_f; t++) {\n\n    chemicalPotential(cold,muold,dx,dy,dz,gamma,e_AA,e_BB,e_AB);\n    localFreeEnergyFunctional(cold,fold,dx,dy,dz,gamma,e_AA,e_BB,e_AB);\n    cahnHilliard(cnew,cold,muold,D,dt,dx,dy,dz);\n\n    if (t > 0 && t % (t_freq - 1) == 0) {\n      #pragma omp target update from(cn[0:vol])\n      #pragma omp target update from(mu[0:vol])\n      #pragma omp target update from(f[0:vol])\n\n      integral_c = integral(cnew,nx,ny,nz);\n\n      ofile_c << t << \",\" << integral_c << endl;\n\n      integral_mu = integral(muold,nx,ny,nz);\n\n      ofile_mu << t << \",\" << integral_mu << endl;\n\n      integral_f = integral(fold,nx,ny,nz);\n\n      ofile_f << t << \",\" << integral_f << endl;\n    }\n\n    Swap(cnew, cold);\n  }\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Kernel exeuction time on the GPU (%d iterations) = %.3f (s)\\n\", t_f, time * 1e-9f);\n}\n\n  free(cnew);\n  free(cold);\n  free(muold);\n  free(fold);\n  return 0;\n}\n"}}
{"kernel_name": "che", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <iostream>\n#include <random>\n#include <fstream>\n\n\n\n#define DATAXSIZE 256\n#define DATAYSIZE 256\n#define DATAZSIZE 256\n\n\n\n#define BLKXSIZE 16\n#define BLKYSIZE 4\n#define BLKZSIZE 4\n\nusing namespace std;\n\ndouble Laplacian(const double c[][DATAYSIZE][DATAXSIZE],\n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int xp, xn, yp, yn, zp, zn;\n\n  int nx = (int)DATAXSIZE - 1;\n  int ny = (int)DATAYSIZE - 1;\n  int nz = (int)DATAZSIZE - 1;\n\n  xp = x+1;\n  xn = x-1;\n  yp = y+1;\n  yn = y-1;\n  zp = z+1;\n  zn = z-1;\n\n  if (xp > nx) xp = 0;\n  if (yp > ny) yp = 0;\n  if (zp > nz) zp = 0;\n  if (xn < 0)  xn = nx;\n  if (yn < 0)  yn = ny;\n  if (zn < 0)  zn = nz;\n\n  double cxx = (c[z][y][xp] + c[z][y][xn] - 2.0*c[z][y][x]) / (dx*dx);\n  double cyy = (c[z][yp][x] + c[z][yn][x] - 2.0*c[z][y][x]) / (dy*dy);\n  double czz = (c[zp][y][x] + c[zn][y][x] - 2.0*c[z][y][x]) / (dz*dz);\n\n  return cxx + cyy + czz;\n}\n\ndouble GradientX(const double phi[][DATAYSIZE][DATAXSIZE], \n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int nx = (int)DATAXSIZE - 1;\n  int xp = x+1;\n  int xn = x-1;\n\n  if (xp > nx) xp = 0;\n  if (xn < 0)  xn = nx;\n\n  return (phi[z][y][xp] - phi[z][y][xn]) / (2.0*dx);\n}\n\ndouble GradientY(const double phi[][DATAYSIZE][DATAXSIZE], \n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int ny = (int)DATAYSIZE - 1;\n  int yp = y+1;\n  int yn = y-1;\n\n  if (yp > ny) yp = 0;\n  if (yn < 0)  yn = ny;\n\n  return (phi[z][yp][x] - phi[z][yn][x]) / (2.0*dy);\n}\n\ndouble GradientZ(const double phi[][DATAYSIZE][DATAXSIZE],\n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int nz = (int)DATAZSIZE - 1;\n  int zp = z+1;\n  int zn = z-1;\n\n  if (zp > nz) zp = 0;\n  if (zn < 0)  zn = nz;\n\n  return (phi[zp][y][x] - phi[zn][y][x]) / (2.0*dz);\n}\n\ndouble freeEnergy(double c, double e_AA, double e_BB, double e_AB)\n{\n  return (((9.0 / 4.0) * ((c*c+2.0*c+1.0)*e_AA+(c*c-2.0*c+1.0)*e_BB+\n          2.0*(1.0-c*c)*e_AB)) + ((3.0/2.0) * c * c) + ((3.0/12.0) * c * c * c * c));\n}\n\n\nvoid chemicalPotential(\n    const double c[][DATAYSIZE][DATAXSIZE], \n    double mu[][DATAYSIZE][DATAXSIZE], \n    double dx,\n    double dy,\n    double dz,\n    double gamma,\n    double e_AA,\n    double e_BB,\n    double e_AB)\n{\n    for (int idz = 0; idz < DATAZSIZE; idz++) {\n    for (int idy = 0; idy < DATAYSIZE; idy++) {\n      for (int idx = 0; idx < DATAXSIZE; idx++) {\n\n        mu[idz][idy][idx] = 4.5 * ( ( c[idz][idy][idx] + 1.0 ) * e_AA + \n            ( c[idz][idy][idx] - 1 ) * e_BB - 2.0 * c[idz][idy][idx] * e_AB ) + \n          3.0 * c[idz][idy][idx] + c[idz][idy][idx] * c[idz][idy][idx] * c[idz][idy][idx] - \n          gamma * Laplacian(c,dx,dy,dz,idx,idy,idz);\n      }\n    }\n  }\n}\n\nvoid localFreeEnergyFunctional(\n    const double c[][DATAYSIZE][DATAXSIZE],\n    double f[][DATAYSIZE][DATAXSIZE], \n    double dx,\n    double dy,\n    double dz,\n    double gamma,\n    double e_AA,\n    double e_BB,\n    double e_AB)\n{\n    for (int idz = 0; idz < DATAZSIZE; idz++) {\n    for (int idy = 0; idy < DATAYSIZE; idy++) {\n      for (int idx = 0; idx < DATAXSIZE; idx++) {\n\n        f[idz][idy][idx] = freeEnergy(c[idz][idy][idx],e_AA,e_BB,e_AB) + (gamma / 2.0) * (\n            GradientX(c,dx,dy,dz,idx,idy,idz) * GradientX(c,dx,dy,dz,idx,idy,idz) + \n            GradientY(c,dx,dy,dz,idx,idy,idz) * GradientY(c,dx,dy,dz,idx,idy,idz) + \n            GradientZ(c,dx,dy,dz,idx,idy,idz) * GradientZ(c,dx,dy,dz,idx,idy,idz));\n      }\n    }\n  }\n}\n\nvoid cahnHilliard(\n    double cnew[][DATAYSIZE][DATAXSIZE], \n    const double cold[][DATAYSIZE][DATAXSIZE], \n    const double mu[][DATAYSIZE][DATAXSIZE],\n    double D,\n    double dt,\n    double dx,\n    double dy,\n    double dz)\n{\n    for (int idz = 0; idz < DATAZSIZE; idz++) {\n    for (int idy = 0; idy < DATAYSIZE; idy++) {\n      for (int idx = 0; idx < DATAXSIZE; idx++) {\n        cnew[idz][idy][idx] = cold[idz][idy][idx] + dt * D * Laplacian(mu,dx,dy,dz,idx,idy,idz);\n      }\n    }\n  }\n}\n\nvoid Swap(double cnew[][DATAYSIZE][DATAXSIZE], double cold[][DATAYSIZE][DATAXSIZE])\n{\n    for (int idz = 0; idz < DATAZSIZE; idz++) {\n    for (int idy = 0; idy < DATAYSIZE; idy++) {\n      for (int idx = 0; idx < DATAXSIZE; idx++) {\n        double tmp = cnew[idz][idy][idx];\n        cnew[idz][idy][idx] = cold[idz][idy][idx];\n        cold[idz][idy][idx] = tmp;\n      }\n    }\n  }\n\n}\n\nvoid initialization(double c[][DATAYSIZE][DATAXSIZE])\n{\n  srand(2);\n  for (unsigned int idz = 0.0; idz < DATAZSIZE; idz++) {\n    for (unsigned int idy = 0.0; idy < DATAYSIZE; idy++) {\n      for (unsigned int idx = 0.0; idx < DATAXSIZE; idx++) {\n        double f = (double)rand() / RAND_MAX;\n        c[idz][idy][idx] = -1.0 + 2.0*f;\n      }\n    }\n  }\n}\n\ndouble integral(const double c[][DATAYSIZE][DATAXSIZE], int nx, int ny, int nz)\n{\n  double summation = 0.0;  \n\n  for (int k = 0; k < nz; k++)\n    for(int j = 0; j < ny; j++)\n      for(int i = 0; i < nx; i++)\n        summation = summation + c[k][j][i];\n\n  return summation;\n}\n\nint main(int argc, char *argv[])\n{\n  const double dx = 1.0;\n  const double dy = 1.0;\n  const double dz = 1.0;\n  const double dt = 0.01;\n  const double e_AA = -(2.0/9.0);\n  const double e_BB = -(2.0/9.0);\n  const double e_AB = (2.0/9.0);\n  const int t_f = atoi(argv[1]);    \n\n#ifndef DEBUG\n  const int t_freq = t_f; \n#else\n  const int t_freq = 10;\n#endif\n  const double gamma = 0.5;\n  const double D = 1.0;\n\n  string name_c = \"./out/integral_c.txt\";\n  ofstream ofile_c (name_c);\n\n  string name_mu = \"./out/integral_mu.txt\";\n  ofstream ofile_mu (name_mu);\n\n  string name_f = \"./out/integral_f.txt\";\n  ofstream ofile_f (name_f);\n\n  typedef double nRarray[DATAYSIZE][DATAXSIZE];\n\n  \n\n  const int nx = DATAXSIZE;\n  const int ny = DATAYSIZE;\n  const int nz = DATAZSIZE;\n  const int vol = nx * ny * nz;\n  const size_t vol_bytes = vol * sizeof(double);\n\n  \n\n  nRarray *cold; \n\n  nRarray *cnew; \n\n  nRarray *muold;\n  nRarray *fold;\n\n  if ((cold = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"cold host malloc failed\\n\"); \n    return 1;\n  }\n  if ((cnew = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"cnew host malloc failed\\n\"); \n    return 1;\n  }\n  if ((muold = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"muold host malloc failed\\n\"); \n    return 1;\n  }\n  if ((fold = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"fold host malloc failed\\n\"); \n    return 1;\n  }\n\n  initialization(cold);\n\n  \n\n  double *co  = (double*) cold;\n  double *mu = (double*) muold;\n  double *f  = (double*) fold;\n  double *cn  = (double*) cnew;\n\n\n{\n  auto start = std::chrono::steady_clock::now();\n  double integral_c = 0.0;\n  double integral_mu = 0.0;\n  double integral_f = 0.0;\n\n  for (int t = 0; t < t_f; t++) {\n\n    chemicalPotential(cold,muold,dx,dy,dz,gamma,e_AA,e_BB,e_AB);\n    localFreeEnergyFunctional(cold,fold,dx,dy,dz,gamma,e_AA,e_BB,e_AB);\n    cahnHilliard(cnew,cold,muold,D,dt,dx,dy,dz);\n\n    if (t > 0 && t % (t_freq - 1) == 0) {\n                  \n      integral_c = integral(cnew,nx,ny,nz);\n\n      ofile_c << t << \",\" << integral_c << endl;\n\n      integral_mu = integral(muold,nx,ny,nz);\n\n      ofile_mu << t << \",\" << integral_mu << endl;\n\n      integral_f = integral(fold,nx,ny,nz);\n\n      ofile_f << t << \",\" << integral_f << endl;\n    }\n\n    Swap(cnew, cold);\n  }\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Kernel exeuction time on the GPU (%d iterations) = %.3f (s)\\n\", t_f, time * 1e-9f);\n}\n\n  free(cnew);\n  free(cold);\n  free(muold);\n  free(fold);\n  return 0;\n}"}}
{"kernel_name": "che", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <iostream>\n#include <random>\n#include <fstream>\n#include <sycl/sycl.hpp>\n\n\n\n#define DATAXSIZE 256\n#define DATAYSIZE 256\n#define DATAZSIZE 256\n\n\n\n#define BLKXSIZE 16\n#define BLKYSIZE 4\n#define BLKZSIZE 4\n\nusing namespace std;\n\ndouble Laplacian(const double c[][DATAYSIZE][DATAXSIZE],\n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int xp, xn, yp, yn, zp, zn;\n\n  int nx = (int)DATAXSIZE - 1;\n  int ny = (int)DATAYSIZE - 1;\n  int nz = (int)DATAZSIZE - 1;\n\n  xp = x+1;\n  xn = x-1;\n  yp = y+1;\n  yn = y-1;\n  zp = z+1;\n  zn = z-1;\n\n  if (xp > nx) xp = 0;\n  if (yp > ny) yp = 0;\n  if (zp > nz) zp = 0;\n  if (xn < 0)  xn = nx;\n  if (yn < 0)  yn = ny;\n  if (zn < 0)  zn = nz;\n\n  double cxx = (c[z][y][xp] + c[z][y][xn] - 2.0*c[z][y][x]) / (dx*dx);\n  double cyy = (c[z][yp][x] + c[z][yn][x] - 2.0*c[z][y][x]) / (dy*dy);\n  double czz = (c[zp][y][x] + c[zn][y][x] - 2.0*c[z][y][x]) / (dz*dz);\n\n  return cxx + cyy + czz;\n}\n\ndouble GradientX(const double phi[][DATAYSIZE][DATAXSIZE], \n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int nx = (int)DATAXSIZE - 1;\n  int xp = x+1;\n  int xn = x-1;\n\n  if (xp > nx) xp = 0;\n  if (xn < 0)  xn = nx;\n\n  return (phi[z][y][xp] - phi[z][y][xn]) / (2.0*dx);\n}\n\ndouble GradientY(const double phi[][DATAYSIZE][DATAXSIZE], \n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int ny = (int)DATAYSIZE - 1;\n  int yp = y+1;\n  int yn = y-1;\n\n  if (yp > ny) yp = 0;\n  if (yn < 0)  yn = ny;\n\n  return (phi[z][yp][x] - phi[z][yn][x]) / (2.0*dy);\n}\n\ndouble GradientZ(const double phi[][DATAYSIZE][DATAXSIZE],\n                 double dx, double dy, double dz, int x, int y, int z)\n{\n  int nz = (int)DATAZSIZE - 1;\n  int zp = z+1;\n  int zn = z-1;\n\n  if (zp > nz) zp = 0;\n  if (zn < 0)  zn = nz;\n\n  return (phi[zp][y][x] - phi[zn][y][x]) / (2.0*dz);\n}\n\nvoid chemicalPotential(\n    sycl::nd_item<3> &item,\n    const double c[][DATAYSIZE][DATAXSIZE], \n    double mu[][DATAYSIZE][DATAXSIZE], \n    double dx,\n    double dy,\n    double dz,\n    double gamma,\n    double e_AA,\n    double e_BB,\n    double e_AB)\n{\n  unsigned idx = item.get_global_id(2);\n  unsigned idy = item.get_global_id(1);\n  unsigned idz = item.get_global_id(0);\n\n  if ((idx < DATAXSIZE) && (idy < DATAYSIZE) && (idz < DATAZSIZE)) {\n\n    mu[idz][idy][idx] = 4.5 * ( ( c[idz][idy][idx] + 1.0 ) * e_AA + \n        ( c[idz][idy][idx] - 1 ) * e_BB - 2.0 * c[idz][idy][idx] * e_AB ) + \n      3.0 * c[idz][idy][idx] + c[idz][idy][idx] * c[idz][idy][idx] * c[idz][idy][idx] - \n      gamma * Laplacian(c,dx,dy,dz,idx,idy,idz);\n  }\n}\n\ndouble freeEnergy(double c, double e_AA, double e_BB, double e_AB)\n{\n  return (((9.0 / 4.0) * ((c*c+2.0*c+1.0)*e_AA+(c*c-2.0*c+1.0)*e_BB+\n          2.0*(1.0-c*c)*e_AB)) + ((3.0/2.0) * c * c) + ((3.0/12.0) * c * c * c * c));\n}\n\nvoid localFreeEnergyFunctional(\n    sycl::nd_item<3> &item,\n    const double c[][DATAYSIZE][DATAXSIZE],\n    double f[][DATAYSIZE][DATAXSIZE], \n    double dx,\n    double dy,\n    double dz,\n    double gamma,\n    double e_AA,\n    double e_BB,\n    double e_AB)\n{\n  unsigned idx = item.get_global_id(2);\n  unsigned idy = item.get_global_id(1);\n  unsigned idz = item.get_global_id(0);\n\n  if ((idx < DATAXSIZE) && (idy < DATAYSIZE) && (idz < DATAZSIZE)) {\n\n    f[idz][idy][idx] = freeEnergy(c[idz][idy][idx],e_AA,e_BB,e_AB) + (gamma / 2.0) * (\n        GradientX(c,dx,dy,dz,idx,idy,idz) * GradientX(c,dx,dy,dz,idx,idy,idz) + \n        GradientY(c,dx,dy,dz,idx,idy,idz) * GradientY(c,dx,dy,dz,idx,idy,idz) + \n        GradientZ(c,dx,dy,dz,idx,idy,idz) * GradientZ(c,dx,dy,dz,idx,idy,idz));\n  }\n}\n\nvoid cahnHilliard(\n    sycl::nd_item<3> &item,\n    double cnew[][DATAYSIZE][DATAXSIZE], \n    const double cold[][DATAYSIZE][DATAXSIZE], \n    const double mu[][DATAYSIZE][DATAXSIZE],\n    double D,\n    double dt,\n    double dx,\n    double dy,\n    double dz)\n{\n  unsigned idx = item.get_global_id(2);\n  unsigned idy = item.get_global_id(1);\n  unsigned idz = item.get_global_id(0);\n\n  if ((idx < DATAXSIZE) && (idy < DATAYSIZE) && (idz < DATAZSIZE)) {\n    cnew[idz][idy][idx] = cold[idz][idy][idx] + dt * D * Laplacian(mu,dx,dy,dz,idx,idy,idz);\n  }\n}\n\nvoid Swap(sycl::nd_item<3> &item, double cnew[][DATAYSIZE][DATAXSIZE], double cold[][DATAYSIZE][DATAXSIZE])\n{\n  unsigned idx = item.get_global_id(2);\n  unsigned idy = item.get_global_id(1);\n  unsigned idz = item.get_global_id(0);\n  double tmp;    \n\n  if ((idx < DATAXSIZE) && (idy < DATAYSIZE) && (idz < DATAZSIZE)) {\n    tmp = cnew[idz][idy][idx];\n    cnew[idz][idy][idx] = cold[idz][idy][idx];\n    cold[idz][idy][idx] = tmp;\n  }\n}\n\nvoid initialization(double c[][DATAYSIZE][DATAXSIZE])\n{\n  srand(2);\n  for (unsigned int idz = 0.0; idz < DATAZSIZE; idz++) {\n    for (unsigned int idy = 0.0; idy < DATAYSIZE; idy++) {\n      for (unsigned int idx = 0.0; idx < DATAXSIZE; idx++) {\n        double f = (double)rand() / RAND_MAX;\n        c[idz][idy][idx] = -1.0 + 2.0*f;\n      }\n    }\n  }\n}\n\ndouble integral(const double c[][DATAYSIZE][DATAXSIZE], int nx, int ny, int nz)\n{\n  double summation = 0.0;  \n\n  for (int k = 0; k < nz; k++)\n    for(int j = 0; j < ny; j++)\n      for(int i = 0; i < nx; i++)\n        summation = summation + c[k][j][i];\n\n  return summation;\n}\n\nint main(int argc, char *argv[])\n{\n  const double dx = 1.0;\n  const double dy = 1.0;\n  const double dz = 1.0;\n  const double dt = 0.01;\n  const double e_AA = -(2.0/9.0);\n  const double e_BB = -(2.0/9.0);\n  const double e_AB = (2.0/9.0);\n  const int t_f = atoi(argv[1]);    \n\n#ifndef DEBUG\n  const int t_freq = t_f; \n#else\n  const int t_freq = 10;\n#endif\n  const double gamma = 0.5;\n  const double D = 1.0;\n\n  string name_c = \"./out/integral_c.txt\";\n  ofstream ofile_c (name_c);\n\n  string name_mu = \"./out/integral_mu.txt\";\n  ofstream ofile_mu (name_mu);\n\n  string name_f = \"./out/integral_f.txt\";\n  ofstream ofile_f (name_f);\n\n  typedef double nRarray[DATAYSIZE][DATAXSIZE];\n\n  \n\n  const int nx = DATAXSIZE;\n  const int ny = DATAYSIZE;\n  const int nz = DATAZSIZE;\n  const int vol = nx * ny * nz;\n  const size_t vol_bytes = vol * sizeof(double);\n\n  \n\n  nRarray *c_host; \n\n  nRarray *mu_host;\n  nRarray *f_host;\n\n  if ((c_host = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"c_host malloc failed\\n\"); \n    return 1;\n  }\n  if ((mu_host = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"mu_host malloc failed\\n\"); \n    return 1;\n  }\n  if ((f_host = (nRarray *)malloc(vol_bytes)) == 0) {\n    fprintf(stderr,\"f_host malloc failed\\n\"); \n    return 1;\n  }\n\n  initialization(c_host);\n\n  double integral_c = 0.0;\n  double integral_mu = 0.0;\n  double integral_f = 0.0;\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  nRarray *d_cold = (nRarray*) sycl::malloc_device(vol_bytes, q);\n  nRarray *d_cnew = (nRarray*) sycl::malloc_device(vol_bytes, q);\n  nRarray *d_muold = (nRarray*) sycl::malloc_device(vol_bytes, q);\n  nRarray *d_fold = (nRarray*) sycl::malloc_device(vol_bytes, q);\n\n  q.memcpy(d_cold, c_host, vol_bytes);\n\n  sycl::range<3> lws (BLKZSIZE, BLKYSIZE, BLKXSIZE);\n  sycl::range<3> gws ((DATAZSIZE+BLKZSIZE-1)/BLKZSIZE * BLKZSIZE,\n                      (DATAYSIZE+BLKYSIZE-1)/BLKYSIZE * BLKYSIZE,\n                      (DATAXSIZE+BLKXSIZE-1)/BLKXSIZE * BLKXSIZE);\n\n  q.wait(); \n  auto start = std::chrono::steady_clock::now();\n\n  for (int t = 0; t < t_f; t++) {\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class chemical_potential>(\n        sycl::nd_range<3>(gws, lws), [=] (sycl::nd_item<3> item) {\n        chemicalPotential(item, d_cold, d_muold,\n                          dx,dy,dz,gamma,e_AA,e_BB,e_AB);\n      });\n    });\n        \n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class energy>(\n        sycl::nd_range<3>(gws, lws), [=] (sycl::nd_item<3> item) {\n        localFreeEnergyFunctional(item, d_cold, d_fold,\n                                  dx,dy,dz,gamma,e_AA,e_BB,e_AB);\n      });\n    });\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class cahn_hilliard>(\n        sycl::nd_range<3>(gws, lws), [=] (sycl::nd_item<3> item) {\n        cahnHilliard(item, d_cnew, d_cold, d_muold,\n                     D,dt,dx,dy,dz);\n      });\n    });\n\n    if (t > 0 && t % (t_freq - 1) == 0) {\n\n      q.memcpy(c_host, d_cnew, vol_bytes);\n\n      q.memcpy(mu_host, d_muold, vol_bytes);\n\n      q.memcpy(f_host, d_fold, vol_bytes);\n\n      q.wait();\n      integral_c = integral(c_host,nx,ny,nz);\n\n      ofile_c << t << \",\" << integral_c << std::endl;\n\n      integral_mu = integral(mu_host,nx,ny,nz);\n\n      ofile_mu << t << \",\" << integral_mu << std::endl;\n\n      integral_f = integral(f_host,nx,ny,nz);\n\n      ofile_f << t << \",\" << integral_f << std::endl;\n    }\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class swap_grid>(\n        sycl::nd_range<3>(gws, lws), [=] (sycl::nd_item<3> item) {\n        Swap(item, d_cnew, d_cold);\n      });\n    });\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Kernel exeuction time on the GPU (%d iterations) = %.3f (s)\\n\", t_f, time * 1e-9f);\n\n  free(c_host);\n  free(mu_host);\n  free(f_host);\n  sycl::free(d_cold, q);\n  sycl::free(d_cnew, q);\n  sycl::free(d_muold, q);\n  sycl::free(d_fold, q);\n  return 0;\n}\n"}}
{"kernel_name": "chi2", "parallel_api": "cuda", "code": {"chi2.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <iostream>\n#include <chrono>\n#include <random>\n#include <cuda.h>\n#include \"reference.h\"\n\n__global__ void kernel(\n  const unsigned int rows,\n  const unsigned int cols,\n  const int cRows,\n  const int contRows,\n  const unsigned char *__restrict__ snpdata,\n  float *__restrict__ results)\n{\n  unsigned char y;\n  int m, n;\n  unsigned int p = 0;\n  int tot_cases = 1;\n  int tot_controls= 1;\n  int total = 1;\n  float chisquare = 0.0f;\n  float exp[3];        \n  float Conexpected[3];        \n  float Cexpected[3];\n  float numerator1;\n  float numerator2;\n\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= cols) return;\n\n  int cases[3] = {1,1,1};\n  int controls[3] = {1,1,1};\n\n  \n\n  for ( m = 0 ; m < cRows ; m++ ) {\n    y = snpdata[(size_t)m * (size_t)cols + tid];\n    if ( y == '0') { cases[0]++; }\n    else if ( y == '1') { cases[1]++; }\n    else if ( y == '2') { cases[2]++; }\n  }\n\n  \n\n  for ( n = cRows ; n < cRows + contRows ; n++ ) {\n    y = snpdata[(size_t)n * (size_t)cols + tid];\n    if ( y == '0' ) { controls[0]++; }\n    else if ( y == '1') { controls[1]++; }\n    else if ( y == '2') { controls[2]++; }\n  }\n\n  for( p = 0 ; p < 3; p++ ) {\n    tot_cases += cases[p];\n    tot_controls += controls[p];\n  }\n  total = tot_cases + tot_controls;\n\n  for( p = 0 ; p < 3; p++ ) {\n    exp[p] = (float)cases[p] + controls[p]; \n    Cexpected[p] = tot_cases * exp[p] / total;\n    Conexpected[p] = tot_controls * exp[p] / total;\n    numerator1 = (float)cases[p] - Cexpected[p];\n    numerator2 = (float)controls[p] - Conexpected[p];\n    chisquare += numerator1 * numerator1 / Cexpected[p] +\n                 numerator2 * numerator2 / Conexpected[p];\n  }\n  results[tid] = chisquare;\n}\n\nint main(int argc, char* argv[]) {\n\n  if (argc != 7) {\n    printf(\"Usage: %s <rows> <cols> <cases> <controls> \"\n           \"<threads> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  unsigned int rows = atoi(argv[1]);\n  unsigned int cols = atoi(argv[2]);\n  int ncases = atoi(argv[3]);\n  int ncontrols = atoi(argv[4]);\n  int nthreads = atoi(argv[5]);\n  int repeat = atoi(argv[6]);\n\n  printf(\"Individuals=%d SNPs=%d cases=%d controls=%d nthreads=%d\\n\",\n         rows,cols,ncases,ncontrols,nthreads);\n\n  size_t size = (size_t)rows * (size_t)cols;\n  printf(\"Size of the data = %lu\\n\",size);\n\n  \n\n  size_t snpdata_size = sizeof(unsigned char) * size;\n  size_t result_size = sizeof(float) * cols;\n\n  unsigned char *dataT = (unsigned char*)malloc(snpdata_size);\n  float* h_results = (float*) malloc(result_size);\n  float* cpu_results = (float*) malloc(result_size);\n\n  if(dataT == NULL || h_results == NULL || cpu_results == NULL) {\n    printf(\"ERROR: Memory for data not allocated.\\n\");\n    if (dataT) free(dataT);\n    if (h_results) free(h_results);\n    return 1;\n  }\n\n  std::mt19937 gen(19937); \n\n  std::uniform_int_distribution<> distrib(0, 2);\n  for (size_t i = 0; i < snpdata_size; i++) {\n    dataT[i] = distrib(gen) + '0';\n  }\n\n  \n\n  unsigned char *d_data;\n  float *d_results;\n  cudaMalloc((void**) &d_data, snpdata_size);\n  cudaMalloc((void**) &d_results, result_size);\n\n  cudaMemcpy(d_data, dataT, snpdata_size, cudaMemcpyHostToDevice);\n\n  unsigned jobs = cols;\n  int nblocks = (jobs + nthreads - 1)/nthreads;\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::high_resolution_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    kernel <<< dim3(nblocks), dim3(nthreads) >>> (rows,cols,ncases,ncontrols,d_data,d_results);\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::high_resolution_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time = %f (s)\\n\", time * 1e-9f / repeat);\n\n  cudaMemcpy(h_results, d_results, result_size, cudaMemcpyDeviceToHost);\n\n  cudaFree(d_data);\n  cudaFree(d_results);\n\n  start = std::chrono::high_resolution_clock::now();\n\n  cpu_kernel(rows,cols,ncases,ncontrols,dataT,cpu_results);\n\n  end = std::chrono::high_resolution_clock::now();\n  time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Host execution time = %f (s)\\n\", time * 1e-9f);\n\n  \n\n  int error = 0;\n  for(unsigned int k = 0; k < jobs; k++) {\n    if (fabs(cpu_results[k] - h_results[k]) > 1e-4) error++;\n  }\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  free(dataT);\n  free(h_results);\n  free(cpu_results);\n\n  return 0;\n}\n"}}
{"kernel_name": "chi2", "parallel_api": "hip", "code": {"chi2.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <iostream>\n#include <chrono>\n#include <random>\n#include <hip/hip_runtime.h>\n#include \"reference.h\"\n\n__global__ void kernel(\n  const unsigned int rows,\n  const unsigned int cols,\n  const int cRows,\n  const int contRows,\n  const unsigned char *__restrict__ snpdata,\n  float *__restrict__ results)\n{\n  unsigned char y;\n  int m, n;\n  unsigned int p = 0;\n  int tot_cases = 1;\n  int tot_controls= 1;\n  int total = 1;\n  float chisquare = 0.0f;\n  float exp[3];        \n  float Conexpected[3];        \n  float Cexpected[3];\n  float numerator1;\n  float numerator2;\n\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= cols) return;\n\n  int cases[3] = {1,1,1};\n  int controls[3] = {1,1,1};\n\n  \n\n  for ( m = 0 ; m < cRows ; m++ ) {\n    y = snpdata[(size_t)m * (size_t)cols + tid];\n    if ( y == '0') { cases[0]++; }\n    else if ( y == '1') { cases[1]++; }\n    else if ( y == '2') { cases[2]++; }\n  }\n\n  \n\n  for ( n = cRows ; n < cRows + contRows ; n++ ) {\n    y = snpdata[(size_t)n * (size_t)cols + tid];\n    if ( y == '0' ) { controls[0]++; }\n    else if ( y == '1') { controls[1]++; }\n    else if ( y == '2') { controls[2]++; }\n  }\n\n  for( p = 0 ; p < 3; p++ ) {\n    tot_cases += cases[p];\n    tot_controls += controls[p];\n  }\n  total = tot_cases + tot_controls;\n\n  for( p = 0 ; p < 3; p++ ) {\n    exp[p] = (float)cases[p] + controls[p]; \n    Cexpected[p] = tot_cases * exp[p] / total;\n    Conexpected[p] = tot_controls * exp[p] / total;\n    numerator1 = (float)cases[p] - Cexpected[p];\n    numerator2 = (float)controls[p] - Conexpected[p];\n    chisquare += numerator1 * numerator1 / Cexpected[p] +\n                 numerator2 * numerator2 / Conexpected[p];\n  }\n  results[tid] = chisquare;\n}\n\nint main(int argc, char* argv[]) {\n\n  if (argc != 7) {\n    printf(\"Usage: %s <rows> <cols> <cases> <controls> \"\n           \"<threads> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  unsigned int rows = atoi(argv[1]);\n  unsigned int cols = atoi(argv[2]);\n  int ncases = atoi(argv[3]);\n  int ncontrols = atoi(argv[4]);\n  int nthreads = atoi(argv[5]);\n  int repeat = atoi(argv[6]);\n\n  printf(\"Individuals=%d SNPs=%d cases=%d controls=%d nthreads=%d\\n\",\n         rows,cols,ncases,ncontrols,nthreads);\n\n  size_t size = (size_t)rows * (size_t)cols;\n  printf(\"Size of the data = %lu\\n\",size);\n\n  \n\n  size_t snpdata_size = sizeof(unsigned char) * size;\n  size_t result_size = sizeof(float) * cols;\n\n  unsigned char *dataT = (unsigned char*)malloc(snpdata_size);\n  float* h_results = (float*) malloc(result_size);\n  float* cpu_results = (float*) malloc(result_size);\n\n  if(dataT == NULL || h_results == NULL || cpu_results == NULL) {\n    printf(\"ERROR: Memory for data not allocated.\\n\");\n    if (dataT) free(dataT);\n    if (h_results) free(h_results);\n    return 1;\n  }\n\n  std::mt19937 gen(19937); \n\n  std::uniform_int_distribution<> distrib(0, 2);\n  for (size_t i = 0; i < snpdata_size; i++) {\n    dataT[i] = distrib(gen) + '0';\n  }\n\n  \n\n  unsigned char *d_data;\n  float *d_results;\n  hipMalloc((void**) &d_data, snpdata_size);\n  hipMalloc((void**) &d_results, result_size);\n\n  hipMemcpy(d_data, dataT, snpdata_size, hipMemcpyHostToDevice);\n\n  unsigned jobs = cols;\n  int nblocks = (jobs + nthreads - 1)/nthreads;\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::high_resolution_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    kernel <<< dim3(nblocks), dim3(nthreads) >>> (rows,cols,ncases,ncontrols,d_data,d_results);\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::high_resolution_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time = %f (s)\\n\", time * 1e-9f / repeat);\n\n  hipMemcpy(h_results, d_results, result_size, hipMemcpyDeviceToHost);\n\n  hipFree(d_data);\n  hipFree(d_results);\n\n  start = std::chrono::high_resolution_clock::now();\n\n  cpu_kernel(rows,cols,ncases,ncontrols,dataT,cpu_results);\n\n  end = std::chrono::high_resolution_clock::now();\n  time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Host execution time = %f (s)\\n\", time * 1e-9f);\n\n  \n\n  int error = 0;\n  for(unsigned int k = 0; k < jobs; k++) {\n    if (fabs(cpu_results[k] - h_results[k]) > 1e-4) error++;\n  }\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  free(dataT);\n  free(h_results);\n  free(cpu_results);\n\n  return 0;\n}\n"}}
{"kernel_name": "chi2", "parallel_api": "omp", "code": {"chi2.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <iostream>\n#include <chrono>\n#include <random>\n#include <omp.h>\n#include \"reference.h\"\n\nint main(int argc, char* argv[]) {\n\n  if (argc != 7) {\n    printf(\"Usage: %s <rows> <cols> <cases> <controls> \"\n           \"<threads> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  unsigned int rows = atoi(argv[1]);\n  unsigned int cols = atoi(argv[2]);\n  int ncases = atoi(argv[3]);\n  int ncontrols = atoi(argv[4]);\n  int nthreads = atoi(argv[5]);\n  int repeat = atoi(argv[6]);\n\n  printf(\"Individuals=%d SNPs=%d cases=%d controls=%d nthreads=%d\\n\",\n         rows,cols,ncases,ncontrols,nthreads);\n\n  size_t size = (size_t)rows * (size_t)cols;\n  printf(\"Size of the data = %lu\\n\",size);\n\n  \n\n  unsigned char *dataT = (unsigned char*)malloc(size);\n  float* h_results = (float*) malloc(cols * sizeof(float)); \n  float* cpu_results = (float*) malloc(cols * sizeof(float)); \n\n  if(dataT == NULL || h_results == NULL || cpu_results == NULL) {\n    printf(\"ERROR: Memory for data not allocated.\\n\");\n    if (dataT) free(dataT);\n    if (h_results) free(h_results);\n    return 1;\n  }\n\n  std::mt19937 gen(19937); \n\n  std::uniform_int_distribution<> distrib(0, 2);\n  for (size_t i = 0; i < size; i++) {\n    dataT[i] = distrib(gen) + '0';\n  }\n\n  #pragma omp target data map(to: dataT[0:size]) map(from: h_results[0:cols])\n  {\n    auto start = std::chrono::high_resolution_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      #pragma omp target teams distribute parallel for simd thread_limit(nthreads)\n      for (int i = 0; i < cols; i++) {\n        unsigned char y;\n        int m, n;\n        unsigned int p = 0;\n        int cases[3] = {1,1,1};\n        int controls[3] = {1,1,1};\n        int tot_cases = 1;\n        int tot_controls= 1;\n        int total = 1;\n        float chisquare = 0.0f;\n        float exp[3];        \n        float Conexpected[3];        \n        float Cexpected[3];\n        float numerator1;\n        float numerator2;\n\n        \n\n        for ( m = 0 ; m < ncases ; m++ ) {\n          y = dataT[(size_t)m * (size_t)cols + i];\n          if ( y == '0') { cases[0]++; }\n          else if ( y == '1') { cases[1]++; }\n          else if ( y == '2') { cases[2]++; }\n        }\n\n        \n\n        for ( n = ncases ; n < ncases + ncontrols ; n++ ) {\n          y = dataT[(size_t)n * (size_t)cols + i];\n          if ( y == '0' ) { controls[0]++; }\n          else if ( y == '1') { controls[1]++; }\n          else if ( y == '2') { controls[2]++; }\n        }\n\n        for( p = 0 ; p < 3; p++ ) {\n          tot_cases += cases[p];\n          tot_controls += controls[p];\n        }\n        total = tot_cases + tot_controls;\n\n        for( p = 0 ; p < 3; p++ ) {\n          exp[p] = (float)cases[p] + controls[p]; \n          Cexpected[p] = tot_cases * exp[p] / total;\n          Conexpected[p] = tot_controls * exp[p] / total;\n          numerator1 = (float)cases[p] - Cexpected[p];\n          numerator2 = (float)controls[p] - Conexpected[p];\n          chisquare += numerator1 * numerator1 / Cexpected[p] +  numerator2 * numerator2 / Conexpected[p];\n        }\n        h_results[i] = chisquare;\n      }\n    }\n    auto end = std::chrono::high_resolution_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time = %f (s)\\n\", time * 1e-9f / repeat);\n  }\n\n  auto start = std::chrono::high_resolution_clock::now();\n\n  cpu_kernel(rows,cols,ncases,ncontrols,dataT,cpu_results);\n\n  auto end = std::chrono::high_resolution_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Host execution time = %f (s)\\n\", time * 1e-9f);\n\n  \n\n  int error = 0;\n  for(unsigned int k = 0; k < cols; k++) {\n    if (fabs(cpu_results[k] - h_results[k]) > 1e-4) error++;\n  }\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  free(dataT);\n  free(h_results);\n  free(cpu_results);\n \n  return 0;\n}\n"}}
{"kernel_name": "chi2", "parallel_api": "serial", "code": {"chi2.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <iostream>\n#include <chrono>\n#include <random>\n#include \"reference.h\"\n\nint main(int argc, char* argv[]) {\n\n  if (argc != 7) {\n    printf(\"Usage: %s <rows> <cols> <cases> <controls> \"\n           \"<threads> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  unsigned int rows = atoi(argv[1]);\n  unsigned int cols = atoi(argv[2]);\n  int ncases = atoi(argv[3]);\n  int ncontrols = atoi(argv[4]);\n  int nthreads = atoi(argv[5]);\n  int repeat = atoi(argv[6]);\n\n  printf(\"Individuals=%d SNPs=%d cases=%d controls=%d nthreads=%d\\n\",\n         rows,cols,ncases,ncontrols,nthreads);\n\n  size_t size = (size_t)rows * (size_t)cols;\n  printf(\"Size of the data = %lu\\n\",size);\n\n  \n\n  unsigned char *dataT = (unsigned char*)malloc(size);\n  float* h_results = (float*) malloc(cols * sizeof(float)); \n  float* cpu_results = (float*) malloc(cols * sizeof(float)); \n\n  if(dataT == NULL || h_results == NULL || cpu_results == NULL) {\n    printf(\"ERROR: Memory for data not allocated.\\n\");\n    if (dataT) free(dataT);\n    if (h_results) free(h_results);\n    return 1;\n  }\n\n  std::mt19937 gen(19937); \n\n  std::uniform_int_distribution<> distrib(0, 2);\n  for (size_t i = 0; i < size; i++) {\n    dataT[i] = distrib(gen) + '0';\n  }\n\n    {\n    auto start = std::chrono::high_resolution_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n            for (int i = 0; i < cols; i++) {\n        unsigned char y;\n        int m, n;\n        unsigned int p = 0;\n        int cases[3] = {1,1,1};\n        int controls[3] = {1,1,1};\n        int tot_cases = 1;\n        int tot_controls= 1;\n        int total = 1;\n        float chisquare = 0.0f;\n        float exp[3];        \n        float Conexpected[3];        \n        float Cexpected[3];\n        float numerator1;\n        float numerator2;\n\n        \n\n        for ( m = 0 ; m < ncases ; m++ ) {\n          y = dataT[(size_t)m * (size_t)cols + i];\n          if ( y == '0') { cases[0]++; }\n          else if ( y == '1') { cases[1]++; }\n          else if ( y == '2') { cases[2]++; }\n        }\n\n        \n\n        for ( n = ncases ; n < ncases + ncontrols ; n++ ) {\n          y = dataT[(size_t)n * (size_t)cols + i];\n          if ( y == '0' ) { controls[0]++; }\n          else if ( y == '1') { controls[1]++; }\n          else if ( y == '2') { controls[2]++; }\n        }\n\n        for( p = 0 ; p < 3; p++ ) {\n          tot_cases += cases[p];\n          tot_controls += controls[p];\n        }\n        total = tot_cases + tot_controls;\n\n        for( p = 0 ; p < 3; p++ ) {\n          exp[p] = (float)cases[p] + controls[p]; \n          Cexpected[p] = tot_cases * exp[p] / total;\n          Conexpected[p] = tot_controls * exp[p] / total;\n          numerator1 = (float)cases[p] - Cexpected[p];\n          numerator2 = (float)controls[p] - Conexpected[p];\n          chisquare += numerator1 * numerator1 / Cexpected[p] +  numerator2 * numerator2 / Conexpected[p];\n        }\n        h_results[i] = chisquare;\n      }\n    }\n    auto end = std::chrono::high_resolution_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time = %f (s)\\n\", time * 1e-9f / repeat);\n  }\n\n  auto start = std::chrono::high_resolution_clock::now();\n\n  cpu_kernel(rows,cols,ncases,ncontrols,dataT,cpu_results);\n\n  auto end = std::chrono::high_resolution_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Host execution time = %f (s)\\n\", time * 1e-9f);\n\n  \n\n  int error = 0;\n  for(unsigned int k = 0; k < cols; k++) {\n    if (fabs(cpu_results[k] - h_results[k]) > 1e-4) error++;\n  }\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  free(dataT);\n  free(h_results);\n  free(cpu_results);\n \n  return 0;\n}"}}
{"kernel_name": "chi2", "parallel_api": "sycl", "code": {"chi2.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <iostream>\n#include <chrono>\n#include <random>\n#include <sycl/sycl.hpp>\n#include \"reference.h\"\n\nint main(int argc, char* argv[]) {\n\n  if (argc != 7) {\n    printf(\"Usage: %s <rows> <cols> <cases> <controls> \"\n           \"<threads> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  unsigned int rows = atoi(argv[1]);\n  unsigned int cols = atoi(argv[2]);\n  int ncases = atoi(argv[3]);\n  int ncontrols = atoi(argv[4]);\n  int nthreads = atoi(argv[5]);\n  int repeat = atoi(argv[6]);\n\n  printf(\"Individuals=%d SNPs=%d cases=%d controls=%d nthreads=%d\\n\",\n         rows,cols,ncases,ncontrols,nthreads);\n\n  size_t size = (size_t)rows * (size_t)cols;\n  printf(\"Size of the data = %lu\\n\",size);\n\n  \n\n  size_t snpdata_size = sizeof(unsigned char) * size;\n  size_t result_size = sizeof(float) * cols;\n\n  unsigned char *dataT = (unsigned char*)malloc(snpdata_size);\n  float* h_results = (float*) malloc(result_size);\n  float* cpu_results = (float*) malloc(result_size);\n\n  if(dataT == NULL || h_results == NULL || cpu_results == NULL) {\n    printf(\"ERROR: Memory for data not allocated.\\n\");\n    if (dataT) free(dataT);\n    if (h_results) free(h_results);\n    return 1;\n  }\n\n  std::mt19937 gen(19937); \n\n  std::uniform_int_distribution<> distrib(0, 2);\n  for (size_t i = 0; i < snpdata_size; i++) {\n    dataT[i] = distrib(gen) + '0';\n  }\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  \n\n  unsigned char *snpdata = sycl::malloc_device<unsigned char>(size, q);\n  float *chi_result = sycl::malloc_device<float>(cols, q);\n\n  q.memcpy(snpdata, dataT, snpdata_size);\n\n  unsigned jobs = cols;\n  sycl::range<1> gws ((jobs + nthreads - 1)/nthreads * nthreads);\n  sycl::range<1> lws (nthreads);\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    q.submit([&](sycl::handler& cgh) {\n      cgh.parallel_for<class chi2>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        unsigned char y;\n        int m, n;\n        unsigned int p = 0;\n        int tot_cases = 1;\n        int tot_controls= 1;\n        int total = 1;\n        float chisquare = 0.0f;\n        float exp[3];        \n        float Conexpected[3];        \n        float Cexpected[3];\n        float numerator1;\n        float numerator2;\n\n        int tid  = item.get_global_id(0);\n        if (tid >= cols) return;\n\n        int cases[3] = {1,1,1};\n        int controls[3] = {1,1,1};\n\n        \n\n        for ( m = 0 ; m < ncases ; m++ ) {\n          y = snpdata[(size_t)m * (size_t)cols + tid];\n          if ( y == '0') { cases[0]++; }\n          else if ( y == '1') { cases[1]++; }\n          else if ( y == '2') { cases[2]++; }\n        }\n\n        \n\n        for ( n = ncases ; n < ncases + ncontrols ; n++ ) {\n          y = snpdata[(size_t)n * (size_t)cols + tid];\n          if ( y == '0' ) { controls[0]++; }\n          else if ( y == '1') { controls[1]++; }\n          else if ( y == '2') { controls[2]++; }\n        }\n\n        for( p = 0 ; p < 3; p++ ) {\n          tot_cases += cases[p];\n          tot_controls += controls[p];\n        }\n        total = tot_cases + tot_controls;\n\n        for( p = 0 ; p < 3; p++ ) {\n          exp[p] = (float)cases[p] + controls[p]; \n          Cexpected[p] = tot_cases * exp[p] / total;\n          Conexpected[p] = tot_controls * exp[p] / total;\n          numerator1 = (float)cases[p] - Cexpected[p];\n          numerator2 = (float)controls[p] - Conexpected[p];\n          chisquare += numerator1 * numerator1 / Cexpected[p] +\n                       numerator2 * numerator2 / Conexpected[p];\n        }\n        chi_result[tid] = chisquare;\n      });\n    });\n  }\n  q.wait();\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time = %f (s)\\n\", time * 1e-9f / repeat);\n\n  q.memcpy(h_results, chi_result, result_size).wait();\n\n  sycl::free(snpdata, q);\n  sycl::free(chi_result, q);\n\n  start = std::chrono::steady_clock::now();\n\n  cpu_kernel(rows,cols,ncases,ncontrols,dataT,cpu_results);\n\n  end = std::chrono::steady_clock::now();\n  time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Host execution time = %f (s)\\n\", time * 1e-9f);\n\n  \n\n  int error = 0;\n  for(unsigned int k = 0; k < jobs; k++) {\n    if (std::fabs(cpu_results[k] - h_results[k]) > 1e-4) error++;\n  }\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  free(dataT);\n  free(h_results);\n  free(cpu_results);\n\n  return 0;\n}\n"}}
{"kernel_name": "cobahh", "parallel_api": "cuda", "code": {"main.cu": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\n#include \"neuron_update.h\"\n#include \"neuron_update_host.h\"\n\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <neurons> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  int N = atoi(argv[1]);\n  int iteration = atoi(argv[2]);\n  srand(2);\n\n  float *h_ge, *h_gi, *h_h, *h_m, *h_n, *h_v, *h_lastspike, *h_dt, *h_t;\n  h_ge = h_gi = h_h = h_m = h_n = h_v = h_lastspike = h_dt = h_t = NULL;\n  char *h_not_refract = NULL;\n\n  posix_memalign((void**)&h_ge, 1024, N * sizeof(float));\n  posix_memalign((void**)&h_gi, 1024, N * sizeof(float));\n  posix_memalign((void**)&h_h, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_m, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_n, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_v, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_lastspike, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_dt, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&h_t, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&h_not_refract, 1024,  N * sizeof(char));\n\n  float *ge, *gi, *h, *m, *n, *v, *lastspike, *dt, *t;\n  ge = gi = h = m = n = v = lastspike = dt = t = NULL;\n  char *not_refract = NULL;\n\n  posix_memalign((void**)&ge, 1024, N * sizeof(float));\n  posix_memalign((void**)&gi, 1024, N * sizeof(float));\n  posix_memalign((void**)&h, 1024,  N * sizeof(float));\n  posix_memalign((void**)&m, 1024,  N * sizeof(float));\n  posix_memalign((void**)&n, 1024,  N * sizeof(float));\n  posix_memalign((void**)&v, 1024,  N * sizeof(float));\n  posix_memalign((void**)&lastspike, 1024,  N * sizeof(float));\n  posix_memalign((void**)&dt, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&t, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&not_refract, 1024,  N * sizeof(char));\n\n  printf(\"initializing ... \");\n  for (int i = 1; i < N; i++) {\n    h_ge[i] = ge[i] = 0.15f + ((rand()%2 == 0) ? 0.1 : -0.1);\n    h_gi[i] = gi[i] = 0.25f + ((rand()%2 == 0) ? 0.2 : -0.2);\n    h_h[i]  =  h[i] = 0.35f + ((rand()%2 == 0) ? 0.3 : -0.3);\n    h_m[i]  =  m[i] = 0.45f + ((rand()%2 == 0) ? 0.4 : -0.4);\n    h_n[i]  =  n[i] = 0.55f + ((rand()%2 == 0) ? 0.5 : -0.5);\n    h_v[i]  =  v[i] = 0.65f + ((rand()%2 == 0) ? 0.6 : -0.6);\n    h_lastspike[i] = lastspike[i] = 1.0f / (rand() % 1000 + 1);\n  }\n\n  for (int i = 0; i < 1; i++) { \n    h_dt[i] = dt[i] = 0.0001;\n    h_t[i] = t[i] = 0.01;\n  }\n  printf(\"done.\\n\");\n\n  \n\n\n  neurongroup_stateupdater_host (h_ge, h_gi, h_h, h_m, h_n, h_v, h_lastspike, \n                                 h_dt, h_t, h_not_refract, N, iteration);\n\n  neurongroup_stateupdater (ge, gi, h, m, n, v, lastspike, dt, \n                            t, not_refract, N, iteration);\n\n  double rsme = 0.0;\n  for (int i = 0; i < N; i++) {\n    rsme += (ge[i]-h_ge[i])*(ge[i]-h_ge[i]); \n    rsme += (gi[i]-h_gi[i])*(gi[i]-h_gi[i]); \n    rsme += (h[i]-h_h[i])*(h[i]-h_h[i]); \n    rsme += (m[i]-h_m[i])*(m[i]-h_m[i]); \n    rsme += (n[i]-h_n[i])*(n[i]-h_n[i]); \n    rsme += (v[i]-h_v[i])*(v[i]-h_v[i]); \n    rsme += (not_refract[i]-h_not_refract[i])*(not_refract[i]-h_not_refract[i]); \n  }\n  printf(\"RSME = %lf\\n\", sqrt(rsme/N));\n\n  free(ge);\n  free(h_ge);\n  free(gi);\n  free(h_gi);\n  free(h);\n  free(h_h);\n  free(m);\n  free(h_m);\n  free(n);\n  free(h_n);\n  free(v);\n  free(h_v);\n  free(lastspike);\n  free(h_lastspike);\n  free(dt);\n  free(h_dt);\n  free(t);\n  free(h_t);\n  free(not_refract);\n  free(h_not_refract);\n  return 0;\n}\n"}}
{"kernel_name": "cobahh", "parallel_api": "hip", "code": {"main.cu": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\n#include \"neuron_update.h\"\n#include \"neuron_update_host.h\"\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <neurons> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  int N = atoi(argv[1]);\n  int iteration = atoi(argv[2]);\n  srand(2);\n\n  float *h_ge, *h_gi, *h_h, *h_m, *h_n, *h_v, *h_lastspike, *h_dt, *h_t;\n  h_ge = h_gi = h_h = h_m = h_n = h_v = h_lastspike = h_dt = h_t = NULL;\n  char *h_not_refract = NULL;\n\n  posix_memalign((void**)&h_ge, 1024, N * sizeof(float));\n  posix_memalign((void**)&h_gi, 1024, N * sizeof(float));\n  posix_memalign((void**)&h_h, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_m, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_n, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_v, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_lastspike, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_dt, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&h_t, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&h_not_refract, 1024,  N * sizeof(char));\n\n  float *ge, *gi, *h, *m, *n, *v, *lastspike, *dt, *t;\n  ge = gi = h = m = n = v = lastspike = dt = t = NULL;\n  char *not_refract = NULL;\n\n  posix_memalign((void**)&ge, 1024, N * sizeof(float));\n  posix_memalign((void**)&gi, 1024, N * sizeof(float));\n  posix_memalign((void**)&h, 1024,  N * sizeof(float));\n  posix_memalign((void**)&m, 1024,  N * sizeof(float));\n  posix_memalign((void**)&n, 1024,  N * sizeof(float));\n  posix_memalign((void**)&v, 1024,  N * sizeof(float));\n  posix_memalign((void**)&lastspike, 1024,  N * sizeof(float));\n  posix_memalign((void**)&dt, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&t, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&not_refract, 1024,  N * sizeof(char));\n\n  printf(\"initializing ... \");\n  for (int i = 1; i < N; i++) {\n    h_ge[i] = ge[i] = 0.15f + ((rand()%2 == 0) ? 0.1 : -0.1);\n    h_gi[i] = gi[i] = 0.25f + ((rand()%2 == 0) ? 0.2 : -0.2);\n    h_h[i]  =  h[i] = 0.35f + ((rand()%2 == 0) ? 0.3 : -0.3);\n    h_m[i]  =  m[i] = 0.45f + ((rand()%2 == 0) ? 0.4 : -0.4);\n    h_n[i]  =  n[i] = 0.55f + ((rand()%2 == 0) ? 0.5 : -0.5);\n    h_v[i]  =  v[i] = 0.65f + ((rand()%2 == 0) ? 0.6 : -0.6);\n    h_lastspike[i] = lastspike[i] = 1.0f / (rand() % 1000 + 1);\n  }\n\n  for (int i = 0; i < 1; i++) { \n    h_dt[i] = dt[i] = 0.0001;\n    h_t[i] = t[i] = 0.01;\n  }\n  printf(\"done.\\n\");\n\n  \n\n\n  neurongroup_stateupdater_host (h_ge, h_gi, h_h, h_m, h_n, h_v, h_lastspike, \n                                 h_dt, h_t, h_not_refract, N, iteration);\n\n  neurongroup_stateupdater (ge, gi, h, m, n, v, lastspike, dt, \n                            t, not_refract, N, iteration);\n\n  double rsme = 0.0;\n  for (int i = 0; i < N; i++) {\n    rsme += (ge[i]-h_ge[i])*(ge[i]-h_ge[i]); \n    rsme += (gi[i]-h_gi[i])*(gi[i]-h_gi[i]); \n    rsme += (h[i]-h_h[i])*(h[i]-h_h[i]); \n    rsme += (m[i]-h_m[i])*(m[i]-h_m[i]); \n    rsme += (n[i]-h_n[i])*(n[i]-h_n[i]); \n    rsme += (v[i]-h_v[i])*(v[i]-h_v[i]); \n    rsme += (not_refract[i]-h_not_refract[i])*(not_refract[i]-h_not_refract[i]); \n  }\n  printf(\"RSME = %lf\\n\", sqrt(rsme/N));\n\n  free(ge);\n  free(h_ge);\n  free(gi);\n  free(h_gi);\n  free(h);\n  free(h_h);\n  free(m);\n  free(h_m);\n  free(n);\n  free(h_n);\n  free(v);\n  free(h_v);\n  free(lastspike);\n  free(h_lastspike);\n  free(dt);\n  free(h_dt);\n  free(t);\n  free(h_t);\n  free(not_refract);\n  free(h_not_refract);\n  return 0;\n}\n"}}
{"kernel_name": "cobahh", "parallel_api": "omp", "code": {"main.cpp": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\n#include \"neuron_update.h\"\n#include \"neuron_update_host.h\"\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <neurons> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  int N = atoi(argv[1]);\n  int iteration = atoi(argv[2]);\n  srand(2);\n\n  float *h_ge, *h_gi, *h_h, *h_m, *h_n, *h_v, *h_lastspike, *h_dt, *h_t;\n  h_ge = h_gi = h_h = h_m = h_n = h_v = h_lastspike = h_dt = h_t = NULL;\n  char *h_not_refract = NULL;\n\n  posix_memalign((void**)&h_ge, 1024, N * sizeof(float));\n  posix_memalign((void**)&h_gi, 1024, N * sizeof(float));\n  posix_memalign((void**)&h_h, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_m, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_n, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_v, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_lastspike, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_dt, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&h_t, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&h_not_refract, 1024,  N * sizeof(char));\n\n  float *ge, *gi, *h, *m, *n, *v, *lastspike, *dt, *t;\n  ge = gi = h = m = n = v = lastspike = dt = t = NULL;\n  char *not_refract = NULL;\n\n  posix_memalign((void**)&ge, 1024, N * sizeof(float));\n  posix_memalign((void**)&gi, 1024, N * sizeof(float));\n  posix_memalign((void**)&h, 1024,  N * sizeof(float));\n  posix_memalign((void**)&m, 1024,  N * sizeof(float));\n  posix_memalign((void**)&n, 1024,  N * sizeof(float));\n  posix_memalign((void**)&v, 1024,  N * sizeof(float));\n  posix_memalign((void**)&lastspike, 1024,  N * sizeof(float));\n  posix_memalign((void**)&dt, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&t, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&not_refract, 1024,  N * sizeof(char));\n\n  printf(\"initializing ... \");\n  for (int i = 1; i < N; i++) {\n    h_ge[i] = ge[i] = 0.15f + ((rand()%2 == 0) ? 0.1 : -0.1);\n    h_gi[i] = gi[i] = 0.25f + ((rand()%2 == 0) ? 0.2 : -0.2);\n    h_h[i]  =  h[i] = 0.35f + ((rand()%2 == 0) ? 0.3 : -0.3);\n    h_m[i]  =  m[i] = 0.45f + ((rand()%2 == 0) ? 0.4 : -0.4);\n    h_n[i]  =  n[i] = 0.55f + ((rand()%2 == 0) ? 0.5 : -0.5);\n    h_v[i]  =  v[i] = 0.65f + ((rand()%2 == 0) ? 0.6 : -0.6);\n    h_lastspike[i] = lastspike[i] = 1.0f / (rand() % 1000 + 1);\n  }\n\n  for (int i = 0; i < 1; i++) { \n    h_dt[i] = dt[i] = 0.0001;\n    h_t[i] = t[i] = 0.01;\n  }\n  printf(\"done.\\n\");\n\n  \n\n\n  neurongroup_stateupdater_host (h_ge, h_gi, h_h, h_m, h_n, h_v, h_lastspike, \n                                 h_dt, h_t, h_not_refract, N, iteration);\n\n  neurongroup_stateupdater (ge, gi, h, m, n, v, lastspike, dt, \n                            t, not_refract, N, iteration);\n\n  double rsme = 0.0;\n  for (int i = 0; i < N; i++) {\n    rsme += (ge[i]-h_ge[i])*(ge[i]-h_ge[i]); \n    rsme += (gi[i]-h_gi[i])*(gi[i]-h_gi[i]); \n    rsme += (h[i]-h_h[i])*(h[i]-h_h[i]); \n    rsme += (m[i]-h_m[i])*(m[i]-h_m[i]); \n    rsme += (n[i]-h_n[i])*(n[i]-h_n[i]); \n    rsme += (v[i]-h_v[i])*(v[i]-h_v[i]); \n    rsme += (not_refract[i]-h_not_refract[i])*(not_refract[i]-h_not_refract[i]); \n  }\n  printf(\"RSME = %lf\\n\", sqrt(rsme/N));\n\n  free(ge);\n  free(h_ge);\n  free(gi);\n  free(h_gi);\n  free(h);\n  free(h_h);\n  free(m);\n  free(h_m);\n  free(n);\n  free(h_n);\n  free(v);\n  free(h_v);\n  free(lastspike);\n  free(h_lastspike);\n  free(dt);\n  free(h_dt);\n  free(t);\n  free(h_t);\n  free(not_refract);\n  free(h_not_refract);\n  return 0;\n}\n"}}
{"kernel_name": "cobahh", "parallel_api": "serial", "code": {"main.cpp": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\n#include \"neuron_update.h\"\n#include \"neuron_update_host.h\"\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <neurons> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  int N = atoi(argv[1]);\n  int iteration = atoi(argv[2]);\n  srand(2);\n\n  float *h_ge, *h_gi, *h_h, *h_m, *h_n, *h_v, *h_lastspike, *h_dt, *h_t;\n  h_ge = h_gi = h_h = h_m = h_n = h_v = h_lastspike = h_dt = h_t = NULL;\n  char *h_not_refract = NULL;\n\n  posix_memalign((void**)&h_ge, 1024, N * sizeof(float));\n  posix_memalign((void**)&h_gi, 1024, N * sizeof(float));\n  posix_memalign((void**)&h_h, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_m, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_n, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_v, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_lastspike, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_dt, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&h_t, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&h_not_refract, 1024,  N * sizeof(char));\n\n  float *ge, *gi, *h, *m, *n, *v, *lastspike, *dt, *t;\n  ge = gi = h = m = n = v = lastspike = dt = t = NULL;\n  char *not_refract = NULL;\n\n  posix_memalign((void**)&ge, 1024, N * sizeof(float));\n  posix_memalign((void**)&gi, 1024, N * sizeof(float));\n  posix_memalign((void**)&h, 1024,  N * sizeof(float));\n  posix_memalign((void**)&m, 1024,  N * sizeof(float));\n  posix_memalign((void**)&n, 1024,  N * sizeof(float));\n  posix_memalign((void**)&v, 1024,  N * sizeof(float));\n  posix_memalign((void**)&lastspike, 1024,  N * sizeof(float));\n  posix_memalign((void**)&dt, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&t, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&not_refract, 1024,  N * sizeof(char));\n\n  printf(\"initializing ... \");\n  for (int i = 1; i < N; i++) {\n    h_ge[i] = ge[i] = 0.15f + ((rand()%2 == 0) ? 0.1 : -0.1);\n    h_gi[i] = gi[i] = 0.25f + ((rand()%2 == 0) ? 0.2 : -0.2);\n    h_h[i]  =  h[i] = 0.35f + ((rand()%2 == 0) ? 0.3 : -0.3);\n    h_m[i]  =  m[i] = 0.45f + ((rand()%2 == 0) ? 0.4 : -0.4);\n    h_n[i]  =  n[i] = 0.55f + ((rand()%2 == 0) ? 0.5 : -0.5);\n    h_v[i]  =  v[i] = 0.65f + ((rand()%2 == 0) ? 0.6 : -0.6);\n    h_lastspike[i] = lastspike[i] = 1.0f / (rand() % 1000 + 1);\n  }\n\n  for (int i = 0; i < 1; i++) { \n    h_dt[i] = dt[i] = 0.0001;\n    h_t[i] = t[i] = 0.01;\n  }\n  printf(\"done.\\n\");\n\n  \n\n\n  neurongroup_stateupdater_host (h_ge, h_gi, h_h, h_m, h_n, h_v, h_lastspike, \n                                 h_dt, h_t, h_not_refract, N, iteration);\n\n  neurongroup_stateupdater (ge, gi, h, m, n, v, lastspike, dt, \n                            t, not_refract, N, iteration);\n\n  double rsme = 0.0;\n  for (int i = 0; i < N; i++) {\n    rsme += (ge[i]-h_ge[i])*(ge[i]-h_ge[i]); \n    rsme += (gi[i]-h_gi[i])*(gi[i]-h_gi[i]); \n    rsme += (h[i]-h_h[i])*(h[i]-h_h[i]); \n    rsme += (m[i]-h_m[i])*(m[i]-h_m[i]); \n    rsme += (n[i]-h_n[i])*(n[i]-h_n[i]); \n    rsme += (v[i]-h_v[i])*(v[i]-h_v[i]); \n    rsme += (not_refract[i]-h_not_refract[i])*(not_refract[i]-h_not_refract[i]); \n  }\n  printf(\"RSME = %lf\\n\", sqrt(rsme/N));\n\n  free(ge);\n  free(h_ge);\n  free(gi);\n  free(h_gi);\n  free(h);\n  free(h_h);\n  free(m);\n  free(h_m);\n  free(n);\n  free(h_n);\n  free(v);\n  free(h_v);\n  free(lastspike);\n  free(h_lastspike);\n  free(dt);\n  free(h_dt);\n  free(t);\n  free(h_t);\n  free(not_refract);\n  free(h_not_refract);\n  return 0;\n}"}}
{"kernel_name": "cobahh", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\n#include \"neuron_update.h\"\n#include \"neuron_update_host.h\"\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <neurons> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  int N = atoi(argv[1]);\n  int iteration = atoi(argv[2]);\n  srand(2);\n\n  float *h_ge, *h_gi, *h_h, *h_m, *h_n, *h_v, *h_lastspike, *h_dt, *h_t;\n  h_ge = h_gi = h_h = h_m = h_n = h_v = h_lastspike = h_dt = h_t = NULL;\n  char *h_not_refract = NULL;\n\n  posix_memalign((void**)&h_ge, 1024, N * sizeof(float));\n  posix_memalign((void**)&h_gi, 1024, N * sizeof(float));\n  posix_memalign((void**)&h_h, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_m, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_n, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_v, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_lastspike, 1024,  N * sizeof(float));\n  posix_memalign((void**)&h_dt, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&h_t, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&h_not_refract, 1024,  N * sizeof(char));\n\n  float *ge, *gi, *h, *m, *n, *v, *lastspike, *dt, *t;\n  ge = gi = h = m = n = v = lastspike = dt = t = NULL;\n  char *not_refract = NULL;\n\n  posix_memalign((void**)&ge, 1024, N * sizeof(float));\n  posix_memalign((void**)&gi, 1024, N * sizeof(float));\n  posix_memalign((void**)&h, 1024,  N * sizeof(float));\n  posix_memalign((void**)&m, 1024,  N * sizeof(float));\n  posix_memalign((void**)&n, 1024,  N * sizeof(float));\n  posix_memalign((void**)&v, 1024,  N * sizeof(float));\n  posix_memalign((void**)&lastspike, 1024,  N * sizeof(float));\n  posix_memalign((void**)&dt, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&t, 1024,  1 * sizeof(float));\n  posix_memalign((void**)&not_refract, 1024,  N * sizeof(char));\n\n  printf(\"initializing ... \");\n  for (int i = 1; i < N; i++) {\n    h_ge[i] = ge[i] = 0.15f + ((rand()%2 == 0) ? 0.1 : -0.1);\n    h_gi[i] = gi[i] = 0.25f + ((rand()%2 == 0) ? 0.2 : -0.2);\n    h_h[i]  =  h[i] = 0.35f + ((rand()%2 == 0) ? 0.3 : -0.3);\n    h_m[i]  =  m[i] = 0.45f + ((rand()%2 == 0) ? 0.4 : -0.4);\n    h_n[i]  =  n[i] = 0.55f + ((rand()%2 == 0) ? 0.5 : -0.5);\n    h_v[i]  =  v[i] = 0.65f + ((rand()%2 == 0) ? 0.6 : -0.6);\n    h_lastspike[i] = lastspike[i] = 1.0f / (rand() % 1000 + 1);\n  }\n\n  for (int i = 0; i < 1; i++) { \n    h_dt[i] = dt[i] = 0.0001;\n    h_t[i] = t[i] = 0.01;\n  }\n  printf(\"done.\\n\");\n\n  \n\n\n  neurongroup_stateupdater_host (h_ge, h_gi, h_h, h_m, h_n, h_v, h_lastspike, \n                                 h_dt, h_t, h_not_refract, N, iteration);\n\n  neurongroup_stateupdater (ge, gi, h, m, n, v, lastspike, dt, \n                            t, not_refract, N, iteration);\n\n  double rsme = 0.0;\n  for (int i = 0; i < N; i++) {\n    rsme += (ge[i]-h_ge[i])*(ge[i]-h_ge[i]); \n    rsme += (gi[i]-h_gi[i])*(gi[i]-h_gi[i]); \n    rsme += (h[i]-h_h[i])*(h[i]-h_h[i]); \n    rsme += (m[i]-h_m[i])*(m[i]-h_m[i]); \n    rsme += (n[i]-h_n[i])*(n[i]-h_n[i]); \n    rsme += (v[i]-h_v[i])*(v[i]-h_v[i]); \n    rsme += (not_refract[i]-h_not_refract[i])*(not_refract[i]-h_not_refract[i]); \n  }\n  printf(\"RSME = %lf\\n\", sqrt(rsme/N));\n\n  free(ge);\n  free(h_ge);\n  free(gi);\n  free(h_gi);\n  free(h);\n  free(h_h);\n  free(m);\n  free(h_m);\n  free(n);\n  free(h_n);\n  free(v);\n  free(h_v);\n  free(lastspike);\n  free(h_lastspike);\n  free(dt);\n  free(h_dt);\n  free(t);\n  free(h_t);\n  free(not_refract);\n  free(h_not_refract);\n  return 0;\n}\n"}}
{"kernel_name": "cooling", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <cuda.h>\n\ntypedef double Real;\n\n\n\n\n\n__host__ __device__\nReal primordial_cool(Real n, Real T, int heat_flag)\n{\n  Real n_h, Y, y, g_ff, cool;\n  Real n_h0, n_hp, n_he0, n_hep, n_hepp, n_e, n_e_old;\n  Real alpha_hp, alpha_hep, alpha_d, alpha_hepp, gamma_eh0, gamma_ehe0, gamma_ehep;\n  Real le_h0, le_hep, li_h0, li_he0, li_hep, lr_hp, lr_hep, lr_hepp, ld_hep, l_ff;\n  Real gamma_lh0, gamma_lhe0, gamma_lhep, e_h0, e_he0, e_hep, H;\n  int n_iter;\n  Real diff, tol;\n\n  Y = 0.24; \n\n  y = Y/(4 - 4*Y);\n\n  \n\n  n_h = n;\n\n  \n\n  \n\n  alpha_hp   = (8.4e-11) * (1.0/sqrt(T)) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7))));\n  alpha_hep  = (1.5e-10) * (pow(T,(-0.6353)));\n  alpha_d    = (1.9e-3)  * (pow(T,(-1.5))) * exp(-470000.0/T) * (1.0 + 0.3*exp(-94000.0/T));\n  alpha_hepp = (3.36e-10)* (1.0/sqrt(T)) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7))));\n  gamma_eh0  = (5.85e-11)* sqrt(T) * exp(-157809.1/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  gamma_ehe0 = (2.38e-11)* sqrt(T) * exp(-285335.4/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  gamma_ehep = (5.68e-12)* sqrt(T) * exp(-631515.0/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  \n\n  \n\n  gamma_lh0 = 3.19851e-13;\n  gamma_lhe0 = 3.13029e-13;\n  gamma_lhep = 2.00541e-14;\n  \n\n  e_h0 = 2.4796e-24;\n  e_he0 = 6.86167e-24;\n  e_hep = 6.21868e-25;\n\n  \n\n  \n\n  n_e = n_h; \n\n  n_iter = 20;\n  diff = 1.0;\n  tol = 1.0e-6;\n  if (heat_flag) {\n    for (int i=0; i<n_iter; i++) {\n      n_e_old = n_e;\n      n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0 + gamma_lh0/n_e);\n      n_hp   = n_h - n_h0;\n      n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0 + gamma_lhe0/n_e) + (gamma_ehep + gamma_lhep/n_e)/alpha_hepp);\n      n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0 + gamma_lhe0/n_e);\n      n_hepp = n_hep*(gamma_ehep + gamma_lhep/n_e)/alpha_hepp;\n      n_e    = n_hp + n_hep + 2*n_hepp;\n      diff = fabs(n_e_old - n_e);\n      if (diff < tol) break;\n    }\n  }\n  else {\n    n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0);\n    n_hp   = n_h - n_h0;\n    n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0) + (gamma_ehep)/alpha_hepp);\n    n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0);\n    n_hepp = n_hep*(gamma_ehep)/alpha_hepp;\n    n_e    = n_hp + n_hep + 2*n_hepp;\n  }\n\n  \n\n  \n\n  le_h0 = (7.50e-19) * exp(-118348.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_h0;\n  le_hep = (5.54e-17) * pow(T,(-0.397)) * exp(-473638.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_hep;\n  li_h0 = (1.27e-21) * sqrt(T) * exp(-157809.1/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_h0;\n  li_he0 = (9.38e-22) * sqrt(T) * exp(-285335.4/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_he0;\n  li_hep = (4.95e-22) * sqrt(T) * exp(-631515.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_hep;\n  lr_hp = (8.70e-27) * sqrt(T) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7)))) * n_e * n_hp;\n  lr_hep = (1.55e-26) * pow(T,(0.3647)) * n_e * n_hep;\n  lr_hepp = (3.48e-26) * sqrt(T) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7)))) * n_e * n_hepp;\n  ld_hep = (1.24e-13) * pow(T,(-1.5)) * exp(-470000.0/T) * (1.0 + 0.3*exp(-94000.0/T)) * n_e * n_hep;\n  g_ff = 1.1 + 0.34*exp(-(5.5-log(T))*(5.5-log(T))/3.0); \n\n  l_ff = (1.42e-27) * g_ff * sqrt(T) * (n_hp + n_hep + 4*n_hepp) * n_e;\n\n  \n\n  cool = le_h0 + le_hep + li_h0 + li_he0 + li_hep + lr_hp + lr_hep + lr_hepp + ld_hep + l_ff;\n\n  \n\n  H = 0.0;\n  if (heat_flag) {\n    H = n_h0*e_h0 + n_he0*e_he0 + n_hep*e_hep;\n  }\n\n  cool -= H;\n\n  return cool;\n}\n\n__global__\nvoid cool_kernel (\n  const int  num,\n  const Real n,\n  const Real *__restrict__ T,\n        Real *__restrict__ r,\n  const int  heat_flag)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < num)\n    r[i] = primordial_cool(n, T[i], heat_flag);\n}\n\nvoid reference (\n  const int  num,\n  const Real n,\n  const Real *__restrict__ T,\n        Real *__restrict__ r,\n  const int  heat_flag)\n{\n  for (int i = 0; i < num; i++) \n    r[i] = primordial_cool(n, T[i], heat_flag);\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <number of points> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int num = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n    \n  const size_t size_bytes = sizeof(Real) * num;\n\n  const Real n = 0.0899; \n\n\n  Real *T = (Real*) malloc (size_bytes);\n  for (int i = 0; i < num; i++) {\n    T[i] = -275.0 + i * 275 * 2.0 / num;\n  }\n\n  Real *r = (Real*) malloc (size_bytes);\n  Real *h_r = (Real*) malloc (size_bytes);\n\n  Real *d_T, *d_r;\n  cudaMalloc((void**)&d_T, size_bytes);\n  cudaMemcpy(d_T, T, size_bytes, cudaMemcpyHostToDevice);\n  cudaMalloc((void**)&d_r, size_bytes);\n\n  dim3 grids ((num + 255) / 256);\n  dim3 blocks (256);\n\n  \n\n  for (int i = 0; i < repeat; i++) {\n    cool_kernel <<< grids, blocks >>> (num, n, d_T, d_r, 0);\n  }\n  cudaDeviceSynchronize();\n\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    cool_kernel <<< grids, blocks >>> (num, n, d_T, d_r, 1);\n  }\n  cudaDeviceSynchronize();\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time %f (ms)\\n\", (time * 1e-6f) / repeat);\n\n  \n\n  cudaMemcpy(r, d_r, size_bytes, cudaMemcpyDeviceToHost);\n\n  reference(num, n, T, h_r, 1);\n  \n  bool error = false;\n  for (int i = 0; i < num; i++) {\n    if (fabs(r[i] - h_r[i]) > 1e-3) {\n      error = true;\n      break;\n    }\n  }\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  cudaFree(d_T);\n  cudaFree(d_r);\n  free(T);\n  free(r);\n  free(h_r);\n  return 0;\n}\n"}}
{"kernel_name": "cooling", "parallel_api": "hip", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n\ntypedef double Real;\n\n\n\n\n\n__host__ __device__\nReal primordial_cool(Real n, Real T, int heat_flag)\n{\n  Real n_h, Y, y, g_ff, cool;\n  Real n_h0, n_hp, n_he0, n_hep, n_hepp, n_e, n_e_old;\n  Real alpha_hp, alpha_hep, alpha_d, alpha_hepp, gamma_eh0, gamma_ehe0, gamma_ehep;\n  Real le_h0, le_hep, li_h0, li_he0, li_hep, lr_hp, lr_hep, lr_hepp, ld_hep, l_ff;\n  Real gamma_lh0, gamma_lhe0, gamma_lhep, e_h0, e_he0, e_hep, H;\n  int n_iter;\n  Real diff, tol;\n\n  Y = 0.24; \n\n  y = Y/(4 - 4*Y);\n\n  \n\n  n_h = n;\n\n  \n\n  \n\n  alpha_hp   = (8.4e-11) * (1.0/sqrt(T)) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7))));\n  alpha_hep  = (1.5e-10) * (pow(T,(-0.6353)));\n  alpha_d    = (1.9e-3)  * (pow(T,(-1.5))) * exp(-470000.0/T) * (1.0 + 0.3*exp(-94000.0/T));\n  alpha_hepp = (3.36e-10)* (1.0/sqrt(T)) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7))));\n  gamma_eh0  = (5.85e-11)* sqrt(T) * exp(-157809.1/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  gamma_ehe0 = (2.38e-11)* sqrt(T) * exp(-285335.4/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  gamma_ehep = (5.68e-12)* sqrt(T) * exp(-631515.0/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  \n\n  \n\n  gamma_lh0 = 3.19851e-13;\n  gamma_lhe0 = 3.13029e-13;\n  gamma_lhep = 2.00541e-14;\n  \n\n  e_h0 = 2.4796e-24;\n  e_he0 = 6.86167e-24;\n  e_hep = 6.21868e-25;\n\n  \n\n  \n\n  n_e = n_h; \n\n  n_iter = 20;\n  diff = 1.0;\n  tol = 1.0e-6;\n  if (heat_flag) {\n    for (int i=0; i<n_iter; i++) {\n      n_e_old = n_e;\n      n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0 + gamma_lh0/n_e);\n      n_hp   = n_h - n_h0;\n      n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0 + gamma_lhe0/n_e) + (gamma_ehep + gamma_lhep/n_e)/alpha_hepp);\n      n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0 + gamma_lhe0/n_e);\n      n_hepp = n_hep*(gamma_ehep + gamma_lhep/n_e)/alpha_hepp;\n      n_e    = n_hp + n_hep + 2*n_hepp;\n      diff = fabs(n_e_old - n_e);\n      if (diff < tol) break;\n    }\n  }\n  else {\n    n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0);\n    n_hp   = n_h - n_h0;\n    n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0) + (gamma_ehep)/alpha_hepp);\n    n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0);\n    n_hepp = n_hep*(gamma_ehep)/alpha_hepp;\n    n_e    = n_hp + n_hep + 2*n_hepp;\n  }\n\n  \n\n  \n\n  le_h0 = (7.50e-19) * exp(-118348.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_h0;\n  le_hep = (5.54e-17) * pow(T,(-0.397)) * exp(-473638.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_hep;\n  li_h0 = (1.27e-21) * sqrt(T) * exp(-157809.1/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_h0;\n  li_he0 = (9.38e-22) * sqrt(T) * exp(-285335.4/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_he0;\n  li_hep = (4.95e-22) * sqrt(T) * exp(-631515.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_hep;\n  lr_hp = (8.70e-27) * sqrt(T) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7)))) * n_e * n_hp;\n  lr_hep = (1.55e-26) * pow(T,(0.3647)) * n_e * n_hep;\n  lr_hepp = (3.48e-26) * sqrt(T) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7)))) * n_e * n_hepp;\n  ld_hep = (1.24e-13) * pow(T,(-1.5)) * exp(-470000.0/T) * (1.0 + 0.3*exp(-94000.0/T)) * n_e * n_hep;\n  g_ff = 1.1 + 0.34*exp(-(5.5-log(T))*(5.5-log(T))/3.0); \n\n  l_ff = (1.42e-27) * g_ff * sqrt(T) * (n_hp + n_hep + 4*n_hepp) * n_e;\n\n  \n\n  cool = le_h0 + le_hep + li_h0 + li_he0 + li_hep + lr_hp + lr_hep + lr_hepp + ld_hep + l_ff;\n\n  \n\n  H = 0.0;\n  if (heat_flag) {\n    H = n_h0*e_h0 + n_he0*e_he0 + n_hep*e_hep;\n  }\n\n  cool -= H;\n\n  return cool;\n}\n\n__global__\nvoid cool_kernel (\n  const int  num,\n  const Real n,\n  const Real *__restrict__ T,\n        Real *__restrict__ r,\n  const int  heat_flag)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < num)\n    r[i] = primordial_cool(n, T[i], heat_flag);\n}\n\nvoid reference (\n  const int  num,\n  const Real n,\n  const Real *__restrict__ T,\n        Real *__restrict__ r,\n  const int  heat_flag)\n{\n  for (int i = 0; i < num; i++) \n    r[i] = primordial_cool(n, T[i], heat_flag);\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <number of points> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int num = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n    \n  const size_t size_bytes = sizeof(Real) * num;\n\n  const Real n = 0.0899; \n\n\n  Real *T = (Real*) malloc (size_bytes);\n  for (int i = 0; i < num; i++) {\n    T[i] = -275.0 + i * 275 * 2.0 / num;\n  }\n\n  Real *r = (Real*) malloc (size_bytes);\n  Real *h_r = (Real*) malloc (size_bytes);\n\n  Real *d_T, *d_r;\n  hipMalloc((void**)&d_T, size_bytes);\n  hipMemcpy(d_T, T, size_bytes, hipMemcpyHostToDevice);\n  hipMalloc((void**)&d_r, size_bytes);\n\n  dim3 grids ((num + 255) / 256);\n  dim3 blocks (256);\n\n  \n\n  for (int i = 0; i < repeat; i++) {\n    hipLaunchKernelGGL(cool_kernel, grids, blocks , 0, 0, num, n, d_T, d_r, 0);\n  }\n  hipDeviceSynchronize();\n\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    hipLaunchKernelGGL(cool_kernel, grids, blocks , 0, 0, num, n, d_T, d_r, 1);\n  }\n  hipDeviceSynchronize();\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time %f (ms)\\n\", (time * 1e-6f) / repeat);\n\n  \n\n  hipMemcpy(r, d_r, size_bytes, hipMemcpyDeviceToHost);\n\n  reference(num, n, T, h_r, 1);\n  \n  bool error = false;\n  for (int i = 0; i < num; i++) {\n    if (fabs(r[i] - h_r[i]) > 1e-3) {\n      error = true;\n      break;\n    }\n  }\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  hipFree(d_T);\n  hipFree(d_r);\n  free(T);\n  free(r);\n  free(h_r);\n  return 0;\n}\n"}}
{"kernel_name": "cooling", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <omp.h>\n\ntypedef double Real;\n\n\n\n\n\n#pragma omp declare target\nReal primordial_cool(Real n, Real T, int heat_flag)\n{\n  Real n_h, Y, y, g_ff, cool;\n  Real n_h0, n_hp, n_he0, n_hep, n_hepp, n_e, n_e_old;\n  Real alpha_hp, alpha_hep, alpha_d, alpha_hepp, gamma_eh0, gamma_ehe0, gamma_ehep;\n  Real le_h0, le_hep, li_h0, li_he0, li_hep, lr_hp, lr_hep, lr_hepp, ld_hep, l_ff;\n  Real gamma_lh0, gamma_lhe0, gamma_lhep, e_h0, e_he0, e_hep, H;\n  int n_iter;\n  Real diff, tol;\n\n  Y = 0.24; \n\n  y = Y/(4 - 4*Y);\n\n  \n\n  n_h = n;\n\n  \n\n  \n\n  alpha_hp   = (8.4e-11) * (1.0/sqrt(T)) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7))));\n  alpha_hep  = (1.5e-10) * (pow(T,(-0.6353)));\n  alpha_d    = (1.9e-3)  * (pow(T,(-1.5))) * exp(-470000.0/T) * (1.0 + 0.3*exp(-94000.0/T));\n  alpha_hepp = (3.36e-10)* (1.0/sqrt(T)) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7))));\n  gamma_eh0  = (5.85e-11)* sqrt(T) * exp(-157809.1/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  gamma_ehe0 = (2.38e-11)* sqrt(T) * exp(-285335.4/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  gamma_ehep = (5.68e-12)* sqrt(T) * exp(-631515.0/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  \n\n  \n\n  gamma_lh0 = 3.19851e-13;\n  gamma_lhe0 = 3.13029e-13;\n  gamma_lhep = 2.00541e-14;\n  \n\n  e_h0 = 2.4796e-24;\n  e_he0 = 6.86167e-24;\n  e_hep = 6.21868e-25;\n\n  \n\n  \n\n  n_e = n_h; \n\n  n_iter = 20;\n  diff = 1.0;\n  tol = 1.0e-6;\n  if (heat_flag) {\n    for (int i=0; i<n_iter; i++) {\n      n_e_old = n_e;\n      n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0 + gamma_lh0/n_e);\n      n_hp   = n_h - n_h0;\n      n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0 + gamma_lhe0/n_e) + (gamma_ehep + gamma_lhep/n_e)/alpha_hepp);\n      n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0 + gamma_lhe0/n_e);\n      n_hepp = n_hep*(gamma_ehep + gamma_lhep/n_e)/alpha_hepp;\n      n_e    = n_hp + n_hep + 2*n_hepp;\n      diff = fabs(n_e_old - n_e);\n      if (diff < tol) break;\n    }\n  }\n  else {\n    n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0);\n    n_hp   = n_h - n_h0;\n    n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0) + (gamma_ehep)/alpha_hepp);\n    n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0);\n    n_hepp = n_hep*(gamma_ehep)/alpha_hepp;\n    n_e    = n_hp + n_hep + 2*n_hepp;\n  }\n\n  \n\n  \n\n  le_h0 = (7.50e-19) * exp(-118348.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_h0;\n  le_hep = (5.54e-17) * pow(T,(-0.397)) * exp(-473638.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_hep;\n  li_h0 = (1.27e-21) * sqrt(T) * exp(-157809.1/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_h0;\n  li_he0 = (9.38e-22) * sqrt(T) * exp(-285335.4/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_he0;\n  li_hep = (4.95e-22) * sqrt(T) * exp(-631515.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_hep;\n  lr_hp = (8.70e-27) * sqrt(T) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7)))) * n_e * n_hp;\n  lr_hep = (1.55e-26) * pow(T,(0.3647)) * n_e * n_hep;\n  lr_hepp = (3.48e-26) * sqrt(T) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7)))) * n_e * n_hepp;\n  ld_hep = (1.24e-13) * pow(T,(-1.5)) * exp(-470000.0/T) * (1.0 + 0.3*exp(-94000.0/T)) * n_e * n_hep;\n  g_ff = 1.1 + 0.34*exp(-(5.5-log(T))*(5.5-log(T))/3.0); \n\n  l_ff = (1.42e-27) * g_ff * sqrt(T) * (n_hp + n_hep + 4*n_hepp) * n_e;\n\n  \n\n  cool = le_h0 + le_hep + li_h0 + li_he0 + li_hep + lr_hp + lr_hep + lr_hepp + ld_hep + l_ff;\n\n  \n\n  H = 0.0;\n  if (heat_flag) {\n    H = n_h0*e_h0 + n_he0*e_he0 + n_hep*e_hep;\n  }\n\n  cool -= H;\n\n  return cool;\n}\n#pragma omp end declare target\n\nvoid cool_kernel (\n  const int  num,\n  const Real n,\n  const Real *__restrict T,\n        Real *__restrict r,\n  const int  heat_flag)\n{\n  #pragma omp target teams distribute parallel for thread_limit(256)\n  for (int i = 0; i < num; i++)\n    r[i] = primordial_cool(n, T[i], heat_flag);\n}\n\nvoid reference (\n  const int  num,\n  const Real n,\n  const Real *__restrict T,\n        Real *__restrict r,\n  const int  heat_flag)\n{\n  for (int i = 0; i < num; i++) \n    r[i] = primordial_cool(n, T[i], heat_flag);\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <number of points> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int num = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n    \n  const size_t size_bytes = sizeof(Real) * num;\n\n  const Real n = 0.0899; \n\n\n  Real *T = (Real*) malloc (size_bytes);\n  for (int i = 0; i < num; i++) {\n    T[i] = -275.0 + i * 275 * 2.0 / num;\n  }\n\n  Real *h_r = (Real*) malloc (size_bytes);\n  Real *d_r = (Real*) malloc (size_bytes);\n\n  #pragma omp target data map (from: d_r[0:num]) \\\n                          map (to: T[0:num])\n  {\n    \n\n    for (int i = 0; i < repeat; i++) {\n      cool_kernel(num, n, T, d_r, 0);\n    }\n\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      cool_kernel(num, n, T, d_r, 1);\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time %f (ms)\\n\", (time * 1e-6f) / repeat);\n  }\n\n  \n\n  reference(num, n, T, h_r, 1);\n  \n  bool error = false;\n  for (int i = 0; i < num; i++) {\n    if (fabs(d_r[i] - h_r[i]) > 1e-3) {\n      error = true;\n      break;\n    }\n  }\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  free(T);\n  free(h_r);\n  free(d_r);\n  return 0;\n}\n"}}
{"kernel_name": "cooling", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n\ntypedef double Real;\n\n\n\n\n\nReal primordial_cool(Real n, Real T, int heat_flag)\n{\n  Real n_h, Y, y, g_ff, cool;\n  Real n_h0, n_hp, n_he0, n_hep, n_hepp, n_e, n_e_old;\n  Real alpha_hp, alpha_hep, alpha_d, alpha_hepp, gamma_eh0, gamma_ehe0, gamma_ehep;\n  Real le_h0, le_hep, li_h0, li_he0, li_hep, lr_hp, lr_hep, lr_hepp, ld_hep, l_ff;\n  Real gamma_lh0, gamma_lhe0, gamma_lhep, e_h0, e_he0, e_hep, H;\n  int n_iter;\n  Real diff, tol;\n\n  Y = 0.24; \n\n  y = Y/(4 - 4*Y);\n\n  \n\n  n_h = n;\n\n  \n\n  \n\n  alpha_hp   = (8.4e-11) * (1.0/sqrt(T)) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7))));\n  alpha_hep  = (1.5e-10) * (pow(T,(-0.6353)));\n  alpha_d    = (1.9e-3)  * (pow(T,(-1.5))) * exp(-470000.0/T) * (1.0 + 0.3*exp(-94000.0/T));\n  alpha_hepp = (3.36e-10)* (1.0/sqrt(T)) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7))));\n  gamma_eh0  = (5.85e-11)* sqrt(T) * exp(-157809.1/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  gamma_ehe0 = (2.38e-11)* sqrt(T) * exp(-285335.4/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  gamma_ehep = (5.68e-12)* sqrt(T) * exp(-631515.0/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  \n\n  \n\n  gamma_lh0 = 3.19851e-13;\n  gamma_lhe0 = 3.13029e-13;\n  gamma_lhep = 2.00541e-14;\n  \n\n  e_h0 = 2.4796e-24;\n  e_he0 = 6.86167e-24;\n  e_hep = 6.21868e-25;\n\n  \n\n  \n\n  n_e = n_h; \n\n  n_iter = 20;\n  diff = 1.0;\n  tol = 1.0e-6;\n  if (heat_flag) {\n    for (int i=0; i<n_iter; i++) {\n      n_e_old = n_e;\n      n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0 + gamma_lh0/n_e);\n      n_hp   = n_h - n_h0;\n      n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0 + gamma_lhe0/n_e) + (gamma_ehep + gamma_lhep/n_e)/alpha_hepp);\n      n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0 + gamma_lhe0/n_e);\n      n_hepp = n_hep*(gamma_ehep + gamma_lhep/n_e)/alpha_hepp;\n      n_e    = n_hp + n_hep + 2*n_hepp;\n      diff = fabs(n_e_old - n_e);\n      if (diff < tol) break;\n    }\n  }\n  else {\n    n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0);\n    n_hp   = n_h - n_h0;\n    n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0) + (gamma_ehep)/alpha_hepp);\n    n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0);\n    n_hepp = n_hep*(gamma_ehep)/alpha_hepp;\n    n_e    = n_hp + n_hep + 2*n_hepp;\n  }\n\n  \n\n  \n\n  le_h0 = (7.50e-19) * exp(-118348.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_h0;\n  le_hep = (5.54e-17) * pow(T,(-0.397)) * exp(-473638.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_hep;\n  li_h0 = (1.27e-21) * sqrt(T) * exp(-157809.1/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_h0;\n  li_he0 = (9.38e-22) * sqrt(T) * exp(-285335.4/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_he0;\n  li_hep = (4.95e-22) * sqrt(T) * exp(-631515.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_hep;\n  lr_hp = (8.70e-27) * sqrt(T) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7)))) * n_e * n_hp;\n  lr_hep = (1.55e-26) * pow(T,(0.3647)) * n_e * n_hep;\n  lr_hepp = (3.48e-26) * sqrt(T) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7)))) * n_e * n_hepp;\n  ld_hep = (1.24e-13) * pow(T,(-1.5)) * exp(-470000.0/T) * (1.0 + 0.3*exp(-94000.0/T)) * n_e * n_hep;\n  g_ff = 1.1 + 0.34*exp(-(5.5-log(T))*(5.5-log(T))/3.0); \n\n  l_ff = (1.42e-27) * g_ff * sqrt(T) * (n_hp + n_hep + 4*n_hepp) * n_e;\n\n  \n\n  cool = le_h0 + le_hep + li_h0 + li_he0 + li_hep + lr_hp + lr_hep + lr_hepp + ld_hep + l_ff;\n\n  \n\n  H = 0.0;\n  if (heat_flag) {\n    H = n_h0*e_h0 + n_he0*e_he0 + n_hep*e_hep;\n  }\n\n  cool -= H;\n\n  return cool;\n}\n\nvoid cool_kernel (\n  const int  num,\n  const Real n,\n  const Real *__restrict T,\n        Real *__restrict r,\n  const int  heat_flag)\n{\n    for (int i = 0; i < num; i++)\n    r[i] = primordial_cool(n, T[i], heat_flag);\n}\n\nvoid reference (\n  const int  num,\n  const Real n,\n  const Real *__restrict T,\n        Real *__restrict r,\n  const int  heat_flag)\n{\n  for (int i = 0; i < num; i++) \n    r[i] = primordial_cool(n, T[i], heat_flag);\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <number of points> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int num = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n    \n  const size_t size_bytes = sizeof(Real) * num;\n\n  const Real n = 0.0899; \n\n\n  Real *T = (Real*) malloc (size_bytes);\n  for (int i = 0; i < num; i++) {\n    T[i] = -275.0 + i * 275 * 2.0 / num;\n  }\n\n  Real *h_r = (Real*) malloc (size_bytes);\n  Real *d_r = (Real*) malloc (size_bytes);\n\n    {\n    \n\n    for (int i = 0; i < repeat; i++) {\n      cool_kernel(num, n, T, d_r, 0);\n    }\n\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      cool_kernel(num, n, T, d_r, 1);\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time %f (ms)\\n\", (time * 1e-6f) / repeat);\n  }\n\n  \n\n  reference(num, n, T, h_r, 1);\n  \n  bool error = false;\n  for (int i = 0; i < num; i++) {\n    if (fabs(d_r[i] - h_r[i]) > 1e-3) {\n      error = true;\n      break;\n    }\n  }\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  free(T);\n  free(h_r);\n  free(d_r);\n  return 0;\n}"}}
{"kernel_name": "cooling", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n\ntypedef double Real;\n\n\n\n\n\n\nReal primordial_cool(Real n, Real T, int heat_flag)\n{\n  Real n_h, Y, y, g_ff, cool;\n  Real n_h0, n_hp, n_he0, n_hep, n_hepp, n_e, n_e_old;\n  Real alpha_hp, alpha_hep, alpha_d, alpha_hepp, gamma_eh0, gamma_ehe0, gamma_ehep;\n  Real le_h0, le_hep, li_h0, li_he0, li_hep, lr_hp, lr_hep, lr_hepp, ld_hep, l_ff;\n  Real gamma_lh0, gamma_lhe0, gamma_lhep, e_h0, e_he0, e_hep, H;\n  int n_iter;\n  Real diff, tol;\n\n  Y = 0.24; \n\n  y = Y/(4 - 4*Y);\n\n  \n\n  n_h = n;\n\n  \n\n  \n\n  alpha_hp = (8.4e-11) * (1.0 / sycl::sqrt(T)) *\n             sycl::pow<double>((T / 1e3), (-0.2)) *\n             (1.0 / (1.0 + sycl::pow<double>((T / 1e6), (0.7))));\n  alpha_hep = (1.5e-10) * (sycl::pow<double>(T, (-0.6353)));\n  alpha_d = (1.9e-3) * (sycl::pow<double>(T, (-1.5))) *\n            sycl::exp(-470000.0 / T) * (1.0 + 0.3 * sycl::exp(-94000.0 / T));\n  alpha_hepp = (3.36e-10) * (1.0 / sycl::sqrt(T)) *\n               sycl::pow<double>((T / 1e3), (-0.2)) *\n               (1.0 / (1.0 + sycl::pow<double>((T / 1e6), (0.7))));\n  gamma_eh0 = (5.85e-11) * sycl::sqrt(T) * sycl::exp(-157809.1 / T) *\n              (1.0 / (1.0 + sycl::sqrt(T / 1e5)));\n  gamma_ehe0 = (2.38e-11) * sycl::sqrt(T) * sycl::exp(-285335.4 / T) *\n               (1.0 / (1.0 + sycl::sqrt(T / 1e5)));\n  gamma_ehep = (5.68e-12) * sycl::sqrt(T) * sycl::exp(-631515.0 / T) *\n               (1.0 / (1.0 + sycl::sqrt(T / 1e5)));\n  \n\n  \n\n  gamma_lh0 = 3.19851e-13;\n  gamma_lhe0 = 3.13029e-13;\n  gamma_lhep = 2.00541e-14;\n  \n\n  e_h0 = 2.4796e-24;\n  e_he0 = 6.86167e-24;\n  e_hep = 6.21868e-25;\n\n  \n\n  \n\n  n_e = n_h; \n\n  n_iter = 20;\n  diff = 1.0;\n  tol = 1.0e-6;\n  if (heat_flag) {\n    for (int i=0; i<n_iter; i++) {\n      n_e_old = n_e;\n      n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0 + gamma_lh0/n_e);\n      n_hp   = n_h - n_h0;\n      n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0 + gamma_lhe0/n_e) + (gamma_ehep + gamma_lhep/n_e)/alpha_hepp);\n      n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0 + gamma_lhe0/n_e);\n      n_hepp = n_hep*(gamma_ehep + gamma_lhep/n_e)/alpha_hepp;\n      n_e    = n_hp + n_hep + 2*n_hepp;\n      diff = sycl::fabs(n_e_old - n_e);\n      if (diff < tol) break;\n    }\n  }\n  else {\n    n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0);\n    n_hp   = n_h - n_h0;\n    n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0) + (gamma_ehep)/alpha_hepp);\n    n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0);\n    n_hepp = n_hep*(gamma_ehep)/alpha_hepp;\n    n_e    = n_hp + n_hep + 2*n_hepp;\n  }\n\n  \n\n  \n\n  le_h0 = (7.50e-19) * sycl::exp(-118348.0 / T) *\n          (1.0 / (1.0 + sycl::sqrt(T / 1e5))) * n_e * n_h0;\n  le_hep = (5.54e-17) * sycl::pow<double>(T, (-0.397)) *\n           sycl::exp(-473638.0 / T) * (1.0 / (1.0 + sycl::sqrt(T / 1e5))) *\n           n_e * n_hep;\n  li_h0 = (1.27e-21) * sycl::sqrt(T) * sycl::exp(-157809.1 / T) *\n          (1.0 / (1.0 + sycl::sqrt(T / 1e5))) * n_e * n_h0;\n  li_he0 = (9.38e-22) * sycl::sqrt(T) * sycl::exp(-285335.4 / T) *\n           (1.0 / (1.0 + sycl::sqrt(T / 1e5))) * n_e * n_he0;\n  li_hep = (4.95e-22) * sycl::sqrt(T) * sycl::exp(-631515.0 / T) *\n           (1.0 / (1.0 + sycl::sqrt(T / 1e5))) * n_e * n_hep;\n  lr_hp = (8.70e-27) * sycl::sqrt(T) * sycl::pow<double>((T / 1e3), (-0.2)) *\n          (1.0 / (1.0 + sycl::pow<double>((T / 1e6), (0.7)))) * n_e * n_hp;\n  lr_hep = (1.55e-26) * sycl::pow<double>(T, (0.3647)) * n_e * n_hep;\n  lr_hepp = (3.48e-26) * sycl::sqrt(T) * sycl::pow<double>((T / 1e3), (-0.2)) *\n            (1.0 / (1.0 + sycl::pow<double>((T / 1e6), (0.7)))) * n_e * n_hepp;\n  ld_hep = (1.24e-13) * sycl::pow<double>(T, (-1.5)) *\n           sycl::exp(-470000.0 / T) * (1.0 + 0.3 * sycl::exp(-94000.0 / T)) *\n           n_e * n_hep;\n  g_ff = 1.1 + 0.34 * sycl::exp(-(5.5 - sycl::log(T)) * (5.5 - sycl::log(T)) /\n                                3.0); \n\n  l_ff = (1.42e-27) * g_ff * sycl::sqrt(T) * (n_hp + n_hep + 4 * n_hepp) * n_e;\n\n  \n\n  cool = le_h0 + le_hep + li_h0 + li_he0 + li_hep + lr_hp + lr_hep + lr_hepp + ld_hep + l_ff;\n\n  \n\n  H = 0.0;\n  if (heat_flag) {\n    H = n_h0*e_h0 + n_he0*e_he0 + n_hep*e_hep;\n  }\n\n  cool -= H;\n\n  return cool;\n}\n\n\nvoid cool_kernel (\n  const int  num,\n  const Real n,\n  const Real *__restrict T,\n        Real *__restrict r,\n  const int  heat_flag,\n  sycl::nd_item<1> &item)\n{\n  int i = item.get_global_id(0);\n  if (i < num)\n    r[i] = primordial_cool(n, T[i], heat_flag);\n}\n\nvoid reference (\n  const int  num,\n  const Real n,\n  const Real *__restrict T,\n        Real *__restrict r,\n  const int  heat_flag)\n{\n  for (int i = 0; i < num; i++) \n    r[i] = primordial_cool(n, T[i], heat_flag);\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <number of points> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int num = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n    \n  const size_t size_bytes = sizeof(Real) * num;\n\n  const Real n = 0.0899; \n\n\n  Real *T = (Real*) malloc (size_bytes);\n  for (int i = 0; i < num; i++) {\n    T[i] = -275.0 + i * 275 * 2.0 / num;\n  }\n\n  Real *r = (Real*) malloc (size_bytes);\n  Real *h_r = (Real*) malloc (size_bytes);\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  Real *d_T, *d_r;\n  d_T = (Real *)sycl::malloc_device(size_bytes, q);\n  q.memcpy(d_T, T, size_bytes);\n\n  d_r = (Real *)sycl::malloc_device(size_bytes, q);\n\n  sycl::range<1> gws ((num + 255) / 256 * 256);\n  sycl::range<1> lws (256);\n\n  \n\n  for (int i = 0; i < repeat; i++) {\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class noheat>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        cool_kernel(num, n, d_T, d_r, 0, item);\n      });\n    });\n  }\n  q.wait();\n\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class heat>(\n      sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        cool_kernel(num, n, d_T, d_r, 1, item);\n      });\n    });\n  }\n  q.wait();\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time %f (ms)\\n\", (time * 1e-6f) / repeat);\n\n  \n\n  q.memcpy(r, d_r, size_bytes).wait();\n\n  reference(num, n, T, h_r, 1);\n  \n  bool error = false;\n  for (int i = 0; i < num; i++) {\n    if (fabs(r[i] - h_r[i]) > 1e-3) {\n      error = true;\n      break;\n    }\n  }\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  sycl::free(d_T, q);\n  sycl::free(d_r, q);\n  free(T);\n  free(r);\n  free(h_r);\n  return 0;\n}\n"}}
{"kernel_name": "damage", "parallel_api": "cuda", "code": {"main.cu": "#include <stdlib.h>\n#include <stdio.h>\n#include <cuda.h>\n#include <chrono>\n#include \"kernel.h\"\n\n\n\n#define BS 256\n\ndouble LCG_random_double(uint64_t * seed)\n{\n  const unsigned long m = 9223372036854775808ULL; \n\n  const unsigned long a = 2806196910506780709ULL;\n  const unsigned long c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 3) {\n    printf(\"Usage: %s <number of points> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int n = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n  const int m = (n + BS - 1) / BS; \n\n\n  int *nlist = (int*) malloc (sizeof(int) * n);\n  int *family = (int*) malloc (sizeof(int) * m);\n  int *n_neigh = (int*) malloc (sizeof(int) * m);\n  double *damage = (double*) malloc (sizeof(double) * m);\n\n  unsigned long seed = 123;\n  for (int i = 0; i < n; i++)\n    nlist[i] = (LCG_random_double(&seed) > 0.5) ? 1 : -1;\n\n  for (int i = 0; i < m; i++) {\n    int s = 0;\n    for (int j = 0; j < BS; j++) {\n      s += (nlist[i*BS+j] != -1) ? 1 : 0;\n    }\n    \n\n    family[i] = s + 1 + s * LCG_random_double(&seed);\n  }\n\n  int *d_nlist;\n  cudaMalloc((void**)&d_nlist, sizeof(int)*n);\n  cudaMemcpy(d_nlist, nlist, sizeof(int)*n, cudaMemcpyHostToDevice);\n\n  int *d_family;\n  cudaMalloc((void**)&d_family, sizeof(int)*m);\n  cudaMemcpy(d_family, family, sizeof(int)*m, cudaMemcpyHostToDevice);\n\n  int *d_n_neigh;\n  cudaMalloc((void**)&d_n_neigh, sizeof(int)*m);\n\n  double *d_damage;\n  cudaMalloc((void**)&d_damage, sizeof(double)*m);\n\n  dim3 blocks (BS);\n  dim3 grids (m);\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) \n    damage_of_node <<< grids, blocks, BS*sizeof(int) >>> (\n      n, d_nlist, d_family, d_n_neigh, d_damage);\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  cudaMemcpy(n_neigh, d_n_neigh, sizeof(int)*m, cudaMemcpyDeviceToHost);\n  cudaMemcpy(damage, d_damage, sizeof(double)*m, cudaMemcpyDeviceToHost);\n\n  double sum = 0.0;\n  for (int i = 0; i < m; i++) sum += damage[i]; \n  printf(\"Checksum: total damage = %lf\\n\", sum);\n\n  cudaFree(d_nlist);\n  cudaFree(d_family);\n  cudaFree(d_n_neigh);\n  cudaFree(d_damage);\n\n  free(nlist);\n  free(family);\n  free(n_neigh);\n  free(damage);\n\n  return 0;\n}\n"}}
{"kernel_name": "damage", "parallel_api": "hip", "code": {"main.cu": "#include <stdlib.h>\n#include <stdio.h>\n#include <hip/hip_runtime.h>\n#include <chrono>\n#include \"kernel.h\"\n\n\n\n#define BS 256\n\ndouble LCG_random_double(uint64_t * seed)\n{\n  const unsigned long m = 9223372036854775808ULL; \n\n  const unsigned long a = 2806196910506780709ULL;\n  const unsigned long c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 3) {\n    printf(\"Usage: %s <number of points> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int n = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n  const int m = (n + BS - 1) / BS; \n\n\n  int *nlist = (int*) malloc (sizeof(int) * n);\n  int *family = (int*) malloc (sizeof(int) * m);\n  int *n_neigh = (int*) malloc (sizeof(int) * m);\n  double *damage = (double*) malloc (sizeof(double) * m);\n\n  unsigned long seed = 123;\n  for (int i = 0; i < n; i++)\n    nlist[i] = (LCG_random_double(&seed) > 0.5) ? 1 : -1;\n\n  for (int i = 0; i < m; i++) {\n    int s = 0;\n    for (int j = 0; j < BS; j++) {\n      s += (nlist[i*BS+j] != -1) ? 1 : 0;\n    }\n    \n\n    family[i] = s + 1 + s * LCG_random_double(&seed);\n  }\n\n  int *d_nlist;\n  hipMalloc((void**)&d_nlist, sizeof(int)*n);\n  hipMemcpy(d_nlist, nlist, sizeof(int)*n, hipMemcpyHostToDevice);\n\n  int *d_family;\n  hipMalloc((void**)&d_family, sizeof(int)*m);\n  hipMemcpy(d_family, family, sizeof(int)*m, hipMemcpyHostToDevice);\n\n  int *d_n_neigh;\n  hipMalloc((void**)&d_n_neigh, sizeof(int)*m);\n\n  double *d_damage;\n  hipMalloc((void**)&d_damage, sizeof(double)*m);\n\n  dim3 blocks (BS);\n  dim3 grids (m);\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) \n    damage_of_node <<< grids, blocks, BS*sizeof(int) >>> (\n      n, d_nlist, d_family, d_n_neigh, d_damage);\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  hipMemcpy(n_neigh, d_n_neigh, sizeof(int)*m, hipMemcpyDeviceToHost);\n  hipMemcpy(damage, d_damage, sizeof(double)*m, hipMemcpyDeviceToHost);\n\n  double sum = 0.0;\n  for (int i = 0; i < m; i++) sum += damage[i]; \n  printf(\"Checksum: total damage = %lf\\n\", sum);\n\n  hipFree(d_nlist);\n  hipFree(d_family);\n  hipFree(d_n_neigh);\n  hipFree(d_damage);\n\n  free(nlist);\n  free(family);\n  free(n_neigh);\n  free(damage);\n\n  return 0;\n}\n"}}
{"kernel_name": "damage", "parallel_api": "omp", "code": {"main.cpp": "#include <stdlib.h>\n#include <stdio.h>\n#include <chrono>\n#include <omp.h>\n\n\n\n#define BS 256\n\n#include \"kernel.h\"\n\ndouble LCG_random_double(uint64_t * seed)\n{\n  const unsigned long m = 9223372036854775808ULL; \n\n  const unsigned long a = 2806196910506780709ULL;\n  const unsigned long c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 3) {\n    printf(\"Usage: %s <number of points> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int n = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n  const int m = (n + BS - 1) / BS; \n\n\n  int *nlist = (int*) malloc (sizeof(int) * n);\n  int *family = (int*) malloc (sizeof(int) * m);\n  int *n_neigh = (int*) malloc (sizeof(int) * m);\n  double *damage = (double*) malloc (sizeof(double) * m);\n\n  unsigned long seed = 123;\n  for (int i = 0; i < n; i++) {\n    nlist[i] = (LCG_random_double(&seed) > 0.5) ? 1 : -1;\n  }\n\n  for (int i = 0; i < m; i++) {\n    int s = 0;\n    for (int j = 0; j < BS; j++) {\n      s += (nlist[i*BS+j] != -1) ? 1 : 0;\n    }\n    \n\n    family[i] = s + 1 + s * LCG_random_double(&seed);\n  }\n\n  #pragma omp target data map(to: nlist[0:n], family[0:m]) \\\n                          map(from: n_neigh[0:m], damage[0:m])\n  {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) \n      damage_of_node (n, nlist, family, n_neigh, damage);\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time %f (s)\\n\", (time * 1e-9f) / repeat);\n  }\n\n  double sum = 0.0;\n  for (int i = 0; i < m; i++) sum += damage[i]; \n  printf(\"Checksum: total damage = %lf\\n\", sum);\n\n  free(nlist);\n  free(family);\n  free(n_neigh);\n  free(damage);\n\n  return 0;\n}\n"}}
{"kernel_name": "damage", "parallel_api": "serial", "code": {"main.cpp": "#include <stdlib.h>\n#include <stdio.h>\n#include <chrono>\n\n\n\n#define BS 256\n\n#include \"kernel.h\"\n\ndouble LCG_random_double(uint64_t * seed)\n{\n  const unsigned long m = 9223372036854775808ULL; \n\n  const unsigned long a = 2806196910506780709ULL;\n  const unsigned long c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 3) {\n    printf(\"Usage: %s <number of points> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int n = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n  const int m = (n + BS - 1) / BS; \n\n\n  int *nlist = (int*) malloc (sizeof(int) * n);\n  int *family = (int*) malloc (sizeof(int) * m);\n  int *n_neigh = (int*) malloc (sizeof(int) * m);\n  double *damage = (double*) malloc (sizeof(double) * m);\n\n  unsigned long seed = 123;\n  for (int i = 0; i < n; i++) {\n    nlist[i] = (LCG_random_double(&seed) > 0.5) ? 1 : -1;\n  }\n\n  for (int i = 0; i < m; i++) {\n    int s = 0;\n    for (int j = 0; j < BS; j++) {\n      s += (nlist[i*BS+j] != -1) ? 1 : 0;\n    }\n    \n\n    family[i] = s + 1 + s * LCG_random_double(&seed);\n  }\n\n    {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) \n      damage_of_node (n, nlist, family, n_neigh, damage);\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time %f (s)\\n\", (time * 1e-9f) / repeat);\n  }\n\n  double sum = 0.0;\n  for (int i = 0; i < m; i++) sum += damage[i]; \n  printf(\"Checksum: total damage = %lf\\n\", sum);\n\n  free(nlist);\n  free(family);\n  free(n_neigh);\n  free(damage);\n\n  return 0;\n}"}}
{"kernel_name": "damage", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdlib.h>\n#include <stdio.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n#include \"kernel.h\"\n\n\n\n#define BS 256\n\ndouble LCG_random_double(uint64_t * seed)\n{\n  const unsigned long m = 9223372036854775808ULL; \n\n  const unsigned long a = 2806196910506780709ULL;\n  const unsigned long c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 3) {\n    printf(\"Usage: %s <number of points> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int n = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n  const int m = (n + BS - 1) / BS; \n\n\n  int *nlist = (int*) malloc (sizeof(int) * n);\n  int *family = (int*) malloc (sizeof(int) * m);\n  int *n_neigh = (int*) malloc (sizeof(int) * m);\n  double *damage = (double*) malloc (sizeof(double) * m);\n\n  unsigned long seed = 123;\n  for (int i = 0; i < n; i++) {\n    nlist[i] = (LCG_random_double(&seed) > 0.5) ? 1 : -1;\n  }\n\n  for (int i = 0; i < m; i++) {\n    int s = 0;\n    for (int j = 0; j < BS; j++) {\n      s += (nlist[i*BS+j] != -1) ? 1 : 0;\n    }\n    \n\n    family[i] = s + 1 + s * LCG_random_double(&seed);\n  }\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  int *d_nlist = sycl::malloc_device<int>(n, q);\n  q.memcpy(d_nlist, nlist, sizeof(int) * n);\n\n  int *d_family = sycl::malloc_device<int>(m, q);\n  q.memcpy(d_family, family, sizeof(int) * m);\n\n  int *d_n_neigh = sycl::malloc_device<int>(m, q);\n  q.memcpy(d_n_neigh, n_neigh, sizeof(int) * m);\n\n  double *d_damage = sycl::malloc_device<double>(m, q);\n\n  sycl::range<1> lws (BS);\n  sycl::range<1> gws (m*BS);\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    q.submit([&] (sycl::handler &cgh) {\n      sycl::local_accessor<int, 1> sm (sycl::range<1>(BS), cgh);\n      cgh.parallel_for<class compute>(\n         sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n         damage_of_node(item,\n                        n,\n                        d_nlist,\n                        d_family,\n                        d_n_neigh,\n                        d_damage,\n                        sm.get_pointer());\n      });\n    });\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  q.memcpy(damage, d_damage, sizeof(double) * m).wait();\n  double sum = 0.0;\n  for (int i = 0; i < m; i++) sum += damage[i]; \n  printf(\"Checksum: total damage = %lf\\n\", sum);\n\n  sycl::free(d_nlist, q);\n  sycl::free(d_family, q);\n  sycl::free(d_n_neigh, q);\n  sycl::free(d_damage, q);\n  free(nlist);\n  free(family);\n  free(n_neigh);\n  free(damage);\n\n  return 0;\n}\n"}}
{"kernel_name": "ddbp", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <cuda.h>\n\n\n\n#define BLOCK_SIZE 256\n\n\n\n#define integrateXcoord 1\n#define integrateYcoord 0\n\n__global__ void pad_projections_kernel(\n    double* d_img,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nElem,\n    const int np)\n{\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < nElem)\n    d_img[(np*nDetYMap *nDetXMap) + (gid*nDetYMap)] = 0;\n}\n\n__global__ void map_boudaries_kernel(\n    double* d_pBound,\n    const int nElem,\n    const double valueLeftBound,\n    const double sizeElem,\n    const double offset)\n{\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < nElem)\n    d_pBound[gid] = (gid - valueLeftBound) * sizeElem + offset;\n}\n\n__global__ void rot_detector_kernel(\n          double* __restrict__ d_pRdetY,\n          double* __restrict__ d_pRdetZ,\n    const double* __restrict__ d_pYcoord,\n    const double* __restrict__ d_pZcoord,\n    const double yOffset,\n    const double zOffset,\n    const double phi,\n    const int nElem)\n{\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < nElem) {\n    \n\n    d_pRdetY[gid] = ((d_pYcoord[gid] - yOffset) * cos(phi) - \n                     (d_pZcoord[gid] - zOffset) * sin(phi)) + yOffset;\n    d_pRdetZ[gid] = ((d_pYcoord[gid] - yOffset) * sin(phi) +\n                     (d_pZcoord[gid] - zOffset) * cos(phi)) + zOffset;\n  }\n}\n\n__global__ void mapDet2Slice_kernel(\n           double* __restrict__ const pXmapp,\n           double* __restrict__ const pYmapp,\n    double tubeX,\n    double tubeY,\n    double tubeZ,\n    const double* __restrict__ const pXcoord,\n    const double* __restrict__ const pYcoord,\n    const double* __restrict__ const pZcoord,\n    const double* __restrict__ const pZSlicecoord,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nz)\n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (px < nDetYMap && py < nDetXMap) {\n\n    const int pos = py * nDetYMap + px;\n\n    pXmapp[pos] = ((pXcoord[py] - tubeX)*(pZSlicecoord[nz] - pZcoord[px]) - \n        (pXcoord[py] * tubeZ) + (pXcoord[py] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n\n    if (py == 0)\n      pYmapp[px] = ((pYcoord[px] - tubeY)*(pZSlicecoord[nz] - pZcoord[px]) -\n          (pYcoord[px] * tubeZ) + (pYcoord[px] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n  }\n}\n\n__global__ void img_integration_kernel(\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const bool direction,\n    const int offsetX,\n    const int offsetY,\n    const int nSlices)\n{\n  \n\n\n  const int tx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int ty = blockIdx.y * blockDim.y + threadIdx.y;\n  const int px = tx + offsetX;\n  const int py = ty + offsetY;\n  const int pz = blockIdx.z * blockDim.z + threadIdx.z;\n\n  if (px >= nPixY || py >= nPixX || pz >= nSlices) return;\n\n  if (direction == integrateXcoord) {\n\n    for (int s = 1; s <= blockDim.y; s *= 2) {\n\n      int spot = ty - s;\n\n      double val = 0;\n\n      if (spot >= 0) {\n        val = d_img[(pz*nPixY*nPixX) + (offsetY + spot) * nPixY + px];\n      }\n\n      if (spot >= 0) {\n        d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n      }\n    }\n  }\n  else\n  {\n    for (int s = 1; s <= blockDim.x; s *= 2) {\n\n      int spot = tx - s;\n\n      double val = 0;\n\n      if (spot >= 0) {\n        val = d_img[(pz*nPixY*nPixX) + py * nPixY + spot + offsetX];\n      }\n\n      if (spot >= 0) {\n        d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n      }\n    }\n  }\n}\n\n__global__ void bilinear_interpolation_kernel(\n          double* __restrict__ d_sliceI,\n    const double* __restrict__ d_pProj,\n    const double* __restrict__ d_pObjX,\n    const double* __restrict__ d_pObjY,\n    const double* __restrict__ d_pDetmX,\n    const double* __restrict__ d_pDetmY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nDetX,\n    const int nDetY,\n    const int np) \n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n\n  \n\n  \n\n  if (px >= nPixYMap || py >= nPixXMap) return;\n\n  \n\n\n  \n\n  \n\n  const double xNormData = nDetX - d_pObjX[py] / d_pDetmX[0];\n  const int    xData = floor(xNormData);\n  const double alpha = xNormData - xData;\n\n  \n\n  \n\n  const double yNormData = (d_pObjY[px] / d_pDetmX[0]) - (d_pDetmY[0] / d_pDetmX[0]);\n  const int    yData = floor(yNormData);\n  const double beta = yNormData - yData;\n\n  double d00, d01, d10, d11;\n  if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY)) \n    d00 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData)];\n  else\n    d00 = 0.0;\n\n  if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY))\n    d10 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData)];\n  else\n    d10 = 0.0;\n\n  if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n    d01 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData + 1)];\n  else\n    d01 = 0.0;\n\n  if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n    d11 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData + 1)];\n  else\n    d11 = 0.0;\n\n  double result_temp1 = alpha * d10 + (-d00 * alpha + d00);\n  double result_temp2 = alpha * d11 + (-d01 * alpha + d01);\n\n  d_sliceI[py * nPixYMap + px] = beta * result_temp2 + (-result_temp1 * beta + result_temp1);\n}\n\n__global__ void differentiation_kernel(\n          double* __restrict__ d_pVolume,\n    const double* __restrict__ d_sliceI,\n    double tubeX,\n    double rtubeY,\n    double rtubeZ,\n    const double* __restrict__ const d_pObjX,\n    const double* __restrict__ const d_pObjY,\n    const double* __restrict__ const d_pObjZ,\n    const int nPixX,\n    const int nPixY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const double du,\n    const double dv,\n    const double dx,\n    const double dy,\n    const double dz,\n    const int nz) \n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n\n  \n\n\n  if (px < nPixY && py < nPixX) {\n\n    const int pos = (nPixX*nPixY*nz) + (py * nPixY) + px;\n\n    int coordA = py * nPixYMap + px;\n    int coordB = ((py + 1) * nPixYMap) + px;\n    int coordC = coordA + 1;\n    int coordD = coordB + 1;\n\n    \n\n    double gamma = atan((d_pObjX[py] + (dx / 2.0) - tubeX) / (rtubeZ - d_pObjZ[nz]));\n\n    \n\n    double alpha = atan((d_pObjY[px] + (dy / 2.0) - rtubeY) / (rtubeZ - d_pObjZ[nz]));\n\n    double dA, dB, dC, dD;\n\n    dA = d_sliceI[coordA];\n    dB = d_sliceI[coordB];\n    dC = d_sliceI[coordC];\n    dD = d_sliceI[coordD];\n\n    \n\n    if (dC == 0 && dD == 0) {\n      dC = dA;\n      dD = dB;\n    }\n\n    \n\n    d_pVolume[pos] += ((dD - dC - dB + dA)*(du*dv*dz / (cos(alpha)*cos(gamma)*dx*dy)));\n  }\n}\n\n__global__ void division_kernel(\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const int nSlices,\n    const int nProj)\n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n  const int pz = blockIdx.z * blockDim.z + threadIdx.z;\n\n  if (px < nPixY && py < nPixX && pz < nSlices) {\n    const int pos = (nPixX*nPixY*pz) + (py * nPixY) + px;\n    d_img[pos] /= (double) nProj;\n  }\n}\n\n\n\n\nvoid backprojectionDDb(\n          double* const h_pVolume,\n    const double* const h_pProj,\n    const double* const h_pTubeAngle,\n    const double* const h_pDetAngle,\n    const int idXProj,\n    const int nProj,\n    const int nPixX,\n    const int nPixY,\n    const int nSlices,\n    const int nDetX,\n    const int nDetY,\n    const double dx,\n    const double dy,\n    const double dz,\n    const double du,\n    const double dv,\n    const double DSD,\n    const double DDR,\n    const double DAG)\n{\n  \n\n  const int nDetXMap = nDetX + 1;\n  const int nDetYMap = nDetY + 1;\n\n  \n\n  const int nPixXMap = nPixX + 1;\n  const int nPixYMap = nPixY + 1;\n\n  double *d_pProj, *d_sliceI, *d_pVolume;\n\n  cudaMalloc((void **)&d_pProj, nDetXMap*nDetYMap*nProj * sizeof(double)); \n  cudaMalloc((void **)&d_sliceI, nPixXMap*nPixYMap * sizeof(double));\n  cudaMalloc((void **)&d_pVolume, nPixX*nPixY*nSlices * sizeof(double));\n\n  \n\n  double *d_pDetX, *d_pDetY, *d_pDetZ, *d_pObjX, *d_pObjY, *d_pObjZ;\n\n  cudaMalloc((void **)&d_pDetX, nDetXMap * sizeof(double));\n  cudaMalloc((void **)&d_pDetY, nDetYMap * sizeof(double));\n  cudaMalloc((void **)&d_pDetZ, nDetYMap * sizeof(double));\n  cudaMalloc((void **)&d_pObjX, nPixXMap * sizeof(double));\n  cudaMalloc((void **)&d_pObjY, nPixYMap * sizeof(double));\n  cudaMalloc((void **)&d_pObjZ, nSlices * sizeof(double));\n\n  \n\n  double *d_pDetmY, *d_pDetmX;\n\n  cudaMalloc((void **)&d_pDetmY, nDetYMap * sizeof(double));\n  cudaMalloc((void **)&d_pDetmX, nDetYMap * nDetXMap * sizeof(double));\n\n  \n\n  double *d_pRdetY, *d_pRdetZ;\n\n  cudaMalloc((void **)&d_pRdetY, nDetYMap * sizeof(double));\n  cudaMalloc((void **)&d_pRdetZ, nDetYMap * sizeof(double));\n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  dim3 threadsPerBlock (1,1,1);\n  dim3 blockSize (1,1,1);\n\n  const int maxThreadsPerBlock = BLOCK_SIZE;\n\n  \n\n\n  \n\n  const double* h_pProj_tmp;\n  double* d_pProj_tmp;\n\n  threadsPerBlock.x = maxThreadsPerBlock;\n  blockSize.x = (nDetXMap / maxThreadsPerBlock) + 1;\n\n  for (int np = 0; np < nProj; np++) {\n\n    \n\n    pad_projections_kernel <<<blockSize, threadsPerBlock>>> (d_pProj, nDetXMap, nDetYMap, nDetXMap, np);\n\n    \n\n    d_pProj_tmp = d_pProj + (nDetXMap*nDetYMap*np) + 1;\n    cudaMemset(d_pProj_tmp, 0, nPixY * sizeof(double));\n  }\n\n  \n\n  for (int np = 0; np < nProj; np++)\n    for (int c = 0; c < nDetX; c++) {\n      h_pProj_tmp = h_pProj + (c * nDetY) + (nDetX*nDetY*np);\n      d_pProj_tmp = d_pProj + (((c + 1) * nDetYMap) + 1) + (nDetXMap*nDetYMap*np);\n      cudaMemcpy(d_pProj_tmp, h_pProj_tmp, nDetY * sizeof(double), cudaMemcpyHostToDevice);\n    }\n\n  \n\n\n  threadsPerBlock.x = maxThreadsPerBlock;\n\n  blockSize.x = (nDetX / maxThreadsPerBlock) + 1;\n\n  map_boudaries_kernel <<<blockSize, threadsPerBlock>>> (d_pDetX, nDetXMap, (double)nDetX, -du, 0.0);\n\n  blockSize.x = (nDetY / maxThreadsPerBlock) + 1;\n\n  map_boudaries_kernel <<<blockSize, threadsPerBlock>>> (d_pDetY, nDetYMap, nDetY / 2.0, dv, 0.0);\n\n  blockSize.x = (nPixX / maxThreadsPerBlock) + 1;\n\n  map_boudaries_kernel <<<blockSize, threadsPerBlock>>> (d_pObjX, nPixXMap, (double)nPixX, -dx, 0.0);\n\n  blockSize.x = (nPixY / maxThreadsPerBlock) + 1;\n\n  map_boudaries_kernel <<<blockSize, threadsPerBlock>>> (d_pObjY, nPixYMap, nPixY / 2.0, dy, 0.0);\n\n  blockSize.x = (nSlices / maxThreadsPerBlock) + 1;\n\n  map_boudaries_kernel <<<blockSize, threadsPerBlock>>> (d_pObjZ, nSlices, 0.0, dz, DAG + (dz / 2.0));\n\n  \n\n  cudaMemset(d_pDetZ, 0, nDetYMap * sizeof(double));\n  cudaMemset(d_pVolume, 0, nPixX * nPixY * nSlices * sizeof(double));\n\n  \n\n  double tubeX = 0;\n  double tubeY = 0;\n  double tubeZ = DSD;\n\n  \n\n  double isoY = 0;\n  double isoZ = DDR;\n\n  \n\n  \n\n\n  \n\n  threadsPerBlock.x = 8;\n  threadsPerBlock.y = 4;\n  threadsPerBlock.z = 8;\n\n  blockSize.x = (int)ceilf((float)nDetYMap / (threadsPerBlock.x - 1));\n  blockSize.y = 1;\n  blockSize.z = (int)ceilf((float)nProj / threadsPerBlock.z);\n\n  int Xk = (int)ceilf((float)nDetXMap / (threadsPerBlock.x - 1));\n  for (int k = 0; k < Xk; k++) {\n\n    img_integration_kernel <<<blockSize, threadsPerBlock>>> (\n        d_pProj, nDetXMap, nDetYMap, integrateXcoord, 0, k * 9, nProj);\n  }\n\n  \n\n  threadsPerBlock.x = 4;\n  threadsPerBlock.y = 8;\n  threadsPerBlock.z = 8;\n\n  blockSize.x = 1;\n  blockSize.y = (int)ceilf((float)nDetXMap / (threadsPerBlock.y - 1));\n  blockSize.z = (int)ceilf((float)nProj / threadsPerBlock.z);\n\n  int Yk = (int)ceilf((float)nDetYMap / (threadsPerBlock.y - 1));\n  for (int k = 0; k < Yk; k++) {\n\n    img_integration_kernel <<<blockSize, threadsPerBlock>>> (\n        d_pProj, nDetXMap, nDetYMap, integrateYcoord, k * 9, 0, nProj);\n  }\n\n  double* d_pDetmX_tmp = d_pDetmX + (nDetYMap * (nDetXMap-2));\n\n  int projIni, projEnd, nProj2Run;\n  if (idXProj == -1) {\n    projIni = 0;\n    projEnd = nProj;\n    nProj2Run = nProj;\n  }\n  else {\n    projIni = idXProj;\n    projEnd = idXProj + 1;\n    nProj2Run = 1;\n  }\n\n  \n\n  for (int p = projIni; p < projEnd; p++) {\n\n    \n\n    double theta = h_pTubeAngle[p] * M_PI / 180.0;\n\n    \n\n    double phi = h_pDetAngle[p] * M_PI / 180.0;\n\n    \n\n\n    \n\n    double rtubeY = ((tubeY - isoY)*cos(theta) - (tubeZ - isoZ)*sin(theta)) + isoY;\n    double rtubeZ = ((tubeY - isoY)*sin(theta) + (tubeZ - isoZ)*cos(theta)) + isoZ;\n\n    \n\n\n    \n\n    threadsPerBlock.x = maxThreadsPerBlock;\n    threadsPerBlock.y = 1;\n    threadsPerBlock.z = 1;\n\n    blockSize.x = (nDetYMap / maxThreadsPerBlock) + 1;\n    blockSize.y = 1;\n    blockSize.z = 1;\n\n    rot_detector_kernel <<<blockSize, threadsPerBlock>>> (\n        d_pRdetY, d_pRdetZ, d_pDetY, d_pDetZ, isoY, isoZ, phi, nDetYMap);\n\n    threadsPerBlock.x = 16;\n    threadsPerBlock.y = 16;\n    threadsPerBlock.z = 1;\n\n    \n\n    for (int nz = 0; nz < nSlices; nz++) {\n\n      \n\n\n      blockSize.x = (nDetYMap / threadsPerBlock.x) + 1;\n      blockSize.y = (nDetXMap / threadsPerBlock.y) + 1;\n      blockSize.z = 1;\n\n      mapDet2Slice_kernel <<<blockSize, threadsPerBlock>>> (\n          d_pDetmX, d_pDetmY, tubeX, rtubeY, rtubeZ, d_pDetX,\n          d_pRdetY, d_pRdetZ, d_pObjZ, nDetXMap, nDetYMap, nz);\n\n      \n\n\n      blockSize.x = (nPixYMap / threadsPerBlock.x) + 1;\n      blockSize.y = (nPixXMap / threadsPerBlock.y) + 1;\n\n      bilinear_interpolation_kernel <<<blockSize, threadsPerBlock>>> (\n          d_sliceI, d_pProj, d_pObjX, d_pObjY, d_pDetmX_tmp, d_pDetmY,\n          nPixXMap, nPixYMap, nDetXMap, nDetYMap, nDetX, nDetY, p);\n\n      \n\n\n      blockSize.x = (nPixY / threadsPerBlock.x) + 1;\n      blockSize.y = (nPixX / threadsPerBlock.y) + 1;\n\n      differentiation_kernel <<<blockSize, threadsPerBlock>>> (\n          d_pVolume, d_sliceI, tubeX, rtubeY, rtubeZ, d_pObjX, d_pObjY, d_pObjZ,\n          nPixX, nPixY, nPixXMap, nPixYMap, du, dv, dx, dy, dz, nz);\n\n    } \n\n\n  } \n\n\n\n  \n\n  threadsPerBlock.x = 8;\n  threadsPerBlock.y = 8;\n  threadsPerBlock.z = 4;\n\n  blockSize.x = (nPixY / threadsPerBlock.x) + 1;\n  blockSize.y = (nPixX / threadsPerBlock.y) + 1;\n  blockSize.z = (nSlices / threadsPerBlock.z) + 1;\n\n  division_kernel <<<blockSize, threadsPerBlock>>> (d_pVolume, nPixX, nPixY, nSlices, nProj2Run);\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Total kernel execution %f (s)\\n\", time * 1e-9f);\n\n  cudaMemcpy(h_pVolume, d_pVolume, nSlices* nPixX * nPixY * sizeof(double), cudaMemcpyDeviceToHost);\n\n  cudaFree(d_pProj);\n  cudaFree(d_sliceI);\n  cudaFree(d_pVolume);\n  cudaFree(d_pDetX);\n  cudaFree(d_pDetY);\n  cudaFree(d_pDetZ);\n  cudaFree(d_pObjX);\n  cudaFree(d_pObjY);\n  cudaFree(d_pObjZ);\n  cudaFree(d_pDetmY);\n  cudaFree(d_pDetmX);\n  cudaFree(d_pRdetY);\n  cudaFree(d_pRdetZ);\n}\n\nint main() \n{\n                            \n\n  const int nPixX = 1996;   \n\n  const int nPixY = 2457;   \n\n  const int nSlices = 78;  \n\n                            \n\n  const int nDetX = 1664;   \n\n  const int nDetY = 2048;   \n\n\n  const int nProj = 15;     \n\n  const int idXProj = -1;   \n\n\n  const double dx = 0.112;  \n\n  const double dy = 0.112;\n  const double dz = 1.0;\n\n  const double du = 0.14;   \n\n  const double dv = 0.14;\n\n  const double DSD = 700;   \n\n  const double DDR = 0.0;   \n\n  const double DAG = 25.0;  \n\n\n  const size_t pixVol = nPixX * nPixY * nSlices;\n  const size_t detVol = nDetX * nDetY * nProj;\n  double *h_pVolume = (double*) malloc (pixVol * sizeof(double));\n  double *h_pProj = (double*) malloc (detVol * sizeof(double));\n\n  double *h_pTubeAngle = (double*) malloc (nProj * sizeof(double));\n  double *h_pDetAngle = (double*) malloc (nProj * sizeof(double));\n  \n  \n\n  for (int i = 0; i < nProj; i++) \n    h_pTubeAngle[i] = -7.5 + i * 15.0/nProj;\n\n  \n\n  for (int i = 0; i < nProj; i++) \n    h_pDetAngle[i] = -2.1 + i * 4.2/nProj;\n\n  \n\n  srand(123);\n  for (size_t i = 0; i < pixVol; i++) \n    h_pVolume[i] = (double)rand() / (double)RAND_MAX;\n\n  for (size_t i = 0; i < detVol; i++) \n    h_pProj[i] = (double)rand() / (double)RAND_MAX;\n\n  backprojectionDDb(\n    h_pVolume,\n    h_pProj,\n    h_pTubeAngle,\n    h_pDetAngle,\n    idXProj,\n    nProj,\n    nPixX, nPixY,\n    nSlices,\n    nDetX, nDetY,\n    dx, dy, dz,\n    du, dv,\n    DSD, DDR, DAG);\n\n  double checkSum = 0;\n  for (size_t i = 0; i < pixVol; i++)\n    checkSum += h_pVolume[i];\n  printf(\"checksum = %lf\\n\", checkSum);\n\n  free(h_pVolume);\n  free(h_pTubeAngle);\n  free(h_pDetAngle);\n  free(h_pProj);\n  return 0;\n}\n"}}
{"kernel_name": "ddbp", "parallel_api": "hip", "code": {"main.cu": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n\n\n\n#define BLOCK_SIZE 256\n\n\n\n#define integrateXcoord 1\n#define integrateYcoord 0\n\n__global__ void pad_projections_kernel(\n    double* d_img,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nElem,\n    const int np)\n{\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < nElem)\n    d_img[(np*nDetYMap *nDetXMap) + (gid*nDetYMap)] = 0;\n}\n\n__global__ void map_boudaries_kernel(\n    double* d_pBound,\n    const int nElem,\n    const double valueLeftBound,\n    const double sizeElem,\n    const double offset)\n{\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < nElem)\n    d_pBound[gid] = (gid - valueLeftBound) * sizeElem + offset;\n}\n\n__global__ void rot_detector_kernel(\n          double* __restrict__ d_pRdetY,\n          double* __restrict__ d_pRdetZ,\n    const double* __restrict__ d_pYcoord,\n    const double* __restrict__ d_pZcoord,\n    const double yOffset,\n    const double zOffset,\n    const double phi,\n    const int nElem)\n{\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < nElem) {\n    \n\n    d_pRdetY[gid] = ((d_pYcoord[gid] - yOffset) * cos(phi) - \n                     (d_pZcoord[gid] - zOffset) * sin(phi)) + yOffset;\n    d_pRdetZ[gid] = ((d_pYcoord[gid] - yOffset) * sin(phi) +\n                     (d_pZcoord[gid] - zOffset) * cos(phi)) + zOffset;\n  }\n}\n\n__global__ void mapDet2Slice_kernel(\n           double* __restrict__ const pXmapp,\n           double* __restrict__ const pYmapp,\n    double tubeX,\n    double tubeY,\n    double tubeZ,\n    const double* __restrict__ const pXcoord,\n    const double* __restrict__ const pYcoord,\n    const double* __restrict__ const pZcoord,\n    const double* __restrict__ const pZSlicecoord,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nz)\n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (px < nDetYMap && py < nDetXMap) {\n\n    const int pos = py * nDetYMap + px;\n\n    pXmapp[pos] = ((pXcoord[py] - tubeX)*(pZSlicecoord[nz] - pZcoord[px]) - \n        (pXcoord[py] * tubeZ) + (pXcoord[py] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n\n    if (py == 0)\n      pYmapp[px] = ((pYcoord[px] - tubeY)*(pZSlicecoord[nz] - pZcoord[px]) -\n          (pYcoord[px] * tubeZ) + (pYcoord[px] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n  }\n}\n\n__global__ void img_integration_kernel(\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const bool direction,\n    const int offsetX,\n    const int offsetY,\n    const int nSlices)\n{\n  \n\n\n  const int tx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int ty = blockIdx.y * blockDim.y + threadIdx.y;\n  const int px = tx + offsetX;\n  const int py = ty + offsetY;\n  const int pz = blockIdx.z * blockDim.z + threadIdx.z;\n\n  if (px >= nPixY || py >= nPixX || pz >= nSlices) return;\n\n  if (direction == integrateXcoord) {\n\n    for (int s = 1; s <= blockDim.y; s *= 2) {\n\n      int spot = ty - s;\n\n      double val = 0;\n\n      if (spot >= 0) {\n        val = d_img[(pz*nPixY*nPixX) + (offsetY + spot) * nPixY + px];\n      }\n\n      if (spot >= 0) {\n        d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n      }\n    }\n  }\n  else\n  {\n    for (int s = 1; s <= blockDim.x; s *= 2) {\n\n      int spot = tx - s;\n\n      double val = 0;\n\n      if (spot >= 0) {\n        val = d_img[(pz*nPixY*nPixX) + py * nPixY + spot + offsetX];\n      }\n\n      if (spot >= 0) {\n        d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n      }\n    }\n  }\n}\n\n__global__ void bilinear_interpolation_kernel(\n          double* __restrict__ d_sliceI,\n    const double* __restrict__ d_pProj,\n    const double* __restrict__ d_pObjX,\n    const double* __restrict__ d_pObjY,\n    const double* __restrict__ d_pDetmX,\n    const double* __restrict__ d_pDetmY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nDetX,\n    const int nDetY,\n    const int np) \n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n\n  \n\n  \n\n  if (px >= nPixYMap || py >= nPixXMap) return;\n\n  \n\n\n  \n\n  \n\n  const double xNormData = nDetX - d_pObjX[py] / d_pDetmX[0];\n  const int    xData = floor(xNormData);\n  const double alpha = xNormData - xData;\n\n  \n\n  \n\n  const double yNormData = (d_pObjY[px] / d_pDetmX[0]) - (d_pDetmY[0] / d_pDetmX[0]);\n  const int    yData = floor(yNormData);\n  const double beta = yNormData - yData;\n\n  double d00, d01, d10, d11;\n  if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY)) \n    d00 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData)];\n  else\n    d00 = 0.0;\n\n  if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY))\n    d10 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData)];\n  else\n    d10 = 0.0;\n\n  if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n    d01 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData + 1)];\n  else\n    d01 = 0.0;\n\n  if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n    d11 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData + 1)];\n  else\n    d11 = 0.0;\n\n  double result_temp1 = alpha * d10 + (-d00 * alpha + d00);\n  double result_temp2 = alpha * d11 + (-d01 * alpha + d01);\n\n  d_sliceI[py * nPixYMap + px] = beta * result_temp2 + (-result_temp1 * beta + result_temp1);\n}\n\n__global__ void differentiation_kernel(\n          double* __restrict__ d_pVolume,\n    const double* __restrict__ d_sliceI,\n    double tubeX,\n    double rtubeY,\n    double rtubeZ,\n    const double* __restrict__ const d_pObjX,\n    const double* __restrict__ const d_pObjY,\n    const double* __restrict__ const d_pObjZ,\n    const int nPixX,\n    const int nPixY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const double du,\n    const double dv,\n    const double dx,\n    const double dy,\n    const double dz,\n    const int nz) \n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n\n  \n\n\n  if (px < nPixY && py < nPixX) {\n\n    const int pos = (nPixX*nPixY*nz) + (py * nPixY) + px;\n\n    int coordA = py * nPixYMap + px;\n    int coordB = ((py + 1) * nPixYMap) + px;\n    int coordC = coordA + 1;\n    int coordD = coordB + 1;\n\n    \n\n    double gamma = atan((d_pObjX[py] + (dx / 2.0) - tubeX) / (rtubeZ - d_pObjZ[nz]));\n\n    \n\n    double alpha = atan((d_pObjY[px] + (dy / 2.0) - rtubeY) / (rtubeZ - d_pObjZ[nz]));\n\n    double dA, dB, dC, dD;\n\n    dA = d_sliceI[coordA];\n    dB = d_sliceI[coordB];\n    dC = d_sliceI[coordC];\n    dD = d_sliceI[coordD];\n\n    \n\n    if (dC == 0 && dD == 0) {\n      dC = dA;\n      dD = dB;\n    }\n\n    \n\n    d_pVolume[pos] += ((dD - dC - dB + dA)*(du*dv*dz / (cos(alpha)*cos(gamma)*dx*dy)));\n  }\n}\n\n__global__ void division_kernel(\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const int nSlices,\n    const int nProj)\n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n  const int pz = blockIdx.z * blockDim.z + threadIdx.z;\n\n  if (px < nPixY && py < nPixX && pz < nSlices) {\n    const int pos = (nPixX*nPixY*pz) + (py * nPixY) + px;\n    d_img[pos] /= (double) nProj;\n  }\n}\n\n\n\n\nvoid backprojectionDDb(\n          double* const h_pVolume,\n    const double* const h_pProj,\n    const double* const h_pTubeAngle,\n    const double* const h_pDetAngle,\n    const int idXProj,\n    const int nProj,\n    const int nPixX,\n    const int nPixY,\n    const int nSlices,\n    const int nDetX,\n    const int nDetY,\n    const double dx,\n    const double dy,\n    const double dz,\n    const double du,\n    const double dv,\n    const double DSD,\n    const double DDR,\n    const double DAG)\n{\n  \n\n  const int nDetXMap = nDetX + 1;\n  const int nDetYMap = nDetY + 1;\n\n  \n\n  const int nPixXMap = nPixX + 1;\n  const int nPixYMap = nPixY + 1;\n\n  double *d_pProj, *d_sliceI, *d_pVolume;\n\n  hipMalloc((void **)&d_pProj, nDetXMap*nDetYMap*nProj * sizeof(double)); \n  hipMalloc((void **)&d_sliceI, nPixXMap*nPixYMap * sizeof(double));\n  hipMalloc((void **)&d_pVolume, nPixX*nPixY*nSlices * sizeof(double));\n\n  \n\n  double *d_pDetX, *d_pDetY, *d_pDetZ, *d_pObjX, *d_pObjY, *d_pObjZ;\n\n  hipMalloc((void **)&d_pDetX, nDetXMap * sizeof(double));\n  hipMalloc((void **)&d_pDetY, nDetYMap * sizeof(double));\n  hipMalloc((void **)&d_pDetZ, nDetYMap * sizeof(double));\n  hipMalloc((void **)&d_pObjX, nPixXMap * sizeof(double));\n  hipMalloc((void **)&d_pObjY, nPixYMap * sizeof(double));\n  hipMalloc((void **)&d_pObjZ, nSlices * sizeof(double));\n\n  \n\n  double *d_pDetmY, *d_pDetmX;\n\n  hipMalloc((void **)&d_pDetmY, nDetYMap * sizeof(double));\n  hipMalloc((void **)&d_pDetmX, nDetYMap * nDetXMap * sizeof(double));\n\n  \n\n  double *d_pRdetY, *d_pRdetZ;\n\n  hipMalloc((void **)&d_pRdetY, nDetYMap * sizeof(double));\n  hipMalloc((void **)&d_pRdetZ, nDetYMap * sizeof(double));\n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  dim3 threadsPerBlock (1,1,1);\n  dim3 blockSize (1,1,1);\n\n  const int maxThreadsPerBlock = BLOCK_SIZE;\n\n  \n\n\n  \n\n  const double* h_pProj_tmp;\n  double* d_pProj_tmp;\n\n  threadsPerBlock.x = maxThreadsPerBlock;\n  blockSize.x = (nDetXMap / maxThreadsPerBlock) + 1;\n\n  for (int np = 0; np < nProj; np++) {\n\n    \n\n    pad_projections_kernel <<<blockSize, threadsPerBlock>>> (d_pProj, nDetXMap, nDetYMap, nDetXMap, np);\n\n    \n\n    d_pProj_tmp = d_pProj + (nDetXMap*nDetYMap*np) + 1;\n    hipMemset(d_pProj_tmp, 0, nPixY * sizeof(double));\n  }\n\n  \n\n  for (int np = 0; np < nProj; np++)\n    for (int c = 0; c < nDetX; c++) {\n      h_pProj_tmp = h_pProj + (c * nDetY) + (nDetX*nDetY*np);\n      d_pProj_tmp = d_pProj + (((c + 1) * nDetYMap) + 1) + (nDetXMap*nDetYMap*np);\n      hipMemcpy(d_pProj_tmp, h_pProj_tmp, nDetY * sizeof(double), hipMemcpyHostToDevice);\n    }\n\n  \n\n\n  threadsPerBlock.x = maxThreadsPerBlock;\n\n  blockSize.x = (nDetX / maxThreadsPerBlock) + 1;\n\n  map_boudaries_kernel <<<blockSize, threadsPerBlock>>> (d_pDetX, nDetXMap, (double)nDetX, -du, 0.0);\n\n  blockSize.x = (nDetY / maxThreadsPerBlock) + 1;\n\n  map_boudaries_kernel <<<blockSize, threadsPerBlock>>> (d_pDetY, nDetYMap, nDetY / 2.0, dv, 0.0);\n\n  blockSize.x = (nPixX / maxThreadsPerBlock) + 1;\n\n  map_boudaries_kernel <<<blockSize, threadsPerBlock>>> (d_pObjX, nPixXMap, (double)nPixX, -dx, 0.0);\n\n  blockSize.x = (nPixY / maxThreadsPerBlock) + 1;\n\n  map_boudaries_kernel <<<blockSize, threadsPerBlock>>> (d_pObjY, nPixYMap, nPixY / 2.0, dy, 0.0);\n\n  blockSize.x = (nSlices / maxThreadsPerBlock) + 1;\n\n  map_boudaries_kernel <<<blockSize, threadsPerBlock>>> (d_pObjZ, nSlices, 0.0, dz, DAG + (dz / 2.0));\n\n  \n\n  hipMemset(d_pDetZ, 0, nDetYMap * sizeof(double));\n  hipMemset(d_pVolume, 0, nPixX * nPixY * nSlices * sizeof(double));\n\n  \n\n  double tubeX = 0;\n  double tubeY = 0;\n  double tubeZ = DSD;\n\n  \n\n  double isoY = 0;\n  double isoZ = DDR;\n\n  \n\n  \n\n\n  \n\n  threadsPerBlock.x = 8;\n  threadsPerBlock.y = 4;\n  threadsPerBlock.z = 8;\n\n  blockSize.x = (int)ceilf((float)nDetYMap / (threadsPerBlock.x - 1));\n  blockSize.y = 1;\n  blockSize.z = (int)ceilf((float)nProj / threadsPerBlock.z);\n\n  int Xk = (int)ceilf((float)nDetXMap / (threadsPerBlock.x - 1));\n  for (int k = 0; k < Xk; k++) {\n\n    img_integration_kernel <<<blockSize, threadsPerBlock>>> (\n        d_pProj, nDetXMap, nDetYMap, integrateXcoord, 0, k * 9, nProj);\n  }\n\n  \n\n  threadsPerBlock.x = 4;\n  threadsPerBlock.y = 8;\n  threadsPerBlock.z = 8;\n\n  blockSize.x = 1;\n  blockSize.y = (int)ceilf((float)nDetXMap / (threadsPerBlock.y - 1));\n  blockSize.z = (int)ceilf((float)nProj / threadsPerBlock.z);\n\n  int Yk = (int)ceilf((float)nDetYMap / (threadsPerBlock.y - 1));\n  for (int k = 0; k < Yk; k++) {\n\n    img_integration_kernel <<<blockSize, threadsPerBlock>>> (\n        d_pProj, nDetXMap, nDetYMap, integrateYcoord, k * 9, 0, nProj);\n  }\n\n  double* d_pDetmX_tmp = d_pDetmX + (nDetYMap * (nDetXMap-2));\n\n  int projIni, projEnd, nProj2Run;\n  if (idXProj == -1) {\n    projIni = 0;\n    projEnd = nProj;\n    nProj2Run = nProj;\n  }\n  else {\n    projIni = idXProj;\n    projEnd = idXProj + 1;\n    nProj2Run = 1;\n  }\n\n  \n\n  for (int p = projIni; p < projEnd; p++) {\n\n    \n\n    double theta = h_pTubeAngle[p] * M_PI / 180.0;\n\n    \n\n    double phi = h_pDetAngle[p] * M_PI / 180.0;\n\n    \n\n\n    \n\n    double rtubeY = ((tubeY - isoY)*cos(theta) - (tubeZ - isoZ)*sin(theta)) + isoY;\n    double rtubeZ = ((tubeY - isoY)*sin(theta) + (tubeZ - isoZ)*cos(theta)) + isoZ;\n\n    \n\n\n    \n\n    threadsPerBlock.x = maxThreadsPerBlock;\n    threadsPerBlock.y = 1;\n    threadsPerBlock.z = 1;\n\n    blockSize.x = (nDetYMap / maxThreadsPerBlock) + 1;\n    blockSize.y = 1;\n    blockSize.z = 1;\n\n    rot_detector_kernel <<<blockSize, threadsPerBlock>>> (\n        d_pRdetY, d_pRdetZ, d_pDetY, d_pDetZ, isoY, isoZ, phi, nDetYMap);\n\n    threadsPerBlock.x = 16;\n    threadsPerBlock.y = 16;\n    threadsPerBlock.z = 1;\n\n    \n\n    for (int nz = 0; nz < nSlices; nz++) {\n\n      \n\n\n      blockSize.x = (nDetYMap / threadsPerBlock.x) + 1;\n      blockSize.y = (nDetXMap / threadsPerBlock.y) + 1;\n      blockSize.z = 1;\n\n      mapDet2Slice_kernel <<<blockSize, threadsPerBlock>>> (\n          d_pDetmX, d_pDetmY, tubeX, rtubeY, rtubeZ, d_pDetX,\n          d_pRdetY, d_pRdetZ, d_pObjZ, nDetXMap, nDetYMap, nz);\n\n      \n\n\n      blockSize.x = (nPixYMap / threadsPerBlock.x) + 1;\n      blockSize.y = (nPixXMap / threadsPerBlock.y) + 1;\n\n      bilinear_interpolation_kernel <<<blockSize, threadsPerBlock>>> (\n          d_sliceI, d_pProj, d_pObjX, d_pObjY, d_pDetmX_tmp, d_pDetmY,\n          nPixXMap, nPixYMap, nDetXMap, nDetYMap, nDetX, nDetY, p);\n\n      \n\n\n      blockSize.x = (nPixY / threadsPerBlock.x) + 1;\n      blockSize.y = (nPixX / threadsPerBlock.y) + 1;\n\n      differentiation_kernel <<<blockSize, threadsPerBlock>>> (\n          d_pVolume, d_sliceI, tubeX, rtubeY, rtubeZ, d_pObjX, d_pObjY, d_pObjZ,\n          nPixX, nPixY, nPixXMap, nPixYMap, du, dv, dx, dy, dz, nz);\n\n    } \n\n\n  } \n\n\n\n  \n\n  threadsPerBlock.x = 8;\n  threadsPerBlock.y = 8;\n  threadsPerBlock.z = 4;\n\n  blockSize.x = (nPixY / threadsPerBlock.x) + 1;\n  blockSize.y = (nPixX / threadsPerBlock.y) + 1;\n  blockSize.z = (nSlices / threadsPerBlock.z) + 1;\n\n  division_kernel <<<blockSize, threadsPerBlock>>> (d_pVolume, nPixX, nPixY, nSlices, nProj2Run);\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Total kernel execution %f (s)\\n\", time * 1e-9f);\n\n  hipMemcpy(h_pVolume, d_pVolume, nSlices* nPixX * nPixY * sizeof(double), hipMemcpyDeviceToHost);\n\n  hipFree(d_pProj);\n  hipFree(d_sliceI);\n  hipFree(d_pVolume);\n  hipFree(d_pDetX);\n  hipFree(d_pDetY);\n  hipFree(d_pDetZ);\n  hipFree(d_pObjX);\n  hipFree(d_pObjY);\n  hipFree(d_pObjZ);\n  hipFree(d_pDetmY);\n  hipFree(d_pDetmX);\n  hipFree(d_pRdetY);\n  hipFree(d_pRdetZ);\n}\n\nint main() \n{\n                            \n\n  const int nPixX = 1996;   \n\n  const int nPixY = 2457;   \n\n  const int nSlices = 78;  \n\n                            \n\n  const int nDetX = 1664;   \n\n  const int nDetY = 2048;   \n\n\n  const int nProj = 15;     \n\n  const int idXProj = -1;   \n\n\n  const double dx = 0.112;  \n\n  const double dy = 0.112;\n  const double dz = 1.0;\n\n  const double du = 0.14;   \n\n  const double dv = 0.14;\n\n  const double DSD = 700;   \n\n  const double DDR = 0.0;   \n\n  const double DAG = 25.0;  \n\n\n  const size_t pixVol = nPixX * nPixY * nSlices;\n  const size_t detVol = nDetX * nDetY * nProj;\n  double *h_pVolume = (double*) malloc (pixVol * sizeof(double));\n  double *h_pProj = (double*) malloc (detVol * sizeof(double));\n\n  double *h_pTubeAngle = (double*) malloc (nProj * sizeof(double));\n  double *h_pDetAngle = (double*) malloc (nProj * sizeof(double));\n  \n  \n\n  for (int i = 0; i < nProj; i++) \n    h_pTubeAngle[i] = -7.5 + i * 15.0/nProj;\n\n  \n\n  for (int i = 0; i < nProj; i++) \n    h_pDetAngle[i] = -2.1 + i * 4.2/nProj;\n\n  \n\n  srand(123);\n  for (size_t i = 0; i < pixVol; i++) \n    h_pVolume[i] = (double)rand() / (double)RAND_MAX;\n\n  for (size_t i = 0; i < detVol; i++) \n    h_pProj[i] = (double)rand() / (double)RAND_MAX;\n\n  backprojectionDDb(\n    h_pVolume,\n    h_pProj,\n    h_pTubeAngle,\n    h_pDetAngle,\n    idXProj,\n    nProj,\n    nPixX, nPixY,\n    nSlices,\n    nDetX, nDetY,\n    dx, dy, dz,\n    du, dv,\n    DSD, DDR, DAG);\n\n  double checkSum = 0;\n  for (size_t i = 0; i < pixVol; i++)\n    checkSum += h_pVolume[i];\n  printf(\"checksum = %lf\\n\", checkSum);\n\n  free(h_pVolume);\n  free(h_pTubeAngle);\n  free(h_pDetAngle);\n  free(h_pProj);\n  return 0;\n}\n"}}
{"kernel_name": "ddbp", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n#include <omp.h>\n\n\n\n#define BLOCK_SIZE 256\n\n\n\n#define integrateXcoord 1\n#define integrateYcoord 0\n\nvoid pad_projections(\n    double* d_img,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nElem,\n    const int np)\n{\n  for (int gid = 0; gid < nElem; gid++)\n    d_img[(np*nDetYMap *nDetXMap) + (gid*nDetYMap)] = 0;\n}\n\nvoid map_boudaries_kernel(\n    double* d_pBound,\n    const int nElem,\n    const double valueLeftBound,\n    const double sizeElem,\n    const double offset)\n{\n  #pragma omp target teams distribute parallel for thread_limit(BLOCK_SIZE)\n  for (int gid = 0; gid < nElem; gid++)\n    d_pBound[gid] = (gid - valueLeftBound) * sizeElem + offset;\n}\n\nvoid rot_detector_kernel(\n          double* __restrict d_pRdetY,\n          double* __restrict d_pRdetZ,\n    const double* __restrict d_pYcoord,\n    const double* __restrict d_pZcoord,\n    const double yOffset,\n    const double zOffset,\n    const double phi,\n    const int nElem)\n{\n  #pragma omp target teams distribute parallel for thread_limit(BLOCK_SIZE)\n  for (int gid = 0; gid < nElem; gid++) {\n    \n\n    d_pRdetY[gid] = ((d_pYcoord[gid] - yOffset) * cos(phi) - \n                     (d_pZcoord[gid] - zOffset) * sin(phi)) + yOffset;\n    d_pRdetZ[gid] = ((d_pYcoord[gid] - yOffset) * sin(phi) +\n                     (d_pZcoord[gid] - zOffset) * cos(phi)) + zOffset;\n  }\n}\n\nvoid mapDet2Slice_kernel(\n           double* __restrict const pXmapp,\n           double* __restrict const pYmapp,\n    double tubeX,\n    double tubeY,\n    double tubeZ,\n    const double* __restrict const pXcoord,\n    const double* __restrict const pYcoord,\n    const double* __restrict const pZcoord,\n    const double* __restrict const pZSlicecoord,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nz)\n{\n  #pragma omp target teams distribute parallel for collapse(2) thread_limit(BLOCK_SIZE)\n  for (int py = 0; py < nDetXMap; py++) {\n    for (int px = 0; px < nDetYMap; px++) {\n\n      const int pos = py * nDetYMap + px;\n\n      pXmapp[pos] = ((pXcoord[py] - tubeX)*(pZSlicecoord[nz] - pZcoord[px]) - \n          (pXcoord[py] * tubeZ) + (pXcoord[py] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n\n      if (py == 0)\n        pYmapp[px] = ((pYcoord[px] - tubeY)*(pZSlicecoord[nz] - pZcoord[px]) -\n            (pYcoord[px] * tubeZ) + (pYcoord[px] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n    }\n  }\n}\n\nvoid img_integration_kernel(\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const bool direction,\n    const int offsetX,\n    const int offsetY,\n    const int nSlices,\n    const int teams,\n    const int teamX,\n    const int teamY,\n    const int threads,\n    const int threadX,\n    const int threadY,\n    const int threadZ)\n{\n  #pragma omp target teams num_teams(teams) thread_limit(threads)\n  {\n  #pragma omp parallel \n  {\n  const int lx = omp_get_thread_num() % threadX;\n  const int ly = omp_get_thread_num() / threadX % threadY;\n  const int lz = omp_get_thread_num() / (threadX * threadY);\n  const int bx = omp_get_team_num() % teamX;\n  const int by = omp_get_team_num() / teamX % teamY;\n  const int bz = omp_get_team_num() / (teamX * teamY);\n  const int tx = bx * threadX + lx;\n  const int ty = by * threadY + ly;\n  const int px = tx + offsetX;\n  const int py = ty + offsetY;\n  const int pz = bz * threadZ + lz;\n  \n\n\n  if (px < nPixY && py < nPixX && pz < nSlices) {\n    if (direction == integrateXcoord) {\n\n      for (int s = 1; s <= threadY; s *= 2) {\n\n        int spot = ty - s;\n\n        double val = 0;\n\n        if (spot >= 0) {\n          val = d_img[(pz*nPixY*nPixX) + (offsetY + spot) * nPixY + px];\n        }\n\n        if (spot >= 0) {\n          d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n        }\n      }\n    }\n    else\n    {\n      for (int s = 1; s <= threadX; s *= 2) {\n\n        int spot = tx - s;\n\n        double val = 0;\n\n        if (spot >= 0) {\n          val = d_img[(pz*nPixY*nPixX) + py * nPixY + spot + offsetX];\n        }\n\n        if (spot >= 0) {\n          d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n        }\n      }\n    }\n  }\n  }\n  }\n}\n\nvoid bilinear_interpolation_kernel(\n          double* __restrict d_sliceI,\n    const double* __restrict d_pProj,\n    const double* __restrict d_pObjX,\n    const double* __restrict d_pObjY,\n    const double* __restrict d_pDetmX,\n    const double* __restrict d_pDetmY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nDetX,\n    const int nDetY,\n    const int np) \n{\n  #pragma omp target teams distribute parallel for collapse(2) thread_limit(BLOCK_SIZE)\n  for (int py = 0; py < nPixXMap; py++) {\n    for (int px = 0; px < nPixYMap; px++) {\n\n      \n\n\n      \n\n      \n\n      const double xNormData = nDetX - d_pObjX[py] / d_pDetmX[0];\n      const int    xData = floor(xNormData);\n      const double alpha = xNormData - xData;\n\n      \n\n      \n\n      const double yNormData = (d_pObjY[px] / d_pDetmX[0]) - (d_pDetmY[0] / d_pDetmX[0]);\n      const int    yData = floor(yNormData);\n      const double beta = yNormData - yData;\n\n      double d00, d01, d10, d11;\n      if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY)) \n        d00 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData)];\n      else\n        d00 = 0.0;\n\n      if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY))\n        d10 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData)];\n      else\n        d10 = 0.0;\n\n      if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n        d01 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData + 1)];\n      else\n        d01 = 0.0;\n\n      if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n        d11 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData + 1)];\n      else\n        d11 = 0.0;\n\n      double result_temp1 = alpha * d10 + (-d00 * alpha + d00);\n      double result_temp2 = alpha * d11 + (-d01 * alpha + d01);\n\n      d_sliceI[py * nPixYMap + px] = beta * result_temp2 + (-result_temp1 * beta + result_temp1);\n    }\n  }\n}\n\nvoid differentiation_kernel(\n          double* __restrict d_pVolume,\n    const double* __restrict d_sliceI,\n    double tubeX,\n    double rtubeY,\n    double rtubeZ,\n    const double* __restrict const d_pObjX,\n    const double* __restrict const d_pObjY,\n    const double* __restrict const d_pObjZ,\n    const int nPixX,\n    const int nPixY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const double du,\n    const double dv,\n    const double dx,\n    const double dy,\n    const double dz,\n    const int nz) \n{\n  \n\n\n  #pragma omp target teams distribute parallel for collapse(2) thread_limit(BLOCK_SIZE)\n  for (int py = 0; py < nPixX; py++) {\n    for (int px = 0; px < nPixY; px++) {\n\n      const int pos = (nPixX*nPixY*nz) + (py * nPixY) + px;\n\n      int coordA = py * nPixYMap + px;\n      int coordB = ((py + 1) * nPixYMap) + px;\n      int coordC = coordA + 1;\n      int coordD = coordB + 1;\n\n      \n\n      double gamma = atan((d_pObjX[py] + (dx / 2.0) - tubeX) / (rtubeZ - d_pObjZ[nz]));\n\n      \n\n      double alpha = atan((d_pObjY[px] + (dy / 2.0) - rtubeY) / (rtubeZ - d_pObjZ[nz]));\n\n      double dA, dB, dC, dD;\n\n      dA = d_sliceI[coordA];\n      dB = d_sliceI[coordB];\n      dC = d_sliceI[coordC];\n      dD = d_sliceI[coordD];\n\n      \n\n      if (dC == 0 && dD == 0) {\n        dC = dA;\n        dD = dB;\n      }\n\n      \n\n      d_pVolume[pos] += ((dD - dC - dB + dA)*(du*dv*dz / (cos(alpha)*cos(gamma)*dx*dy)));\n    }\n  }\n}\n\nvoid division_kernel(\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const int nSlices,\n    const int nProj)\n{\n  #pragma omp target teams distribute parallel for collapse(3) thread_limit(BLOCK_SIZE)\n  for (int pz = 0; pz < nSlices; pz++) {\n    for (int py = 0; py < nPixX; py++) {\n      for (int px = 0; px < nPixY; px++) {\n        const int pos = (nPixX*nPixY*pz) + (py * nPixY) + px;\n        d_img[pos] /= (double) nProj;\n      }\n    }\n  }\n}\n\n\n\n\nvoid backprojectionDDb(double* const h_pVolume,\n    const double* const h_pProj,\n    const double* const h_pTubeAngle,\n    const double* const h_pDetAngle,\n    const int idXProj,\n    const int nProj,\n    const int nPixX,\n    const int nPixY,\n    const int nSlices,\n    const int nDetX,\n    const int nDetY,\n    const double dx,\n    const double dy,\n    const double dz,\n    const double du,\n    const double dv,\n    const double DSD,\n    const double DDR,\n    const double DAG)\n{\n  \n\n  const int nDetXMap = nDetX + 1;\n  const int nDetYMap = nDetY + 1;\n\n  \n\n  const int nPixXMap = nPixX + 1;\n  const int nPixYMap = nPixY + 1;\n\n  double* d_pProj = (double*) malloc (nDetXMap*nDetYMap*nProj * sizeof(double));\n  double* d_sliceI = (double*) malloc (nPixXMap*nPixYMap * sizeof(double));\n  double* d_pVolume = h_pVolume;\n\n  \n\n\n  \n\n  const double* h_pProj_tmp;\n  double* d_pProj_tmp;\n\n  for (int np = 0; np < nProj; np++) {\n\n    \n\n    pad_projections (d_pProj, nDetXMap, nDetYMap, nDetXMap, np);\n\n    \n\n    d_pProj_tmp = d_pProj + (nDetXMap*nDetYMap*np) + 1;\n    memset(d_pProj_tmp, 0, nPixY * sizeof(double));\n  }\n\n  \n\n  for (int np = 0; np < nProj; np++)\n    for (int c = 0; c < nDetX; c++) {\n      h_pProj_tmp = h_pProj + (c * nDetY) + (nDetX*nDetY*np);\n      d_pProj_tmp = d_pProj + (((c + 1) * nDetYMap) + 1) + (nDetXMap*nDetYMap*np);\n      memcpy(d_pProj_tmp, h_pProj_tmp, nDetY * sizeof(double));\n    }\n\n  \n\n  double* d_pDetX = (double*) malloc (nDetXMap * sizeof(double));\n  double* d_pDetY = (double*) malloc (nDetYMap * sizeof(double));\n  double* d_pDetZ = (double*) malloc (nDetYMap * sizeof(double));\n  double* d_pObjX = (double*) malloc (nPixXMap * sizeof(double));\n  double* d_pObjY = (double*) malloc (nPixYMap * sizeof(double));\n  double* d_pObjZ = (double*) malloc (nSlices * sizeof(double));\n\n  \n\n  double* d_pDetmY = (double*) malloc (nDetYMap * sizeof(double));\n  double* d_pDetmX = (double*) malloc (nDetYMap * nDetXMap * sizeof(double));\n\n  \n\n  double* d_pRdetY = (double*) malloc (nDetYMap * sizeof(double));\n  double* d_pRdetZ = (double*) malloc (nDetYMap * sizeof(double));\n\n  \n\n  #pragma omp target data map (to: d_pProj[0:nDetXMap*nDetYMap*nProj]) \\\n                          map (from: d_pVolume[0:nPixX*nPixY*nSlices]) \\\n                          map (alloc: d_sliceI[0:nPixXMap*nPixYMap],\\\n                                      d_pDetX[0:nDetXMap],\\\n                                      d_pDetY[0:nDetYMap],\\\n                                      d_pDetZ[0:nDetYMap],\\\n                                      d_pObjX[0:nPixXMap],\\\n                                      d_pObjY[0:nPixYMap],\\\n                                      d_pObjZ[0:nSlices],\\\n                                      d_pDetmY[0:nDetYMap],\\\n                                      d_pDetmX[0:nDetYMap*nDetXMap],\\\n                                      d_pRdetY[0:nDetYMap],\\\n                                      d_pRdetZ[0:nDetYMap])\n  {\n\n  auto start = std::chrono::steady_clock::now();\n\n  map_boudaries_kernel(d_pDetX, nDetXMap, (double)nDetX, -du, 0.0);\n\n  map_boudaries_kernel(d_pDetY, nDetYMap, nDetY / 2.0, dv, 0.0);\n\n  map_boudaries_kernel(d_pDetZ, nDetYMap, 0.0, 0.0, 0.0);\n\n  map_boudaries_kernel(d_pObjX, nPixXMap, (double)nPixX, -dx, 0.0);\n\n  map_boudaries_kernel(d_pObjY, nPixYMap, nPixY / 2.0, dy, 0.0);\n\n  map_boudaries_kernel(d_pObjZ, nSlices, 0.0, dz, DAG + (dz / 2.0));\n\n  #pragma omp target teams distribute parallel for thread_limit(BLOCK_SIZE)\n  for (int i = 0; i < nPixX * nPixY * nSlices; i++)\n    d_pVolume[i] = 0.0;\n\n  \n\n  double tubeX = 0;\n  double tubeY = 0;\n  double tubeZ = DSD;\n\n  \n\n  double isoY = 0;\n  double isoZ = DDR;\n\n  int threadX = 8;\n  int threadY = 4;\n  int threadZ = 8;\n  int threads = threadX * threadY * threadZ;\n\n  int teamX = (int)ceilf((float)nDetYMap / (threadX - 1));\n  int teamY = 1;\n  int teamZ = (int)ceilf((float)nProj / threadZ);\n  int teams = teamX * teamY * teamZ;\n  \n\n  \n\n\n  \n\n  int Xk = (int)ceilf((float)nDetXMap / (threadX - 1));\n  for (int k = 0; k < Xk; k++) {\n    img_integration_kernel(\n        d_pProj, nDetXMap, nDetYMap, integrateXcoord, 0, k * 9, nProj,\n        teams, teamX, teamY, threads, threadX, threadY, threadZ);\n  }\n\n  threadX = 4;\n  threadY = 8;\n  threadZ = 8;\n  threads = threadX * threadY * threadZ;\n\n  teamX = 1;\n  teamY = (int)ceilf((float)nDetXMap / (threadY - 1));\n  teamZ = (int)ceilf((float)nProj / threadZ);\n  teams = teamX * teamY * teamZ;\n\n  \n\n  int Yk = (int)ceilf((float)nDetYMap / (threadY - 1));\n  for (int k = 0; k < Yk; k++) {\n    img_integration_kernel(\n        d_pProj, nDetXMap, nDetYMap, integrateYcoord, k * 9, 0, nProj,\n        teams, teamX, teamY, threads, threadX, threadY, threadZ);\n  }\n\n  double* d_pDetmX_tmp = d_pDetmX + (nDetYMap * (nDetXMap-2));\n\n  \n\n  int projIni, projEnd, nProj2Run;\n  if (idXProj == -1) {\n    projIni = 0;\n    projEnd = nProj;\n    nProj2Run = nProj;\n  }\n  else {\n    projIni = idXProj;\n    projEnd = idXProj + 1;\n    nProj2Run = 1;\n  }\n\n  \n\n  for (int p = projIni; p < projEnd; p++) {\n\n    \n\n    double theta = h_pTubeAngle[p] * M_PI / 180.0;\n\n    \n\n    double phi = h_pDetAngle[p] * M_PI / 180.0;\n\n    \n\n\n    \n\n    double rtubeY = ((tubeY - isoY)*cos(theta) - (tubeZ - isoZ)*sin(theta)) + isoY;\n    double rtubeZ = ((tubeY - isoY)*sin(theta) + (tubeZ - isoZ)*cos(theta)) + isoZ;\n\n    \n\n\n    \n\n    rot_detector_kernel(\n        d_pRdetY, d_pRdetZ, d_pDetY, d_pDetZ, isoY, isoZ, phi, nDetYMap);\n\n    \n\n    for (int nz = 0; nz < nSlices; nz++) {\n\n      \n\n\n      mapDet2Slice_kernel(\n          d_pDetmX, d_pDetmY, tubeX, rtubeY, rtubeZ, d_pDetX,\n          d_pRdetY, d_pRdetZ, d_pObjZ, nDetXMap, nDetYMap, nz);\n\n      \n\n\n      bilinear_interpolation_kernel(\n          d_sliceI, d_pProj, d_pObjX, d_pObjY, d_pDetmX_tmp, d_pDetmY,\n          nPixXMap, nPixYMap, nDetXMap, nDetYMap, nDetX, nDetY, p);\n\n      \n\n\n      differentiation_kernel(\n          d_pVolume, d_sliceI, tubeX, rtubeY, rtubeZ, d_pObjX, d_pObjY, d_pObjZ,\n          nPixX, nPixY, nPixXMap, nPixYMap, du, dv, dx, dy, dz, nz);\n\n    } \n\n\n  } \n\n\n  \n\n  division_kernel(d_pVolume, nPixX, nPixY, nSlices, nProj2Run);\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Total kernel execution %f (s)\\n\", time * 1e-9f);\n\n  }\n\n  free(d_pProj);\n  free(d_pDetX);\n  free(d_pDetY);\n  free(d_pDetZ);\n  free(d_pObjX);\n  free(d_pObjY);\n  free(d_pObjZ);\n  free(d_pDetmY);\n  free(d_pDetmX);\n  free(d_pRdetY);\n  free(d_pRdetZ);\n}\n\nint main() \n{\n                            \n\n  const int nPixX = 1996;   \n\n  const int nPixY = 2457;   \n\n  const int nSlices = 78;  \n\n                            \n\n  const int nDetX = 1664;   \n\n  const int nDetY = 2048;   \n\n\n  const int nProj = 15;     \n\n  const int idXProj = -1;   \n\n\n  const double dx = 0.112;  \n\n  const double dy = 0.112;\n  const double dz = 1.0;\n\n  const double du = 0.14;   \n\n  const double dv = 0.14;\n\n  const double DSD = 700;   \n\n  const double DDR = 0.0;   \n\n  const double DAG = 25.0;  \n\n\n  const size_t pixVol = nPixX * nPixY * nSlices;\n  const size_t detVol = nDetX * nDetY * nProj;\n  double *h_pVolume = (double*) malloc (pixVol * sizeof(double));\n  double *h_pProj = (double*) malloc (detVol * sizeof(double));\n\n  double *h_pTubeAngle = (double*) malloc (nProj * sizeof(double));\n  double *h_pDetAngle = (double*) malloc (nProj * sizeof(double));\n  \n  \n\n  for (int i = 0; i < nProj; i++) \n    h_pTubeAngle[i] = -7.5 + i * 15.0/nProj;\n\n  \n\n  for (int i = 0; i < nProj; i++) \n    h_pDetAngle[i] = -2.1 + i * 4.2/nProj;\n\n  \n\n  srand(123);\n  for (size_t i = 0; i < pixVol; i++) \n    h_pVolume[i] = (double)rand() / (double)RAND_MAX;\n\n  for (size_t i = 0; i < detVol; i++) \n    h_pProj[i] = (double)rand() / (double)RAND_MAX;\n\n  backprojectionDDb(\n    h_pVolume,\n    h_pProj,\n    h_pTubeAngle,\n    h_pDetAngle,\n    idXProj,\n    nProj,\n    nPixX, nPixY,\n    nSlices,\n    nDetX, nDetY,\n    dx, dy, dz,\n    du, dv,\n    DSD, DDR, DAG);\n\n  double checkSum = 0;\n  for (size_t i = 0; i < pixVol; i++)\n    checkSum += h_pVolume[i];\n  printf(\"checksum = %lf\\n\", checkSum);\n\n  free(h_pVolume);\n  free(h_pTubeAngle);\n  free(h_pDetAngle);\n  free(h_pProj);\n  return 0;\n}\n"}}
{"kernel_name": "ddbp", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n\n\n\n#define BLOCK_SIZE 256\n\n\n\n#define integrateXcoord 1\n#define integrateYcoord 0\n\nvoid pad_projections(\n    double* d_img,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nElem,\n    const int np)\n{\n  for (int gid = 0; gid < nElem; gid++)\n    d_img[(np*nDetYMap *nDetXMap) + (gid*nDetYMap)] = 0;\n}\n\nvoid map_boudaries_kernel(\n    double* d_pBound,\n    const int nElem,\n    const double valueLeftBound,\n    const double sizeElem,\n    const double offset)\n{\n    for (int gid = 0; gid < nElem; gid++)\n    d_pBound[gid] = (gid - valueLeftBound) * sizeElem + offset;\n}\n\nvoid rot_detector_kernel(\n          double* __restrict d_pRdetY,\n          double* __restrict d_pRdetZ,\n    const double* __restrict d_pYcoord,\n    const double* __restrict d_pZcoord,\n    const double yOffset,\n    const double zOffset,\n    const double phi,\n    const int nElem)\n{\n    for (int gid = 0; gid < nElem; gid++) {\n    \n\n    d_pRdetY[gid] = ((d_pYcoord[gid] - yOffset) * cos(phi) - \n                     (d_pZcoord[gid] - zOffset) * sin(phi)) + yOffset;\n    d_pRdetZ[gid] = ((d_pYcoord[gid] - yOffset) * sin(phi) +\n                     (d_pZcoord[gid] - zOffset) * cos(phi)) + zOffset;\n  }\n}\n\nvoid mapDet2Slice_kernel(\n           double* __restrict const pXmapp,\n           double* __restrict const pYmapp,\n    double tubeX,\n    double tubeY,\n    double tubeZ,\n    const double* __restrict const pXcoord,\n    const double* __restrict const pYcoord,\n    const double* __restrict const pZcoord,\n    const double* __restrict const pZSlicecoord,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nz)\n{\n    for (int py = 0; py < nDetXMap; py++) {\n    for (int px = 0; px < nDetYMap; px++) {\n\n      const int pos = py * nDetYMap + px;\n\n      pXmapp[pos] = ((pXcoord[py] - tubeX)*(pZSlicecoord[nz] - pZcoord[px]) - \n          (pXcoord[py] * tubeZ) + (pXcoord[py] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n\n      if (py == 0)\n        pYmapp[px] = ((pYcoord[px] - tubeY)*(pZSlicecoord[nz] - pZcoord[px]) -\n            (pYcoord[px] * tubeZ) + (pYcoord[px] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n    }\n  }\n}\n\nvoid img_integration_kernel(\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const bool direction,\n    const int offsetX,\n    const int offsetY,\n    const int nSlices,\n    const int teams,\n    const int teamX,\n    const int teamY,\n    const int threads,\n    const int threadX,\n    const int threadY,\n    const int threadZ)\n{\n    {\n    {\n  const int lx = omp_get_thread_num() % threadX;\n  const int ly = omp_get_thread_num() / threadX % threadY;\n  const int lz = omp_get_thread_num() / (threadX * threadY);\n  const int bx = omp_get_team_num() % teamX;\n  const int by = omp_get_team_num() / teamX % teamY;\n  const int bz = omp_get_team_num() / (teamX * teamY);\n  const int tx = bx * threadX + lx;\n  const int ty = by * threadY + ly;\n  const int px = tx + offsetX;\n  const int py = ty + offsetY;\n  const int pz = bz * threadZ + lz;\n  \n\n\n  if (px < nPixY && py < nPixX && pz < nSlices) {\n    if (direction == integrateXcoord) {\n\n      for (int s = 1; s <= threadY; s *= 2) {\n\n        int spot = ty - s;\n\n        double val = 0;\n\n        if (spot >= 0) {\n          val = d_img[(pz*nPixY*nPixX) + (offsetY + spot) * nPixY + px];\n        }\n\n        if (spot >= 0) {\n          d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n        }\n      }\n    }\n    else\n    {\n      for (int s = 1; s <= threadX; s *= 2) {\n\n        int spot = tx - s;\n\n        double val = 0;\n\n        if (spot >= 0) {\n          val = d_img[(pz*nPixY*nPixX) + py * nPixY + spot + offsetX];\n        }\n\n        if (spot >= 0) {\n          d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n        }\n      }\n    }\n  }\n  }\n  }\n}\n\nvoid bilinear_interpolation_kernel(\n          double* __restrict d_sliceI,\n    const double* __restrict d_pProj,\n    const double* __restrict d_pObjX,\n    const double* __restrict d_pObjY,\n    const double* __restrict d_pDetmX,\n    const double* __restrict d_pDetmY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nDetX,\n    const int nDetY,\n    const int np) \n{\n    for (int py = 0; py < nPixXMap; py++) {\n    for (int px = 0; px < nPixYMap; px++) {\n\n      \n\n\n      \n\n      \n\n      const double xNormData = nDetX - d_pObjX[py] / d_pDetmX[0];\n      const int    xData = floor(xNormData);\n      const double alpha = xNormData - xData;\n\n      \n\n      \n\n      const double yNormData = (d_pObjY[px] / d_pDetmX[0]) - (d_pDetmY[0] / d_pDetmX[0]);\n      const int    yData = floor(yNormData);\n      const double beta = yNormData - yData;\n\n      double d00, d01, d10, d11;\n      if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY)) \n        d00 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData)];\n      else\n        d00 = 0.0;\n\n      if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY))\n        d10 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData)];\n      else\n        d10 = 0.0;\n\n      if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n        d01 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData + 1)];\n      else\n        d01 = 0.0;\n\n      if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n        d11 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData + 1)];\n      else\n        d11 = 0.0;\n\n      double result_temp1 = alpha * d10 + (-d00 * alpha + d00);\n      double result_temp2 = alpha * d11 + (-d01 * alpha + d01);\n\n      d_sliceI[py * nPixYMap + px] = beta * result_temp2 + (-result_temp1 * beta + result_temp1);\n    }\n  }\n}\n\nvoid differentiation_kernel(\n          double* __restrict d_pVolume,\n    const double* __restrict d_sliceI,\n    double tubeX,\n    double rtubeY,\n    double rtubeZ,\n    const double* __restrict const d_pObjX,\n    const double* __restrict const d_pObjY,\n    const double* __restrict const d_pObjZ,\n    const int nPixX,\n    const int nPixY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const double du,\n    const double dv,\n    const double dx,\n    const double dy,\n    const double dz,\n    const int nz) \n{\n  \n\n\n    for (int py = 0; py < nPixX; py++) {\n    for (int px = 0; px < nPixY; px++) {\n\n      const int pos = (nPixX*nPixY*nz) + (py * nPixY) + px;\n\n      int coordA = py * nPixYMap + px;\n      int coordB = ((py + 1) * nPixYMap) + px;\n      int coordC = coordA + 1;\n      int coordD = coordB + 1;\n\n      \n\n      double gamma = atan((d_pObjX[py] + (dx / 2.0) - tubeX) / (rtubeZ - d_pObjZ[nz]));\n\n      \n\n      double alpha = atan((d_pObjY[px] + (dy / 2.0) - rtubeY) / (rtubeZ - d_pObjZ[nz]));\n\n      double dA, dB, dC, dD;\n\n      dA = d_sliceI[coordA];\n      dB = d_sliceI[coordB];\n      dC = d_sliceI[coordC];\n      dD = d_sliceI[coordD];\n\n      \n\n      if (dC == 0 && dD == 0) {\n        dC = dA;\n        dD = dB;\n      }\n\n      \n\n      d_pVolume[pos] += ((dD - dC - dB + dA)*(du*dv*dz / (cos(alpha)*cos(gamma)*dx*dy)));\n    }\n  }\n}\n\nvoid division_kernel(\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const int nSlices,\n    const int nProj)\n{\n    for (int pz = 0; pz < nSlices; pz++) {\n    for (int py = 0; py < nPixX; py++) {\n      for (int px = 0; px < nPixY; px++) {\n        const int pos = (nPixX*nPixY*pz) + (py * nPixY) + px;\n        d_img[pos] /= (double) nProj;\n      }\n    }\n  }\n}\n\n\n\n\nvoid backprojectionDDb(double* const h_pVolume,\n    const double* const h_pProj,\n    const double* const h_pTubeAngle,\n    const double* const h_pDetAngle,\n    const int idXProj,\n    const int nProj,\n    const int nPixX,\n    const int nPixY,\n    const int nSlices,\n    const int nDetX,\n    const int nDetY,\n    const double dx,\n    const double dy,\n    const double dz,\n    const double du,\n    const double dv,\n    const double DSD,\n    const double DDR,\n    const double DAG)\n{\n  \n\n  const int nDetXMap = nDetX + 1;\n  const int nDetYMap = nDetY + 1;\n\n  \n\n  const int nPixXMap = nPixX + 1;\n  const int nPixYMap = nPixY + 1;\n\n  double* d_pProj = (double*) malloc (nDetXMap*nDetYMap*nProj * sizeof(double));\n  double* d_sliceI = (double*) malloc (nPixXMap*nPixYMap * sizeof(double));\n  double* d_pVolume = h_pVolume;\n\n  \n\n\n  \n\n  const double* h_pProj_tmp;\n  double* d_pProj_tmp;\n\n  for (int np = 0; np < nProj; np++) {\n\n    \n\n    pad_projections (d_pProj, nDetXMap, nDetYMap, nDetXMap, np);\n\n    \n\n    d_pProj_tmp = d_pProj + (nDetXMap*nDetYMap*np) + 1;\n    memset(d_pProj_tmp, 0, nPixY * sizeof(double));\n  }\n\n  \n\n  for (int np = 0; np < nProj; np++)\n    for (int c = 0; c < nDetX; c++) {\n      h_pProj_tmp = h_pProj + (c * nDetY) + (nDetX*nDetY*np);\n      d_pProj_tmp = d_pProj + (((c + 1) * nDetYMap) + 1) + (nDetXMap*nDetYMap*np);\n      memcpy(d_pProj_tmp, h_pProj_tmp, nDetY * sizeof(double));\n    }\n\n  \n\n  double* d_pDetX = (double*) malloc (nDetXMap * sizeof(double));\n  double* d_pDetY = (double*) malloc (nDetYMap * sizeof(double));\n  double* d_pDetZ = (double*) malloc (nDetYMap * sizeof(double));\n  double* d_pObjX = (double*) malloc (nPixXMap * sizeof(double));\n  double* d_pObjY = (double*) malloc (nPixYMap * sizeof(double));\n  double* d_pObjZ = (double*) malloc (nSlices * sizeof(double));\n\n  \n\n  double* d_pDetmY = (double*) malloc (nDetYMap * sizeof(double));\n  double* d_pDetmX = (double*) malloc (nDetYMap * nDetXMap * sizeof(double));\n\n  \n\n  double* d_pRdetY = (double*) malloc (nDetYMap * sizeof(double));\n  double* d_pRdetZ = (double*) malloc (nDetYMap * sizeof(double));\n\n  \n\n    {\n\n  auto start = std::chrono::steady_clock::now();\n\n  map_boudaries_kernel(d_pDetX, nDetXMap, (double)nDetX, -du, 0.0);\n\n  map_boudaries_kernel(d_pDetY, nDetYMap, nDetY / 2.0, dv, 0.0);\n\n  map_boudaries_kernel(d_pDetZ, nDetYMap, 0.0, 0.0, 0.0);\n\n  map_boudaries_kernel(d_pObjX, nPixXMap, (double)nPixX, -dx, 0.0);\n\n  map_boudaries_kernel(d_pObjY, nPixYMap, nPixY / 2.0, dy, 0.0);\n\n  map_boudaries_kernel(d_pObjZ, nSlices, 0.0, dz, DAG + (dz / 2.0));\n\n    for (int i = 0; i < nPixX * nPixY * nSlices; i++)\n    d_pVolume[i] = 0.0;\n\n  \n\n  double tubeX = 0;\n  double tubeY = 0;\n  double tubeZ = DSD;\n\n  \n\n  double isoY = 0;\n  double isoZ = DDR;\n\n  int threadX = 8;\n  int threadY = 4;\n  int threadZ = 8;\n  int threads = threadX * threadY * threadZ;\n\n  int teamX = (int)ceilf((float)nDetYMap / (threadX - 1));\n  int teamY = 1;\n  int teamZ = (int)ceilf((float)nProj / threadZ);\n  int teams = teamX * teamY * teamZ;\n  \n\n  \n\n\n  \n\n  int Xk = (int)ceilf((float)nDetXMap / (threadX - 1));\n  for (int k = 0; k < Xk; k++) {\n    img_integration_kernel(\n        d_pProj, nDetXMap, nDetYMap, integrateXcoord, 0, k * 9, nProj,\n        teams, teamX, teamY, threads, threadX, threadY, threadZ);\n  }\n\n  threadX = 4;\n  threadY = 8;\n  threadZ = 8;\n  threads = threadX * threadY * threadZ;\n\n  teamX = 1;\n  teamY = (int)ceilf((float)nDetXMap / (threadY - 1));\n  teamZ = (int)ceilf((float)nProj / threadZ);\n  teams = teamX * teamY * teamZ;\n\n  \n\n  int Yk = (int)ceilf((float)nDetYMap / (threadY - 1));\n  for (int k = 0; k < Yk; k++) {\n    img_integration_kernel(\n        d_pProj, nDetXMap, nDetYMap, integrateYcoord, k * 9, 0, nProj,\n        teams, teamX, teamY, threads, threadX, threadY, threadZ);\n  }\n\n  double* d_pDetmX_tmp = d_pDetmX + (nDetYMap * (nDetXMap-2));\n\n  \n\n  int projIni, projEnd, nProj2Run;\n  if (idXProj == -1) {\n    projIni = 0;\n    projEnd = nProj;\n    nProj2Run = nProj;\n  }\n  else {\n    projIni = idXProj;\n    projEnd = idXProj + 1;\n    nProj2Run = 1;\n  }\n\n  \n\n  for (int p = projIni; p < projEnd; p++) {\n\n    \n\n    double theta = h_pTubeAngle[p] * M_PI / 180.0;\n\n    \n\n    double phi = h_pDetAngle[p] * M_PI / 180.0;\n\n    \n\n\n    \n\n    double rtubeY = ((tubeY - isoY)*cos(theta) - (tubeZ - isoZ)*sin(theta)) + isoY;\n    double rtubeZ = ((tubeY - isoY)*sin(theta) + (tubeZ - isoZ)*cos(theta)) + isoZ;\n\n    \n\n\n    \n\n    rot_detector_kernel(\n        d_pRdetY, d_pRdetZ, d_pDetY, d_pDetZ, isoY, isoZ, phi, nDetYMap);\n\n    \n\n    for (int nz = 0; nz < nSlices; nz++) {\n\n      \n\n\n      mapDet2Slice_kernel(\n          d_pDetmX, d_pDetmY, tubeX, rtubeY, rtubeZ, d_pDetX,\n          d_pRdetY, d_pRdetZ, d_pObjZ, nDetXMap, nDetYMap, nz);\n\n      \n\n\n      bilinear_interpolation_kernel(\n          d_sliceI, d_pProj, d_pObjX, d_pObjY, d_pDetmX_tmp, d_pDetmY,\n          nPixXMap, nPixYMap, nDetXMap, nDetYMap, nDetX, nDetY, p);\n\n      \n\n\n      differentiation_kernel(\n          d_pVolume, d_sliceI, tubeX, rtubeY, rtubeZ, d_pObjX, d_pObjY, d_pObjZ,\n          nPixX, nPixY, nPixXMap, nPixYMap, du, dv, dx, dy, dz, nz);\n\n    } \n\n\n  } \n\n\n  \n\n  division_kernel(d_pVolume, nPixX, nPixY, nSlices, nProj2Run);\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Total kernel execution %f (s)\\n\", time * 1e-9f);\n\n  }\n\n  free(d_pProj);\n  free(d_pDetX);\n  free(d_pDetY);\n  free(d_pDetZ);\n  free(d_pObjX);\n  free(d_pObjY);\n  free(d_pObjZ);\n  free(d_pDetmY);\n  free(d_pDetmX);\n  free(d_pRdetY);\n  free(d_pRdetZ);\n}\n\nint main() \n{\n                            \n\n  const int nPixX = 1996;   \n\n  const int nPixY = 2457;   \n\n  const int nSlices = 78;  \n\n                            \n\n  const int nDetX = 1664;   \n\n  const int nDetY = 2048;   \n\n\n  const int nProj = 15;     \n\n  const int idXProj = -1;   \n\n\n  const double dx = 0.112;  \n\n  const double dy = 0.112;\n  const double dz = 1.0;\n\n  const double du = 0.14;   \n\n  const double dv = 0.14;\n\n  const double DSD = 700;   \n\n  const double DDR = 0.0;   \n\n  const double DAG = 25.0;  \n\n\n  const size_t pixVol = nPixX * nPixY * nSlices;\n  const size_t detVol = nDetX * nDetY * nProj;\n  double *h_pVolume = (double*) malloc (pixVol * sizeof(double));\n  double *h_pProj = (double*) malloc (detVol * sizeof(double));\n\n  double *h_pTubeAngle = (double*) malloc (nProj * sizeof(double));\n  double *h_pDetAngle = (double*) malloc (nProj * sizeof(double));\n  \n  \n\n  for (int i = 0; i < nProj; i++) \n    h_pTubeAngle[i] = -7.5 + i * 15.0/nProj;\n\n  \n\n  for (int i = 0; i < nProj; i++) \n    h_pDetAngle[i] = -2.1 + i * 4.2/nProj;\n\n  \n\n  srand(123);\n  for (size_t i = 0; i < pixVol; i++) \n    h_pVolume[i] = (double)rand() / (double)RAND_MAX;\n\n  for (size_t i = 0; i < detVol; i++) \n    h_pProj[i] = (double)rand() / (double)RAND_MAX;\n\n  backprojectionDDb(\n    h_pVolume,\n    h_pProj,\n    h_pTubeAngle,\n    h_pDetAngle,\n    idXProj,\n    nProj,\n    nPixX, nPixY,\n    nSlices,\n    nDetX, nDetY,\n    dx, dy, dz,\n    du, dv,\n    DSD, DDR, DAG);\n\n  double checkSum = 0;\n  for (size_t i = 0; i < pixVol; i++)\n    checkSum += h_pVolume[i];\n  printf(\"checksum = %lf\\n\", checkSum);\n\n  free(h_pVolume);\n  free(h_pTubeAngle);\n  free(h_pDetAngle);\n  free(h_pProj);\n  return 0;\n}"}}
{"kernel_name": "ddbp", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n\nusing namespace sycl;\n\n\n\n#define BLOCK_SIZE 256\n\n\n\n#define integrateXcoord 1\n#define integrateYcoord 0\n\nvoid pad_projections_kernel(\n    nd_item<3> &item,\n    double* d_img,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nElem,\n    const int np)\n{\n  const int gid = item.get_global_id(2);\n  if (gid < nElem)\n    d_img[(np*nDetYMap *nDetXMap) + (gid*nDetYMap)] = 0;\n}\n\nvoid map_boudaries_kernel(\n    nd_item<3> &item,\n    double* d_pBound,\n    const int nElem,\n    const double valueLeftBound,\n    const double sizeElem,\n    const double offset)\n{\n  const int gid = item.get_global_id(2);\n  if (gid < nElem)\n    d_pBound[gid] = (gid - valueLeftBound) * sizeElem + offset;\n}\n\nvoid rot_detector_kernel(\n    nd_item<3> &item,\n          double* __restrict d_pRdetY,\n          double* __restrict d_pRdetZ,\n    const double* __restrict d_pYcoord,\n    const double* __restrict d_pZcoord,\n    const double yOffset,\n    const double zOffset,\n    const double phi,\n    const int nElem)\n{\n  const int gid = item.get_global_id(2);\n  if (gid < nElem) {\n    \n\n    d_pRdetY[gid] = ((d_pYcoord[gid] - yOffset) * sycl::cos(phi) - \n                     (d_pZcoord[gid] - zOffset) * sycl::sin(phi)) + yOffset;\n    d_pRdetZ[gid] = ((d_pYcoord[gid] - yOffset) * sycl::sin(phi) +\n                     (d_pZcoord[gid] - zOffset) * sycl::cos(phi)) + zOffset;\n  }\n}\n\nvoid mapDet2Slice_kernel(\n    nd_item<3> &item,\n           double* __restrict const pXmapp,\n           double* __restrict const pYmapp,\n    double tubeX,\n    double tubeY,\n    double tubeZ,\n    const double* __restrict const pXcoord,\n    const double* __restrict const pYcoord,\n    const double* __restrict const pZcoord,\n    const double* __restrict const pZSlicecoord,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nz)\n{\n  const int px = item.get_global_id(2);\n  const int py = item.get_global_id(1);\n\n  if (px < nDetYMap && py < nDetXMap) {\n\n    const int pos = py * nDetYMap + px;\n\n    pXmapp[pos] = ((pXcoord[py] - tubeX)*(pZSlicecoord[nz] - pZcoord[px]) - \n        (pXcoord[py] * tubeZ) + (pXcoord[py] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n\n    if (py == 0)\n      pYmapp[px] = ((pYcoord[px] - tubeY)*(pZSlicecoord[nz] - pZcoord[px]) -\n          (pYcoord[px] * tubeZ) + (pYcoord[px] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n  }\n}\n\nvoid img_integration_kernel(\n    nd_item<3> &item,\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const bool direction,\n    const int offsetX,\n    const int offsetY,\n    const int nSlices)\n{\n  \n\n\n  const int tx = item.get_global_id(2);\n  const int ty = item.get_global_id(1);\n  const int px = tx + offsetX;\n  const int py = ty + offsetY;\n  const int pz = item.get_global_id(0);\n\n  if (px >= nPixY || py >= nPixX || pz >= nSlices) return;\n\n  if (direction == integrateXcoord) {\n\n    for (int s = 1; s <= item.get_local_range(1); s *= 2) {\n\n      int spot = ty - s;\n\n      double val = 0;\n\n      if (spot >= 0) {\n        val = d_img[(pz*nPixY*nPixX) + (offsetY + spot) * nPixY + px];\n      }\n\n      if (spot >= 0) {\n        d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n      }\n    }\n  }\n  else\n  {\n    for (int s = 1; s <= item.get_local_range(2); s *= 2) {\n\n      int spot = tx - s;\n\n      double val = 0;\n\n      if (spot >= 0) {\n        val = d_img[(pz*nPixY*nPixX) + py * nPixY + spot + offsetX];\n      }\n\n      if (spot >= 0) {\n        d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n      }\n    }\n  }\n}\n\nvoid bilinear_interpolation_kernel(\n    nd_item<3> &item,\n          double* __restrict d_sliceI,\n    const double* __restrict d_pProj,\n    const double* __restrict d_pObjX,\n    const double* __restrict d_pObjY,\n    const double* __restrict d_pDetmX,\n    const double* __restrict d_pDetmY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nDetX,\n    const int nDetY,\n    const int np) \n{\n  const int px = item.get_global_id(2);\n  const int py = item.get_global_id(1);\n\n  \n\n  \n\n  if (px >= nPixYMap || py >= nPixXMap) return;\n\n  \n\n\n  \n\n  \n\n  const double xNormData = nDetX - d_pObjX[py] / d_pDetmX[0];\n  const int    xData = sycl::floor(xNormData);\n  const double alpha = xNormData - xData;\n\n  \n\n  \n\n  const double yNormData = (d_pObjY[px] / d_pDetmX[0]) - (d_pDetmY[0] / d_pDetmX[0]);\n  const int    yData = sycl::floor(yNormData);\n  const double beta = yNormData - yData;\n\n  double d00, d01, d10, d11;\n  if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY)) \n    d00 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData)];\n  else\n    d00 = 0.0;\n\n  if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY))\n    d10 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData)];\n  else\n    d10 = 0.0;\n\n  if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n    d01 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData + 1)];\n  else\n    d01 = 0.0;\n\n  if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n    d11 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData + 1)];\n  else\n    d11 = 0.0;\n\n  double result_temp1 = alpha * d10 + (-d00 * alpha + d00);\n  double result_temp2 = alpha * d11 + (-d01 * alpha + d01);\n\n  d_sliceI[py * nPixYMap + px] = beta * result_temp2 + (-result_temp1 * beta + result_temp1);\n}\n\nvoid differentiation_kernel(\n    nd_item<3> &item,\n          double* __restrict d_pVolume,\n    const double* __restrict d_sliceI,\n    double tubeX,\n    double rtubeY,\n    double rtubeZ,\n    const double* __restrict const d_pObjX,\n    const double* __restrict const d_pObjY,\n    const double* __restrict const d_pObjZ,\n    const int nPixX,\n    const int nPixY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const double du,\n    const double dv,\n    const double dx,\n    const double dy,\n    const double dz,\n    const int nz) \n{\n  const int px = item.get_global_id(2);\n  const int py = item.get_global_id(1);\n\n  \n\n\n  if (px < nPixY && py < nPixX) {\n\n    const int pos = (nPixX*nPixY*nz) + (py * nPixY) + px;\n\n    int coordA = py * nPixYMap + px;\n    int coordB = ((py + 1) * nPixYMap) + px;\n    int coordC = coordA + 1;\n    int coordD = coordB + 1;\n\n    \n\n    double gamma = sycl::atan((d_pObjX[py] + (dx / 2.0) - tubeX) / (rtubeZ - d_pObjZ[nz]));\n\n    \n\n    double alpha = sycl::atan((d_pObjY[px] + (dy / 2.0) - rtubeY) / (rtubeZ - d_pObjZ[nz]));\n\n    double dA, dB, dC, dD;\n\n    dA = d_sliceI[coordA];\n    dB = d_sliceI[coordB];\n    dC = d_sliceI[coordC];\n    dD = d_sliceI[coordD];\n\n    \n\n    if (dC == 0 && dD == 0) {\n      dC = dA;\n      dD = dB;\n    }\n\n    \n\n    d_pVolume[pos] += ((dD - dC - dB + dA)*(du*dv*dz / \n                       (sycl::cos(alpha)*sycl::cos(gamma)*dx*dy)));\n  }\n}\n\nvoid division_kernel(\n    nd_item<3> &item,\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const int nSlices,\n    const int nProj)\n{\n  const int px = item.get_global_id(2);\n  const int py = item.get_global_id(1);\n  const int pz = item.get_global_id(0);\n  if (px < nPixY && py < nPixX && pz < nSlices) {\n    const int pos = (nPixX*nPixY*pz) + (py * nPixY) + px;\n    d_img[pos] /= (double) nProj;\n  }\n}\n\n\n\nvoid backprojectionDDb(\n          double* const h_pVolume,\n    const double* const h_pProj,\n    const double* const h_pTubeAngle,\n    const double* const h_pDetAngle,\n    const int idXProj,\n    const int nProj,\n    const int nPixX,\n    const int nPixY,\n    const int nSlices,\n    const int nDetX,\n    const int nDetY,\n    const double dx,\n    const double dy,\n    const double dz,\n    const double du,\n    const double dv,\n    const double DSD,\n    const double DDR,\n    const double DAG)\n{\n  \n\n  const int nDetXMap = nDetX + 1;\n  const int nDetYMap = nDetY + 1;\n\n  \n\n  const int nPixXMap = nPixX + 1;\n  const int nPixYMap = nPixY + 1;\n\n#ifdef USE_GPU\n  queue q(gpu_selector_v, property::queue::in_order());\n#else\n  queue q(cpu_selector_v, property::queue::in_order());\n#endif\n\n  double *d_pProj = malloc_device<double>(nDetXMap*nDetYMap*nProj, q);\n  double *d_sliceI = malloc_device<double>(nPixXMap*nPixYMap, q);\n  double *d_pVolume = malloc_device<double>(nPixX*nPixY*nSlices, q);\n\n  \n\n  double *d_pDetX = malloc_device<double>( nDetXMap , q);\n  double *d_pDetY = malloc_device<double>( nDetYMap , q);\n  double *d_pDetZ = malloc_device<double>( nDetYMap , q);\n  double *d_pObjX = malloc_device<double>( nPixXMap , q);\n  double *d_pObjY = malloc_device<double>( nPixYMap , q);\n  double *d_pObjZ = malloc_device<double>( nSlices , q);\n\n  \n\n  double *d_pDetmY = malloc_device<double>( nDetYMap , q);\n  double *d_pDetmX = malloc_device<double>( nDetYMap * nDetXMap , q);\n\n  \n\n  double *d_pRdetY = malloc_device<double>( nDetYMap , q);\n  double *d_pRdetZ = malloc_device<double>( nDetYMap , q);\n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  range<3> gws (1,1,1);\n  range<3> lws (1,1,1);\n\n  const int maxThreadsPerBlock = BLOCK_SIZE;\n\n  \n\n\n  \n\n  const double* h_pProj_tmp;\n  double* d_pProj_tmp;\n\n  lws[2] = maxThreadsPerBlock;\n  gws[2] = (nDetXMap / maxThreadsPerBlock + 1) * maxThreadsPerBlock;\n\n  for (int np = 0; np < nProj; np++) {\n\n    \n\n    q.submit([&] (handler &cgh) {\n      cgh.parallel_for<class pad>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n        pad_projections_kernel (item, d_pProj,\n                                nDetXMap, nDetYMap, nDetXMap, np);\n      });\n    });\n\n    \n\n    d_pProj_tmp = d_pProj + (nDetXMap*nDetYMap*np) + 1;\n    q.memset(d_pProj_tmp, 0, nPixY * sizeof(double));\n  }\n\n  \n\n  for (int np = 0; np < nProj; np++)\n    for (int c = 0; c < nDetX; c++) {\n      h_pProj_tmp = h_pProj + (c * nDetY) + (nDetX*nDetY*np);\n      d_pProj_tmp = d_pProj + (((c + 1) * nDetYMap) + 1) + (nDetXMap*nDetYMap*np);\n      q.memcpy(d_pProj_tmp, h_pProj_tmp, nDetY * sizeof(double));\n    }\n\n  \n\n\n  lws[2] = maxThreadsPerBlock;\n\n  gws[2] = (nDetX / maxThreadsPerBlock + 1) * maxThreadsPerBlock;\n\n  q.submit([&] (handler &cgh) {\n    cgh.parallel_for<class map_detX>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n      map_boudaries_kernel(item, d_pDetX, nDetXMap, (double)nDetX, -du, 0.0);\n    });\n  });\n\n  gws[2] = (nDetY / maxThreadsPerBlock + 1) * maxThreadsPerBlock;\n\n  q.submit([&] (handler &cgh) {\n    cgh.parallel_for<class map_detY>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n      map_boudaries_kernel(item, d_pDetY, nDetYMap, nDetY / 2.0, dv, 0.0);\n    });\n  });\n\n  gws[2] = (nPixX / maxThreadsPerBlock + 1) * maxThreadsPerBlock;\n\n  q.submit([&] (handler &cgh) {\n    cgh.parallel_for<class map_pixX>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n      map_boudaries_kernel(item, d_pObjX, nPixXMap, (double)nPixX, -dx, 0.0);\n    });\n  });\n\n  gws[2] = (nPixY / maxThreadsPerBlock + 1) * maxThreadsPerBlock;\n\n  q.submit([&] (handler &cgh) {\n    cgh.parallel_for<class map_pixY>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n      map_boudaries_kernel(item, d_pObjY, nPixYMap, nPixY / 2.0, dy, 0.0);\n    });\n  });\n\n  gws[2] = (nSlices / maxThreadsPerBlock + 1) * maxThreadsPerBlock;\n\n  q.submit([&] (handler &cgh) {\n    cgh.parallel_for<class map_pixZ>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n      map_boudaries_kernel(item, d_pObjZ, nSlices, 0.0, dz, DAG + (dz / 2.0));\n    });\n  });\n\n  \n\n  q.memset(d_pDetZ, 0, nDetYMap * sizeof(double));\n  q.memset(d_pVolume, 0, nPixX * nPixY * nSlices * sizeof(double));\n\n  \n\n  double tubeX = 0;\n  double tubeY = 0;\n  double tubeZ = DSD;\n\n  \n\n  double isoY = 0;\n  double isoZ = DDR;\n\n  \n\n  \n\n\n  \n\n  lws[2] = 8;\n  lws[1] = 4;\n  lws[0] = 8;\n\n  gws[2] = 8 * (int)ceilf((float)nDetYMap / (8 - 1));\n  gws[1] = 4;\n  gws[0] = 8 * (int)ceilf((float)nProj / 8);\n\n  int Xk = (int)ceilf((float)nDetXMap / (8 - 1));\n  for (int k = 0; k < Xk; k++) {\n\n    q.submit([&] (handler &cgh) {\n      cgh.parallel_for<class img_integralX>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n        img_integration_kernel(item, d_pProj,\n          nDetXMap, nDetYMap, integrateXcoord, 0, k * 9, nProj);\n      });\n    });\n  }\n\n  \n\n  lws[2] = 4;\n  lws[1] = 8;\n  lws[0] = 8;\n\n  gws[2] = 4;\n  gws[1] = 8 * (int)ceilf((float)nDetXMap / (8 - 1));\n  gws[0] = 8 * (int)ceilf((float)nProj / 8);\n\n  int Yk = (int)ceilf((float)nDetYMap / (8 - 1));\n  for (int k = 0; k < Yk; k++) {\n\n    q.submit([&] (handler &cgh) {\n      cgh.parallel_for<class img_integralY>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n        img_integration_kernel(item, d_pProj,\n            nDetXMap, nDetYMap, integrateYcoord, k * 9, 0, nProj);\n      });\n    });\n  }\n\n  \n\n  int projIni, projEnd, nProj2Run;\n  if (idXProj == -1) {\n    projIni = 0;\n    projEnd = nProj;\n    nProj2Run = nProj;\n  }\n  else {\n    projIni = idXProj;\n    projEnd = idXProj + 1;\n    nProj2Run = 1;\n  }\n\n  \n\n  for (int p = projIni; p < projEnd; p++) {\n\n    \n\n    double theta = h_pTubeAngle[p] * M_PI / 180.0;\n\n    \n\n    double phi = h_pDetAngle[p] * M_PI / 180.0;\n\n    \n\n\n    \n\n    double rtubeY = ((tubeY - isoY)*cos(theta) - (tubeZ - isoZ)*sin(theta)) + isoY;\n    double rtubeZ = ((tubeY - isoY)*sin(theta) + (tubeZ - isoZ)*cos(theta)) + isoZ;\n\n    \n\n\n    \n\n    lws[2] = maxThreadsPerBlock;\n    lws[1] = 1;\n    lws[0] = 1;\n\n    gws[2] = (nDetYMap / maxThreadsPerBlock + 1) * maxThreadsPerBlock;\n    gws[1] = 1;\n    gws[0] = 1;\n\n    q.submit([&] (handler &cgh) {\n      cgh.parallel_for<class rot_det>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n        rot_detector_kernel(item, d_pRdetY, d_pRdetZ, d_pDetY, d_pDetZ,\n                            isoY, isoZ, phi, nDetYMap);\n      });\n    });\n\n    lws[2] = 16;\n    lws[1] = 16;\n    lws[0] = 1;\n\n    \n\n    for (int nz = 0; nz < nSlices; nz++) {\n\n      \n\n\n      gws[2] = (nDetYMap / 16 + 1) * 16;\n      gws[1] = (nDetXMap / 16 + 1) * 16;\n      gws[0] = 1;\n\n      q.submit([&] (handler &cgh) {\n        cgh.parallel_for<class mapDet2Slice>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n          mapDet2Slice_kernel (\n            item,\n            d_pDetmX, d_pDetmY, tubeX, rtubeY, rtubeZ, \n            d_pDetX, d_pRdetY, d_pRdetZ,\n            d_pObjZ, nDetXMap, nDetYMap, nz);\n        });\n      });\n\n      \n\n\n      gws[2] = (nPixYMap / 16 + 1) * 16;\n      gws[1] = (nPixXMap / 16 + 1) * 16;\n\n      q.submit([&] (handler &cgh) {\n        cgh.parallel_for<class bilinear_interp>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n          bilinear_interpolation_kernel (item,\n            d_sliceI, d_pProj,\n            d_pObjX, d_pObjY, \n            d_pDetmX + nDetYMap * (nDetXMap-2), d_pDetmY,\n            nPixXMap, nPixYMap, nDetXMap, nDetYMap, nDetX, nDetY, p);\n        });\n      });\n\n      \n\n\n      gws[2] = (nPixY / 16 + 1) * 16;\n      gws[1] = (nPixX / 16 + 1) * 16;\n\n      q.submit([&] (handler &cgh) {\n        cgh.parallel_for<class diff>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n          differentiation_kernel (\n            item, d_pVolume, d_sliceI, \n            tubeX, rtubeY, rtubeZ,\n            d_pObjX, d_pObjY, d_pObjZ,\n            nPixX, nPixY, nPixXMap, nPixYMap, du, dv, dx, dy, dz, nz);\n        });\n      });\n\n    } \n\n\n  } \n\n\n\n  \n\n  lws[2] = 8;\n  lws[1] = 8;\n  lws[0] = 4;\n\n  gws[2] = (nPixY / 8 + 1) * 8;\n  gws[1] = (nPixX / 8 + 1) * 8;\n  gws[0] = (nSlices / 4 + 1) * 4;\n\n  q.submit([&] (handler &cgh) {\n    cgh.parallel_for<class div>(nd_range<3>(gws, lws), [=] (nd_item<3> item) {\n      division_kernel (item, d_pVolume, \n                       nPixX, nPixY, nSlices, nProj2Run);\n    });\n  }).wait();\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Total kernel execution time %f (s)\\n\", time * 1e-9f);\n\n  q.memcpy(h_pVolume, d_pVolume, nSlices* nPixX * nPixY * sizeof(double));\n\n  free(d_pProj, q);\n  free(d_sliceI, q);\n  free(d_pVolume, q);\n  free(d_pDetX, q);\n  free(d_pDetY, q);\n  free(d_pDetZ, q);\n  free(d_pObjX, q);\n  free(d_pObjY, q);\n  free(d_pObjZ, q);\n  free(d_pDetmY, q);\n  free(d_pDetmX, q);\n  free(d_pRdetY, q);\n  free(d_pRdetZ, q);\n}\n\nint main() \n{\n                            \n\n  const int nPixX = 1996;   \n\n  const int nPixY = 2457;   \n\n  const int nSlices = 78;  \n\n                            \n\n  const int nDetX = 1664;   \n\n  const int nDetY = 2048;   \n\n\n  const int nProj = 15;     \n\n  const int idXProj = -1;   \n\n\n  const double dx = 0.112;  \n\n  const double dy = 0.112;\n  const double dz = 1.0;\n\n  const double du = 0.14;   \n\n  const double dv = 0.14;\n\n  const double DSD = 700;   \n\n  const double DDR = 0.0;   \n\n  const double DAG = 25.0;  \n\n\n  const size_t pixVol = nPixX * nPixY * nSlices;\n  const size_t detVol = nDetX * nDetY * nProj;\n  double *h_pVolume = (double*) malloc (pixVol * sizeof(double));\n  double *h_pProj = (double*) malloc (detVol * sizeof(double));\n\n  double *h_pTubeAngle = (double*) malloc (nProj * sizeof(double));\n  double *h_pDetAngle = (double*) malloc (nProj * sizeof(double));\n  \n  \n\n  for (int i = 0; i < nProj; i++) \n    h_pTubeAngle[i] = -7.5 + i * 15.0/nProj;\n\n  \n\n  for (int i = 0; i < nProj; i++) \n    h_pDetAngle[i] = -2.1 + i * 4.2/nProj;\n\n  \n\n  srand(123);\n  for (size_t i = 0; i < pixVol; i++) \n    h_pVolume[i] = (double)rand() / (double)RAND_MAX;\n\n  for (size_t i = 0; i < detVol; i++) \n    h_pProj[i] = (double)rand() / (double)RAND_MAX;\n\n  backprojectionDDb(\n    h_pVolume,\n    h_pProj,\n    h_pTubeAngle,\n    h_pDetAngle,\n    idXProj,\n    nProj,\n    nPixX, nPixY,\n    nSlices,\n    nDetX, nDetY,\n    dx, dy, dz,\n    du, dv,\n    DSD, DDR, DAG);\n\n  double checkSum = 0;\n  for (size_t i = 0; i < pixVol; i++)\n    checkSum += h_pVolume[i];\n  printf(\"checksum = %lf\\n\", checkSum);\n\n  free(h_pVolume);\n  free(h_pTubeAngle);\n  free(h_pDetAngle);\n  free(h_pProj);\n  return 0;\n}\n"}}
{"kernel_name": "feynman-kac", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <chrono>\n#include <cuda.h>\n#include \"util.h\"\n#include \"kernel.h\"\n\nint main ( int argc, char **argv )\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <iterations>\\n\", argv[0]); \n    return 1;\n  }\n\n  const int repeat = atoi(argv[1]);\n  double a = 2.0;\n  double b = 1.0;\n  int dim = 2;\n  double err;\n  double h = 0.001;\n  int N = 1000;\n  int n_inside;\n  int ni;\n  int nj;\n  double rth;\n  int seed = 123456789;\n\n  printf ( \"\\n\" );\n\n  printf ( \"\\n\" );\n  printf ( \"FEYNMAN_KAC_2D:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  Program parameters:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  The calculation takes place inside a 2D ellipse.\\n\" );\n  printf ( \"  A rectangular grid of points will be defined.\\n\" );\n  printf ( \"  The solution will be estimated for those grid points\\n\" );\n  printf ( \"  that lie inside the ellipse.\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  Each solution will be estimated by computing %d trajectories\\n\", N );\n  printf ( \"  from the point to the boundary.\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"    (X/A)^2 + (Y/B)^2 = 1\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  The ellipse parameters A, B are set to:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"    A = %f\\n\", a );\n  printf ( \"    B = %f\\n\", b );\n  printf ( \"  Stepsize H = %6.4f\\n\", h );\n\n  \n\n  rth = sqrt ( ( double ) dim * h );\n\n  \n\n  nj = 128;\n  ni = 1 + i4_ceiling ( a / b ) * ( nj - 1 );\n\n  printf ( \"\\n\" );\n  printf ( \"  X coordinate marked by %d points\\n\", ni );\n  printf ( \"  Y coordinate marked by %d points\\n\", nj );\n\n  err = 0.0;\n  n_inside = 0;\n\n  dim3 grids ((ni+15)/16, (nj+15)/16);\n  dim3 blocks (16, 16);\n\n  double *d_err;\n  int *d_n_inside;\n\n  cudaMalloc((void**)&d_err, sizeof(double));\n  cudaMalloc((void**)&d_n_inside, sizeof(int));\n\n  long time = 0;\n  for (int i = 0; i < repeat; i++) {\n    cudaMemcpy(d_err, &err, sizeof(double), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_n_inside, &n_inside, sizeof(int), cudaMemcpyHostToDevice);\n\n    cudaDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    fk <<< grids, blocks >>> (ni, nj, seed, N, a, b, h, rth, d_n_inside, d_err);\n\n    cudaDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  }\n  printf(\"Average kernel time: %lf (s)\\n\", time * 1e-9 / repeat);\n\n  cudaMemcpy(&err, d_err, sizeof(double), cudaMemcpyDeviceToHost);\n  cudaMemcpy(&n_inside, d_n_inside, sizeof(int), cudaMemcpyDeviceToHost);\n\n  cudaFree(d_err);\n  cudaFree(d_n_inside);\n\n  err = sqrt ( err / ( double ) ( n_inside ) );\n  printf ( \"\\n\" );\n  printf ( \"  RMS absolute error in solution = %e\\n\", err );\n  printf ( \"\\n\" );\n  printf ( \"FEYNMAN_KAC_2D:\\n\" );\n  printf ( \"  Normal end of execution.\\n\" );\n  printf ( \"\\n\" );\n\n  return 0;\n}\n"}}
{"kernel_name": "feynman-kac", "parallel_api": "hip", "code": {"main.cu": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n#include \"util.h\"\n#include \"kernel.h\"\n\nint main ( int argc, char **argv )\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <iterations>\\n\", argv[0]); \n    return 1;\n  }\n\n  const int repeat = atoi(argv[1]);\n  double a = 2.0;\n  double b = 1.0;\n  int dim = 2;\n  double err;\n  double h = 0.001;\n  int N = 1000;\n  int n_inside;\n  int ni;\n  int nj;\n  double rth;\n  int seed = 123456789;\n\n  printf ( \"\\n\" );\n\n  printf ( \"\\n\" );\n  printf ( \"FEYNMAN_KAC_2D:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  Program parameters:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  The calculation takes place inside a 2D ellipse.\\n\" );\n  printf ( \"  A rectangular grid of points will be defined.\\n\" );\n  printf ( \"  The solution will be estimated for those grid points\\n\" );\n  printf ( \"  that lie inside the ellipse.\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  Each solution will be estimated by computing %d trajectories\\n\", N );\n  printf ( \"  from the point to the boundary.\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"    (X/A)^2 + (Y/B)^2 = 1\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  The ellipse parameters A, B are set to:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"    A = %f\\n\", a );\n  printf ( \"    B = %f\\n\", b );\n  printf ( \"  Stepsize H = %6.4f\\n\", h );\n\n  \n\n  rth = sqrt ( ( double ) dim * h );\n\n  \n\n  nj = 128;\n  ni = 1 + i4_ceiling ( a / b ) * ( nj - 1 );\n\n  printf ( \"\\n\" );\n  printf ( \"  X coordinate marked by %d points\\n\", ni );\n  printf ( \"  Y coordinate marked by %d points\\n\", nj );\n\n  err = 0.0;\n  n_inside = 0;\n\n  dim3 grids ((ni+15)/16, (nj+15)/16);\n  dim3 blocks (16, 16);\n\n  double *d_err;\n  int *d_n_inside;\n\n  hipMalloc((void**)&d_err, sizeof(double));\n  hipMalloc((void**)&d_n_inside, sizeof(int));\n\n  long time = 0;\n  for (int i = 0; i < repeat; i++) {\n    hipMemcpy(d_err, &err, sizeof(double), hipMemcpyHostToDevice);\n    hipMemcpy(d_n_inside, &n_inside, sizeof(int), hipMemcpyHostToDevice);\n\n    hipDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    hipLaunchKernelGGL(fk, grids, blocks , 0, 0, ni, nj, seed, N, a, b, h, rth, d_n_inside, d_err);\n\n    hipDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  }\n  printf(\"Average kernel time: %lf (s)\\n\", time * 1e-9 / repeat);\n\n  hipMemcpy(&err, d_err, sizeof(double), hipMemcpyDeviceToHost);\n  hipMemcpy(&n_inside, d_n_inside, sizeof(int), hipMemcpyDeviceToHost);\n\n  hipFree(d_err);\n  hipFree(d_n_inside);\n\n  err = sqrt ( err / ( double ) ( n_inside ) );\n  printf ( \"\\n\" );\n  printf ( \"  RMS absolute error in solution = %e\\n\", err );\n  printf ( \"\\n\" );\n  printf ( \"FEYNMAN_KAC_2D:\\n\" );\n  printf ( \"  Normal end of execution.\\n\" );\n  printf ( \"\\n\" );\n\n  return 0;\n}\n"}}
{"kernel_name": "feynman-kac", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <chrono>\n#include <omp.h>\n#include \"util.h\"\n\nint main ( int argc, char **argv )\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <iterations>\\n\", argv[0]); \n    return 1;\n  }\n\n  const int repeat = atoi(argv[1]);\n  double a = 2.0;\n  double b = 1.0;\n  int dim = 2;\n  double err;\n  double h = 0.001;\n  int N = 1000;\n  int n_inside;\n  int ni;\n  int nj;\n  double rth;\n  int seed = 123456789;\n\n  printf ( \"\\n\" );\n\n  printf ( \"\\n\" );\n  printf ( \"FEYNMAN_KAC_2D:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  Program parameters:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  The calculation takes place inside a 2D ellipse.\\n\" );\n  printf ( \"  A rectangular grid of points will be defined.\\n\" );\n  printf ( \"  The solution will be estimated for those grid points\\n\" );\n  printf ( \"  that lie inside the ellipse.\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  Each solution will be estimated by computing %d trajectories\\n\", N );\n  printf ( \"  from the point to the boundary.\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"    (X/A)^2 + (Y/B)^2 = 1\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  The ellipse parameters A, B are set to:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"    A = %f\\n\", a );\n  printf ( \"    B = %f\\n\", b );\n  printf ( \"  Stepsize H = %6.4f\\n\", h );\n\n  \n\n  rth = sqrt ( ( double ) dim * h );\n\n  \n\n  nj = 128;\n  ni = 1 + i4_ceiling ( a / b ) * ( nj - 1 );\n\n  printf ( \"\\n\" );\n  printf ( \"  X coordinate marked by %d points\\n\", ni );\n  printf ( \"  Y coordinate marked by %d points\\n\", nj );\n\n  err = 0.0;\n  n_inside = 0;\n\n  #pragma omp target data map (to: ni, nj, seed, N, a, b, h, rth) \\\n                          map (from: err, n_inside)\n  {\n    long time = 0;\n    for (int i = 0; i < repeat; i++) {\n      #pragma omp target update to (err) \n      #pragma omp target update to (n_inside) \n\n      auto start = std::chrono::steady_clock::now();\n      #pragma omp target teams distribute parallel for collapse(2) thread_limit(256) \\\n                                                       reduction(+: err, n_inside) \n      for (int j = 0; j < nj; j++) {\n        for (int i = 0; i < ni; i++) {\n          double x = ( ( double ) ( nj - j     ) * ( - a )\n                     + ( double ) (      j - 1 ) *     a )\n                     / ( double ) ( nj     - 1 );\n\n          double y = ( ( double ) ( ni - i     ) * ( - b )\n                     + ( double ) (      i - 1 ) *     b ) \n                     / ( double ) ( ni     - 1 );\n\n          double dx;\n          double dy;\n          double us;\n          double ut;\n          double vh;\n          double vs;\n          double x1;\n          double x2;\n          double w;\n          double w_exact;\n          double we;\n          double wt;\n          double chk = pow ( x / a, 2.0 ) + pow ( y / b, 2.0 );\n\n          if ( 1.0 < chk )\n          {\n            w_exact = 1.0;\n            wt = 1.0;\n          }\n          else {\n            n_inside++;\n            w_exact = exp ( pow ( x / a, 2.0 ) + pow ( y / b, 2.0 ) - 1.0 );\n            wt = 0.0;\n            for ( int k = 0; k < N; k++ )\n            {\n              x1 = x;\n              x2 = y;\n              w = 1.0;  \n              chk = 0.0;\n              while ( chk < 1.0 )\n              {\n                ut = r8_uniform_01 ( &seed );\n                if ( ut < 1.0 / 2.0 )\n                {\n                  us = r8_uniform_01 ( &seed ) - 0.5;\n                  if ( us < 0.0)\n                    dx = - rth;\n                  else\n                    dx = rth;\n                } \n                else\n                {\n                  dx = 0.0;\n                }\n\n                ut = r8_uniform_01 ( &seed );\n                if ( ut < 1.0 / 2.0 )\n                {\n                  us = r8_uniform_01 ( &seed ) - 0.5;\n                  if ( us < 0.0 )\n                    dy = - rth;\n                  else\n                    dy = rth;\n                }\n                else\n                {\n                  dy = 0.0;\n                }\n                vs = potential ( a, b, x1, x2 );\n                x1 = x1 + dx;\n                x2 = x2 + dy;\n\n                vh = potential ( a, b, x1, x2 );\n\n                we = ( 1.0 - h * vs ) * w;\n                w = w - 0.5 * h * ( vh * we + vs * w ); \n\n                chk = pow ( x1 / a, 2.0 ) + pow ( x2 / b, 2.0 );\n              }\n              wt += w;\n            }\n            wt /= ( double ) ( N ); \n            err += pow ( w_exact - wt, 2.0 );\n          }\n        }\n      }\n      auto end = std::chrono::steady_clock::now();\n      time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    }\n    printf(\"Average kernel time: %lf (s)\\n\", time * 1e-9 / repeat);\n  }\n\n  err = sqrt ( err / ( double ) ( n_inside ) );\n  printf ( \"\\n\" );\n  printf ( \"  RMS absolute error in solution = %e\\n\", err );\n  printf ( \"\\n\" );\n  printf ( \"FEYNMAN_KAC_2D:\\n\" );\n  printf ( \"  Normal end of execution.\\n\" );\n  printf ( \"\\n\" );\n\n  return 0;\n}\n"}}
{"kernel_name": "feynman-kac", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <chrono>\n#include \"util.h\"\n\nint main ( int argc, char **argv )\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <iterations>\\n\", argv[0]); \n    return 1;\n  }\n\n  const int repeat = atoi(argv[1]);\n  double a = 2.0;\n  double b = 1.0;\n  int dim = 2;\n  double err;\n  double h = 0.001;\n  int N = 1000;\n  int n_inside;\n  int ni;\n  int nj;\n  double rth;\n  int seed = 123456789;\n\n  printf ( \"\\n\" );\n\n  printf ( \"\\n\" );\n  printf ( \"FEYNMAN_KAC_2D:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  Program parameters:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  The calculation takes place inside a 2D ellipse.\\n\" );\n  printf ( \"  A rectangular grid of points will be defined.\\n\" );\n  printf ( \"  The solution will be estimated for those grid points\\n\" );\n  printf ( \"  that lie inside the ellipse.\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  Each solution will be estimated by computing %d trajectories\\n\", N );\n  printf ( \"  from the point to the boundary.\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"    (X/A)^2 + (Y/B)^2 = 1\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  The ellipse parameters A, B are set to:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"    A = %f\\n\", a );\n  printf ( \"    B = %f\\n\", b );\n  printf ( \"  Stepsize H = %6.4f\\n\", h );\n\n  \n\n  rth = sqrt ( ( double ) dim * h );\n\n  \n\n  nj = 128;\n  ni = 1 + i4_ceiling ( a / b ) * ( nj - 1 );\n\n  printf ( \"\\n\" );\n  printf ( \"  X coordinate marked by %d points\\n\", ni );\n  printf ( \"  Y coordinate marked by %d points\\n\", nj );\n\n  err = 0.0;\n  n_inside = 0;\n\n    {\n    long time = 0;\n    for (int i = 0; i < repeat; i++) {\n            \n      auto start = std::chrono::steady_clock::now();\n            for (int j = 0; j < nj; j++) {\n        for (int i = 0; i < ni; i++) {\n          double x = ( ( double ) ( nj - j     ) * ( - a )\n                     + ( double ) (      j - 1 ) *     a )\n                     / ( double ) ( nj     - 1 );\n\n          double y = ( ( double ) ( ni - i     ) * ( - b )\n                     + ( double ) (      i - 1 ) *     b ) \n                     / ( double ) ( ni     - 1 );\n\n          double dx;\n          double dy;\n          double us;\n          double ut;\n          double vh;\n          double vs;\n          double x1;\n          double x2;\n          double w;\n          double w_exact;\n          double we;\n          double wt;\n          double chk = pow ( x / a, 2.0 ) + pow ( y / b, 2.0 );\n\n          if ( 1.0 < chk )\n          {\n            w_exact = 1.0;\n            wt = 1.0;\n          }\n          else {\n            n_inside++;\n            w_exact = exp ( pow ( x / a, 2.0 ) + pow ( y / b, 2.0 ) - 1.0 );\n            wt = 0.0;\n            for ( int k = 0; k < N; k++ )\n            {\n              x1 = x;\n              x2 = y;\n              w = 1.0;  \n              chk = 0.0;\n              while ( chk < 1.0 )\n              {\n                ut = r8_uniform_01 ( &seed );\n                if ( ut < 1.0 / 2.0 )\n                {\n                  us = r8_uniform_01 ( &seed ) - 0.5;\n                  if ( us < 0.0)\n                    dx = - rth;\n                  else\n                    dx = rth;\n                } \n                else\n                {\n                  dx = 0.0;\n                }\n\n                ut = r8_uniform_01 ( &seed );\n                if ( ut < 1.0 / 2.0 )\n                {\n                  us = r8_uniform_01 ( &seed ) - 0.5;\n                  if ( us < 0.0 )\n                    dy = - rth;\n                  else\n                    dy = rth;\n                }\n                else\n                {\n                  dy = 0.0;\n                }\n                vs = potential ( a, b, x1, x2 );\n                x1 = x1 + dx;\n                x2 = x2 + dy;\n\n                vh = potential ( a, b, x1, x2 );\n\n                we = ( 1.0 - h * vs ) * w;\n                w = w - 0.5 * h * ( vh * we + vs * w ); \n\n                chk = pow ( x1 / a, 2.0 ) + pow ( x2 / b, 2.0 );\n              }\n              wt += w;\n            }\n            wt /= ( double ) ( N ); \n            err += pow ( w_exact - wt, 2.0 );\n          }\n        }\n      }\n      auto end = std::chrono::steady_clock::now();\n      time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    }\n    printf(\"Average kernel time: %lf (s)\\n\", time * 1e-9 / repeat);\n  }\n\n  err = sqrt ( err / ( double ) ( n_inside ) );\n  printf ( \"\\n\" );\n  printf ( \"  RMS absolute error in solution = %e\\n\", err );\n  printf ( \"\\n\" );\n  printf ( \"FEYNMAN_KAC_2D:\\n\" );\n  printf ( \"  Normal end of execution.\\n\" );\n  printf ( \"\\n\" );\n\n  return 0;\n}"}}
{"kernel_name": "feynman-kac", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n#include \"util.h\"\n#include \"kernel.h\"\n\nint main ( int argc, char **argv )\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <iterations>\\n\", argv[0]); \n    return 1;\n  }\n\n  const int repeat = atoi(argv[1]);\n  double a = 2.0;\n  double b = 1.0;\n  int dim = 2;\n  double err;\n  double h = 0.001;\n  int N = 1000;\n  int n_inside;\n  int ni;\n  int nj;\n  double rth;\n  int seed = 123456789;\n\n  printf ( \"\\n\" );\n\n  printf ( \"\\n\" );\n  printf ( \"FEYNMAN_KAC_2D:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  Program parameters:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  The calculation takes place inside a 2D ellipse.\\n\" );\n  printf ( \"  A rectangular grid of points will be defined.\\n\" );\n  printf ( \"  The solution will be estimated for those grid points\\n\" );\n  printf ( \"  that lie inside the ellipse.\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  Each solution will be estimated by computing %d trajectories\\n\", N );\n  printf ( \"  from the point to the boundary.\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"    (X/A)^2 + (Y/B)^2 = 1\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"  The ellipse parameters A, B are set to:\\n\" );\n  printf ( \"\\n\" );\n  printf ( \"    A = %f\\n\", a );\n  printf ( \"    B = %f\\n\", b );\n  printf ( \"  Stepsize H = %6.4f\\n\", h );\n\n  \n\n  rth = sqrt ( ( double ) dim * h );\n\n  \n\n  nj = 128;\n  ni = 1 + i4_ceiling ( a / b ) * ( nj - 1 );\n\n  printf ( \"\\n\" );\n  printf ( \"  X coordinate marked by %d points\\n\", ni );\n  printf ( \"  Y coordinate marked by %d points\\n\", nj );\n\n  err = 0.0;\n  n_inside = 0;\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  sycl::range<2> gws ((nj+15)/16*16, (ni+15)/16);\n  sycl::range<2> lws (16, 16);\n\n  double *d_err = sycl::malloc_device<double>(1, q);\n  int *d_n_inside = sycl::malloc_device<int>(1, q);\n  \n  long time = 0;\n  for (int i = 0; i < repeat; i++) {\n    q.memcpy(d_err, &err, sizeof(double));\n    q.memcpy(d_n_inside, &n_inside, sizeof(int));\n    q.wait();\n\n    auto start = std::chrono::steady_clock::now();\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class solution>(\n        sycl::nd_range<2>(gws, lws), [=] (sycl::nd_item<2> item) {\n        fk (item, ni, nj, seed, N, a, b, h, rth, d_n_inside, d_err);\n      });\n    }).wait();\n\n    auto end = std::chrono::steady_clock::now();\n    time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  }\n  printf(\"Average kernel time: %lf (s)\\n\", time * 1e-9 / repeat);\n  \n  q.memcpy(&err, d_err, sizeof(double));\n  q.memcpy(&n_inside, d_n_inside, sizeof(int));\n  q.wait();\n\n  sycl::free(d_err, q);\n  sycl::free(d_n_inside, q);\n\n  err = sqrt ( err / ( double ) ( n_inside ) );\n  printf ( \"\\n\" );\n  printf ( \"  RMS absolute error in solution = %e\\n\", err );\n  printf ( \"\\n\" );\n  printf ( \"FEYNMAN_KAC_2D:\\n\" );\n  printf ( \"  Normal end of execution.\\n\" );\n  printf ( \"\\n\" );\n\n  return 0;\n}\n"}}
{"kernel_name": "fhd", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <cuda.h>\n\n#define CHUNK_S 4096\n\ntypedef struct {\n  float x, y, z;\n} kdata;\n\n__constant__ kdata k[CHUNK_S];\n\n__global__\nvoid cmpfhd(const float*__restrict__ rmu, \n            const float*__restrict__ imu,\n                  float*__restrict__ rfhd,\n                  float*__restrict__ ifhd,\n            const float*__restrict__ x, \n            const float*__restrict__ y,\n            const float*__restrict__ z,\n            const int samples,\n            const int voxels) \n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (n < samples) {\n    float xn = x[n], yn = y[n], zn = z[n];\n    float rfhdn = rfhd[n], ifhdn = ifhd[n];\n    for (int m = 0; m < voxels; m++) {\n      float e = 2.f * (float)M_PI * (k[m].x * xn + k[m].y * yn + k[m].z * zn);\n      float c = __cosf(e);\n      float s = __sinf(e);\n      rfhdn += rmu[m] * c - imu[m] * s;\n      ifhdn += imu[m] * c + rmu[m] * s;\n    }\n    rfhd[n] = rfhdn, ifhd[n] = ifhdn;\n  }\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 4) {\n    printf(\"Usage: %s <#samples> <#voxels> <verify>\\n\", argv[0]);\n    exit(1);\n  }\n  const int samples = atoi(argv[1]); \n\n  const int voxels = atoi(argv[2]);  \n\n  const int verify = atoi(argv[3]);\n  const int sampleSize = samples * sizeof(float);\n  const int voxelSize = voxels * sizeof(float);\n\n  float *h_rmu = (float*) malloc (voxelSize);\n  float *h_imu = (float*) malloc (voxelSize);\n  float *h_kx = (float*) malloc (voxelSize);\n  float *h_ky = (float*) malloc (voxelSize);\n  float *h_kz = (float*) malloc (voxelSize);\n  kdata *h_k = (kdata*) malloc (voxels * sizeof(kdata));\n\n  float *h_rfhd = (float*) malloc (sampleSize);\n  float *h_ifhd = (float*) malloc (sampleSize);\n  float *h_x = (float*) malloc (sampleSize);\n  float *h_y = (float*) malloc (sampleSize);\n  float *h_z = (float*) malloc (sampleSize);\n\n  \n\n  float *rfhd = (float*) malloc (sampleSize);\n  float *ifhd = (float*) malloc (sampleSize);\n\n  srand(2);\n  for (int i = 0; i < samples; i++) {\n    h_rfhd[i] = (float)i/samples;\n    h_ifhd[i] = (float)i/samples;\n    h_x[i] = 0.3f + (rand()%2 ? 0.1 : -0.1);\n    h_y[i] = 0.2f + (rand()%2 ? 0.1 : -0.1);\n    h_z[i] = 0.1f + (rand()%2 ? 0.1 : -0.1);\n  }\n\n  for (int i = 0; i < voxels; i++) {\n    h_rmu[i] = (float)i/voxels;\n    h_imu[i] = (float)i/voxels;\n    h_k[i].x = h_kx[i] = 0.1f + (rand()%2 ? 0.1 : -0.1);\n    h_k[i].y = h_ky[i] = 0.2f + (rand()%2 ? 0.1 : -0.1);\n    h_k[i].z = h_kz[i] = 0.3f + (rand()%2 ? 0.1 : -0.1);\n  }\n\n  printf(\"Run FHd on a device\\n\");\n  float *d_rmu, *d_imu;\n  float *d_rfhd, *d_ifhd;\n  float *d_x, *d_y, *d_z;\n\n  cudaMalloc((void**)&d_rmu, voxelSize);\n  cudaMemcpy(d_rmu, h_rmu, voxelSize, cudaMemcpyHostToDevice);\n\n  cudaMalloc((void**)&d_imu, voxelSize);\n  cudaMemcpy(d_imu, h_imu, voxelSize, cudaMemcpyHostToDevice);\n\n  cudaMalloc((void**)&d_rfhd, sampleSize);\n  cudaMemcpy(d_rfhd, h_rfhd, sampleSize, cudaMemcpyHostToDevice);\n\n  cudaMalloc((void**)&d_ifhd, sampleSize);\n  cudaMemcpy(d_ifhd, h_ifhd, sampleSize, cudaMemcpyHostToDevice);\n\n  cudaMalloc((void**)&d_x, sampleSize);\n  cudaMemcpy(d_x, h_x, sampleSize, cudaMemcpyHostToDevice);\n\n  cudaMalloc((void**)&d_y, sampleSize);\n  cudaMemcpy(d_y, h_y, sampleSize, cudaMemcpyHostToDevice);\n\n  cudaMalloc((void**)&d_z, sampleSize);\n  cudaMemcpy(d_z, h_z, sampleSize, cudaMemcpyHostToDevice);\n  \n  const int ntpb = 256;\n  const int nblks = (samples + ntpb - 1) / ntpb;\n  dim3 grid (nblks);\n  dim3 block (ntpb);\n\n  int c = CHUNK_S;\n  int s = sizeof(kdata) * c;\n  int nchunks = (voxels + c - 1) / c;\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < nchunks; i++) {\n    if (i == nchunks - 1) {\n      c = voxels - CHUNK_S * i;\n      s = sizeof(kdata) * c;\n    }\n    cudaMemcpyToSymbol(k, &h_k[i * CHUNK_S], s);\n\n    cmpfhd<<<grid, block>>>(d_rmu + i*CHUNK_S,\n                            d_imu + i*CHUNK_S, \n                            d_rfhd, d_ifhd, \n                            d_x, d_y, d_z, \n                            samples, c);\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Device execution time %f (s)\\n\", time * 1e-9f);\n\n  cudaMemcpy(rfhd, d_rfhd, sampleSize, cudaMemcpyDeviceToHost);\n  cudaMemcpy(ifhd, d_ifhd, sampleSize, cudaMemcpyDeviceToHost);\n\n  if (verify) {\n    printf(\"Computing root mean square error between host and device results.\\n\");\n    printf(\"This will take a while..\\n\");\n\n    #pragma omp parallel for \n    for (int n = 0; n < samples; n++) {\n      float r = h_rfhd[n];\n      float i = h_ifhd[n];\n      #pragma omp parallel for simd reduction(+:r,i)\n      for (int m = 0; m < voxels; m++) {\n        float e = 2.f * (float)M_PI * \n                  (h_kx[m] * h_x[n] + h_ky[m] * h_y[n] + h_kz[m] * h_z[n]);\n        float c = cosf(e);\n        float s = sinf(e);\n        r += h_rmu[m] * c - h_imu[m] * s;\n        i += h_imu[m] * c + h_rmu[m] * s;\n      }\n      h_rfhd[n] = r;\n      h_ifhd[n] = i;   \n    }\n\n    float err = 0.f;\n    for (int i = 0; i < samples; i++) {\n      err += (h_rfhd[i] - rfhd[i]) * (h_rfhd[i] - rfhd[i]) +\n             (h_ifhd[i] - ifhd[i]) * (h_ifhd[i] - ifhd[i]) ;\n    }\n    printf(\"RMSE = %f\\n\", sqrtf(err / (2*samples)));\n  }\n \n  cudaFree(d_rmu);\n  cudaFree(d_imu);\n  cudaFree(d_rfhd);\n  cudaFree(d_ifhd);\n  cudaFree(d_x);\n  cudaFree(d_y);\n  cudaFree(d_z);\n  free(h_rmu);\n  free(h_imu);\n  free(h_kx);\n  free(h_ky);\n  free(h_kz);\n  free(h_k);\n  free(h_rfhd);\n  free(h_ifhd);\n  free(rfhd);\n  free(ifhd);\n  free(h_x);\n  free(h_y);\n  free(h_z);\n\n  return 0;\n}\n"}}
{"kernel_name": "fhd", "parallel_api": "hip", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n\n#define CHUNK_S 4096\n\ntypedef struct {\n  float x, y, z;\n} kdata;\n\n__constant__ kdata k[CHUNK_S];\n\n__global__\nvoid cmpfhd(const float*__restrict__ rmu, \n            const float*__restrict__ imu,\n                  float*__restrict__ rfhd,\n                  float*__restrict__ ifhd,\n            const float*__restrict__ x, \n            const float*__restrict__ y,\n            const float*__restrict__ z,\n            const int samples,\n            const int voxels) \n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (n < samples) {\n    float xn = x[n], yn = y[n], zn = z[n];\n    float rfhdn = rfhd[n], ifhdn = ifhd[n];\n    for (int m = 0; m < voxels; m++) {\n      float e = 2.f * (float)M_PI * (k[m].x * xn + k[m].y * yn + k[m].z * zn);\n      float c = __cosf(e);\n      float s = __sinf(e);\n      rfhdn += rmu[m] * c - imu[m] * s;\n      ifhdn += imu[m] * c + rmu[m] * s;\n    }\n    rfhd[n] = rfhdn, ifhd[n] = ifhdn;\n  }\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 4) {\n    printf(\"Usage: %s <#samples> <#voxels> <verify>\\n\", argv[0]);\n    exit(1);\n  }\n  const int samples = atoi(argv[1]); \n\n  const int voxels = atoi(argv[2]);  \n\n  const int verify = atoi(argv[3]);\n  const int sampleSize = samples * sizeof(float);\n  const int voxelSize = voxels * sizeof(float);\n\n  float *h_rmu = (float*) malloc (voxelSize);\n  float *h_imu = (float*) malloc (voxelSize);\n  float *h_kx = (float*) malloc (voxelSize);\n  float *h_ky = (float*) malloc (voxelSize);\n  float *h_kz = (float*) malloc (voxelSize);\n  kdata *h_k = (kdata*) malloc (voxels * sizeof(kdata));\n\n  float *h_rfhd = (float*) malloc (sampleSize);\n  float *h_ifhd = (float*) malloc (sampleSize);\n  float *h_x = (float*) malloc (sampleSize);\n  float *h_y = (float*) malloc (sampleSize);\n  float *h_z = (float*) malloc (sampleSize);\n\n  \n\n  float *rfhd = (float*) malloc (sampleSize);\n  float *ifhd = (float*) malloc (sampleSize);\n\n  srand(2);\n  for (int i = 0; i < samples; i++) {\n    h_rfhd[i] = (float)i/samples;\n    h_ifhd[i] = (float)i/samples;\n    h_x[i] = 0.3f + (rand()%2 ? 0.1 : -0.1);\n    h_y[i] = 0.2f + (rand()%2 ? 0.1 : -0.1);\n    h_z[i] = 0.1f + (rand()%2 ? 0.1 : -0.1);\n  }\n\n  for (int i = 0; i < voxels; i++) {\n    h_rmu[i] = (float)i/voxels;\n    h_imu[i] = (float)i/voxels;\n    h_k[i].x = h_kx[i] = 0.1f + (rand()%2 ? 0.1 : -0.1);\n    h_k[i].y = h_ky[i] = 0.2f + (rand()%2 ? 0.1 : -0.1);\n    h_k[i].z = h_kz[i] = 0.3f + (rand()%2 ? 0.1 : -0.1);\n  }\n\n  printf(\"Run FHd on a device\\n\");\n  float *d_rmu, *d_imu;\n  float *d_rfhd, *d_ifhd;\n  float *d_x, *d_y, *d_z;\n\n  hipMalloc((void**)&d_rmu, voxelSize);\n  hipMemcpy(d_rmu, h_rmu, voxelSize, hipMemcpyHostToDevice);\n\n  hipMalloc((void**)&d_imu, voxelSize);\n  hipMemcpy(d_imu, h_imu, voxelSize, hipMemcpyHostToDevice);\n\n  hipMalloc((void**)&d_rfhd, sampleSize);\n  hipMemcpy(d_rfhd, h_rfhd, sampleSize, hipMemcpyHostToDevice);\n\n  hipMalloc((void**)&d_ifhd, sampleSize);\n  hipMemcpy(d_ifhd, h_ifhd, sampleSize, hipMemcpyHostToDevice);\n\n  hipMalloc((void**)&d_x, sampleSize);\n  hipMemcpy(d_x, h_x, sampleSize, hipMemcpyHostToDevice);\n\n  hipMalloc((void**)&d_y, sampleSize);\n  hipMemcpy(d_y, h_y, sampleSize, hipMemcpyHostToDevice);\n\n  hipMalloc((void**)&d_z, sampleSize);\n  hipMemcpy(d_z, h_z, sampleSize, hipMemcpyHostToDevice);\n  \n  const int ntpb = 256;\n  const int nblks = (samples + ntpb - 1) / ntpb;\n  dim3 grid (nblks);\n  dim3 block (ntpb);\n\n  int c = CHUNK_S;\n  int s = sizeof(kdata) * c;\n  int nchunks = (voxels + c - 1) / c;\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < nchunks; i++) {\n    if (i == nchunks - 1) {\n      c = voxels - CHUNK_S * i;\n      s = sizeof(kdata) * c;\n    }\n    hipMemcpyToSymbol(k, &h_k[i * CHUNK_S], s);\n\n    cmpfhd<<<grid, block>>>(d_rmu + i*CHUNK_S,\n                            d_imu + i*CHUNK_S, \n                            d_rfhd, d_ifhd, \n                            d_x, d_y, d_z, \n                            samples, c);\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Device execution time %f (s)\\n\", time * 1e-9f);\n\n  hipMemcpy(rfhd, d_rfhd, sampleSize, hipMemcpyDeviceToHost);\n  hipMemcpy(ifhd, d_ifhd, sampleSize, hipMemcpyDeviceToHost);\n\n  if (verify) {\n    printf(\"Computing root mean square error between host and device results.\\n\");\n    printf(\"This will take a while..\\n\");\n\n    #pragma omp parallel for \n    for (int n = 0; n < samples; n++) {\n      float r = h_rfhd[n];\n      float i = h_ifhd[n];\n      #pragma omp parallel for simd reduction(+:r,i)\n      for (int m = 0; m < voxels; m++) {\n        float e = 2.f * (float)M_PI * \n                  (h_kx[m] * h_x[n] + h_ky[m] * h_y[n] + h_kz[m] * h_z[n]);\n        float c = cosf(e);\n        float s = sinf(e);\n        r += h_rmu[m] * c - h_imu[m] * s;\n        i += h_imu[m] * c + h_rmu[m] * s;\n      }\n      h_rfhd[n] = r;\n      h_ifhd[n] = i;   \n    }\n\n    float err = 0.f;\n    for (int i = 0; i < samples; i++) {\n      err += (h_rfhd[i] - rfhd[i]) * (h_rfhd[i] - rfhd[i]) +\n             (h_ifhd[i] - ifhd[i]) * (h_ifhd[i] - ifhd[i]) ;\n    }\n    printf(\"RMSE = %f\\n\", sqrtf(err / (2*samples)));\n  }\n \n  hipFree(d_rmu);\n  hipFree(d_imu);\n  hipFree(d_rfhd);\n  hipFree(d_ifhd);\n  hipFree(d_x);\n  hipFree(d_y);\n  hipFree(d_z);\n  free(h_rmu);\n  free(h_imu);\n  free(h_kx);\n  free(h_ky);\n  free(h_kz);\n  free(h_k);\n  free(h_rfhd);\n  free(h_ifhd);\n  free(rfhd);\n  free(ifhd);\n  free(h_x);\n  free(h_y);\n  free(h_z);\n\n  return 0;\n}\n"}}
{"kernel_name": "fhd", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <omp.h>\n\n#define CHUNK_S 4096\n\nint main(int argc, char* argv[]) {\n  if (argc != 4) {\n    printf(\"Usage: %s <#samples> <#voxels> <verify>\\n\", argv[0]);\n    exit(1);\n  }\n  const int samples = atoi(argv[1]); \n\n  const int voxels = atoi(argv[2]);  \n\n  const int verify = atoi(argv[3]);\n  const int sampleSize = samples * sizeof(float);\n  const int voxelSize = voxels * sizeof(float);\n\n  float *h_rmu = (float*) malloc (voxelSize);\n  float *h_imu = (float*) malloc (voxelSize);\n  float *h_kx = (float*) malloc (voxelSize);\n  float *h_ky = (float*) malloc (voxelSize);\n  float *h_kz = (float*) malloc (voxelSize);\n\n  float *h_rfhd = (float*) malloc (sampleSize);\n  float *h_ifhd = (float*) malloc (sampleSize);\n  float *h_x = (float*) malloc (sampleSize);\n  float *h_y = (float*) malloc (sampleSize);\n  float *h_z = (float*) malloc (sampleSize);\n\n  \n\n  float *rfhd = (float*) malloc (sampleSize);\n  float *ifhd = (float*) malloc (sampleSize);\n\n  srand(2);\n  for (int i = 0; i < samples; i++) {\n    rfhd[i] = h_rfhd[i] = (float)i/samples;\n    ifhd[i] = h_ifhd[i] = (float)i/samples;\n    h_x[i] = 0.3f + (rand()%2 ? 0.1 : -0.1);\n    h_y[i] = 0.2f + (rand()%2 ? 0.1 : -0.1);\n    h_z[i] = 0.1f + (rand()%2 ? 0.1 : -0.1);\n  }\n\n  for (int i = 0; i < voxels; i++) {\n    h_rmu[i] = (float)i/voxels;\n    h_imu[i] = (float)i/voxels;\n    h_kx[i] = 0.1f + (rand()%2 ? 0.1 : -0.1);\n    h_ky[i] = 0.2f + (rand()%2 ? 0.1 : -0.1);\n    h_kz[i] = 0.3f + (rand()%2 ? 0.1 : -0.1);\n  }\n\n  printf(\"Run FHd on a device\\n\");\n\n  #pragma omp target data map(to: h_rmu[0:voxels],\\\n                                  h_imu[0:voxels],\\\n                                  h_kx[0:voxels],\\\n                                  h_ky[0:voxels],\\\n                                  h_kz[0:voxels],\\\n                                  h_x[0:samples],\\\n                                  h_y[0:samples],\\\n                                  h_z[0:samples]) \\\n                          map(tofrom: rfhd[0:samples],\\\n                                      ifhd[0:samples])\n  {\n    auto start = std::chrono::steady_clock::now();\n\n    #pragma omp target teams distribute parallel for\n    for (int n = 0; n < samples; n++) {\n      float r = rfhd[n];\n      float i = ifhd[n];\n      float xn = h_x[n];\n      float yn = h_y[n];\n      float zn = h_z[n];\n      #pragma omp parallel for simd reduction(+:r,i)\n      for (int m = 0; m < voxels; m++) {\n        float e = 2.f * (float)M_PI * \n                  (h_kx[m] * xn + h_ky[m] * yn + h_kz[m] * zn);\n        float c = cosf(e);\n        float s = sinf(e);\n        r += h_rmu[m] * c - h_imu[m] * s;\n        i += h_imu[m] * c + h_rmu[m] * s;\n      }\n      rfhd[n] = r;\n      ifhd[n] = i;   \n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Device execution time %f (s)\\n\", time * 1e-9f);\n  }\n  \n  if (verify) {\n    printf(\"Computing root mean square error between host and device results.\\n\");\n    printf(\"This will take a while..\\n\");\n\n    #pragma omp parallel for \n    for (int n = 0; n < samples; n++) {\n      float r = h_rfhd[n];\n      float i = h_ifhd[n];\n      #pragma omp parallel for simd reduction(+:r,i)\n      for (int m = 0; m < voxels; m++) {\n        float e = 2.f * (float)M_PI * \n                  (h_kx[m] * h_x[n] + h_ky[m] * h_y[n] + h_kz[m] * h_z[n]);\n        float c = cosf(e);\n        float s = sinf(e);\n        r += h_rmu[m] * c - h_imu[m] * s;\n        i += h_imu[m] * c + h_rmu[m] * s;\n      }\n      h_rfhd[n] = r;\n      h_ifhd[n] = i;   \n    }\n\n    float err = 0.f;\n    for (int i = 0; i < samples; i++) {\n      err += (h_rfhd[i] - rfhd[i]) * (h_rfhd[i] - rfhd[i]) +\n             (h_ifhd[i] - ifhd[i]) * (h_ifhd[i] - ifhd[i]) ;\n    }\n    printf(\"RMSE = %f\\n\", sqrtf(err / (2*samples)));\n  }\n \n  free(h_rmu);\n  free(h_imu);\n  free(h_kx);\n  free(h_ky);\n  free(h_kz);\n  free(h_rfhd);\n  free(h_ifhd);\n  free(rfhd);\n  free(ifhd);\n  free(h_x);\n  free(h_y);\n  free(h_z);\n\n  return 0;\n}\n"}}
{"kernel_name": "fhd", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n\n#define CHUNK_S 4096\n\nint main(int argc, char* argv[]) {\n  if (argc != 4) {\n    printf(\"Usage: %s <#samples> <#voxels> <verify>\\n\", argv[0]);\n    exit(1);\n  }\n  const int samples = atoi(argv[1]); \n\n  const int voxels = atoi(argv[2]);  \n\n  const int verify = atoi(argv[3]);\n  const int sampleSize = samples * sizeof(float);\n  const int voxelSize = voxels * sizeof(float);\n\n  float *h_rmu = (float*) malloc (voxelSize);\n  float *h_imu = (float*) malloc (voxelSize);\n  float *h_kx = (float*) malloc (voxelSize);\n  float *h_ky = (float*) malloc (voxelSize);\n  float *h_kz = (float*) malloc (voxelSize);\n\n  float *h_rfhd = (float*) malloc (sampleSize);\n  float *h_ifhd = (float*) malloc (sampleSize);\n  float *h_x = (float*) malloc (sampleSize);\n  float *h_y = (float*) malloc (sampleSize);\n  float *h_z = (float*) malloc (sampleSize);\n\n  \n\n  float *rfhd = (float*) malloc (sampleSize);\n  float *ifhd = (float*) malloc (sampleSize);\n\n  srand(2);\n  for (int i = 0; i < samples; i++) {\n    rfhd[i] = h_rfhd[i] = (float)i/samples;\n    ifhd[i] = h_ifhd[i] = (float)i/samples;\n    h_x[i] = 0.3f + (rand()%2 ? 0.1 : -0.1);\n    h_y[i] = 0.2f + (rand()%2 ? 0.1 : -0.1);\n    h_z[i] = 0.1f + (rand()%2 ? 0.1 : -0.1);\n  }\n\n  for (int i = 0; i < voxels; i++) {\n    h_rmu[i] = (float)i/voxels;\n    h_imu[i] = (float)i/voxels;\n    h_kx[i] = 0.1f + (rand()%2 ? 0.1 : -0.1);\n    h_ky[i] = 0.2f + (rand()%2 ? 0.1 : -0.1);\n    h_kz[i] = 0.3f + (rand()%2 ? 0.1 : -0.1);\n  }\n\n  printf(\"Run FHd on a device\\n\");\n\n    {\n    auto start = std::chrono::steady_clock::now();\n\n        for (int n = 0; n < samples; n++) {\n      float r = rfhd[n];\n      float i = ifhd[n];\n      float xn = h_x[n];\n      float yn = h_y[n];\n      float zn = h_z[n];\n            for (int m = 0; m < voxels; m++) {\n        float e = 2.f * (float)M_PI * \n                  (h_kx[m] * xn + h_ky[m] * yn + h_kz[m] * zn);\n        float c = cosf(e);\n        float s = sinf(e);\n        r += h_rmu[m] * c - h_imu[m] * s;\n        i += h_imu[m] * c + h_rmu[m] * s;\n      }\n      rfhd[n] = r;\n      ifhd[n] = i;   \n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Device execution time %f (s)\\n\", time * 1e-9f);\n  }\n  \n  if (verify) {\n    printf(\"Computing root mean square error between host and device results.\\n\");\n    printf(\"This will take a while..\\n\");\n\n        for (int n = 0; n < samples; n++) {\n      float r = h_rfhd[n];\n      float i = h_ifhd[n];\n            for (int m = 0; m < voxels; m++) {\n        float e = 2.f * (float)M_PI * \n                  (h_kx[m] * h_x[n] + h_ky[m] * h_y[n] + h_kz[m] * h_z[n]);\n        float c = cosf(e);\n        float s = sinf(e);\n        r += h_rmu[m] * c - h_imu[m] * s;\n        i += h_imu[m] * c + h_rmu[m] * s;\n      }\n      h_rfhd[n] = r;\n      h_ifhd[n] = i;   \n    }\n\n    float err = 0.f;\n    for (int i = 0; i < samples; i++) {\n      err += (h_rfhd[i] - rfhd[i]) * (h_rfhd[i] - rfhd[i]) +\n             (h_ifhd[i] - ifhd[i]) * (h_ifhd[i] - ifhd[i]) ;\n    }\n    printf(\"RMSE = %f\\n\", sqrtf(err / (2*samples)));\n  }\n \n  free(h_rmu);\n  free(h_imu);\n  free(h_kx);\n  free(h_ky);\n  free(h_kz);\n  free(h_rfhd);\n  free(h_ifhd);\n  free(rfhd);\n  free(ifhd);\n  free(h_x);\n  free(h_y);\n  free(h_z);\n\n  return 0;\n}"}}
{"kernel_name": "fhd", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n\n#define CHUNK_S 4096\n\n#ifdef __NVPTX__\n  #include <sycl/ext/oneapi/experimental/cuda/builtins.hpp>\n  using namespace sycl::ext::oneapi::experimental::cuda;\n#else\n  #define ldg(a) (*(a))\n#endif\n\n\ntypedef struct {\n  float x, y, z;\n} kdata;\n\nvoid cmpfhd(const float*__restrict rmu,\n            const float*__restrict imu,\n                  float*__restrict rfhd,\n                  float*__restrict ifhd,\n            const float*__restrict x,\n            const float*__restrict y,\n            const float*__restrict z,\n            const kdata*__restrict k,\n            const int samples,\n            const int voxels,\n            sycl::nd_item<1> &item )\n{\n  int n = item.get_global_id(0);\n\n  if (n < samples) {\n    float xn = ldg(&x[n]), yn = ldg(&y[n]), zn = ldg(&z[n]);\n    float rfhdn = ldg(&rfhd[n]), ifhdn = ldg(&ifhd[n]);\n    for (int m = 0; m < voxels; m++) {\n      float e = 2.f * (float)M_PI * (k[m].x * xn + k[m].y * yn + k[m].z * zn);\n      float c = sycl::native::cos(e);\n      float s = sycl::native::sin(e);\n      rfhdn += ldg(&rmu[m]) * c - ldg(&imu[m]) * s;\n      ifhdn += ldg(&imu[m]) * c + ldg(&rmu[m]) * s;\n    }\n    rfhd[n] = rfhdn, ifhd[n] = ifhdn;\n  }\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 4) {\n    printf(\"Usage: %s <#samples> <#voxels> <verify>\\n\", argv[0]);\n    exit(1);\n  }\n  const int samples = atoi(argv[1]); \n\n  const int voxels = atoi(argv[2]);  \n\n  const int verify = atoi(argv[3]);\n  const int sampleSize = samples * sizeof(float);\n  const int voxelSize = voxels * sizeof(float);\n\n  float *h_rmu = (float*) malloc (voxelSize);\n  float *h_imu = (float*) malloc (voxelSize);\n  float *h_kx = (float*) malloc (voxelSize);\n  float *h_ky = (float*) malloc (voxelSize);\n  float *h_kz = (float*) malloc (voxelSize);\n  kdata *h_k = (kdata*) malloc (voxels * sizeof(kdata));\n\n  float *h_rfhd = (float*) malloc (sampleSize);\n  float *h_ifhd = (float*) malloc (sampleSize);\n  float *h_x = (float*) malloc (sampleSize);\n  float *h_y = (float*) malloc (sampleSize);\n  float *h_z = (float*) malloc (sampleSize);\n\n  \n\n  float *rfhd = (float*) malloc (sampleSize);\n  float *ifhd = (float*) malloc (sampleSize);\n\n  srand(2);\n  for (int i = 0; i < samples; i++) {\n    h_rfhd[i] = (float)i/samples;\n    h_ifhd[i] = (float)i/samples;\n    h_x[i] = 0.3f + (rand()%2 ? 0.1 : -0.1);\n    h_y[i] = 0.2f + (rand()%2 ? 0.1 : -0.1);\n    h_z[i] = 0.1f + (rand()%2 ? 0.1 : -0.1);\n  }\n\n  for (int i = 0; i < voxels; i++) {\n    h_rmu[i] = (float)i/voxels;\n    h_imu[i] = (float)i/voxels;\n    h_k[i].x = h_kx[i] = 0.1f + (rand()%2 ? 0.1 : -0.1);\n    h_k[i].y = h_ky[i] = 0.2f + (rand()%2 ? 0.1 : -0.1);\n    h_k[i].z = h_kz[i] = 0.3f + (rand()%2 ? 0.1 : -0.1);\n  }\n\n  printf(\"Run FHd on a device\\n\");\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  float *d_rmu = sycl::malloc_device<float>(voxels, q);\n  q.memcpy(d_rmu, h_rmu, voxelSize);\n\n  float *d_imu = sycl::malloc_device<float>(voxels, q);\n  q.memcpy(d_imu, h_imu, voxelSize);\n\n  float *d_rfhd = sycl::malloc_device<float>(samples, q);\n  q.memcpy(d_rfhd, h_rfhd, sampleSize);\n\n  float *d_ifhd = sycl::malloc_device<float>(samples, q);\n  q.memcpy(d_ifhd, h_ifhd, sampleSize);\n\n  float *d_x = sycl::malloc_device<float>(samples, q);\n  q.memcpy(d_x, h_x, sampleSize);\n\n  float *d_y = sycl::malloc_device<float>(samples, q);\n  q.memcpy(d_y, h_y, sampleSize);\n\n  float *d_z = sycl::malloc_device<float>(samples, q);\n  q.memcpy(d_z, h_z, sampleSize);\n\n  kdata *d_k = sycl::malloc_device<kdata>(CHUNK_S, q);\n\n  const int ntpb = 256;\n  const int nblks = (samples + ntpb - 1) / ntpb * ntpb;\n  sycl::range<1> gws (nblks);\n  sycl::range<1> lws (ntpb);\n\n  int c = CHUNK_S;\n  int s = sizeof(kdata) * c;\n  int nchunks = (voxels + c - 1) / c;\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < nchunks; i++) {\n    if (i == nchunks - 1) {\n      c = voxels - CHUNK_S * i;\n      s = sizeof(kdata) * c;\n    }\n\n    q.memcpy(d_k, &h_k[i * CHUNK_S], s);\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class fhd>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        cmpfhd( d_rmu + i*CHUNK_S,\n                d_imu + i*CHUNK_S,\n                d_rfhd, d_ifhd,\n                d_x, d_y, d_z, d_k,\n                samples, c, item);\n      });\n    });\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Device execution time %f (s)\\n\", time * 1e-9f);\n\n  q.memcpy(rfhd, d_rfhd, sampleSize);\n  q.memcpy(ifhd, d_ifhd, sampleSize);\n\n  q.wait();\n\n  if (verify) {\n    printf(\"Computing root mean square error between host and device results.\\n\");\n    printf(\"This will take a while..\\n\");\n\n    #pragma omp parallel for\n    for (int n = 0; n < samples; n++) {\n      float r = h_rfhd[n];\n      float i = h_ifhd[n];\n      #pragma omp parallel for simd reduction(+:r,i)\n      for (int m = 0; m < voxels; m++) {\n        float e = 2.f * (float)M_PI *\n                  (h_kx[m] * h_x[n] + h_ky[m] * h_y[n] + h_kz[m] * h_z[n]);\n        float c = cosf(e);\n        float s = sinf(e);\n        r += h_rmu[m] * c - h_imu[m] * s;\n        i += h_imu[m] * c + h_rmu[m] * s;\n      }\n      h_rfhd[n] = r;\n      h_ifhd[n] = i;\n    }\n\n    float err = 0.f;\n    for (int i = 0; i < samples; i++) {\n      err += (h_rfhd[i] - rfhd[i]) * (h_rfhd[i] - rfhd[i]) +\n             (h_ifhd[i] - ifhd[i]) * (h_ifhd[i] - ifhd[i]) ;\n    }\n    printf(\"RMSE = %f\\n\", sqrtf(err / (2*samples)));\n  }\n\n  sycl::free(d_rmu, q);\n  sycl::free(d_imu, q);\n  sycl::free(d_rfhd, q);\n  sycl::free(d_ifhd, q);\n  sycl::free(d_x, q);\n  sycl::free(d_y, q);\n  sycl::free(d_z, q);\n  free(h_rmu);\n  free(h_imu);\n  free(h_kx);\n  free(h_ky);\n  free(h_kz);\n  free(h_k);\n  free(h_rfhd);\n  free(h_ifhd);\n  free(rfhd);\n  free(ifhd);\n  free(h_x);\n  free(h_y);\n  free(h_z);\n\n  return 0;\n}\n"}}
{"kernel_name": "goulash", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <sys/time.h>\n#include <cuda.h>\n#include \"utils.h\"\n\n\n\n#define CUDACHECK(x) {cudaError_t err=x; if (err != cudaSuccess) \\\n\tprintf (\"ERROR CUDA failed: %s\", cudaGetErrorString(err));}\n\n__global__ \nvoid gate(double* __restrict__ m_gate, const long nCells, const double* __restrict__ Vm) \n{\n  long ii = blockIdx.x*blockDim.x + threadIdx.x;\n  if (ii >= nCells) return;\n\n  double sum1,sum2;\n  const double x = Vm[ii];\n  const int Mhu_l = 10;\n  const int Mhu_m = 5;\n  const double Mhu_a[] = { 9.9632117206253790e-01,  4.0825738726469545e-02,  6.3401613233199589e-04,  4.4158436861700431e-06,  1.1622058324043520e-08,  1.0000000000000000e+00,  4.0568375699663400e-02,  6.4216825832642788e-04,  4.2661664422410096e-06,  1.3559930396321903e-08, -1.3573468728873069e-11, -4.2594802366702580e-13,  7.6779952208246166e-15,  1.4260675804433780e-16, -2.6656212072499249e-18};\n\n  sum1 = 0;\n  for (int j = Mhu_m-1; j >= 0; j--)\n    sum1 = Mhu_a[j] + x*sum1;\n  sum2 = 0;\n  int k = Mhu_m + Mhu_l - 1;\n  for (int j = k; j >= Mhu_m; j--)\n    sum2 = Mhu_a[j] + x * sum2;\n  double mhu = sum1/sum2;\n\n  const int Tau_m = 18;\n  const double Tau_a[] = {1.7765862602413648e+01*0.02,  5.0010202770602419e-02*0.02, -7.8002064070783474e-04*0.02, -6.9399661775931530e-05*0.02,  1.6936588308244311e-06*0.02,  5.4629017090963798e-07*0.02, -1.3805420990037933e-08*0.02, -8.0678945216155694e-10*0.02,  1.6209833004622630e-11*0.02,  6.5130101230170358e-13*0.02, -6.9931705949674988e-15*0.02, -3.1161210504114690e-16*0.02,  5.0166191902609083e-19*0.02,  7.8608831661430381e-20*0.02,  4.3936315597226053e-22*0.02, -7.0535966258003289e-24*0.02, -9.0473475495087118e-26*0.02, -2.9878427692323621e-28*0.02,  1.0000000000000000e+00};\n\n  sum1 = 0;\n  for (int j = Tau_m-1; j >= 0; j--)\n    sum1 = Tau_a[j] + x*sum1;\n  double tauR = sum1;\n  m_gate[ii] += (mhu - m_gate[ii])*(1-exp(-tauR));\n}\n\nint main(int argc, char* argv[]) \n{\n  if (argc != 3)\n  {\n    printf (\"Usage: %s <Iterations> <Kernel_GBs_used>\\n\\n\", argv[0]);\n    exit (1);\n  }\n\n  \n\n  long iterations = atol(argv[1]);\n  double kernel_mem_used=atof(argv[2]);\n\n  \n\n  long nCells = (long) ((kernel_mem_used * 1024.0 * 1024.0 * 1024.0) / (sizeof(double) * 2));\n  printf(\"Number of cells: %ld\\n\", nCells);\n\n  double* m_gate = (double*)calloc(nCells,sizeof(double));\n  if (m_gate == NULL) printf (\"failed calloc m_gate\\n\");\n\n  \n\n  double* m_gate_h = (double*)calloc(nCells,sizeof(double));\n  if (m_gate_h == NULL) printf (\"failed calloc m_gate_h\\n\");\n\n  double* Vm = (double*)calloc(nCells,sizeof(double));\n  if (Vm == NULL) printf (\"failed calloc Vm\\n\");\n\n  double *d_m_gate, *d_Vm;\n  CUDACHECK(cudaMalloc(&d_m_gate, sizeof(double)*nCells));\n  CUDACHECK(cudaMalloc(&d_Vm, sizeof(double)*nCells));\n\n  CUDACHECK(cudaMemcpy(d_m_gate, m_gate, sizeof(double)*nCells, cudaMemcpyHostToDevice));\n  CUDACHECK(cudaMemcpy(d_Vm, Vm, sizeof(double)*nCells, cudaMemcpyHostToDevice));\n\n  dim3 gridSize ((nCells + 255)/256);\n  dim3 blockSize (256);\n\n  double kernel_starttime, kernel_endtime, kernel_runtime;\n\n  for (long itime=0; itime<=iterations; itime++) {\n    \n\n    if (itime == 1) {\n      CUDACHECK(cudaMemcpy(m_gate, d_m_gate, sizeof(double)*nCells, cudaMemcpyDeviceToHost));\n      kernel_starttime = secs_elapsed();\n    }\n\n    gate<<<gridSize, blockSize>>>(d_m_gate, nCells, d_Vm);\n  }\n\n  cudaDeviceSynchronize();\n  kernel_endtime = secs_elapsed();\n  kernel_runtime = kernel_endtime-kernel_starttime;\n  printf(\"total kernel time %lf(s) for %ld iterations\\n\", kernel_runtime, iterations-1);\n\n  CUDACHECK(cudaFree(d_Vm));\n  CUDACHECK(cudaFree(d_m_gate));\n\n  \n\n  reference(m_gate_h, nCells, Vm);\n\n  bool ok = true;\n  for (long ii = 0; ii < nCells; ii++) {\n    if (fabs(m_gate[ii] - m_gate_h[ii]) > 1e-6) {\n      ok = false;\n      break;\n    }\n  }\n\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  if (m_gate != NULL) free(m_gate);\n  if (m_gate_h != NULL) free(m_gate_h);\n  if (Vm != NULL) free(Vm);\n  return 0;\n}\n"}}
{"kernel_name": "goulash", "parallel_api": "hip", "code": {"main.cu": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <sys/time.h>\n#include <hip/hip_runtime.h>\n#include \"utils.h\"\n\n\n\n#define HIPCHECK(x) {hipError_t err=x; if (err != hipSuccess) \\\n\tprintf (\"ERROR HIP failed: %s\", hipGetErrorString(err));}\n\n__global__ \nvoid gate(double* __restrict__ m_gate, const long nCells, const double* __restrict__ Vm) \n{\n  long i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i >= nCells) return;\n\n  double sum1,sum2;\n  const double x = Vm[i];\n  const int Mhu_l = 10;\n  const int Mhu_m = 5;\n  const double Mhu_a[] = { 9.9632117206253790e-01,  4.0825738726469545e-02,  6.3401613233199589e-04,  4.4158436861700431e-06,  1.1622058324043520e-08,  1.0000000000000000e+00,  4.0568375699663400e-02,  6.4216825832642788e-04,  4.2661664422410096e-06,  1.3559930396321903e-08, -1.3573468728873069e-11, -4.2594802366702580e-13,  7.6779952208246166e-15,  1.4260675804433780e-16, -2.6656212072499249e-18};\n\n  sum1 = 0;\n  for (int j = Mhu_m-1; j >= 0; j--)\n    sum1 = Mhu_a[j] + x*sum1;\n  sum2 = 0;\n  int k = Mhu_m + Mhu_l - 1;\n  for (int j = k; j >= Mhu_m; j--)\n    sum2 = Mhu_a[j] + x * sum2;\n  double mhu = sum1/sum2;\n\n  const int Tau_m = 18;\n  const double Tau_a[] = {1.7765862602413648e+01*0.02,  5.0010202770602419e-02*0.02, -7.8002064070783474e-04*0.02, -6.9399661775931530e-05*0.02,  1.6936588308244311e-06*0.02,  5.4629017090963798e-07*0.02, -1.3805420990037933e-08*0.02, -8.0678945216155694e-10*0.02,  1.6209833004622630e-11*0.02,  6.5130101230170358e-13*0.02, -6.9931705949674988e-15*0.02, -3.1161210504114690e-16*0.02,  5.0166191902609083e-19*0.02,  7.8608831661430381e-20*0.02,  4.3936315597226053e-22*0.02, -7.0535966258003289e-24*0.02, -9.0473475495087118e-26*0.02, -2.9878427692323621e-28*0.02,  1.0000000000000000e+00};\n\n  sum1 = 0;\n  for (int j = Tau_m-1; j >= 0; j--)\n    sum1 = Tau_a[j] + x*sum1;\n  double tauR = sum1;\n  m_gate[i] += (mhu - m_gate[i])*(1-exp(-tauR));\n}\n\nint main(int argc, char* argv[]) \n{\n  if (argc != 3)\n  {\n    printf (\"Usage: %s <Iterations> <Kernel_GBs_used>\\n\\n\", argv[0]);\n    exit (1);\n  }\n\n  \n\n  long iterations = atol(argv[1]);\n  double kernel_mem_used = atof(argv[2]);\n\n  \n\n  long nCells = (long) ((kernel_mem_used * 1024.0 * 1024.0 * 1024.0) / (sizeof(double) * 2));\n  printf(\"Number of cells: %ld\\n\", nCells);\n\n  double* m_gate = (double*)calloc(nCells,sizeof(double));\n  if (m_gate == NULL) printf (\"failed calloc m_gate\\n\");\n\n  \n\n  double* m_gate_h = (double*)calloc(nCells,sizeof(double));\n  if (m_gate_h == NULL) printf (\"failed calloc m_gate_h\\n\");\n\n  double* Vm = (double*)calloc(nCells,sizeof(double));\n  if (Vm == NULL) printf (\"failed calloc Vm\\n\");\n\n  double *d_m_gate, *d_Vm;\n  HIPCHECK(hipMalloc(&d_m_gate, sizeof(double)*nCells));\n  HIPCHECK(hipMalloc(&d_Vm, sizeof(double)*nCells));\n\n  HIPCHECK(hipMemcpy(d_m_gate, m_gate, sizeof(double)*nCells, hipMemcpyHostToDevice));\n  HIPCHECK(hipMemcpy(d_Vm, Vm, sizeof(double)*nCells, hipMemcpyHostToDevice));\n\n  dim3 gridSize ((nCells + 255)/256);\n  dim3 blockSize (256);\n\n  double kernel_starttime, kernel_endtime, kernel_runtime;\n\n  for (long itime=0; itime<=iterations; itime++) {\n    \n\n    if (itime == 1) {\n      HIPCHECK(hipMemcpy(m_gate, d_m_gate, sizeof(double)*nCells, hipMemcpyDeviceToHost));\n      kernel_starttime = secs_elapsed();\n    }\n\n    hipLaunchKernelGGL(gate, gridSize, blockSize, 0, 0, d_m_gate, nCells, d_Vm);\n  }\n\n  hipDeviceSynchronize();\n  kernel_endtime = secs_elapsed();\n  kernel_runtime = kernel_endtime-kernel_starttime;\n  printf(\"total kernel time %lf(s) for %ld iterations\\n\", kernel_runtime, iterations-1);\n\n  HIPCHECK(hipFree(d_Vm));\n  HIPCHECK(hipFree(d_m_gate));\n\n  \n\n  reference(m_gate_h, nCells, Vm);\n\n  bool ok = true;\n  for (long i = 0; i < nCells; i++) {\n    if (fabs(m_gate[i] - m_gate_h[i]) > 1e-6) {\n      ok = false;\n      break;\n    }\n  }\n\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  if (m_gate != NULL) free(m_gate);\n  if (m_gate_h != NULL) free(m_gate_h);\n  if (Vm != NULL) free(Vm);\n  return 0;\n}\n"}}
{"kernel_name": "goulash", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <sys/time.h>\n#include <omp.h>\n#include \"utils.h\"\n\nvoid gate(double* __restrict m_gate, const long nCells, const double* __restrict Vm) \n{\n  #pragma omp target teams distribute parallel for thread_limit(256)\n  for (long i = 0; i < nCells; i++) {\n\n    double sum1,sum2;\n    const double x = Vm[i];\n    const int Mhu_l = 10;\n    const int Mhu_m = 5;\n    const double Mhu_a[] = { 9.9632117206253790e-01,  4.0825738726469545e-02,  6.3401613233199589e-04,  4.4158436861700431e-06,  1.1622058324043520e-08,  1.0000000000000000e+00,  4.0568375699663400e-02,  6.4216825832642788e-04,  4.2661664422410096e-06,  1.3559930396321903e-08, -1.3573468728873069e-11, -4.2594802366702580e-13,  7.6779952208246166e-15,  1.4260675804433780e-16, -2.6656212072499249e-18};\n\n    sum1 = 0;\n    for (int j = Mhu_m-1; j >= 0; j--)\n      sum1 = Mhu_a[j] + x*sum1;\n    sum2 = 0;\n    int k = Mhu_m + Mhu_l - 1;\n    for (int j = k; j >= Mhu_m; j--)\n      sum2 = Mhu_a[j] + x * sum2;\n    double mhu = sum1/sum2;\n\n    const int Tau_m = 18;\n    const double Tau_a[] = {1.7765862602413648e+01*0.02,  5.0010202770602419e-02*0.02, -7.8002064070783474e-04*0.02, -6.9399661775931530e-05*0.02,  1.6936588308244311e-06*0.02,  5.4629017090963798e-07*0.02, -1.3805420990037933e-08*0.02, -8.0678945216155694e-10*0.02,  1.6209833004622630e-11*0.02,  6.5130101230170358e-13*0.02, -6.9931705949674988e-15*0.02, -3.1161210504114690e-16*0.02,  5.0166191902609083e-19*0.02,  7.8608831661430381e-20*0.02,  4.3936315597226053e-22*0.02, -7.0535966258003289e-24*0.02, -9.0473475495087118e-26*0.02, -2.9878427692323621e-28*0.02,  1.0000000000000000e+00};\n\n    sum1 = 0;\n    for (int j = Tau_m-1; j >= 0; j--)\n      sum1 = Tau_a[j] + x*sum1;\n    double tauR = sum1;\n    m_gate[i] += (mhu - m_gate[i])*(1-exp(-tauR));\n  }\n}\n\nint main(int argc, char* argv[]) \n{\n  if (argc != 3)\n  {\n    printf (\"Usage: %s <Iterations> <Kernel_GBs_used>\\n\\n\", argv[0]);\n    exit (1);\n  }\n\n  \n\n  long iterations = atol(argv[1]);\n  double kernel_mem_used = atof(argv[2]);\n\n  \n\n  long nCells = (long) ((kernel_mem_used * 1024.0 * 1024.0 * 1024.0) / (sizeof(double) * 2));\n  printf(\"Number of cells: %ld\\n\", nCells);\n\n  double* m_gate = (double*)calloc(nCells,sizeof(double));\n  if (m_gate == NULL) printf (\"failed calloc m_gate\\n\");\n\n  \n\n  double* m_gate_h = (double*)calloc(nCells,sizeof(double));\n  if (m_gate_h == NULL) printf (\"failed calloc m_gate_h\\n\");\n\n  double* Vm = (double*)calloc(nCells,sizeof(double));\n  if (Vm == NULL) printf (\"failed calloc Vm\\n\");\n\n  #pragma omp target data map (to: m_gate[0:nCells], Vm[0:nCells])\n  {\n    double kernel_starttime, kernel_endtime, kernel_runtime;\n    for (long itime=0; itime<=iterations; itime++) {\n      \n\n      if (itime == 1) {\n        #pragma omp target update from (m_gate[0:nCells])\n        kernel_starttime = secs_elapsed();\n      }\n      gate(m_gate, nCells, Vm);\n    }\n  \n    kernel_endtime = secs_elapsed();\n    kernel_runtime = kernel_endtime-kernel_starttime;\n    printf(\"total kernel time %lf(s) for %ld iterations\\n\", kernel_runtime, iterations-1);\n  }\n\n  \n\n  reference(m_gate_h, nCells, Vm);\n\n  bool ok = true;\n  for (long i = 0; i < nCells; i++) {\n    if (fabs(m_gate[i] - m_gate_h[i]) > 1e-6) {\n      ok = false;\n      break;\n    }\n  }\n\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  if (m_gate != NULL) free(m_gate);\n  if (m_gate_h != NULL) free(m_gate_h);\n  if (Vm != NULL) free(Vm);\n  return 0;\n}\n"}}
{"kernel_name": "goulash", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <sys/time.h>\n#include \"utils.h\"\n\nvoid gate(double* __restrict m_gate, const long nCells, const double* __restrict Vm) \n{\n    for (long i = 0; i < nCells; i++) {\n\n    double sum1,sum2;\n    const double x = Vm[i];\n    const int Mhu_l = 10;\n    const int Mhu_m = 5;\n    const double Mhu_a[] = { 9.9632117206253790e-01,  4.0825738726469545e-02,  6.3401613233199589e-04,  4.4158436861700431e-06,  1.1622058324043520e-08,  1.0000000000000000e+00,  4.0568375699663400e-02,  6.4216825832642788e-04,  4.2661664422410096e-06,  1.3559930396321903e-08, -1.3573468728873069e-11, -4.2594802366702580e-13,  7.6779952208246166e-15,  1.4260675804433780e-16, -2.6656212072499249e-18};\n\n    sum1 = 0;\n    for (int j = Mhu_m-1; j >= 0; j--)\n      sum1 = Mhu_a[j] + x*sum1;\n    sum2 = 0;\n    int k = Mhu_m + Mhu_l - 1;\n    for (int j = k; j >= Mhu_m; j--)\n      sum2 = Mhu_a[j] + x * sum2;\n    double mhu = sum1/sum2;\n\n    const int Tau_m = 18;\n    const double Tau_a[] = {1.7765862602413648e+01*0.02,  5.0010202770602419e-02*0.02, -7.8002064070783474e-04*0.02, -6.9399661775931530e-05*0.02,  1.6936588308244311e-06*0.02,  5.4629017090963798e-07*0.02, -1.3805420990037933e-08*0.02, -8.0678945216155694e-10*0.02,  1.6209833004622630e-11*0.02,  6.5130101230170358e-13*0.02, -6.9931705949674988e-15*0.02, -3.1161210504114690e-16*0.02,  5.0166191902609083e-19*0.02,  7.8608831661430381e-20*0.02,  4.3936315597226053e-22*0.02, -7.0535966258003289e-24*0.02, -9.0473475495087118e-26*0.02, -2.9878427692323621e-28*0.02,  1.0000000000000000e+00};\n\n    sum1 = 0;\n    for (int j = Tau_m-1; j >= 0; j--)\n      sum1 = Tau_a[j] + x*sum1;\n    double tauR = sum1;\n    m_gate[i] += (mhu - m_gate[i])*(1-exp(-tauR));\n  }\n}\n\nint main(int argc, char* argv[]) \n{\n  if (argc != 3)\n  {\n    printf (\"Usage: %s <Iterations> <Kernel_GBs_used>\\n\\n\", argv[0]);\n    exit (1);\n  }\n\n  \n\n  long iterations = atol(argv[1]);\n  double kernel_mem_used = atof(argv[2]);\n\n  \n\n  long nCells = (long) ((kernel_mem_used * 1024.0 * 1024.0 * 1024.0) / (sizeof(double) * 2));\n  printf(\"Number of cells: %ld\\n\", nCells);\n\n  double* m_gate = (double*)calloc(nCells,sizeof(double));\n  if (m_gate == NULL) printf (\"failed calloc m_gate\\n\");\n\n  \n\n  double* m_gate_h = (double*)calloc(nCells,sizeof(double));\n  if (m_gate_h == NULL) printf (\"failed calloc m_gate_h\\n\");\n\n  double* Vm = (double*)calloc(nCells,sizeof(double));\n  if (Vm == NULL) printf (\"failed calloc Vm\\n\");\n\n    {\n    double kernel_starttime, kernel_endtime, kernel_runtime;\n    for (long itime=0; itime<=iterations; itime++) {\n      \n\n      if (itime == 1) {\n                kernel_starttime = secs_elapsed();\n      }\n      gate(m_gate, nCells, Vm);\n    }\n  \n    kernel_endtime = secs_elapsed();\n    kernel_runtime = kernel_endtime-kernel_starttime;\n    printf(\"total kernel time %lf(s) for %ld iterations\\n\", kernel_runtime, iterations-1);\n  }\n\n  \n\n  reference(m_gate_h, nCells, Vm);\n\n  bool ok = true;\n  for (long i = 0; i < nCells; i++) {\n    if (fabs(m_gate[i] - m_gate_h[i]) > 1e-6) {\n      ok = false;\n      break;\n    }\n  }\n\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  if (m_gate != NULL) free(m_gate);\n  if (m_gate_h != NULL) free(m_gate_h);\n  if (Vm != NULL) free(Vm);\n  return 0;\n}"}}
{"kernel_name": "goulash", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <sys/time.h>\n#include <sycl/sycl.hpp>\n#include \"utils.h\"\n\nvoid gate(sycl::nd_item<1> &item, double* __restrict m_gate, \n          const long nCells, const double* __restrict Vm) \n{\n  long i = item.get_global_id(0);\n  if (i >= nCells) return;\n\n  double sum1,sum2;\n  const double x = Vm[i];\n  const int Mhu_l = 10;\n  const int Mhu_m = 5;\n  const double Mhu_a[] = { 9.9632117206253790e-01,  4.0825738726469545e-02,  6.3401613233199589e-04,  4.4158436861700431e-06,  1.1622058324043520e-08,  1.0000000000000000e+00,  4.0568375699663400e-02,  6.4216825832642788e-04,  4.2661664422410096e-06,  1.3559930396321903e-08, -1.3573468728873069e-11, -4.2594802366702580e-13,  7.6779952208246166e-15,  1.4260675804433780e-16, -2.6656212072499249e-18};\n\n  sum1 = 0;\n  for (int j = Mhu_m-1; j >= 0; j--)\n    sum1 = Mhu_a[j] + x*sum1;\n  sum2 = 0;\n  int k = Mhu_m + Mhu_l - 1;\n  for (int j = k; j >= Mhu_m; j--)\n    sum2 = Mhu_a[j] + x * sum2;\n  double mhu = sum1/sum2;\n\n  const int Tau_m = 18;\n  const double Tau_a[] = {1.7765862602413648e+01*0.02,  5.0010202770602419e-02*0.02, -7.8002064070783474e-04*0.02, -6.9399661775931530e-05*0.02,  1.6936588308244311e-06*0.02,  5.4629017090963798e-07*0.02, -1.3805420990037933e-08*0.02, -8.0678945216155694e-10*0.02,  1.6209833004622630e-11*0.02,  6.5130101230170358e-13*0.02, -6.9931705949674988e-15*0.02, -3.1161210504114690e-16*0.02,  5.0166191902609083e-19*0.02,  7.8608831661430381e-20*0.02,  4.3936315597226053e-22*0.02, -7.0535966258003289e-24*0.02, -9.0473475495087118e-26*0.02, -2.9878427692323621e-28*0.02,  1.0000000000000000e+00};\n\n  sum1 = 0;\n  for (int j = Tau_m-1; j >= 0; j--)\n    sum1 = Tau_a[j] + x*sum1;\n  double tauR = sum1;\n  m_gate[i] += (mhu - m_gate[i])*(1-sycl::exp(-tauR));\n}\n\nint main(int argc, char* argv[]) \n{\n  if (argc != 3)\n  {\n    printf (\"Usage: %s <Iterations> <Kernel_GBs_used>\\n\\n\", argv[0]);\n    exit (1);\n  }\n\n  \n\n  long iterations = atol(argv[1]);\n  double kernel_mem_used = atof(argv[2]);\n\n  \n\n  long nCells = (long) ((kernel_mem_used * 1024.0 * 1024.0 * 1024.0) / (sizeof(double) * 2));\n  printf(\"Number of cells: %ld\\n\", nCells);\n\n  double* m_gate = (double*)calloc(nCells,sizeof(double));\n  if (m_gate == NULL) printf (\"failed calloc m_gate\\n\");\n\n  \n\n  double* m_gate_h = (double*)calloc(nCells,sizeof(double));\n  if (m_gate_h == NULL) printf (\"failed calloc m_gate_h\\n\");\n\n  double* Vm = (double*)calloc(nCells,sizeof(double));\n  if (Vm == NULL) printf (\"failed calloc Vm\\n\");\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  double *d_m_gate = sycl::malloc_device<double>(nCells, q);\n  q.memcpy(d_m_gate, m_gate, sizeof(double) * nCells);\n\n  double *d_Vm = sycl::malloc_device<double>(nCells, q);\n  q.memcpy(d_Vm, Vm, sizeof(double) * nCells);\n\n  sycl::range<1> gws ((nCells + 255)/256*256);\n  sycl::range<1> lws (256);\n\n  double kernel_starttime, kernel_endtime, kernel_runtime;\n\n  for (long itime=0; itime<=iterations; itime++) {\n    \n\n    if (itime == 1) {\n      q.memcpy(m_gate, d_m_gate, sizeof(double) * nCells).wait();\n      kernel_starttime = secs_elapsed();\n    }\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class mgate>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        gate(item, d_m_gate, nCells, d_Vm);\n      });\n    });\n  }\n\n  q.wait();\n  kernel_endtime = secs_elapsed();\n  kernel_runtime = kernel_endtime-kernel_starttime;\n  printf(\"total kernel time %lf(s) for %ld iterations\\n\", kernel_runtime, iterations-1);\n\n  sycl::free(d_Vm, q);\n  sycl::free(d_m_gate, q);\n\n  \n\n  reference(m_gate_h, nCells, Vm);\n\n  bool ok = true;\n  for (long i = 0; i < nCells; i++) {\n    if (fabs(m_gate[i] - m_gate_h[i]) > 1e-6) {\n      ok = false;\n      break;\n    }\n  }\n\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  if (m_gate != NULL) free(m_gate);\n  if (m_gate_h != NULL) free(m_gate_h);\n  if (Vm != NULL) free(Vm);\n  return 0;\n}\n"}}
{"kernel_name": "gpp", "parallel_api": "cuda", "code": {"main.cu": "#include <string.h>\n#include <chrono>\n#include <cuda.h>\n\n#ifndef dataType\n#define dataType double\n#endif\n\n#include \"CustomComplex.h\"\n#include \"utils.h\"\n#include \"kernel.h\"\n\nint main(int argc, char **argv) {\n\n  int number_bands = 0, nvband = 0, ncouls = 0, nodes_per_group = 0;\n  if (argc == 1) {\n    number_bands = 512;\n    nvband = 2;\n    ncouls = 512;\n    nodes_per_group = 20;\n  } else if (argc == 2) {\n    if (strcmp(argv[1], \"benchmark\") == 0) {\n      number_bands = 512;\n      nvband = 2;\n      ncouls = 32768;\n      nodes_per_group = 20;\n    } else if (strcmp(argv[1], \"test\") == 0) {\n      number_bands = 512;\n      nvband = 2;\n      ncouls = 512;\n      nodes_per_group = 20;\n    } else {\n      std::cout\n          << \"Usage: ./main <test or benchmark>\\n\"\n          << \"Problem unrecognized, use 'test' or 'benchmark'\"\n          << std::endl;\n      exit(0);\n    }\n  } else if (argc == 5) {\n    number_bands = atoi(argv[1]);\n    nvband = atoi(argv[2]);\n    ncouls = atoi(argv[3]);\n    nodes_per_group = atoi(argv[4]);\n  } else {\n    std::cout << \"The correct form of input is : \" << std::endl;\n    std::cout << \" ./main <number_bands> <number_valence_bands> \"\n                 \"<number_plane_waves> <nodes_per_mpi_group> \"\n              << std::endl;\n    exit(0);\n  }\n  int ngpown = ncouls / nodes_per_group;\n\n  \n\n  const dataType e_lk = 10;\n  const dataType dw = 1;\n  const dataType to1 = 1e-6;\n  const dataType e_n1kq = 6.0;\n\n  \n\n  std::cout << \"Sizeof(CustomComplex<dataType> = \"\n            << sizeof(CustomComplex<dataType>) << \" bytes\" << std::endl;\n  std::cout << \"number_bands = \" << number_bands << \"\\t nvband = \" << nvband\n            << \"\\t ncouls = \" << ncouls\n            << \"\\t nodes_per_group  = \" << nodes_per_group\n            << \"\\t ngpown = \" << ngpown << \"\\t nend = \" << nend\n            << \"\\t nstart = \" << nstart << std::endl;\n\n  CustomComplex<dataType> expr0(0.0, 0.0);\n  CustomComplex<dataType> expr(0.025, 0.025);\n  size_t memFootPrint = 0;\n\n  \n\n  dataType elapsedTimer;\n  timeval startTimer, endTimer;\n  gettimeofday(&startTimer, NULL);\n\n  CustomComplex<dataType> *achtemp;\n  achtemp = (CustomComplex<dataType> *)safe_malloc(\n      achtemp_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += achtemp_size * sizeof(CustomComplex<dataType>);\n\n  CustomComplex<dataType> *aqsmtemp, *aqsntemp;\n  aqsmtemp = (CustomComplex<dataType> *)safe_malloc(\n      aqsmtemp_size * sizeof(CustomComplex<dataType>));\n\n  aqsntemp = (CustomComplex<dataType> *)safe_malloc(\n      aqsntemp_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += 2 * aqsmtemp_size * sizeof(CustomComplex<dataType>);\n\n  CustomComplex<dataType> *I_eps_array, *wtilde_array;\n  I_eps_array = (CustomComplex<dataType> *)safe_malloc(\n      I_eps_array_size * sizeof(CustomComplex<dataType>));\n\n  wtilde_array = (CustomComplex<dataType> *)safe_malloc(\n      I_eps_array_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += 2 * I_eps_array_size * sizeof(CustomComplex<dataType>);\n\n  dataType *vcoul;\n  vcoul = (dataType *)safe_malloc(vcoul_size * sizeof(dataType));\n\n  memFootPrint += vcoul_size * sizeof(dataType);\n\n  int *inv_igp_index, *indinv;\n  inv_igp_index = (int *)safe_malloc(inv_igp_index_size * sizeof(int));\n  indinv = (int *)safe_malloc(indinv_size * sizeof(int));\n\n  \n\n  dataType *achtemp_re, *achtemp_im, *wx_array;\n  achtemp_re = (dataType *)safe_malloc(achtemp_re_size * sizeof(dataType));\n  achtemp_im = (dataType *)safe_malloc(achtemp_im_size * sizeof(dataType));\n  wx_array = (dataType *)safe_malloc(wx_array_size * sizeof(dataType));\n\n  memFootPrint += 3 * wx_array_size * sizeof(double);\n\n  \n\n  CustomComplex<dataType> *d_aqsmtemp, *d_aqsntemp;\n  cudaMalloc((void **)&d_aqsmtemp, aqsmtemp_size * sizeof(CustomComplex<dataType>));\n  cudaMalloc((void **)&d_aqsntemp, aqsntemp_size * sizeof(CustomComplex<dataType>));\n\n  CustomComplex<dataType> *d_I_eps_array, *d_wtilde_array;\n  cudaMalloc((void **)&d_I_eps_array, I_eps_array_size * sizeof(CustomComplex<dataType>));\n  cudaMalloc((void **)&d_wtilde_array, wtilde_array_size * sizeof(CustomComplex<dataType>));\n\n  dataType *d_vcoul, *d_achtemp_re, *d_achtemp_im, *d_wx_array;\n  cudaMalloc((void **)&d_vcoul, vcoul_size * sizeof(dataType));\n  cudaMalloc((void **)&d_wx_array, wx_array_size * sizeof(dataType));\n  cudaMalloc((void **)&d_achtemp_re, achtemp_re_size * sizeof(dataType));\n  cudaMalloc((void **)&d_achtemp_im, achtemp_im_size * sizeof(dataType));\n\n  int *d_inv_igp_index, *d_indinv;\n  cudaMalloc((void **)&d_inv_igp_index, inv_igp_index_size * sizeof(int));\n  cudaMalloc((void **)&d_indinv, indinv_size * sizeof(int));\n\n  \n\n  std::cout << \"Memory Foot Print = \" << memFootPrint / pow(1024, 3) << \" GBs\"\n            << std::endl;\n\n  for (int n1 = 0; n1 < number_bands; n1++)\n    for (int ig = 0; ig < ncouls; ig++) {\n      aqsmtemp(n1, ig) = expr;\n      aqsntemp(n1, ig) = expr;\n    }\n\n  for (int my_igp = 0; my_igp < ngpown; my_igp++)\n    for (int ig = 0; ig < ncouls; ig++) {\n      I_eps_array(my_igp, ig) = expr;\n      wtilde_array(my_igp, ig) = expr;\n    }\n\n  for (int i = 0; i < ncouls; i++)\n    vcoul[i] = i * 0.025;\n\n  for (int ig = 0; ig < ngpown; ++ig)\n    inv_igp_index[ig] = (ig + 1) * ncouls / ngpown;\n\n  for (int ig = 0; ig < ncouls; ++ig)\n    indinv[ig] = ig;\n  indinv[ncouls] = ncouls - 1;\n\n  for (int iw = nstart; iw < nend; ++iw) {\n    achtemp_re[iw] = 0.0;\n    achtemp_im[iw] = 0.0;\n  }\n\n  for (int iw = nstart; iw < nend; ++iw) {\n    wx_array[iw] = e_lk - e_n1kq + dw * ((iw + 1) - 2);\n    if (wx_array[iw] < to1)\n      wx_array[iw] = to1;\n  }\n\n  cudaMemcpy(d_aqsmtemp, aqsmtemp,\n             aqsmtemp_size * sizeof(CustomComplex<dataType>), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_aqsntemp, aqsntemp,\n             aqsntemp_size * sizeof(CustomComplex<dataType>), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_I_eps_array, I_eps_array,\n             I_eps_array_size * sizeof(CustomComplex<dataType>), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_wtilde_array, wtilde_array,\n             wtilde_array_size * sizeof(CustomComplex<dataType>), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_vcoul, vcoul, vcoul_size * sizeof(dataType), cudaMemcpyHostToDevice);\n\n  cudaMemcpy(d_wx_array, wx_array,\n             wx_array_size * sizeof(dataType), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_inv_igp_index, inv_igp_index,\n             inv_igp_index_size * sizeof(int), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_indinv, indinv, indinv_size * sizeof(int), cudaMemcpyHostToDevice);\n\n  dim3 grid(number_bands, ngpown, 1);\n  dim3 threads(32, 1, 1);  \n\n  printf(\"Launching a kernel with grid = \"\n         \"(%d,%d,%d), and threads = (%d,%d,%d) \\n\",\n         number_bands, ngpown, 1, 32, 1, 1);\n\n  float total_time = 0.f;\n\n  for (int i = 0; i < 10; i++) {\n    \n\n    cudaMemcpy(d_achtemp_re, achtemp_re,\n               achtemp_re_size * sizeof(dataType), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_achtemp_im, achtemp_im,\n               achtemp_im_size * sizeof(dataType), cudaMemcpyHostToDevice);\n\n    cudaDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    solver<<<grid, threads>>>(\n        number_bands, ngpown, ncouls, d_inv_igp_index, d_indinv, d_wx_array,\n        d_wtilde_array, d_aqsmtemp, d_aqsntemp, d_I_eps_array, d_vcoul, d_achtemp_re,\n        d_achtemp_im);\n\n    cudaDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n  }\n\n  printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9f) / 10.f);\n\n  cudaMemcpy(achtemp_re, d_achtemp_re,\n             achtemp_re_size * sizeof(dataType), cudaMemcpyDeviceToHost);\n\n  cudaMemcpy(achtemp_im, d_achtemp_im,\n             achtemp_re_size * sizeof(dataType), cudaMemcpyDeviceToHost);\n\n  for (int iw = nstart; iw < nend; ++iw)\n    achtemp[iw] = CustomComplex<dataType>(achtemp_re[iw], achtemp_im[iw]);\n\n  \n\n  if (argc == 2) {\n    if (strcmp(argv[1], \"benchmark\") == 0)\n      correctness(0, achtemp[0]);\n    else if (strcmp(argv[1], \"test\") == 0)\n      correctness(1, achtemp[0]);\n  } else\n    correctness(1, achtemp[0]);\n\n  printf(\"\\n Final achtemp\\n\");\n  achtemp[0].print();\n\n  \n\n  free(achtemp);\n  free(aqsmtemp);\n  free(aqsntemp);\n  free(I_eps_array);\n  free(wtilde_array);\n  free(vcoul);\n  free(inv_igp_index);\n  free(indinv);\n  free(achtemp_re);\n  free(achtemp_im);\n  free(wx_array);\n\n  cudaFree(d_aqsmtemp);\n  cudaFree(d_aqsntemp);\n  cudaFree(d_I_eps_array);\n  cudaFree(d_wtilde_array);\n  cudaFree(d_vcoul);\n  cudaFree(d_inv_igp_index);\n  cudaFree(d_indinv);\n  cudaFree(d_achtemp_re);\n  cudaFree(d_achtemp_im);\n  cudaFree(d_wx_array);\n\n  gettimeofday(&endTimer, NULL);\n  elapsedTimer = (endTimer.tv_sec - startTimer.tv_sec) +\n                 1e-6 * (endTimer.tv_usec - startTimer.tv_usec);\n\n  std::cout << \"********** Total Time Taken **********= \" << elapsedTimer << \" secs\"\n            << std::endl;\n  return 0;\n}\n"}}
{"kernel_name": "gpp", "parallel_api": "hip", "code": {"main.cu": "#include <string.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n\n#ifndef dataType\n#define dataType double\n#endif\n\n#include \"CustomComplex.h\"\n#include \"utils.h\"\n#include \"kernel.h\"\n\nint main(int argc, char **argv) {\n\n  int number_bands = 0, nvband = 0, ncouls = 0, nodes_per_group = 0;\n  if (argc == 1) {\n    number_bands = 512;\n    nvband = 2;\n    ncouls = 512;\n    nodes_per_group = 20;\n  } else if (argc == 2) {\n    if (strcmp(argv[1], \"benchmark\") == 0) {\n      number_bands = 512;\n      nvband = 2;\n      ncouls = 32768;\n      nodes_per_group = 20;\n    } else if (strcmp(argv[1], \"test\") == 0) {\n      number_bands = 512;\n      nvband = 2;\n      ncouls = 512;\n      nodes_per_group = 20;\n    } else {\n      std::cout\n          << \"Usage: ./main <test or benchmark>\\n\"\n          << \"Problem unrecognized, use 'test' or 'benchmark'\"\n          << std::endl;\n      exit(0);\n    }\n  } else if (argc == 5) {\n    number_bands = atoi(argv[1]);\n    nvband = atoi(argv[2]);\n    ncouls = atoi(argv[3]);\n    nodes_per_group = atoi(argv[4]);\n  } else {\n    std::cout << \"The correct form of input is : \" << std::endl;\n    std::cout << \" ./main <number_bands> <number_valence_bands> \"\n                 \"<number_plane_waves> <nodes_per_mpi_group> \"\n              << std::endl;\n    exit(0);\n  }\n  int ngpown = ncouls / nodes_per_group;\n\n  \n\n  const dataType e_lk = 10;\n  const dataType dw = 1;\n  const dataType to1 = 1e-6;\n  const dataType e_n1kq = 6.0;\n\n  \n\n  std::cout << \"Sizeof(CustomComplex<dataType> = \"\n            << sizeof(CustomComplex<dataType>) << \" bytes\" << std::endl;\n  std::cout << \"number_bands = \" << number_bands << \"\\t nvband = \" << nvband\n            << \"\\t ncouls = \" << ncouls\n            << \"\\t nodes_per_group  = \" << nodes_per_group\n            << \"\\t ngpown = \" << ngpown << \"\\t nend = \" << nend\n            << \"\\t nstart = \" << nstart << std::endl;\n\n  CustomComplex<dataType> expr0(0.0, 0.0);\n  CustomComplex<dataType> expr(0.025, 0.025);\n  size_t memFootPrint = 0;\n\n  \n\n  dataType elapsedTimer;\n  timeval startTimer, endTimer;\n  gettimeofday(&startTimer, NULL);\n\n  CustomComplex<dataType> *achtemp;\n  achtemp = (CustomComplex<dataType> *)safe_malloc(\n      achtemp_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += achtemp_size * sizeof(CustomComplex<dataType>);\n\n  CustomComplex<dataType> *aqsmtemp, *aqsntemp;\n  aqsmtemp = (CustomComplex<dataType> *)safe_malloc(\n      aqsmtemp_size * sizeof(CustomComplex<dataType>));\n\n  aqsntemp = (CustomComplex<dataType> *)safe_malloc(\n      aqsntemp_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += 2 * aqsmtemp_size * sizeof(CustomComplex<dataType>);\n\n  CustomComplex<dataType> *I_eps_array, *wtilde_array;\n  I_eps_array = (CustomComplex<dataType> *)safe_malloc(\n      I_eps_array_size * sizeof(CustomComplex<dataType>));\n\n  wtilde_array = (CustomComplex<dataType> *)safe_malloc(\n      I_eps_array_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += 2 * I_eps_array_size * sizeof(CustomComplex<dataType>);\n\n  dataType *vcoul;\n  vcoul = (dataType *)safe_malloc(vcoul_size * sizeof(dataType));\n\n  memFootPrint += vcoul_size * sizeof(dataType);\n\n  int *inv_igp_index, *indinv;\n  inv_igp_index = (int *)safe_malloc(inv_igp_index_size * sizeof(int));\n  indinv = (int *)safe_malloc(indinv_size * sizeof(int));\n\n  \n\n  dataType *achtemp_re, *achtemp_im, *wx_array;\n  achtemp_re = (dataType *)safe_malloc(achtemp_re_size * sizeof(dataType));\n  achtemp_im = (dataType *)safe_malloc(achtemp_im_size * sizeof(dataType));\n  wx_array = (dataType *)safe_malloc(wx_array_size * sizeof(dataType));\n\n  memFootPrint += 3 * wx_array_size * sizeof(double);\n\n  \n\n  CustomComplex<dataType> *d_aqsmtemp, *d_aqsntemp;\n  hipMalloc((void **)&d_aqsmtemp, aqsmtemp_size * sizeof(CustomComplex<dataType>));\n  hipMalloc((void **)&d_aqsntemp, aqsntemp_size * sizeof(CustomComplex<dataType>));\n\n  CustomComplex<dataType> *d_I_eps_array, *d_wtilde_array;\n  hipMalloc((void **)&d_I_eps_array, I_eps_array_size * sizeof(CustomComplex<dataType>));\n  hipMalloc((void **)&d_wtilde_array, wtilde_array_size * sizeof(CustomComplex<dataType>));\n\n  dataType *d_vcoul, *d_achtemp_re, *d_achtemp_im, *d_wx_array;\n  hipMalloc((void **)&d_vcoul, vcoul_size * sizeof(dataType));\n  hipMalloc((void **)&d_wx_array, wx_array_size * sizeof(dataType));\n  hipMalloc((void **)&d_achtemp_re, achtemp_re_size * sizeof(dataType));\n  hipMalloc((void **)&d_achtemp_im, achtemp_im_size * sizeof(dataType));\n\n  int *d_inv_igp_index, *d_indinv;\n  hipMalloc((void **)&d_inv_igp_index, inv_igp_index_size * sizeof(int));\n  hipMalloc((void **)&d_indinv, indinv_size * sizeof(int));\n\n  \n\n  std::cout << \"Memory Foot Print = \" << memFootPrint / pow(1024, 3) << \" GBs\"\n            << std::endl;\n\n  for (int n1 = 0; n1 < number_bands; n1++)\n    for (int ig = 0; ig < ncouls; ig++) {\n      aqsmtemp(n1, ig) = expr;\n      aqsntemp(n1, ig) = expr;\n    }\n\n  for (int my_igp = 0; my_igp < ngpown; my_igp++)\n    for (int ig = 0; ig < ncouls; ig++) {\n      I_eps_array(my_igp, ig) = expr;\n      wtilde_array(my_igp, ig) = expr;\n    }\n\n  for (int i = 0; i < ncouls; i++)\n    vcoul[i] = i * 0.025;\n\n  for (int ig = 0; ig < ngpown; ++ig)\n    inv_igp_index[ig] = (ig + 1) * ncouls / ngpown;\n\n  for (int ig = 0; ig < ncouls; ++ig)\n    indinv[ig] = ig;\n  indinv[ncouls] = ncouls - 1;\n\n  for (int iw = nstart; iw < nend; ++iw) {\n    achtemp_re[iw] = 0.0;\n    achtemp_im[iw] = 0.0;\n  }\n\n  for (int iw = nstart; iw < nend; ++iw) {\n    wx_array[iw] = e_lk - e_n1kq + dw * ((iw + 1) - 2);\n    if (wx_array[iw] < to1)\n      wx_array[iw] = to1;\n  }\n\n  hipMemcpy(d_aqsmtemp, aqsmtemp,\n             aqsmtemp_size * sizeof(CustomComplex<dataType>), hipMemcpyHostToDevice);\n  hipMemcpy(d_aqsntemp, aqsntemp,\n             aqsntemp_size * sizeof(CustomComplex<dataType>), hipMemcpyHostToDevice);\n  hipMemcpy(d_I_eps_array, I_eps_array,\n             I_eps_array_size * sizeof(CustomComplex<dataType>), hipMemcpyHostToDevice);\n  hipMemcpy(d_wtilde_array, wtilde_array,\n             wtilde_array_size * sizeof(CustomComplex<dataType>), hipMemcpyHostToDevice);\n  hipMemcpy(d_vcoul, vcoul, vcoul_size * sizeof(dataType), hipMemcpyHostToDevice);\n\n  hipMemcpy(d_wx_array, wx_array,\n             wx_array_size * sizeof(dataType), hipMemcpyHostToDevice);\n  hipMemcpy(d_inv_igp_index, inv_igp_index,\n             inv_igp_index_size * sizeof(int), hipMemcpyHostToDevice);\n  hipMemcpy(d_indinv, indinv, indinv_size * sizeof(int), hipMemcpyHostToDevice);\n\n  dim3 grid(number_bands, ngpown, 1);\n  dim3 threads(32, 1, 1);  \n\n  printf(\"Launching a kernel with grid = \"\n         \"(%d,%d,%d), and threads = (%d,%d,%d) \\n\",\n         number_bands, ngpown, 1, 32, 1, 1);\n\n  float total_time = 0.f;\n\n  for (int i = 0; i < 10; i++) {\n    \n\n    hipMemcpy(d_achtemp_re, achtemp_re,\n               achtemp_re_size * sizeof(dataType), hipMemcpyHostToDevice);\n    hipMemcpy(d_achtemp_im, achtemp_im,\n               achtemp_im_size * sizeof(dataType), hipMemcpyHostToDevice);\n\n    hipDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    hipLaunchKernelGGL(solver, grid, threads, 0, 0, \n        number_bands, ngpown, ncouls, d_inv_igp_index, d_indinv, d_wx_array,\n        d_wtilde_array, d_aqsmtemp, d_aqsntemp, d_I_eps_array, d_vcoul, d_achtemp_re,\n        d_achtemp_im);\n\n    hipDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n  }\n\n  printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9f) / 10.f);\n\n  hipMemcpy(achtemp_re, d_achtemp_re,\n             achtemp_re_size * sizeof(dataType), hipMemcpyDeviceToHost);\n\n  hipMemcpy(achtemp_im, d_achtemp_im,\n             achtemp_re_size * sizeof(dataType), hipMemcpyDeviceToHost);\n\n  for (int iw = nstart; iw < nend; ++iw)\n    achtemp[iw] = CustomComplex<dataType>(achtemp_re[iw], achtemp_im[iw]);\n\n  \n\n  if (argc == 2) {\n    if (strcmp(argv[1], \"benchmark\") == 0)\n      correctness(0, achtemp[0]);\n    else if (strcmp(argv[1], \"test\") == 0)\n      correctness(1, achtemp[0]);\n  } else\n    correctness(1, achtemp[0]);\n\n  printf(\"\\n Final achtemp\\n\");\n  achtemp[0].print();\n\n  \n\n  free(achtemp);\n  free(aqsmtemp);\n  free(aqsntemp);\n  free(I_eps_array);\n  free(wtilde_array);\n  free(vcoul);\n  free(inv_igp_index);\n  free(indinv);\n  free(achtemp_re);\n  free(achtemp_im);\n  free(wx_array);\n\n  hipFree(d_aqsmtemp);\n  hipFree(d_aqsntemp);\n  hipFree(d_I_eps_array);\n  hipFree(d_wtilde_array);\n  hipFree(d_vcoul);\n  hipFree(d_inv_igp_index);\n  hipFree(d_indinv);\n  hipFree(d_achtemp_re);\n  hipFree(d_achtemp_im);\n  hipFree(d_wx_array);\n\n  gettimeofday(&endTimer, NULL);\n  elapsedTimer = (endTimer.tv_sec - startTimer.tv_sec) +\n                 1e-6 * (endTimer.tv_usec - startTimer.tv_usec);\n\n  std::cout << \"********** Total Time Taken **********= \" << elapsedTimer << \" secs\"\n            << std::endl;\n  return 0;\n}\n"}}
{"kernel_name": "gpp", "parallel_api": "omp", "code": {"main.cpp": "#include <omp.h>\n#include <chrono>\n#include <string.h>\n\n#ifndef dataType\n#define dataType double\n#endif\n\n#include \"CustomComplex.h\"\n#include \"utils.h\"\n\nint main(int argc, char **argv) {\n\n  int number_bands = 0, nvband = 0, ncouls = 0, nodes_per_group = 0;\n  if (argc == 1) {\n    number_bands = 512;\n    nvband = 2;\n    ncouls = 512;\n    nodes_per_group = 20;\n  } else if (argc == 2) {\n    if (strcmp(argv[1], \"benchmark\") == 0) {\n      number_bands = 512;\n      nvband = 2;\n      ncouls = 32768;\n      nodes_per_group = 20;\n    } else if (strcmp(argv[1], \"test\") == 0) {\n      number_bands = 512;\n      nvband = 2;\n      ncouls = 512;\n      nodes_per_group = 20;\n    } else {\n      std::cout\n          << \"Usage: ./main <test or benchmark>\\n\"\n          << \"Problem unrecognized, use 'test' or 'benchmark'\"\n          << std::endl;\n      exit(0);\n    }\n  } else if (argc == 5) {\n    number_bands = atoi(argv[1]);\n    nvband = atoi(argv[2]);\n    ncouls = atoi(argv[3]);\n    nodes_per_group = atoi(argv[4]);\n  } else {\n    std::cout << \"The correct form of input is : \" << std::endl;\n    std::cout << \" ./main <number_bands> <number_valence_bands> \"\n                 \"<number_plane_waves> <nodes_per_mpi_group> \"\n              << std::endl;\n    exit(0);\n  }\n  int ngpown = ncouls / nodes_per_group;\n\n  \n\n  const dataType e_lk = 10;\n  const dataType dw = 1;\n  const dataType to1 = 1e-6;\n  const dataType e_n1kq = 6.0;\n\n  \n\n  dataType elapsedKernelTimer, elapsedTimer;\n  timeval startTimer, endTimer;\n  gettimeofday(&startTimer, NULL);\n\n  \n\n  std::cout << \"Sizeof(CustomComplex<dataType> = \"\n            << sizeof(CustomComplex<dataType>) << \" bytes\" << std::endl;\n  std::cout << \"number_bands = \" << number_bands << \"\\t nvband = \" << nvband\n            << \"\\t ncouls = \" << ncouls\n            << \"\\t nodes_per_group  = \" << nodes_per_group\n            << \"\\t ngpown = \" << ngpown << \"\\t nend = \" << nend\n            << \"\\t nstart = \" << nstart << std::endl;\n\n  CustomComplex<dataType> expr0(0.0, 0.0);\n  CustomComplex<dataType> expr(0.025, 0.025);\n  size_t memFootPrint = 0;\n\n  CustomComplex<dataType> *achtemp;\n  achtemp = (CustomComplex<dataType> *)safe_malloc(\n      achtemp_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += achtemp_size * sizeof(CustomComplex<dataType>);\n\n  CustomComplex<dataType> *aqsmtemp, *aqsntemp;\n  aqsmtemp = (CustomComplex<dataType> *)safe_malloc(\n      aqsmtemp_size * sizeof(CustomComplex<dataType>));\n\n  aqsntemp = (CustomComplex<dataType> *)safe_malloc(\n      aqsntemp_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += 2 * aqsmtemp_size * sizeof(CustomComplex<dataType>);\n\n  CustomComplex<dataType> *I_eps_array, *wtilde_array;\n  I_eps_array = (CustomComplex<dataType> *)safe_malloc(\n      I_eps_array_size * sizeof(CustomComplex<dataType>));\n\n  wtilde_array = (CustomComplex<dataType> *)safe_malloc(\n      I_eps_array_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += 2 * I_eps_array_size * sizeof(CustomComplex<dataType>);\n\n  dataType *vcoul;\n  vcoul = (dataType *)safe_malloc(vcoul_size * sizeof(dataType));\n\n  memFootPrint += vcoul_size * sizeof(dataType);\n\n  int *inv_igp_index, *indinv;\n  inv_igp_index = (int *)safe_malloc(inv_igp_index_size * sizeof(int));\n  indinv = (int *)safe_malloc(indinv_size * sizeof(int));\n\n  \n\n  dataType *achtemp_re, *achtemp_im, *wx_array;\n  achtemp_re = (dataType *)safe_malloc(achtemp_re_size * sizeof(dataType));\n  achtemp_im = (dataType *)safe_malloc(achtemp_im_size * sizeof(dataType));\n  wx_array = (dataType *)safe_malloc(wx_array_size * sizeof(dataType));\n\n  memFootPrint += 3 * wx_array_size * sizeof(double);\n\n  \n\n  std::cout << \"Memory Foot Print = \" << memFootPrint / pow(1024, 3) << \" GBs\"\n            << std::endl;\n\n  for (int n1 = 0; n1 < number_bands; n1++)\n    for (int ig = 0; ig < ncouls; ig++) {\n      aqsmtemp(n1, ig) = expr;\n      aqsntemp(n1, ig) = expr;\n    }\n\n  for (int my_igp = 0; my_igp < ngpown; my_igp++)\n    for (int ig = 0; ig < ncouls; ig++) {\n      I_eps_array(my_igp, ig) = expr;\n      wtilde_array(my_igp, ig) = expr;\n    }\n\n  for (int i = 0; i < ncouls; i++)\n    vcoul[i] = i * 0.025;\n\n  for (int ig = 0; ig < ngpown; ++ig)\n    inv_igp_index[ig] = (ig + 1) * ncouls / ngpown;\n\n  for (int ig = 0; ig < ncouls; ++ig)\n    indinv[ig] = ig;\n  indinv[ncouls] = ncouls - 1;\n\n  for (int iw = nstart; iw < nend; ++iw) {\n    achtemp_re[iw] = 0.0;\n    achtemp_im[iw] = 0.0;\n  }\n\n  for (int iw = nstart; iw < nend; ++iw) {\n    wx_array[iw] = e_lk - e_n1kq + dw * ((iw + 1) - 2);\n    if (wx_array[iw] < to1)\n      wx_array[iw] = to1;\n  }\n\n  #pragma omp target data map(to \\\n    : aqsmtemp [0:aqsmtemp_size], vcoul [0:vcoul_size],                        \\\n      inv_igp_index [0:inv_igp_index_size], indinv [0:indinv_size],            \\\n      aqsntemp [0:aqsntemp_size], I_eps_array [0:I_eps_array_size],            \\\n      wx_array [0:wx_array_size], wtilde_array [0:wtilde_array_size])\n  {\n    dataType ach_re0, ach_re1, ach_re2, ach_im0, ach_im1, ach_im2;\n\n    float total_time = 0.f;\n\n    for (int i = 0; i < 10; i++) {\n      ach_re0 = 0.0, ach_re1 = 0.0, ach_re2 = 0.0,\n      ach_im0 = 0.0, ach_im1 = 0.0, ach_im2 = 0.0;\n\n      auto start = std::chrono::steady_clock::now();\n\n      #pragma omp target teams distribute parallel for collapse(2) \\\n       reduction(+:ach_re0, ach_re1, ach_re2, ach_im0, ach_im1, ach_im2)\n      for (int my_igp = 0; my_igp < ngpown; ++my_igp) \n\n      {\n        for (int n1 = 0; n1 < number_bands; ++n1) \n\n        {\n          int indigp = inv_igp_index[my_igp];\n          int igp = indinv[indigp];\n\n          dataType achtemp_re_loc[nend - nstart], achtemp_im_loc[nend - nstart];\n          #pragma unroll\n          for (size_t iw = nstart; iw < nend; ++iw) {\n            achtemp_re_loc[iw] = 0.0;\n            achtemp_im_loc[iw] = 0.0;\n          }\n\n          CustomComplex<dataType> sch_store1 =\n              aqsmtemp(n1, igp).conj() * aqsntemp(n1, igp) * 0.5 *\n              vcoul[igp];\n\n          for (size_t ig = 0; ig < ncouls; ++ig) \n\n          {\n            #pragma unroll\n            for (size_t iw = nstart; iw < nend; ++iw) \n\n            {\n              CustomComplex<dataType> wdiff =\n                  wx_array[iw] - wtilde_array(my_igp, ig);\n              CustomComplex<dataType> delw =\n                  wtilde_array(my_igp, ig) * wdiff.conj() *\n                  (1 / CustomComplex_real((wdiff * wdiff.conj())));\n              CustomComplex<dataType> sch_array =\n                  delw * I_eps_array(my_igp, ig) * sch_store1;\n\n              achtemp_re_loc[iw] += sch_array.real();\n              achtemp_im_loc[iw] += sch_array.imag();\n            }\n          }\n\n          ach_re0 += achtemp_re_loc[0];\n          ach_re1 += achtemp_re_loc[1];\n          ach_re2 += achtemp_re_loc[2];\n          ach_im0 += achtemp_im_loc[0];\n          ach_im1 += achtemp_im_loc[1];\n          ach_im2 += achtemp_im_loc[2];\n        } \n\n      }   \n\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      total_time += time;\n    } \n\n\n    printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9f) / 10.f);\n\n    achtemp_re[0] = ach_re0;\n    achtemp_re[1] = ach_re1;\n    achtemp_re[2] = ach_re2;\n    achtemp_im[0] = ach_im0;\n    achtemp_im[1] = ach_im1;\n    achtemp_im[2] = ach_im2;\n\n    for (int iw = nstart; iw < nend; ++iw)\n      achtemp[iw] = CustomComplex<dataType>(achtemp_re[iw], achtemp_im[iw]);\n\n    \n\n    if (argc == 2) {\n      if (strcmp(argv[1], \"benchmark\") == 0)\n        correctness(0, achtemp[0]);\n      else if (strcmp(argv[1], \"test\") == 0)\n        correctness(1, achtemp[0]);\n    } else\n      correctness(1, achtemp[0]);\n\n    printf(\"\\n Final achtemp\\n\");\n    achtemp[0].print();\n\n    gettimeofday(&endTimer, NULL);\n    elapsedTimer = (endTimer.tv_sec - startTimer.tv_sec) +\n                   1e-6 * (endTimer.tv_usec - startTimer.tv_usec);\n  }\n\n  \n\n  free(achtemp);\n  free(aqsmtemp);\n  free(aqsntemp);\n  free(I_eps_array);\n  free(wtilde_array);\n  free(vcoul);\n  free(inv_igp_index);\n  free(indinv);\n  free(achtemp_re);\n  free(achtemp_im);\n  free(wx_array);\n\n  std::cout << \"********** Total Time Taken **********= \" << elapsedTimer << \" secs\"\n            << std::endl;\n\n  return 0;\n}\n"}}
{"kernel_name": "gpp", "parallel_api": "serial", "code": {"main.cpp": "#include <chrono>\n#include <string.h>\n\n#ifndef dataType\n#define dataType double\n#endif\n\n#include \"CustomComplex.h\"\n#include \"utils.h\"\n\nint main(int argc, char **argv) {\n\n  int number_bands = 0, nvband = 0, ncouls = 0, nodes_per_group = 0;\n  if (argc == 1) {\n    number_bands = 512;\n    nvband = 2;\n    ncouls = 512;\n    nodes_per_group = 20;\n  } else if (argc == 2) {\n    if (strcmp(argv[1], \"benchmark\") == 0) {\n      number_bands = 512;\n      nvband = 2;\n      ncouls = 32768;\n      nodes_per_group = 20;\n    } else if (strcmp(argv[1], \"test\") == 0) {\n      number_bands = 512;\n      nvband = 2;\n      ncouls = 512;\n      nodes_per_group = 20;\n    } else {\n      std::cout\n          << \"Usage: ./main <test or benchmark>\\n\"\n          << \"Problem unrecognized, use 'test' or 'benchmark'\"\n          << std::endl;\n      exit(0);\n    }\n  } else if (argc == 5) {\n    number_bands = atoi(argv[1]);\n    nvband = atoi(argv[2]);\n    ncouls = atoi(argv[3]);\n    nodes_per_group = atoi(argv[4]);\n  } else {\n    std::cout << \"The correct form of input is : \" << std::endl;\n    std::cout << \" ./main <number_bands> <number_valence_bands> \"\n                 \"<number_plane_waves> <nodes_per_mpi_group> \"\n              << std::endl;\n    exit(0);\n  }\n  int ngpown = ncouls / nodes_per_group;\n\n  \n\n  const dataType e_lk = 10;\n  const dataType dw = 1;\n  const dataType to1 = 1e-6;\n  const dataType e_n1kq = 6.0;\n\n  \n\n  dataType elapsedKernelTimer, elapsedTimer;\n  timeval startTimer, endTimer;\n  gettimeofday(&startTimer, NULL);\n\n  \n\n  std::cout << \"Sizeof(CustomComplex<dataType> = \"\n            << sizeof(CustomComplex<dataType>) << \" bytes\" << std::endl;\n  std::cout << \"number_bands = \" << number_bands << \"\\t nvband = \" << nvband\n            << \"\\t ncouls = \" << ncouls\n            << \"\\t nodes_per_group  = \" << nodes_per_group\n            << \"\\t ngpown = \" << ngpown << \"\\t nend = \" << nend\n            << \"\\t nstart = \" << nstart << std::endl;\n\n  CustomComplex<dataType> expr0(0.0, 0.0);\n  CustomComplex<dataType> expr(0.025, 0.025);\n  size_t memFootPrint = 0;\n\n  CustomComplex<dataType> *achtemp;\n  achtemp = (CustomComplex<dataType> *)safe_malloc(\n      achtemp_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += achtemp_size * sizeof(CustomComplex<dataType>);\n\n  CustomComplex<dataType> *aqsmtemp, *aqsntemp;\n  aqsmtemp = (CustomComplex<dataType> *)safe_malloc(\n      aqsmtemp_size * sizeof(CustomComplex<dataType>));\n\n  aqsntemp = (CustomComplex<dataType> *)safe_malloc(\n      aqsntemp_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += 2 * aqsmtemp_size * sizeof(CustomComplex<dataType>);\n\n  CustomComplex<dataType> *I_eps_array, *wtilde_array;\n  I_eps_array = (CustomComplex<dataType> *)safe_malloc(\n      I_eps_array_size * sizeof(CustomComplex<dataType>));\n\n  wtilde_array = (CustomComplex<dataType> *)safe_malloc(\n      I_eps_array_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += 2 * I_eps_array_size * sizeof(CustomComplex<dataType>);\n\n  dataType *vcoul;\n  vcoul = (dataType *)safe_malloc(vcoul_size * sizeof(dataType));\n\n  memFootPrint += vcoul_size * sizeof(dataType);\n\n  int *inv_igp_index, *indinv;\n  inv_igp_index = (int *)safe_malloc(inv_igp_index_size * sizeof(int));\n  indinv = (int *)safe_malloc(indinv_size * sizeof(int));\n\n  \n\n  dataType *achtemp_re, *achtemp_im, *wx_array;\n  achtemp_re = (dataType *)safe_malloc(achtemp_re_size * sizeof(dataType));\n  achtemp_im = (dataType *)safe_malloc(achtemp_im_size * sizeof(dataType));\n  wx_array = (dataType *)safe_malloc(wx_array_size * sizeof(dataType));\n\n  memFootPrint += 3 * wx_array_size * sizeof(double);\n\n  \n\n  std::cout << \"Memory Foot Print = \" << memFootPrint / pow(1024, 3) << \" GBs\"\n            << std::endl;\n\n  for (int n1 = 0; n1 < number_bands; n1++)\n    for (int ig = 0; ig < ncouls; ig++) {\n      aqsmtemp(n1, ig) = expr;\n      aqsntemp(n1, ig) = expr;\n    }\n\n  for (int my_igp = 0; my_igp < ngpown; my_igp++)\n    for (int ig = 0; ig < ncouls; ig++) {\n      I_eps_array(my_igp, ig) = expr;\n      wtilde_array(my_igp, ig) = expr;\n    }\n\n  for (int i = 0; i < ncouls; i++)\n    vcoul[i] = i * 0.025;\n\n  for (int ig = 0; ig < ngpown; ++ig)\n    inv_igp_index[ig] = (ig + 1) * ncouls / ngpown;\n\n  for (int ig = 0; ig < ncouls; ++ig)\n    indinv[ig] = ig;\n  indinv[ncouls] = ncouls - 1;\n\n  for (int iw = nstart; iw < nend; ++iw) {\n    achtemp_re[iw] = 0.0;\n    achtemp_im[iw] = 0.0;\n  }\n\n  for (int iw = nstart; iw < nend; ++iw) {\n    wx_array[iw] = e_lk - e_n1kq + dw * ((iw + 1) - 2);\n    if (wx_array[iw] < to1)\n      wx_array[iw] = to1;\n  }\n\n    {\n    dataType ach_re0, ach_re1, ach_re2, ach_im0, ach_im1, ach_im2;\n\n    float total_time = 0.f;\n\n    for (int i = 0; i < 10; i++) {\n      ach_re0 = 0.0, ach_re1 = 0.0, ach_re2 = 0.0,\n      ach_im0 = 0.0, ach_im1 = 0.0, ach_im2 = 0.0;\n\n      auto start = std::chrono::steady_clock::now();\n\n            for (int my_igp = 0; my_igp < ngpown; ++my_igp) \n\n      {\n        for (int n1 = 0; n1 < number_bands; ++n1) \n\n        {\n          int indigp = inv_igp_index[my_igp];\n          int igp = indinv[indigp];\n\n          dataType achtemp_re_loc[nend - nstart], achtemp_im_loc[nend - nstart];\n                    for (size_t iw = nstart; iw < nend; ++iw) {\n            achtemp_re_loc[iw] = 0.0;\n            achtemp_im_loc[iw] = 0.0;\n          }\n\n          CustomComplex<dataType> sch_store1 =\n              aqsmtemp(n1, igp).conj() * aqsntemp(n1, igp) * 0.5 *\n              vcoul[igp];\n\n          for (size_t ig = 0; ig < ncouls; ++ig) \n\n          {\n                        for (size_t iw = nstart; iw < nend; ++iw) \n\n            {\n              CustomComplex<dataType> wdiff =\n                  wx_array[iw] - wtilde_array(my_igp, ig);\n              CustomComplex<dataType> delw =\n                  wtilde_array(my_igp, ig) * wdiff.conj() *\n                  (1 / CustomComplex_real((wdiff * wdiff.conj())));\n              CustomComplex<dataType> sch_array =\n                  delw * I_eps_array(my_igp, ig) * sch_store1;\n\n              achtemp_re_loc[iw] += sch_array.real();\n              achtemp_im_loc[iw] += sch_array.imag();\n            }\n          }\n\n          ach_re0 += achtemp_re_loc[0];\n          ach_re1 += achtemp_re_loc[1];\n          ach_re2 += achtemp_re_loc[2];\n          ach_im0 += achtemp_im_loc[0];\n          ach_im1 += achtemp_im_loc[1];\n          ach_im2 += achtemp_im_loc[2];\n        } \n\n      }   \n\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      total_time += time;\n    } \n\n\n    printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9f) / 10.f);\n\n    achtemp_re[0] = ach_re0;\n    achtemp_re[1] = ach_re1;\n    achtemp_re[2] = ach_re2;\n    achtemp_im[0] = ach_im0;\n    achtemp_im[1] = ach_im1;\n    achtemp_im[2] = ach_im2;\n\n    for (int iw = nstart; iw < nend; ++iw)\n      achtemp[iw] = CustomComplex<dataType>(achtemp_re[iw], achtemp_im[iw]);\n\n    \n\n    if (argc == 2) {\n      if (strcmp(argv[1], \"benchmark\") == 0)\n        correctness(0, achtemp[0]);\n      else if (strcmp(argv[1], \"test\") == 0)\n        correctness(1, achtemp[0]);\n    } else\n      correctness(1, achtemp[0]);\n\n    printf(\"\\n Final achtemp\\n\");\n    achtemp[0].print();\n\n    gettimeofday(&endTimer, NULL);\n    elapsedTimer = (endTimer.tv_sec - startTimer.tv_sec) +\n                   1e-6 * (endTimer.tv_usec - startTimer.tv_usec);\n  }\n\n  \n\n  free(achtemp);\n  free(aqsmtemp);\n  free(aqsntemp);\n  free(I_eps_array);\n  free(wtilde_array);\n  free(vcoul);\n  free(inv_igp_index);\n  free(indinv);\n  free(achtemp_re);\n  free(achtemp_im);\n  free(wx_array);\n\n  std::cout << \"********** Total Time Taken **********= \" << elapsedTimer << \" secs\"\n            << std::endl;\n\n  return 0;\n}"}}
{"kernel_name": "gpp", "parallel_api": "sycl", "code": {"main.cpp": "#include <string.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n\n#ifndef dataType\n#define dataType double\n#endif\n\n#include \"CustomComplex.h\"\n#include \"utils.h\"\n#include \"kernel.h\"\n\nint main(int argc, char **argv) {\n\n  int number_bands = 0, nvband = 0, ncouls = 0, nodes_per_group = 0;\n  if (argc == 1) {\n    number_bands = 512;\n    nvband = 2;\n    ncouls = 512;\n    nodes_per_group = 20;\n  } else if (argc == 2) {\n    if (strcmp(argv[1], \"benchmark\") == 0) {\n      number_bands = 512;\n      nvband = 2;\n      ncouls = 32768;\n      nodes_per_group = 20;\n    } else if (strcmp(argv[1], \"test\") == 0) {\n      number_bands = 512;\n      nvband = 2;\n      ncouls = 512;\n      nodes_per_group = 20;\n    } else {\n      std::cout\n          << \"Usage: ./main <test or benchmark>\\n\"\n          << \"Problem unrecognized, use 'test' or 'benchmark'\"\n          << std::endl;\n      exit(0);\n    }\n  } else if (argc == 5) {\n    number_bands = atoi(argv[1]);\n    nvband = atoi(argv[2]);\n    ncouls = atoi(argv[3]);\n    nodes_per_group = atoi(argv[4]);\n  } else {\n    std::cout << \"The correct form of input is : \" << std::endl;\n    std::cout << \" ./main <number_bands> <number_valence_bands> \"\n                 \"<number_plane_waves> <nodes_per_mpi_group> \"\n              << std::endl;\n    exit(0);\n  }\n  int ngpown = ncouls / nodes_per_group;\n\n  \n\n  const dataType e_lk = 10;\n  const dataType dw = 1;\n  const dataType to1 = 1e-6;\n  const dataType e_n1kq = 6.0;\n\n  \n\n  dataType elapsedKernelTimer, elapsedTimer;\n  timeval startTimer, endTimer;\n  gettimeofday(&startTimer, NULL);\n\n  \n\n  std::cout << \"Sizeof(CustomComplex<dataType> = \"\n            << sizeof(CustomComplex<dataType>) << \" bytes\" << std::endl;\n  std::cout << \"number_bands = \" << number_bands << \"\\t nvband = \" << nvband\n            << \"\\t ncouls = \" << ncouls\n            << \"\\t nodes_per_group  = \" << nodes_per_group\n            << \"\\t ngpown = \" << ngpown << \"\\t nend = \" << nend\n            << \"\\t nstart = \" << nstart << std::endl;\n\n  CustomComplex<dataType> expr0(0.0, 0.0);\n  CustomComplex<dataType> expr(0.025, 0.025);\n  size_t memFootPrint = 0;\n\n  CustomComplex<dataType> *achtemp;\n  achtemp = (CustomComplex<dataType> *)safe_malloc(\n      achtemp_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += achtemp_size * sizeof(CustomComplex<dataType>);\n\n  CustomComplex<dataType> *aqsmtemp, *aqsntemp;\n  aqsmtemp = (CustomComplex<dataType> *)safe_malloc(\n      aqsmtemp_size * sizeof(CustomComplex<dataType>));\n\n  aqsntemp = (CustomComplex<dataType> *)safe_malloc(\n      aqsntemp_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += 2 * aqsmtemp_size * sizeof(CustomComplex<dataType>);\n\n  CustomComplex<dataType> *I_eps_array, *wtilde_array;\n  I_eps_array = (CustomComplex<dataType> *)safe_malloc(\n      I_eps_array_size * sizeof(CustomComplex<dataType>));\n\n  wtilde_array = (CustomComplex<dataType> *)safe_malloc(\n      I_eps_array_size * sizeof(CustomComplex<dataType>));\n\n  memFootPrint += 2 * I_eps_array_size * sizeof(CustomComplex<dataType>);\n\n  dataType *vcoul;\n  vcoul = (dataType *)safe_malloc(vcoul_size * sizeof(dataType));\n\n  memFootPrint += vcoul_size * sizeof(dataType);\n\n  int *inv_igp_index, *indinv;\n  inv_igp_index = (int *)safe_malloc(inv_igp_index_size * sizeof(int));\n  indinv = (int *)safe_malloc(indinv_size * sizeof(int));\n\n  \n\n  dataType *achtemp_re, *achtemp_im, *wx_array;\n  achtemp_re = (dataType *)safe_malloc(achtemp_re_size * sizeof(dataType));\n  achtemp_im = (dataType *)safe_malloc(achtemp_im_size * sizeof(dataType));\n  wx_array = (dataType *)safe_malloc(wx_array_size * sizeof(dataType));\n\n  memFootPrint += 3 * wx_array_size * sizeof(double);\n\n\n  \n\n  std::cout << \"Memory Foot Print = \" << memFootPrint / pow(1024, 3) << \" GBs\"\n            << std::endl;\n\n  for (int n1 = 0; n1 < number_bands; n1++)\n    for (int ig = 0; ig < ncouls; ig++) {\n      aqsmtemp(n1, ig) = expr;\n      aqsntemp(n1, ig) = expr;\n    }\n\n  for (int my_igp = 0; my_igp < ngpown; my_igp++)\n    for (int ig = 0; ig < ncouls; ig++) {\n      I_eps_array(my_igp, ig) = expr;\n      wtilde_array(my_igp, ig) = expr;\n    }\n\n  for (int i = 0; i < ncouls; i++)\n    vcoul[i] = i * 0.025;\n\n  for (int ig = 0; ig < ngpown; ++ig)\n    inv_igp_index[ig] = (ig + 1) * ncouls / ngpown;\n\n  for (int ig = 0; ig < ncouls; ++ig)\n    indinv[ig] = ig;\n  indinv[ncouls] = ncouls - 1;\n\n  for (int iw = nstart; iw < nend; ++iw) {\n    achtemp_re[iw] = 0.0;\n    achtemp_im[iw] = 0.0;\n  }\n\n  for (int iw = nstart; iw < nend; ++iw) {\n    wx_array[iw] = e_lk - e_n1kq + dw * ((iw + 1) - 2);\n    if (wx_array[iw] < to1)\n      wx_array[iw] = to1;\n  }\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  \n\n  CustomComplex<dataType> *d_aqsmtemp = \n    sycl::malloc_device<CustomComplex<dataType>>(aqsmtemp_size, q);\n  q.memcpy(d_aqsmtemp, aqsmtemp, aqsmtemp_size * sizeof(CustomComplex<dataType>));\n\n  CustomComplex<dataType> *d_aqsntemp =\n    sycl::malloc_device<CustomComplex<dataType>>(aqsntemp_size, q);\n  q.memcpy(d_aqsntemp, aqsntemp, aqsntemp_size * sizeof(CustomComplex<dataType>));\n\n  CustomComplex<dataType> *d_I_eps_array = \n    sycl::malloc_device<CustomComplex<dataType>>(I_eps_array_size, q);\n  q.memcpy(d_I_eps_array, I_eps_array, I_eps_array_size * sizeof(CustomComplex<dataType>));\n\n  CustomComplex<dataType> *d_wtilde_array =\n    sycl::malloc_device<CustomComplex<dataType>>(wtilde_array_size, q);\n  q.memcpy(d_wtilde_array, wtilde_array, wtilde_array_size * sizeof(CustomComplex<dataType>));\n\n  dataType *d_vcoul = sycl::malloc_device<dataType>(vcoul_size, q);\n  q.memcpy(d_vcoul, vcoul, vcoul_size * sizeof(dataType));\n\n  dataType *d_wx_array = sycl::malloc_device<dataType>(wx_array_size, q); \n  q.memcpy(d_wx_array, wx_array, wx_array_size * sizeof(dataType));\n\n  dataType *d_achtemp_re = sycl::malloc_device<dataType>(achtemp_re_size, q);\n  dataType *d_achtemp_im = sycl::malloc_device<dataType>(achtemp_im_size, q);\n\n  int *d_inv_igp_index = sycl::malloc_device<int>(inv_igp_index_size, q);\n  int *d_indinv = sycl::malloc_device<int>(indinv_size, q);\n\n  q.memcpy(d_inv_igp_index, inv_igp_index, inv_igp_index_size * sizeof(int));\n  q.memcpy(d_indinv, indinv, indinv_size * sizeof(int));\n\n  \n\n  timeval startKernelTimer, endKernelTimer;\n  gettimeofday(&startKernelTimer, NULL);\n\n  sycl::range<3> gws(1, ngpown, 32 * number_bands);\n  sycl::range<3> lws(1, 1, 32);\n  printf(\"Launching a kernel with global work size = \"\n         \"(%d,%d,%d), and local work size = (%d,%d,%d) \\n\",\n         number_bands * 32, ngpown, 1, 32, 1, 1);\n\n  float total_time = 0.f;\n\n  for (int i = 0; i < 10; i++) {\n    \n\n    q.memcpy(d_achtemp_re, achtemp_re, achtemp_re_size * sizeof(dataType));\n    q.memcpy(d_achtemp_im, achtemp_im, achtemp_im_size * sizeof(dataType));\n\n    q.wait();\n    auto start = std::chrono::steady_clock::now();\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class gpp_kernel>(\n        sycl::nd_range<3>(gws, lws), [=] (sycl::nd_item<3> item) {\n        solver(item, number_bands, ngpown, ncouls, \n               d_inv_igp_index, d_indinv, d_wx_array, d_wtilde_array,  \n               d_aqsmtemp, d_aqsntemp, d_I_eps_array, d_vcoul, d_achtemp_re, d_achtemp_im);\n      });\n    }).wait();\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n  }\n\n  printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9f) / 10.f);\n\n  q.memcpy(achtemp_re, d_achtemp_re, achtemp_re_size * sizeof(dataType));\n  q.memcpy(achtemp_im, d_achtemp_im, achtemp_im_size * sizeof(dataType));\n  q.wait();\n\n  gettimeofday(&endKernelTimer, NULL);\n\n  elapsedKernelTimer =\n      (endKernelTimer.tv_sec - startKernelTimer.tv_sec) +\n      1e-6 * (endKernelTimer.tv_usec - startKernelTimer.tv_usec);\n\n  for (int iw = nstart; iw < nend; ++iw)\n    achtemp[iw] = CustomComplex<dataType>(achtemp_re[iw], achtemp_im[iw]);\n\n  \n\n  if (argc == 2) {\n    if (strcmp(argv[1], \"benchmark\") == 0)\n      correctness(0, achtemp[0]);\n    else if (strcmp(argv[1], \"test\") == 0)\n      correctness(1, achtemp[0]);\n  } else\n    correctness(1, achtemp[0]);\n\n  printf(\"\\n Final achtemp\\n\");\n  achtemp[0].print();\n\n  gettimeofday(&endTimer, NULL);\n  elapsedTimer = (endTimer.tv_sec - startTimer.tv_sec) +\n                 1e-6 * (endTimer.tv_usec - startTimer.tv_usec);\n\n  \n\n  free(achtemp);\n  free(aqsmtemp);\n  free(aqsntemp);\n  free(I_eps_array);\n  free(wtilde_array);\n  free(vcoul);\n  free(inv_igp_index);\n  free(indinv);\n  free(achtemp_re);\n  free(achtemp_im);\n  free(wx_array);\n\n  sycl::free(d_aqsmtemp, q);\n  sycl::free(d_aqsntemp, q);\n  sycl::free(d_I_eps_array, q);\n  sycl::free(d_wtilde_array, q);\n  sycl::free(d_vcoul, q);\n  sycl::free(d_inv_igp_index, q);\n  sycl::free(d_indinv, q);\n  sycl::free(d_achtemp_re, q);\n  sycl::free(d_achtemp_im, q);\n  sycl::free(d_wx_array, q);\n\n  std::cout << \"********** Kernel Time Taken **********= \" << elapsedKernelTimer\n            << \" secs\" << std::endl;\n  std::cout << \"********** Total Time Taken **********= \" << elapsedTimer << \" secs\"\n            << std::endl;\n\n  return 0;\n}\n"}}
{"kernel_name": "haccmk", "parallel_api": "cuda", "code": {"haccmk.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <cuda.h>\n\n__global__ void\nhaccmk_kernel (\n    const int n1,  \n\n    const int n2,  \n\n    const float *__restrict__ xx, \n    const float *__restrict__ yy,\n    const float *__restrict__ zz,\n    const float *__restrict__ mass,\n          float *__restrict__ vx2,\n          float *__restrict__ vy2,\n          float *__restrict__ vz2,\n    const float fsrmax,\n    const float mp_rsm,\n    const float fcoeff ) \n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n1) return;\n\n  const float ma0 = 0.269327f; \n  const float ma1 = -0.0750978f; \n  const float ma2 = 0.0114808f; \n  const float ma3 = -0.00109313f; \n  const float ma4 = 0.0000605491f; \n  const float ma5 = -0.00000147177f;\n\n  float dxc, dyc, dzc, m, r2, f, xi, yi, zi;\n\n  xi = 0.f; \n  yi = 0.f;\n  zi = 0.f;\n\n  float xxi = xx[i];\n  float yyi = yy[i];\n  float zzi = zz[i];\n\n  for ( int j = 0; j < n2; j++ ) {\n    dxc = xx[j] - xxi;\n    dyc = yy[j] - yyi;\n    dzc = zz[j] - zzi;\n\n    r2 = dxc * dxc + dyc * dyc + dzc * dzc;\n\n    if ( r2 < fsrmax ) m = mass[j]; else m = 0.f;\n\n    f = r2 + mp_rsm;\n    f = m * (1.f / (f * sqrtf(f)) - (ma0 + r2*(ma1 + r2*(ma2 + r2*(ma3 + r2*(ma4 + r2*ma5))))));\n\n    xi = xi + f * dxc;\n    yi = yi + f * dyc;\n    zi = zi + f * dzc;\n  }\n\n  vx2[i] += xi * fcoeff;\n  vy2[i] += yi * fcoeff;\n  vz2[i] += zi * fcoeff;\n}\n\nvoid haccmk (\n    const int repeat,\n    const int n1,\n    const int n2,\n    const float* xx, \n    const float* yy,\n    const float* zz,\n    const float* mass,\n    float* vx2,\n    float* vy2,\n    float* vz2,\n    const float fsrmax,\n    const float mp_rsm,\n    const float fcoeff ) \n{\n  const int block_size = 256;\n\n  float *d_xx, *d_yy, *d_zz, *d_mass;\n  float *d_vx2, *d_vy2, *d_vz2;\n\n  cudaMalloc((void**) &d_xx, sizeof(float) * n2);\n  cudaMalloc((void**) &d_yy, sizeof(float) * n2);\n  cudaMalloc((void**) &d_zz, sizeof(float) * n2);\n  cudaMalloc((void**) &d_mass, sizeof(float) * n2);\n  cudaMalloc((void**) &d_vx2, sizeof(float) * n1);\n  cudaMalloc((void**) &d_vy2, sizeof(float) * n1);\n  cudaMalloc((void**) &d_vz2, sizeof(float) * n1);\n\n  cudaMemcpy(d_xx, xx, sizeof(float) * n2, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_yy, yy, sizeof(float) * n2, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_zz, zz, sizeof(float) * n2, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_mass, mass, sizeof(float) * n2, cudaMemcpyHostToDevice);\n\n  dim3 grids ((n1+block_size-1)/block_size);\n  dim3 blocks (block_size);\n\n  float total_time = 0.f;\n\n  for (int i = 0; i < repeat; i++) {\n    \n\n    cudaMemcpy(d_vx2, vx2, sizeof(float) * n1, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_vy2, vy2, sizeof(float) * n1, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_vz2, vz2, sizeof(float) * n1, cudaMemcpyHostToDevice);\n\n    cudaDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    haccmk_kernel <<< grids, blocks >>> (\n      n1, n2, d_xx, d_yy, d_zz, d_mass,\n      d_vx2, d_vy2, d_vz2, fsrmax, mp_rsm, fcoeff);\n\n    cudaDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n  }\n\n  printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9f) / repeat);\n\n  cudaMemcpy(vx2, d_vx2, sizeof(float) * n1, cudaMemcpyDeviceToHost);\n  cudaMemcpy(vy2, d_vy2, sizeof(float) * n1, cudaMemcpyDeviceToHost);\n  cudaMemcpy(vz2, d_vz2, sizeof(float) * n1, cudaMemcpyDeviceToHost);\n  cudaFree(d_xx);\n  cudaFree(d_yy);\n  cudaFree(d_zz);\n  cudaFree(d_mass);\n  cudaFree(d_vx2);\n  cudaFree(d_vy2);\n  cudaFree(d_vz2);\n}\n\nvoid haccmk_gold(\n    int count1,\n    float xxi,\n    float yyi,\n    float zzi,\n    float fsrrmax2,\n    float mp_rsm2, \n    float *__restrict__ xx1, \n    float *__restrict__ yy1, \n    float *__restrict__ zz1, \n    float *__restrict__ mass1, \n    float *__restrict__ dxi,\n    float *__restrict__ dyi,\n    float *__restrict__ dzi )\n{\n  int j;\n  const float ma0 = 0.269327, ma1 = -0.0750978, ma2 = 0.0114808, \n        ma3 = -0.00109313, ma4 = 0.0000605491, ma5 = -0.00000147177;\n  float dxc, dyc, dzc, m, r2, f, xi, yi, zi;\n\n  xi = 0.f; \n  yi = 0.f;\n  zi = 0.f;\n\n  for ( j = 0; j < count1; j++ ) {\n    dxc = xx1[j] - xxi;\n    dyc = yy1[j] - yyi;\n    dzc = zz1[j] - zzi;\n\n    r2 = dxc * dxc + dyc * dyc + dzc * dzc;\n\n    if ( r2 < fsrrmax2 ) m = mass1[j]; else m = 0.f;\n\n    f = r2 + mp_rsm2;\n    f =  m * ( 1.f / ( f * sqrtf( f ) ) - ( ma0 + r2*(ma1 + r2*(ma2 + r2*(ma3 + r2*(ma4 + r2*ma5))))));\n\n    xi = xi + f * dxc;\n    yi = yi + f * dyc;\n    zi = zi + f * dzc;\n  }\n\n  *dxi = xi;\n  *dyi = yi;\n  *dzi = zi;\n}\n\nint main( int argc, char *argv[] )\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  float fsrrmax2, mp_rsm2, fcoeff, dx1, dy1, dz1, dx2, dy2, dz2;\n  int n1, n2, i;\n  n1 = 784;\n  n2 = 15000;\n  printf( \"Outer loop count is set %d\\n\", n1 );\n  printf( \"Inner loop count is set %d\\n\", n2 );\n\n  float* xx = (float*) malloc (sizeof(float) * n2);\n  float* yy = (float*) malloc (sizeof(float) * n2);\n  float* zz = (float*) malloc (sizeof(float) * n2);\n  float* mass = (float*) malloc (sizeof(float) * n2);\n  float* vx2 = (float*) malloc (sizeof(float) * n2);\n  float* vy2 = (float*) malloc (sizeof(float) * n2);\n  float* vz2 = (float*) malloc (sizeof(float) * n2);\n  float* vx2_hw = (float*) malloc (sizeof(float) * n2);\n  float* vy2_hw = (float*) malloc (sizeof(float) * n2);\n  float* vz2_hw = (float*) malloc (sizeof(float) * n2);\n\n  \n\n  fcoeff = 0.23f;  \n  fsrrmax2 = 0.5f; \n  mp_rsm2 = 0.03f;\n  dx1 = 1.0f/(float)n2;\n  dy1 = 2.0f/(float)n2;\n  dz1 = 3.0f/(float)n2;\n  xx[0] = 0.f;\n  yy[0] = 0.f;\n  zz[0] = 0.f;\n  mass[0] = 2.f;\n\n  for ( i = 1; i < n2; i++ ) {\n    xx[i] = xx[i-1] + dx1;\n    yy[i] = yy[i-1] + dy1;\n    zz[i] = zz[i-1] + dz1;\n    mass[i] = (float)i * 0.01f + xx[i];\n  }\n\n  for ( i = 0; i < n2; i++ ) {\n    vx2[i] = 0.f;\n    vy2[i] = 0.f;\n    vz2[i] = 0.f;\n    vx2_hw[i] = 0.f;\n    vy2_hw[i] = 0.f;\n    vz2_hw[i] = 0.f;\n  }\n\n  for ( i = 0; i < n1; ++i) {\n    haccmk_gold( n2, xx[i], yy[i], zz[i], fsrrmax2, mp_rsm2, xx, yy, zz, mass, &dx2, &dy2, &dz2 );    \n    vx2[i] = vx2[i] + dx2 * fcoeff;\n    vy2[i] = vy2[i] + dy2 * fcoeff;\n    vz2[i] = vz2[i] + dz2 * fcoeff;\n  }\n\n  haccmk(repeat, n1, n2, xx, yy, zz, mass,\n         vx2_hw, vy2_hw, vz2_hw, fsrrmax2, mp_rsm2, fcoeff);\n\n  \n\n  int error = 0;\n  const float eps = 1e-1f;\n  for (i = 0; i < n2; i++) {\n    if (fabsf(vx2[i] - vx2_hw[i]) > eps) {\n      printf(\"error at vx2[%d] %f %f\\n\", i, vx2[i], vx2_hw[i]);\n      error = 1;\n      break;\n    }\n    if (fabsf(vy2[i] - vy2_hw[i]) > eps) {\n      printf(\"error at vy2[%d]: %f %f\\n\", i, vy2[i], vy2_hw[i]);\n      error = 1;\n      break;\n    }\n    if (fabsf(vz2[i] - vz2_hw[i]) > eps) {\n      printf(\"error at vz2[%d]: %f %f\\n\", i, vz2[i], vz2_hw[i]);\n      error = 1;\n      break;\n    }\n  }\n\n  free(xx);\n  free(yy);\n  free(zz);\n  free(mass);\n  free(vx2);\n  free(vy2);\n  free(vz2);\n  free(vx2_hw);\n  free(vy2_hw);\n  free(vz2_hw);\n\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  return 0;\n}\n"}}
{"kernel_name": "haccmk", "parallel_api": "hip", "code": {"haccmk.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n\n__global__ void\nhaccmk_kernel (\n    const int n1,  \n\n    const int n2,  \n\n    const float *__restrict__ xx, \n    const float *__restrict__ yy,\n    const float *__restrict__ zz,\n    const float *__restrict__ mass,\n          float *__restrict__ vx2,\n          float *__restrict__ vy2,\n          float *__restrict__ vz2,\n    const float fsrmax,\n    const float mp_rsm,\n    const float fcoeff ) \n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n1) return;\n\n  const float ma0 = 0.269327f; \n  const float ma1 = -0.0750978f; \n  const float ma2 = 0.0114808f; \n  const float ma3 = -0.00109313f; \n  const float ma4 = 0.0000605491f; \n  const float ma5 = -0.00000147177f;\n\n  float dxc, dyc, dzc, m, r2, f, xi, yi, zi;\n\n  xi = 0.f; \n  yi = 0.f;\n  zi = 0.f;\n\n  float xxi = xx[i];\n  float yyi = yy[i];\n  float zzi = zz[i];\n\n  for ( int j = 0; j < n2; j++ ) {\n    dxc = xx[j] - xxi;\n    dyc = yy[j] - yyi;\n    dzc = zz[j] - zzi;\n\n    r2 = dxc * dxc + dyc * dyc + dzc * dzc;\n\n    if ( r2 < fsrmax ) m = mass[j]; else m = 0.f;\n\n    f = r2 + mp_rsm;\n    f = m * (1.f / (f * sqrtf(f)) - (ma0 + r2*(ma1 + r2*(ma2 + r2*(ma3 + r2*(ma4 + r2*ma5))))));\n\n    xi = xi + f * dxc;\n    yi = yi + f * dyc;\n    zi = zi + f * dzc;\n  }\n\n  vx2[i] += xi * fcoeff;\n  vy2[i] += yi * fcoeff;\n  vz2[i] += zi * fcoeff;\n}\n\nvoid haccmk (\n    const int repeat,\n    const int n1,\n    const int n2,\n    const float* xx, \n    const float* yy,\n    const float* zz,\n    const float* mass,\n    float* vx2,\n    float* vy2,\n    float* vz2,\n    const float fsrmax,\n    const float mp_rsm,\n    const float fcoeff ) \n{\n  const int block_size = 256;\n\n  float *d_xx, *d_yy, *d_zz, *d_mass;\n  float *d_vx2, *d_vy2, *d_vz2;\n\n  hipMalloc((void**) &d_xx, sizeof(float) * n2);\n  hipMalloc((void**) &d_yy, sizeof(float) * n2);\n  hipMalloc((void**) &d_zz, sizeof(float) * n2);\n  hipMalloc((void**) &d_mass, sizeof(float) * n2);\n  hipMalloc((void**) &d_vx2, sizeof(float) * n1);\n  hipMalloc((void**) &d_vy2, sizeof(float) * n1);\n  hipMalloc((void**) &d_vz2, sizeof(float) * n1);\n\n  hipMemcpy(d_xx, xx, sizeof(float) * n2, hipMemcpyHostToDevice);\n  hipMemcpy(d_yy, yy, sizeof(float) * n2, hipMemcpyHostToDevice);\n  hipMemcpy(d_zz, zz, sizeof(float) * n2, hipMemcpyHostToDevice);\n  hipMemcpy(d_mass, mass, sizeof(float) * n2, hipMemcpyHostToDevice);\n\n  dim3 grids ((n1+block_size-1)/block_size);\n  dim3 blocks (block_size);\n\n  float total_time = 0.f;\n\n  for (int i = 0; i < repeat; i++) {\n    \n\n    hipMemcpy(d_vx2, vx2, sizeof(float) * n1, hipMemcpyHostToDevice);\n    hipMemcpy(d_vy2, vy2, sizeof(float) * n1, hipMemcpyHostToDevice);\n    hipMemcpy(d_vz2, vz2, sizeof(float) * n1, hipMemcpyHostToDevice);\n\n    hipDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    hipLaunchKernelGGL(haccmk_kernel, grids, blocks , 0, 0, \n      n1, n2, d_xx, d_yy, d_zz, d_mass,\n      d_vx2, d_vy2, d_vz2, fsrmax, mp_rsm, fcoeff);\n\n    hipDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n  }\n\n  printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9f) / repeat);\n\n  hipMemcpy(vx2, d_vx2, sizeof(float) * n1, hipMemcpyDeviceToHost);\n  hipMemcpy(vy2, d_vy2, sizeof(float) * n1, hipMemcpyDeviceToHost);\n  hipMemcpy(vz2, d_vz2, sizeof(float) * n1, hipMemcpyDeviceToHost);\n  hipFree(d_xx);\n  hipFree(d_yy);\n  hipFree(d_zz);\n  hipFree(d_mass);\n  hipFree(d_vx2);\n  hipFree(d_vy2);\n  hipFree(d_vz2);\n}\n\nvoid haccmk_gold(\n    int count1,\n    float xxi,\n    float yyi,\n    float zzi,\n    float fsrrmax2,\n    float mp_rsm2, \n    float *__restrict__ xx1, \n    float *__restrict__ yy1, \n    float *__restrict__ zz1, \n    float *__restrict__ mass1, \n    float *__restrict__ dxi,\n    float *__restrict__ dyi,\n    float *__restrict__ dzi )\n{\n  int j;\n  const float ma0 = 0.269327, ma1 = -0.0750978, ma2 = 0.0114808, \n        ma3 = -0.00109313, ma4 = 0.0000605491, ma5 = -0.00000147177;\n  float dxc, dyc, dzc, m, r2, f, xi, yi, zi;\n\n  xi = 0.f; \n  yi = 0.f;\n  zi = 0.f;\n\n  for ( j = 0; j < count1; j++ ) {\n    dxc = xx1[j] - xxi;\n    dyc = yy1[j] - yyi;\n    dzc = zz1[j] - zzi;\n\n    r2 = dxc * dxc + dyc * dyc + dzc * dzc;\n\n    if ( r2 < fsrrmax2 ) m = mass1[j]; else m = 0.f;\n\n    f = r2 + mp_rsm2;\n    f =  m * ( 1.f / ( f * sqrtf( f ) ) - ( ma0 + r2*(ma1 + r2*(ma2 + r2*(ma3 + r2*(ma4 + r2*ma5))))));\n\n    xi = xi + f * dxc;\n    yi = yi + f * dyc;\n    zi = zi + f * dzc;\n  }\n\n  *dxi = xi;\n  *dyi = yi;\n  *dzi = zi;\n}\n\nint main( int argc, char *argv[] )\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  float fsrrmax2, mp_rsm2, fcoeff, dx1, dy1, dz1, dx2, dy2, dz2;\n  int n1, n2, i;\n  n1 = 784;\n  n2 = 15000;\n  printf( \"Outer loop count is set %d\\n\", n1 );\n  printf( \"Inner loop count is set %d\\n\", n2 );\n\n  float* xx = (float*) malloc (sizeof(float) * n2);\n  float* yy = (float*) malloc (sizeof(float) * n2);\n  float* zz = (float*) malloc (sizeof(float) * n2);\n  float* mass = (float*) malloc (sizeof(float) * n2);\n  float* vx2 = (float*) malloc (sizeof(float) * n2);\n  float* vy2 = (float*) malloc (sizeof(float) * n2);\n  float* vz2 = (float*) malloc (sizeof(float) * n2);\n  float* vx2_hw = (float*) malloc (sizeof(float) * n2);\n  float* vy2_hw = (float*) malloc (sizeof(float) * n2);\n  float* vz2_hw = (float*) malloc (sizeof(float) * n2);\n\n  \n\n  fcoeff = 0.23f;  \n  fsrrmax2 = 0.5f; \n  mp_rsm2 = 0.03f;\n  dx1 = 1.0f/(float)n2;\n  dy1 = 2.0f/(float)n2;\n  dz1 = 3.0f/(float)n2;\n  xx[0] = 0.f;\n  yy[0] = 0.f;\n  zz[0] = 0.f;\n  mass[0] = 2.f;\n\n  for ( i = 1; i < n2; i++ ) {\n    xx[i] = xx[i-1] + dx1;\n    yy[i] = yy[i-1] + dy1;\n    zz[i] = zz[i-1] + dz1;\n    mass[i] = (float)i * 0.01f + xx[i];\n  }\n\n  for ( i = 0; i < n2; i++ ) {\n    vx2[i] = 0.f;\n    vy2[i] = 0.f;\n    vz2[i] = 0.f;\n    vx2_hw[i] = 0.f;\n    vy2_hw[i] = 0.f;\n    vz2_hw[i] = 0.f;\n  }\n\n  for ( i = 0; i < n1; ++i) {\n    haccmk_gold( n2, xx[i], yy[i], zz[i], fsrrmax2, mp_rsm2, xx, yy, zz, mass, &dx2, &dy2, &dz2 );    \n    vx2[i] = vx2[i] + dx2 * fcoeff;\n    vy2[i] = vy2[i] + dy2 * fcoeff;\n    vz2[i] = vz2[i] + dz2 * fcoeff;\n  }\n\n  haccmk(repeat, n1, n2, xx, yy, zz, mass,\n         vx2_hw, vy2_hw, vz2_hw, fsrrmax2, mp_rsm2, fcoeff);\n\n  \n\n  int error = 0;\n  const float eps = 1e-1f;\n  for (i = 0; i < n2; i++) {\n    if (fabsf(vx2[i] - vx2_hw[i]) > eps) {\n      printf(\"error at vx2[%d] %f %f\\n\", i, vx2[i], vx2_hw[i]);\n      error = 1;\n      break;\n    }\n    if (fabsf(vy2[i] - vy2_hw[i]) > eps) {\n      printf(\"error at vy2[%d]: %f %f\\n\", i, vy2[i], vy2_hw[i]);\n      error = 1;\n      break;\n    }\n    if (fabsf(vz2[i] - vz2_hw[i]) > eps) {\n      printf(\"error at vz2[%d]: %f %f\\n\", i, vz2[i], vz2_hw[i]);\n      error = 1;\n      break;\n    }\n  }\n\n  free(xx);\n  free(yy);\n  free(zz);\n  free(mass);\n  free(vx2);\n  free(vy2);\n  free(vz2);\n  free(vx2_hw);\n  free(vy2_hw);\n  free(vz2_hw);\n\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  return 0;\n}\n"}}
{"kernel_name": "haccmk", "parallel_api": "omp", "code": {"haccmk.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <omp.h>\n\ntemplate <typename T>\nvoid haccmk (\n    const int repeat,\n    const size_t n,  \n\n    const int ilp, \n\n    const T fsrrmax,\n    const T mp_rsm,\n    const T fcoeff,\n    const T*__restrict xx, \n    const T*__restrict yy,\n    const T*__restrict zz,\n    const T*__restrict mass,\n    T*__restrict vx2,\n    T*__restrict vy2,\n    T*__restrict vz2 ) \n{\n  #pragma omp target data map(to: xx[0:ilp], yy[0:ilp], zz[0:ilp], mass[0:ilp]) \\\n                          map(from: vx2[0:n], vy2[0:n], vz2[0:n])\n  {\n    float total_time = 0.f;\n\n    for (int i = 0; i < repeat; i++) {\n      #pragma omp target update to (vx2[0:n])\n      #pragma omp target update to (vy2[0:n])\n      #pragma omp target update to (vz2[0:n])\n\n      auto start = std::chrono::steady_clock::now();\n\n      #pragma omp target teams distribute parallel for\n      for (int i = 0; i < n; i++) {\n\n        const float ma0 = 0.269327f; \n        const float ma1 = -0.0750978f; \n        const float ma2 = 0.0114808f; \n        const float ma3 = -0.00109313f; \n        const float ma4 = 0.0000605491f; \n        const float ma5 = -0.00000147177f;\n\n        float dxc, dyc, dzc, m, r2, f, xi, yi, zi;\n\n        xi = 0.f; \n        yi = 0.f;\n        zi = 0.f;\n\n        float xxi = xx[i];\n        float yyi = yy[i];\n        float zzi = zz[i];\n\n        for ( int j = 0; j < ilp; j++ ) {\n          dxc = xx[j] - xxi;\n          dyc = yy[j] - yyi;\n          dzc = zz[j] - zzi;\n\n          r2 = dxc * dxc + dyc * dyc + dzc * dzc;\n\n          if ( r2 < fsrrmax ) m = mass[j]; else m = 0.f;\n\n          f = r2 + mp_rsm;\n          f = m * ( 1.f / (f * sqrtf(f)) - \n              (ma0 + r2*(ma1 + r2*(ma2 + r2*(ma3 + r2*(ma4 + r2*ma5))))));\n\n          xi = xi + f * dxc;\n          yi = yi + f * dyc;\n          zi = zi + f * dzc;\n        }\n\n        vx2[i] += xi * fcoeff;\n        vy2[i] += yi * fcoeff;\n        vz2[i] += zi * fcoeff;\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      total_time += time;\n    }\n    printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9f) / repeat);\n  }\n}\n\nvoid haccmk_gold(\n    int count1,\n    float xxi,\n    float yyi,\n    float zzi,\n    float fsrrmax2,\n    float mp_rsm2, \n    float *__restrict xx1, \n    float *__restrict yy1, \n    float *__restrict zz1, \n    float *__restrict mass1, \n    float *__restrict dxi,\n    float *__restrict dyi,\n    float *__restrict dzi )\n{\n  const float ma0 = 0.269327f, \n              ma1 = -0.0750978f, \n              ma2 = 0.0114808f,\n              ma3 = -0.00109313f,\n              ma4 = 0.0000605491f,\n              ma5 = -0.00000147177f;\n\n\n  float dxc, dyc, dzc, m, r2, f, xi, yi, zi;\n\n  xi = 0.f; \n  yi = 0.f;\n  zi = 0.f;\n\n  for (int j = 0; j < count1; j++ ) {\n    dxc = xx1[j] - xxi;\n    dyc = yy1[j] - yyi;\n    dzc = zz1[j] - zzi;\n\n    r2 = dxc * dxc + dyc * dyc + dzc * dzc;\n\n    if ( r2 < fsrrmax2 ) m = mass1[j]; else m = 0.f;\n\n    f = r2 + mp_rsm2;\n    f =  m * (1.f / (f * sqrtf(f)) - (ma0 + r2*(ma1 + r2*(ma2 + r2*(ma3 + r2*(ma4 + r2*ma5))))));\n\n    xi = xi + f * dxc;\n    yi = yi + f * dyc;\n    zi = zi + f * dzc;\n  }\n\n  *dxi = xi;\n  *dyi = yi;\n  *dzi = zi;\n}\n\n\nint main( int argc, char *argv[] )\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  float fsrrmax2, mp_rsm2, fcoeff, dx1, dy1, dz1, dx2, dy2, dz2;\n  int n1, n2, i;\n  n1 = 784;\n  n2 = 15000;\n  printf( \"Outer loop count is set %d\\n\", n1 );\n  printf( \"Inner loop count is set %d\\n\", n2 );\n\n  float* xx = (float*) malloc (sizeof(float) * n2);\n  float* yy = (float*) malloc (sizeof(float) * n2);\n  float* zz = (float*) malloc (sizeof(float) * n2);\n  float* mass = (float*) malloc (sizeof(float) * n2);\n  float* vx2 = (float*) malloc (sizeof(float) * n2);\n  float* vy2 = (float*) malloc (sizeof(float) * n2);\n  float* vz2 = (float*) malloc (sizeof(float) * n2);\n  float* vx2_hw = (float*) malloc (sizeof(float) * n2);\n  float* vy2_hw = (float*) malloc (sizeof(float) * n2);\n  float* vz2_hw = (float*) malloc (sizeof(float) * n2);\n\n  \n\n  fcoeff = 0.23f;  \n  fsrrmax2 = 0.5f; \n  mp_rsm2 = 0.03f;\n  dx1 = 1.0f/(float)n2;\n  dy1 = 2.0f/(float)n2;\n  dz1 = 3.0f/(float)n2;\n  xx[0] = 0.f;\n  yy[0] = 0.f;\n  zz[0] = 0.f;\n  mass[0] = 2.f;\n\n  for ( i = 1; i < n2; i++ ) {\n    xx[i] = xx[i-1] + dx1;\n    yy[i] = yy[i-1] + dy1;\n    zz[i] = zz[i-1] + dz1;\n    mass[i] = (float)i * 0.01f + xx[i];\n  }\n\n  for ( i = 0; i < n2; i++ ) {\n    vx2[i] = 0.f;\n    vy2[i] = 0.f;\n    vz2[i] = 0.f;\n    vx2_hw[i] = 0.f; \n    vy2_hw[i] = 0.f; \n    vz2_hw[i] = 0.f;\n  }\n\n  for ( i = 0; i < n1; ++i) {\n    haccmk_gold( n2, xx[i], yy[i], zz[i], fsrrmax2, mp_rsm2, xx, yy, zz, mass, &dx2, &dy2, &dz2 );    \n    vx2[i] = vx2[i] + dx2 * fcoeff;\n    vy2[i] = vy2[i] + dy2 * fcoeff;\n    vz2[i] = vz2[i] + dz2 * fcoeff;\n  }\n\n  haccmk(repeat, n1, n2, fsrrmax2, mp_rsm2, fcoeff, xx,\n      yy, zz, mass, vx2_hw, vy2_hw, vz2_hw); \n\n  \n\n  int error = 0;\n  const float eps = 1e-1f;\n  for (i = 0; i < n2; i++) {\n    if (fabsf(vx2[i] - vx2_hw[i]) > eps) {\n      printf(\"error at vx2[%d] %f %f\\n\", i, vx2[i], vx2_hw[i]);\n      error = 1;\n      break;\n    }\n    if (fabsf(vy2[i] - vy2_hw[i]) > eps) {\n      printf(\"error at vy2[%d]: %f %f\\n\", i, vy2[i], vy2_hw[i]);\n      error = 1;\n      break;\n    }\n    if (fabsf(vz2[i] - vz2_hw[i]) > eps) {\n      printf(\"error at vz2[%d]: %f %f\\n\", i, vz2[i], vz2_hw[i]);\n      error = 1;\n      break;\n    }\n  } \n\n  free(xx);\n  free(yy);\n  free(zz);\n  free(mass);\n  free(vx2);\n  free(vy2);\n  free(vz2);\n  free(vx2_hw);\n  free(vy2_hw);\n  free(vz2_hw);\n\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  return 0;\n}\n"}}
{"kernel_name": "haccmk", "parallel_api": "serial", "code": {"haccmk.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n\ntemplate <typename T>\nvoid haccmk (\n    const int repeat,\n    const size_t n,  \n\n    const int ilp, \n\n    const T fsrrmax,\n    const T mp_rsm,\n    const T fcoeff,\n    const T*__restrict xx, \n    const T*__restrict yy,\n    const T*__restrict zz,\n    const T*__restrict mass,\n    T*__restrict vx2,\n    T*__restrict vy2,\n    T*__restrict vz2 ) \n{\n    {\n    float total_time = 0.f;\n\n    for (int i = 0; i < repeat; i++) {\n                  \n      auto start = std::chrono::steady_clock::now();\n\n            for (int i = 0; i < n; i++) {\n\n        const float ma0 = 0.269327f; \n        const float ma1 = -0.0750978f; \n        const float ma2 = 0.0114808f; \n        const float ma3 = -0.00109313f; \n        const float ma4 = 0.0000605491f; \n        const float ma5 = -0.00000147177f;\n\n        float dxc, dyc, dzc, m, r2, f, xi, yi, zi;\n\n        xi = 0.f; \n        yi = 0.f;\n        zi = 0.f;\n\n        float xxi = xx[i];\n        float yyi = yy[i];\n        float zzi = zz[i];\n\n        for ( int j = 0; j < ilp; j++ ) {\n          dxc = xx[j] - xxi;\n          dyc = yy[j] - yyi;\n          dzc = zz[j] - zzi;\n\n          r2 = dxc * dxc + dyc * dyc + dzc * dzc;\n\n          if ( r2 < fsrrmax ) m = mass[j]; else m = 0.f;\n\n          f = r2 + mp_rsm;\n          f = m * ( 1.f / (f * sqrtf(f)) - \n              (ma0 + r2*(ma1 + r2*(ma2 + r2*(ma3 + r2*(ma4 + r2*ma5))))));\n\n          xi = xi + f * dxc;\n          yi = yi + f * dyc;\n          zi = zi + f * dzc;\n        }\n\n        vx2[i] += xi * fcoeff;\n        vy2[i] += yi * fcoeff;\n        vz2[i] += zi * fcoeff;\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      total_time += time;\n    }\n    printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9f) / repeat);\n  }\n}\n\nvoid haccmk_gold(\n    int count1,\n    float xxi,\n    float yyi,\n    float zzi,\n    float fsrrmax2,\n    float mp_rsm2, \n    float *__restrict xx1, \n    float *__restrict yy1, \n    float *__restrict zz1, \n    float *__restrict mass1, \n    float *__restrict dxi,\n    float *__restrict dyi,\n    float *__restrict dzi )\n{\n  const float ma0 = 0.269327f, \n              ma1 = -0.0750978f, \n              ma2 = 0.0114808f,\n              ma3 = -0.00109313f,\n              ma4 = 0.0000605491f,\n              ma5 = -0.00000147177f;\n\n\n  float dxc, dyc, dzc, m, r2, f, xi, yi, zi;\n\n  xi = 0.f; \n  yi = 0.f;\n  zi = 0.f;\n\n  for (int j = 0; j < count1; j++ ) {\n    dxc = xx1[j] - xxi;\n    dyc = yy1[j] - yyi;\n    dzc = zz1[j] - zzi;\n\n    r2 = dxc * dxc + dyc * dyc + dzc * dzc;\n\n    if ( r2 < fsrrmax2 ) m = mass1[j]; else m = 0.f;\n\n    f = r2 + mp_rsm2;\n    f =  m * (1.f / (f * sqrtf(f)) - (ma0 + r2*(ma1 + r2*(ma2 + r2*(ma3 + r2*(ma4 + r2*ma5))))));\n\n    xi = xi + f * dxc;\n    yi = yi + f * dyc;\n    zi = zi + f * dzc;\n  }\n\n  *dxi = xi;\n  *dyi = yi;\n  *dzi = zi;\n}\n\n\nint main( int argc, char *argv[] )\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  float fsrrmax2, mp_rsm2, fcoeff, dx1, dy1, dz1, dx2, dy2, dz2;\n  int n1, n2, i;\n  n1 = 784;\n  n2 = 15000;\n  printf( \"Outer loop count is set %d\\n\", n1 );\n  printf( \"Inner loop count is set %d\\n\", n2 );\n\n  float* xx = (float*) malloc (sizeof(float) * n2);\n  float* yy = (float*) malloc (sizeof(float) * n2);\n  float* zz = (float*) malloc (sizeof(float) * n2);\n  float* mass = (float*) malloc (sizeof(float) * n2);\n  float* vx2 = (float*) malloc (sizeof(float) * n2);\n  float* vy2 = (float*) malloc (sizeof(float) * n2);\n  float* vz2 = (float*) malloc (sizeof(float) * n2);\n  float* vx2_hw = (float*) malloc (sizeof(float) * n2);\n  float* vy2_hw = (float*) malloc (sizeof(float) * n2);\n  float* vz2_hw = (float*) malloc (sizeof(float) * n2);\n\n  \n\n  fcoeff = 0.23f;  \n  fsrrmax2 = 0.5f; \n  mp_rsm2 = 0.03f;\n  dx1 = 1.0f/(float)n2;\n  dy1 = 2.0f/(float)n2;\n  dz1 = 3.0f/(float)n2;\n  xx[0] = 0.f;\n  yy[0] = 0.f;\n  zz[0] = 0.f;\n  mass[0] = 2.f;\n\n  for ( i = 1; i < n2; i++ ) {\n    xx[i] = xx[i-1] + dx1;\n    yy[i] = yy[i-1] + dy1;\n    zz[i] = zz[i-1] + dz1;\n    mass[i] = (float)i * 0.01f + xx[i];\n  }\n\n  for ( i = 0; i < n2; i++ ) {\n    vx2[i] = 0.f;\n    vy2[i] = 0.f;\n    vz2[i] = 0.f;\n    vx2_hw[i] = 0.f; \n    vy2_hw[i] = 0.f; \n    vz2_hw[i] = 0.f;\n  }\n\n  for ( i = 0; i < n1; ++i) {\n    haccmk_gold( n2, xx[i], yy[i], zz[i], fsrrmax2, mp_rsm2, xx, yy, zz, mass, &dx2, &dy2, &dz2 );    \n    vx2[i] = vx2[i] + dx2 * fcoeff;\n    vy2[i] = vy2[i] + dy2 * fcoeff;\n    vz2[i] = vz2[i] + dz2 * fcoeff;\n  }\n\n  haccmk(repeat, n1, n2, fsrrmax2, mp_rsm2, fcoeff, xx,\n      yy, zz, mass, vx2_hw, vy2_hw, vz2_hw); \n\n  \n\n  int error = 0;\n  const float eps = 1e-1f;\n  for (i = 0; i < n2; i++) {\n    if (fabsf(vx2[i] - vx2_hw[i]) > eps) {\n      printf(\"error at vx2[%d] %f %f\\n\", i, vx2[i], vx2_hw[i]);\n      error = 1;\n      break;\n    }\n    if (fabsf(vy2[i] - vy2_hw[i]) > eps) {\n      printf(\"error at vy2[%d]: %f %f\\n\", i, vy2[i], vy2_hw[i]);\n      error = 1;\n      break;\n    }\n    if (fabsf(vz2[i] - vz2_hw[i]) > eps) {\n      printf(\"error at vz2[%d]: %f %f\\n\", i, vz2[i], vz2_hw[i]);\n      error = 1;\n      break;\n    }\n  } \n\n  free(xx);\n  free(yy);\n  free(zz);\n  free(mass);\n  free(vx2);\n  free(vy2);\n  free(vz2);\n  free(vx2_hw);\n  free(vy2_hw);\n  free(vz2_hw);\n\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  return 0;\n}"}}
{"kernel_name": "haccmk", "parallel_api": "sycl", "code": {"haccmk.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <iostream>\n#include <sycl/sycl.hpp>\n\ntemplate <typename T> \nclass HACCmk;\n\ntemplate <typename T>\nvoid haccmk (\n    const int repeat,\n    const size_t n,\n\n    const int ilp, \n\n    const T fsrrmax,\n    const T mp_rsm,\n    const T fcoeff,\n    const T*__restrict xx, \n    const T*__restrict yy,\n    const T*__restrict zz,\n    const T*__restrict mass,\n          T*__restrict vx2,\n          T*__restrict vy2,\n          T*__restrict vz2 ) \n{\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  T *d_xx = sycl::malloc_device<T>(ilp, q);\n  q.memcpy(d_xx, xx, ilp * sizeof(T));\n\n  T *d_yy = sycl::malloc_device<T>(ilp, q);\n  q.memcpy(d_yy, yy, ilp * sizeof(T));\n\n  T *d_zz = sycl::malloc_device<T>(ilp, q);\n  q.memcpy(d_zz, zz, ilp * sizeof(T));\n\n  T *d_mass = sycl::malloc_device<T>(ilp, q);\n  q.memcpy(d_mass, mass, ilp * sizeof(T));\n\n  T *d_vx2 = sycl::malloc_device<T>(n, q);\n  T *d_vy2 = sycl::malloc_device<T>(n, q);\n  T *d_vz2 = sycl::malloc_device<T>(n, q);\n\n  float total_time = 0.f;\n\n  for (int i = 0; i < repeat; i++) {\n    \n\n    q.memcpy(d_vx2, vx2, n * sizeof(T));\n    q.memcpy(d_vy2, vy2, n * sizeof(T));\n    q.memcpy(d_vz2, vz2, n * sizeof(T));\n    q.wait();\n\n    sycl::range<1> gws ((n + 255) / 256 * 256);\n    sycl::range<1> lws (256);\n\n    auto start = std::chrono::steady_clock::now();\n    \n    q.submit([&](sycl::handler& cgh) {\n      cgh.parallel_for<class HACCmk<T>>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n\n        int i = item.get_global_id(0);\n        if (i >= n) return;\n\n        const float ma0 = 0.269327f; \n        const float ma1 = -0.0750978f; \n        const float ma2 = 0.0114808f; \n        const float ma3 = -0.00109313f; \n        const float ma4 = 0.0000605491f; \n        const float ma5 = -0.00000147177f;\n\n        float dxc, dyc, dzc, m, r2, f, xi, yi, zi;\n\n        xi = 0.f; \n        yi = 0.f;\n        zi = 0.f;\n\n        float xxi = d_xx[i];\n        float yyi = d_yy[i];\n        float zzi = d_zz[i];\n\n        for ( int j = 0; j < ilp; j++ ) {\n          dxc = d_xx[j] - xxi;\n          dyc = d_yy[j] - yyi;\n          dzc = d_zz[j] - zzi;\n\n          r2 = dxc * dxc + dyc * dyc + dzc * dzc;\n\n          if ( r2 < fsrrmax ) m = d_mass[j]; else m = 0.f;\n\n          f = r2 + mp_rsm;\n          f = m * ( 1.f / ( f * sycl::sqrt( f ) ) - \n              ( ma0 + r2*(ma1 + r2*(ma2 + r2*(ma3 + r2*(ma4 + r2*ma5))))));\n\n          xi = xi + f * dxc;\n          yi = yi + f * dyc;\n          zi = zi + f * dzc;\n        }\n\n        d_vx2[i] = d_vx2[i] + xi * fcoeff;\n        d_vy2[i] = d_vy2[i] + yi * fcoeff;\n        d_vz2[i] = d_vz2[i] + zi * fcoeff;\n      });\n    }).wait();\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n  }\n\n  printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9f) / repeat);\n\n  q.memcpy(vx2, d_vx2, sizeof(T) * n);\n  q.memcpy(vy2, d_vy2, sizeof(T) * n);\n  q.memcpy(vz2, d_vz2, sizeof(T) * n);\n  q.wait();\n\n  sycl::free(d_xx, q);\n  sycl::free(d_yy, q);\n  sycl::free(d_zz, q);\n  sycl::free(d_mass, q);\n  sycl::free(d_vx2, q);\n  sycl::free(d_vy2, q);\n  sycl::free(d_vz2, q);\n}\n\nvoid haccmk_gold(\n    int count1,\n    float xxi,\n    float yyi,\n    float zzi,\n    float fsrrmax2,\n    float mp_rsm2, \n    float *__restrict xx1, \n    float *__restrict yy1, \n    float *__restrict zz1, \n    float *__restrict mass1, \n    float *__restrict dxi,\n    float *__restrict dyi,\n    float *__restrict dzi )\n{\n  const float ma0 = 0.269327f, \n              ma1 = -0.0750978f, \n              ma2 = 0.0114808f,\n              ma3 = -0.00109313f,\n              ma4 = 0.0000605491f,\n              ma5 = -0.00000147177f;\n\n  float dxc, dyc, dzc, m, r2, f, xi, yi, zi;\n\n  xi = 0.f; \n  yi = 0.f;\n  zi = 0.f;\n\n  for (int j = 0; j < count1; j++ ) {\n    dxc = xx1[j] - xxi;\n    dyc = yy1[j] - yyi;\n    dzc = zz1[j] - zzi;\n\n    r2 = dxc * dxc + dyc * dyc + dzc * dzc;\n\n    if ( r2 < fsrrmax2 ) m = mass1[j]; else m = 0.f;\n\n    f = r2 + mp_rsm2;\n    f =  m * ( 1.f / ( f * sqrtf( f ) ) - ( ma0 + r2*(ma1 + r2*(ma2 + r2*(ma3 + r2*(ma4 + r2*ma5))))));\n\n    xi = xi + f * dxc;\n    yi = yi + f * dyc;\n    zi = zi + f * dzc;\n  }\n\n  *dxi = xi;\n  *dyi = yi;\n  *dzi = zi;\n}\n\n\nint main( int argc, char *argv[] )\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  float fsrrmax2, mp_rsm2, fcoeff, dx1, dy1, dz1, dx2, dy2, dz2;\n  int n1, n2, i;\n  n1 = 784;\n  n2 = 15000;\n  printf( \"Outer loop count is set %d\\n\", n1 );\n  printf( \"Inner loop count is set %d\\n\", n2 );\n\n  float* xx = (float*) malloc (sizeof(float) * n2);\n  float* yy = (float*) malloc (sizeof(float) * n2);\n  float* zz = (float*) malloc (sizeof(float) * n2);\n  float* mass = (float*) malloc (sizeof(float) * n2);\n  float* vx2 = (float*) malloc (sizeof(float) * n2);\n  float* vy2 = (float*) malloc (sizeof(float) * n2);\n  float* vz2 = (float*) malloc (sizeof(float) * n2);\n  float* vx2_hw = (float*) malloc (sizeof(float) * n2);\n  float* vy2_hw = (float*) malloc (sizeof(float) * n2);\n  float* vz2_hw = (float*) malloc (sizeof(float) * n2);\n\n  \n\n  fcoeff = 0.23f;  \n  fsrrmax2 = 0.5f; \n  mp_rsm2 = 0.03f;\n  dx1 = 1.0f/(float)n2;\n  dy1 = 2.0f/(float)n2;\n  dz1 = 3.0f/(float)n2;\n  xx[0] = 0.f;\n  yy[0] = 0.f;\n  zz[0] = 0.f;\n  mass[0] = 2.f;\n\n  for ( i = 1; i < n2; i++ ) {\n    xx[i] = xx[i-1] + dx1;\n    yy[i] = yy[i-1] + dy1;\n    zz[i] = zz[i-1] + dz1;\n    mass[i] = (float)i * 0.01f + xx[i];\n  }\n\n  for ( i = 0; i < n2; i++ ) {\n    vx2[i] = 0.f;\n    vy2[i] = 0.f;\n    vz2[i] = 0.f;\n    vx2_hw[i] = 0.f; \n    vy2_hw[i] = 0.f; \n    vz2_hw[i] = 0.f;\n  }\n\n  for ( i = 0; i < n1; ++i) {\n    haccmk_gold( n2, xx[i], yy[i], zz[i], fsrrmax2, mp_rsm2, xx, yy, zz, mass, &dx2, &dy2, &dz2 );    \n    vx2[i] = vx2[i] + dx2 * fcoeff;\n    vy2[i] = vy2[i] + dy2 * fcoeff;\n    vz2[i] = vz2[i] + dz2 * fcoeff;\n  }\n\n  haccmk(repeat, n1, n2, fsrrmax2, mp_rsm2, fcoeff, xx,\n         yy, zz, mass, vx2_hw, vy2_hw, vz2_hw); \n\n  \n\n  int error = 0;\n  const float eps = 1e-1f;\n  for (i = 0; i < n2; i++) {\n    if (fabsf(vx2[i] - vx2_hw[i]) > eps) {\n      printf(\"error at vx2[%d] %f %f\\n\", i, vx2[i], vx2_hw[i]);\n      error = 1;\n      break;\n    }\n    if (fabsf(vy2[i] - vy2_hw[i]) > eps) {\n      printf(\"error at vy2[%d]: %f %f\\n\", i, vy2[i], vy2_hw[i]);\n      error = 1;\n      break;\n    }\n    if (fabsf(vz2[i] - vz2_hw[i]) > eps) {\n      printf(\"error at vz2[%d]: %f %f\\n\", i, vz2[i], vz2_hw[i]);\n      error = 1;\n      break;\n    }\n  } \n\n  free(xx);\n  free(yy);\n  free(zz);\n  free(mass);\n  free(vx2);\n  free(vy2);\n  free(vz2);\n  free(vx2_hw);\n  free(vy2_hw);\n  free(vz2_hw);\n\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  return 0;\n}\n\n\n"}}
{"kernel_name": "heat", "parallel_api": "cuda", "code": {"heat.cu": "\n\n\n#include <iostream>\n#include <chrono>\n#include <cmath>\n#include <fstream>\n#include <cuda.h>\n\n\n\n#define PI acos(-1.0) \n\n#define LINE \"--------------------\" \n\n\n__global__ void initial_value(const unsigned int n, const double dx, const double length, double * u);\n__global__ void zero(const unsigned int n, double * u);\n__global__ void solve(const unsigned int n, const double alpha, const double dx, const double dt, const double r, const double r2,\n\t\tdouble * __restrict__ u, double * __restrict__ u_tmp);\ndouble solution(const double t, const double x, const double y, const double alpha, const double length);\ndouble l2norm(const int n, const double * u, const int nsteps, const double dt, const double alpha, const double dx, const double length);\n\nint main(int argc, char *argv[]) {\n\n  \n\n  auto start = std::chrono::high_resolution_clock::now();\n\n  \n\n  int n = 1000;\n\n  \n\n  int nsteps = 10;\n\n  \n\n  \n\n  if (argc == 3) {\n\n    \n\n    n = atoi(argv[1]);\n    if (n < 0) {\n      std::cerr << \"Error: n must be positive\" << std::endl;\n      exit(EXIT_FAILURE);\n    }\n\n    \n\n    nsteps = atoi(argv[2]);\n    if (nsteps < 0) {\n      std::cerr << \"Error: nsteps must be positive\" << std::endl;\n      exit(EXIT_FAILURE);\n    }\n  }\n\n  \n\n  \n\n  \n\n  double alpha = 0.1;          \n\n  double length = 1000.0;      \n\n  double dx = length / (n+1);  \n\n  double dt = 0.5 / nsteps;    \n\n\n  \n\n  double r = alpha * dt / (dx * dx);\n\n  cudaDeviceProp prop;\n  cudaGetDeviceProperties(&prop, 0);\n  char *device_name = prop.name;\n\n  \n\n  std::cout\n    << std::endl\n    << \" MMS heat equation\" << std::endl << std::endl\n    << LINE << std::endl\n    << \"Problem input\" << std::endl << std::endl\n    << \" Grid size: \" << n << \" x \" << n << std::endl\n    << \" Cell width: \" << dx << std::endl\n    << \" Grid length: \" << length << \"x\" << length << std::endl\n    << std::endl\n    << \" Alpha: \" << alpha << std::endl\n    << std::endl\n    << \" Steps: \" <<  nsteps << std::endl\n    << \" Total time: \" << dt*(double)nsteps << std::endl\n    << \" Time step: \" << dt << std::endl\n    << \" GPU device: \" << device_name << std::endl\n    << LINE << std::endl;\n\n  \n\n  std::cout << \"Stability\" << std::endl << std::endl;\n  std::cout << \" r value: \" << r << std::endl;\n  if (r > 0.5)\n    std::cout << \" Warning: unstable\" << std::endl;\n  std::cout << LINE << std::endl;\n\n\n  \n\n  double *u;\n  double *u_tmp;\n  cudaMalloc((void**)&u,     sizeof(double)*n*n);\n  cudaMalloc((void**)&u_tmp, sizeof(double)*n*n);\n\n  \n\n  const int block_size = 256;\n  int n_ceil = (n*n+block_size-1) / block_size;\n  dim3 grid(n_ceil);\n  dim3 block(block_size);\n  initial_value <<< dim3(grid), dim3(block) >>> (n, dx, length, u);\n  zero <<< dim3(grid), dim3(block) >>> (n, u_tmp);\n\n  \n\n  cudaError_t err = cudaDeviceSynchronize();\n  if (err != cudaSuccess) {\n    std::cerr << \"CUDA error after initalisation\" << std::endl;\n    exit(EXIT_FAILURE);\n  }\n\n  \n\n  \n\n  \n\n  \n\n  const double r2 = 1.0 - 4.0*r;\n\n  \n\n  auto tic = std::chrono::high_resolution_clock::now();\n\n  for (int t = 0; t < nsteps; ++t) {\n\n    \n\n    \n\n    \n\n    solve<<< dim3(grid), dim3(block) >>> (n, alpha, dx, dt, r, r2, u, u_tmp);\n\n    \n\n    auto tmp = u;\n    u = u_tmp;\n    u_tmp = tmp;\n  }\n\n  \n\n  cudaDeviceSynchronize();\n  auto toc = std::chrono::high_resolution_clock::now();\n\n  \n\n  double *u_host = new double[n*n];\n  err = cudaMemcpy(u_host, u, sizeof(double)*n*n, cudaMemcpyDeviceToHost);\n  if (err != cudaSuccess) {\n    std::cerr << \"CUDA error on copying back data\" << std::endl;\n    exit(EXIT_FAILURE);\n  }\n\n  \n\n  \n\n  \n\n  \n\n  double norm = l2norm(n, u_host, nsteps, dt, alpha, dx, length);\n\n  \n\n  auto stop = std::chrono::high_resolution_clock::now();\n\n  \n\n  std::cout\n    << \"Results\" << std::endl << std::endl\n    << \"Error (L2norm): \" << norm << std::endl\n    << \"Solve time (s): \" << std::chrono::duration_cast<std::chrono::duration<double>>(toc-tic).count() << std::endl\n    << \"Total time (s): \" << std::chrono::duration_cast<std::chrono::duration<double>>(stop-start).count() << std::endl\n    << \"Bandwidth (GB/s): \" << 1.0E-9*2.0*n*n*nsteps*sizeof(double)/std::chrono::duration_cast<std::chrono::duration<double>>(toc-tic).count() << std::endl\n    << LINE << std::endl;\n\n  delete[] u_host;\n  cudaFree(u);\n  cudaFree(u_tmp);\n}\n\n\n\n__global__ void initial_value(const unsigned int n, const double dx, const double length, double * u) {\n\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < n*n) {\n    int i = idx % n;\n    int j = idx / n;\n    double y = dx * (j+1); \n\n    double x = dx * (i+1); \n\n    u[i+j*n] = sin(PI * x / length) * sin(PI * y / length);\n  }\n}\n\n\n\n\n__global__ void zero(const unsigned int n, double * u) {\n\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < n*n) u[idx] = 0.0;\n}\n\n\n\n\n\n\n__global__ void solve(const unsigned int n, const double alpha, const double dx, const double dt, \n\t\tconst double r, const double r2,\n\t\tdouble * __restrict__ u, double * __restrict__ u_tmp) {\n\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < n * n) {\n    int i = idx % n;\n    int j = idx / n;\n    \n\n    u_tmp[i+j*n] =  r2 * u[i+j*n] +\n    r * ((i < n-1) ? u[i+1+j*n] : 0.0) +\n    r * ((i > 0)   ? u[i-1+j*n] : 0.0) +\n    r * ((j < n-1) ? u[i+(j+1)*n] : 0.0) +\n    r * ((j > 0)   ? u[i+(j-1)*n] : 0.0);\n  }\n}\n\n\n\n\ndouble solution(const double t, const double x, const double y, const double alpha, const double length) {\n\n  return exp(-2.0*alpha*PI*PI*t/(length*length)) * sin(PI*x/length) * sin(PI*y/length);\n\n}\n\n\n\n\n\n\ndouble l2norm(const int n, const double * u, const int nsteps, const double dt, const double alpha, const double dx, const double length) {\n\n  \n\n  double time = dt * (double)nsteps;\n\n  \n\n  double l2norm = 0.0;\n\n  \n\n  double y = dx;\n  for (int j = 0; j < n; ++j) {\n    double x = dx;\n    for (int i = 0; i < n; ++i) {\n      double answer = solution(time, x, y, alpha, length);\n      l2norm += (u[i+j*n] - answer) * (u[i+j*n] - answer);\n\n      x += dx;\n    }\n    y += dx;\n  }\n\n  return sqrt(l2norm);\n}\n"}}
{"kernel_name": "heat", "parallel_api": "hip", "code": {"heat.cu": "\n\n\n#include <iostream>\n#include <chrono>\n#include <cmath>\n#include <fstream>\n\n#include <hip/hip_runtime.h>\n\n\n\n#define PI acos(-1.0) \n\n#define LINE \"--------------------\" \n\n\n__global__ void initial_value(const unsigned int n, const double dx, const double length, double * u);\n__global__ void zero(const unsigned int n, double * u);\n__global__ void solve(const unsigned int n, const double alpha, const double dx, const double dt, const double r, const double r2,\n\t\tdouble * __restrict__ u, double * __restrict__ u_tmp);\ndouble solution(const double t, const double x, const double y, const double alpha, const double length);\ndouble l2norm(const int n, const double * u, const int nsteps, const double dt, const double alpha, const double dx, const double length);\n\nint main(int argc, char *argv[]) {\n\n  \n\n  auto start = std::chrono::high_resolution_clock::now();\n\n  \n\n  int n = 1000;\n\n  \n\n  int nsteps = 10;\n\n  \n\n  \n\n  if (argc == 3) {\n\n    \n\n    n = atoi(argv[1]);\n    if (n < 0) {\n      std::cerr << \"Error: n must be positive\" << std::endl;\n      exit(EXIT_FAILURE);\n    }\n\n    \n\n    nsteps = atoi(argv[2]);\n    if (nsteps < 0) {\n      std::cerr << \"Error: nsteps must be positive\" << std::endl;\n      exit(EXIT_FAILURE);\n    }\n  }\n\n  \n\n  \n\n  \n\n  double alpha = 0.1;          \n\n  double length = 1000.0;      \n\n  double dx = length / (n+1);  \n\n  double dt = 0.5 / nsteps;    \n\n\n  \n\n  double r = alpha * dt / (dx * dx);\n\n  hipDeviceProp_t prop;\n  hipGetDeviceProperties(&prop, 0);\n  char *device_name = prop.name;\n\n  \n\n  std::cout\n    << std::endl\n    << \" MMS heat equation\" << std::endl << std::endl\n    << LINE << std::endl\n    << \"Problem input\" << std::endl << std::endl\n    << \" Grid size: \" << n << \" x \" << n << std::endl\n    << \" Cell width: \" << dx << std::endl\n    << \" Grid length: \" << length << \"x\" << length << std::endl\n    << std::endl\n    << \" Alpha: \" << alpha << std::endl\n    << std::endl\n    << \" Steps: \" <<  nsteps << std::endl\n    << \" Total time: \" << dt*(double)nsteps << std::endl\n    << \" Time step: \" << dt << std::endl\n    << \" GPU device: \" << device_name << std::endl\n    << LINE << std::endl;\n\n  \n\n  std::cout << \"Stability\" << std::endl << std::endl;\n  std::cout << \" r value: \" << r << std::endl;\n  if (r > 0.5)\n    std::cout << \" Warning: unstable\" << std::endl;\n  std::cout << LINE << std::endl;\n\n  \n\n  double *u;\n  double *u_tmp;\n  hipMalloc((void**)&u,     sizeof(double)*n*n);\n  hipMalloc((void**)&u_tmp, sizeof(double)*n*n);\n\n  \n\n  const int block_size = 256;\n  int n_ceil = (n*n+block_size-1) / block_size;\n  dim3 grid(n_ceil);\n  dim3 block(block_size);\n  hipLaunchKernelGGL(initial_value, grid, block, 0, 0, n, dx, length, u);\n  hipLaunchKernelGGL(zero, grid, block, 0, 0, n, u_tmp);\n\n  \n\n  hipError_t err = hipDeviceSynchronize();\n  if (err != hipSuccess) {\n    std::cerr << \"CUDA error after initalisation\" << std::endl;\n    exit(EXIT_FAILURE);\n  }\n\n  \n\n  \n\n  \n\n  \n\n  const double r2 = 1.0 - 4.0*r;\n\n  \n\n  auto tic = std::chrono::high_resolution_clock::now();\n\n  for (int t = 0; t < nsteps; ++t) {\n\n    \n\n    \n\n    \n\n    hipLaunchKernelGGL(solve, grid, block, 0, 0, n, alpha, dx, dt, r, r2, u, u_tmp);\n\n    \n\n    auto tmp = u;\n    u = u_tmp;\n    u_tmp = tmp;\n  }\n\n  \n\n  hipDeviceSynchronize();\n  auto toc = std::chrono::high_resolution_clock::now();\n\n  \n\n  double *u_host = new double[n*n];\n  err = hipMemcpy(u_host, u, sizeof(double)*n*n, hipMemcpyDeviceToHost);\n  if (err != hipSuccess) {\n    std::cerr << \"CUDA error on copying back data\" << std::endl;\n    exit(EXIT_FAILURE);\n  }\n\n  \n\n  \n\n  \n\n  \n\n  double norm = l2norm(n, u_host, nsteps, dt, alpha, dx, length);\n\n  \n\n  auto stop = std::chrono::high_resolution_clock::now();\n\n  \n\n  std::cout\n    << \"Results\" << std::endl << std::endl\n    << \"Error (L2norm): \" << norm << std::endl\n    << \"Solve time (s): \" << std::chrono::duration_cast<std::chrono::duration<double>>(toc-tic).count() << std::endl\n    << \"Total time (s): \" << std::chrono::duration_cast<std::chrono::duration<double>>(stop-start).count() << std::endl\n    << \"Bandwidth (GB/s): \" << 1.0E-9*2.0*n*n*nsteps*sizeof(double)/std::chrono::duration_cast<std::chrono::duration<double>>(toc-tic).count() << std::endl\n    << LINE << std::endl;\n\n  delete[] u_host;\n  hipFree(u);\n  hipFree(u_tmp);\n}\n\n\n\n__global__ void initial_value(const unsigned int n, const double dx, const double length, double * u) {\n\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < n*n) {\n    int i = idx % n;\n    int j = idx / n;\n    double y = dx * (j+1); \n\n    double x = dx * (i+1); \n\n    u[i+j*n] = sin(PI * x / length) * sin(PI * y / length);\n  }\n}\n\n\n\n\n__global__ void zero(const unsigned int n, double * u) {\n\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < n*n) u[idx] = 0.0;\n}\n\n\n\n\n\n\n__global__ void solve(const unsigned int n, const double alpha, const double dx, const double dt, \n\t\tconst double r, const double r2,\n\t\tdouble * __restrict__ u, double * __restrict__ u_tmp) {\n\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < n * n) {\n    int i = idx % n;\n    int j = idx / n;\n    \n\n    u_tmp[i+j*n] =  r2 * u[i+j*n] +\n    r * ((i < n-1) ? u[i+1+j*n] : 0.0) +\n    r * ((i > 0)   ? u[i-1+j*n] : 0.0) +\n    r * ((j < n-1) ? u[i+(j+1)*n] : 0.0) +\n    r * ((j > 0)   ? u[i+(j-1)*n] : 0.0);\n  }\n}\n\n\n\n\ndouble solution(const double t, const double x, const double y, const double alpha, const double length) {\n\n  return exp(-2.0*alpha*PI*PI*t/(length*length)) * sin(PI*x/length) * sin(PI*y/length);\n\n}\n\n\n\n\n\n\ndouble l2norm(const int n, const double * u, const int nsteps, const double dt, const double alpha, const double dx, const double length) {\n\n  \n\n  double time = dt * (double)nsteps;\n\n  \n\n  double l2norm = 0.0;\n\n  \n\n  double y = dx;\n  for (int j = 0; j < n; ++j) {\n    double x = dx;\n    for (int i = 0; i < n; ++i) {\n      double answer = solution(time, x, y, alpha, length);\n      l2norm += (u[i+j*n] - answer) * (u[i+j*n] - answer);\n\n      x += dx;\n    }\n    y += dx;\n  }\n\n  return sqrt(l2norm);\n}\n"}}
{"kernel_name": "heat", "parallel_api": "omp", "code": {"heat.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\n\n\n#define PI acos(-1.0) \n\n#define LINE \"--------------------\\n\" \n\n\ndouble solution(const double t, const double x, const double y, const double alpha, const double length);\ndouble l2norm(const int n, const double * __restrict u, const int nsteps, const double dt, const double alpha, const double dx, const double length);\n\nint main(int argc, char *argv[]) {\n\n  \n\n  double start = omp_get_wtime();\n\n  \n\n  int n = 1000;\n\n  \n\n  int nsteps = 10;\n\n  \n\n  \n\n  if (argc == 3) {\n\n    \n\n    n = atoi(argv[1]);\n    if (n < 0) {\n      fprintf(stderr, \"Error: n must be positive\\n\");\n      exit(EXIT_FAILURE);\n    }\n\n    \n\n    nsteps = atoi(argv[2]);\n    if (nsteps < 0) {\n      fprintf(stderr, \"Error: nsteps must be positive\\n\");\n      exit(EXIT_FAILURE);\n    }\n  }\n\n  \n\n  \n\n  \n\n  double alpha = 0.1;          \n\n  double length = 1000.0;      \n\n  double dx = length / (n+1);  \n\n  double dt = 0.5 / nsteps;    \n\n\n  \n\n  double r = alpha * dt / (dx * dx);\n\n  \n\n  printf(\"\\n\");\n  printf(\" MMS heat equation\\n\\n\");\n  printf(LINE);\n  printf(\"Problem input\\n\\n\");\n  printf(\" Grid size: %d x %d\\n\", n, n);\n  printf(\" Cell width: %E\\n\", dx);\n  printf(\" Grid length: %lf x %lf\\n\", length, length);\n  printf(\"\\n\");\n  printf(\" Alpha: %E\\n\", alpha);\n  printf(\"\\n\");\n  printf(\" Steps: %d\\n\", nsteps);\n  printf(\" Total time: %E\\n\", dt*(double)nsteps);\n  printf(\" Time step: %E\\n\", dt);\n  printf(LINE);\n\n  \n\n  printf(\"Stability\\n\\n\");\n  printf(\" r value: %lf\\n\", r);\n  if (r > 0.5)\n    printf(\" Warning: unstable\\n\");\n  printf(LINE);\n\n  \n\n  double *u     = (double*) malloc(sizeof(double)*n*n);\n  double *u_tmp = (double*) malloc(sizeof(double)*n*n);\n\n  double tic, toc;\n  const int block_size = 256;\n\n#pragma omp target data map(tofrom: u[0:n*n], u_tmp[0:n*n]) \n{\n  \n\n  #pragma omp target teams distribute parallel for simd collapse(2) thread_limit(block_size)\n  for (int j = 0; j < n; ++j) {\n    for (int i = 0; i < n; ++i) {\n      double y = (j+1)*dx; \n\n      double x = (i+1)*dx; \n\n      u[i+j*n] = sin(PI * x / length) * sin(PI * y / length);\n    }\n  }\n\n  #pragma omp target teams distribute parallel for simd collapse(2) thread_limit(block_size)\n  for (int j = 0; j < n; ++j) {\n    for (int i = 0; i < n; ++i) {\n      u_tmp[i+j*n] = 0.0;\n    }\n  }\n\n  \n\n  \n\n  \n\n\n  \n\n  const double r2 = 1.0 - 4.0*r;\n\n  \n\n  tic = omp_get_wtime();\n\n  for (int t = 0; t < nsteps; ++t) {\n\n    \n\n    \n\n    \n\n    \n\n    #pragma omp target teams distribute parallel for simd collapse(2) thread_limit(block_size)\n    for (int j = 0; j < n; ++j) {\n      for (int i = 0; i < n; ++i) {\n        \n\n        \n\n        u_tmp[i+j*n] =  r2 * u[i+j*n] +\n        r * ((i < n-1) ? u[i+1+j*n] : 0.0) +\n        r * ((i > 0)   ? u[i-1+j*n] : 0.0) +\n        r * ((j < n-1) ? u[i+(j+1)*n] : 0.0) +\n        r * ((j > 0)   ? u[i+(j-1)*n] : 0.0);\n      }\n    }\n\n    \n\n    double *tmp = u;\n    u = u_tmp;\n    u_tmp = tmp;\n  }\n  \n\n  toc = omp_get_wtime();\n}\n\n  \n\n  \n\n  \n\n  \n\n  double norm = l2norm(n, u, nsteps, dt, alpha, dx, length);\n\n  \n\n  double stop = omp_get_wtime();\n\n  \n\n  printf(\"Results\\n\\n\");\n  printf(\"Error (L2norm): %E\\n\", norm);\n  printf(\"Solve time (s): %lf\\n\", toc-tic);\n  printf(\"Total time (s): %lf\\n\", stop-start);\n  printf(\"Bandwidth (GB/s): %lf\\n\", 1.0E-9*2.0*n*n*nsteps*sizeof(double)/(toc-tic));\n  printf(LINE);\n\n  \n\n  free(u);\n  free(u_tmp);\n}\n\n\n\n\ndouble solution(const double t, const double x, const double y, const double alpha, const double length) {\n\n  return exp(-2.0*alpha*PI*PI*t/(length*length)) * sin(PI*x/length) * sin(PI*y/length);\n\n}\n\n\n\n\n\n\ndouble l2norm(const int n, const double * u, const int nsteps, const double dt,\n              const double alpha, const double dx, const double length) {\n\n  \n\n  double time = dt * (double)nsteps;\n\n  \n\n  double l2norm = 0.0;\n\n  \n\n  double y = dx;\n  for (int j = 0; j < n; ++j) {\n    double x = dx;\n    for (int i = 0; i < n; ++i) {\n      double answer = solution(time, x, y, alpha, length);\n      l2norm += (u[i+j*n] - answer) * (u[i+j*n] - answer);\n\n      x += dx;\n    }\n    y += dx;\n  }\n\n  return sqrt(l2norm);\n}\n"}}
{"kernel_name": "heat", "parallel_api": "serial", "code": {"heat.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n\n\n\n#define PI acos(-1.0) \n\n#define LINE \"--------------------\\n\" \n\n\ndouble solution(const double t, const double x, const double y, const double alpha, const double length);\ndouble l2norm(const int n, const double * __restrict u, const int nsteps, const double dt, const double alpha, const double dx, const double length);\n\nint main(int argc, char *argv[]) {\n\n  \n\n  double start = omp_get_wtime();\n\n  \n\n  int n = 1000;\n\n  \n\n  int nsteps = 10;\n\n  \n\n  \n\n  if (argc == 3) {\n\n    \n\n    n = atoi(argv[1]);\n    if (n < 0) {\n      fprintf(stderr, \"Error: n must be positive\\n\");\n      exit(EXIT_FAILURE);\n    }\n\n    \n\n    nsteps = atoi(argv[2]);\n    if (nsteps < 0) {\n      fprintf(stderr, \"Error: nsteps must be positive\\n\");\n      exit(EXIT_FAILURE);\n    }\n  }\n\n  \n\n  \n\n  \n\n  double alpha = 0.1;          \n\n  double length = 1000.0;      \n\n  double dx = length / (n+1);  \n\n  double dt = 0.5 / nsteps;    \n\n\n  \n\n  double r = alpha * dt / (dx * dx);\n\n  \n\n  printf(\"\\n\");\n  printf(\" MMS heat equation\\n\\n\");\n  printf(LINE);\n  printf(\"Problem input\\n\\n\");\n  printf(\" Grid size: %d x %d\\n\", n, n);\n  printf(\" Cell width: %E\\n\", dx);\n  printf(\" Grid length: %lf x %lf\\n\", length, length);\n  printf(\"\\n\");\n  printf(\" Alpha: %E\\n\", alpha);\n  printf(\"\\n\");\n  printf(\" Steps: %d\\n\", nsteps);\n  printf(\" Total time: %E\\n\", dt*(double)nsteps);\n  printf(\" Time step: %E\\n\", dt);\n  printf(LINE);\n\n  \n\n  printf(\"Stability\\n\\n\");\n  printf(\" r value: %lf\\n\", r);\n  if (r > 0.5)\n    printf(\" Warning: unstable\\n\");\n  printf(LINE);\n\n  \n\n  double *u     = (double*) malloc(sizeof(double)*n*n);\n  double *u_tmp = (double*) malloc(sizeof(double)*n*n);\n\n  double tic, toc;\n  const int block_size = 256;\n\n{\n  \n\n    for (int j = 0; j < n; ++j) {\n    for (int i = 0; i < n; ++i) {\n      double y = (j+1)*dx; \n\n      double x = (i+1)*dx; \n\n      u[i+j*n] = sin(PI * x / length) * sin(PI * y / length);\n    }\n  }\n\n    for (int j = 0; j < n; ++j) {\n    for (int i = 0; i < n; ++i) {\n      u_tmp[i+j*n] = 0.0;\n    }\n  }\n\n  \n\n  \n\n  \n\n\n  \n\n  const double r2 = 1.0 - 4.0*r;\n\n  \n\n  tic = omp_get_wtime();\n\n  for (int t = 0; t < nsteps; ++t) {\n\n    \n\n    \n\n    \n\n    \n\n        for (int j = 0; j < n; ++j) {\n      for (int i = 0; i < n; ++i) {\n        \n\n        \n\n        u_tmp[i+j*n] =  r2 * u[i+j*n] +\n        r * ((i < n-1) ? u[i+1+j*n] : 0.0) +\n        r * ((i > 0)   ? u[i-1+j*n] : 0.0) +\n        r * ((j < n-1) ? u[i+(j+1)*n] : 0.0) +\n        r * ((j > 0)   ? u[i+(j-1)*n] : 0.0);\n      }\n    }\n\n    \n\n    double *tmp = u;\n    u = u_tmp;\n    u_tmp = tmp;\n  }\n  \n\n  toc = omp_get_wtime();\n}\n\n  \n\n  \n\n  \n\n  \n\n  double norm = l2norm(n, u, nsteps, dt, alpha, dx, length);\n\n  \n\n  double stop = omp_get_wtime();\n\n  \n\n  printf(\"Results\\n\\n\");\n  printf(\"Error (L2norm): %E\\n\", norm);\n  printf(\"Solve time (s): %lf\\n\", toc-tic);\n  printf(\"Total time (s): %lf\\n\", stop-start);\n  printf(\"Bandwidth (GB/s): %lf\\n\", 1.0E-9*2.0*n*n*nsteps*sizeof(double)/(toc-tic));\n  printf(LINE);\n\n  \n\n  free(u);\n  free(u_tmp);\n}\n\n\n\n\ndouble solution(const double t, const double x, const double y, const double alpha, const double length) {\n\n  return exp(-2.0*alpha*PI*PI*t/(length*length)) * sin(PI*x/length) * sin(PI*y/length);\n\n}\n\n\n\n\n\n\ndouble l2norm(const int n, const double * u, const int nsteps, const double dt,\n              const double alpha, const double dx, const double length) {\n\n  \n\n  double time = dt * (double)nsteps;\n\n  \n\n  double l2norm = 0.0;\n\n  \n\n  double y = dx;\n  for (int j = 0; j < n; ++j) {\n    double x = dx;\n    for (int i = 0; i < n; ++i) {\n      double answer = solution(time, x, y, alpha, length);\n      l2norm += (u[i+j*n] - answer) * (u[i+j*n] - answer);\n\n      x += dx;\n    }\n    y += dx;\n  }\n\n  return sqrt(l2norm);\n}"}}
{"kernel_name": "heat", "parallel_api": "sycl", "code": {"heat.cpp": "\n\n\n#include <iostream>\n#include <chrono>\n#include <cmath>\n#include <sycl/sycl.hpp>\n\n\n\n#define PI sycl::acos(-1.0) \n\n#define LINE \"--------------------\" \n\n\nvoid initial_value(sycl::queue &q, const unsigned int n, const double dx, const double length, double *u);\nvoid zero(sycl::queue &q, const unsigned int n, double *u);\nvoid solve(sycl::queue &q, const unsigned int n, const double alpha, const double dx, const double dt, double *u, double *u_tmp);\ndouble solution(const double t, const double x, const double y, const double alpha, const double length);\ndouble l2norm(const unsigned int n, const double * u, const int nsteps, const double dt, const double alpha, const double dx, const double length);\n\n\n\nint main(int argc, char *argv[]) {\n\n  \n\n  auto start = std::chrono::high_resolution_clock::now();\n\n  \n\n  unsigned int n = 1000;\n\n  \n\n  int nsteps = 10;\n\n  \n\n  \n\n  if (argc == 3) {\n\n    \n\n    n = atoi(argv[1]);\n    if (n < 0) {\n      std::cerr << \"Error: n must be positive\" << std::endl;\n      exit(EXIT_FAILURE);\n    }\n\n    \n\n    nsteps = atoi(argv[2]);\n    if (nsteps < 0) {\n      std::cerr << \"Error: nsteps must be positive\" << std::endl;\n      exit(EXIT_FAILURE);\n    }\n  }\n\n  \n\n  \n\n  \n\n  double alpha = 0.1;          \n\n  double length = 1000.0;      \n\n  double dx = length / (n+1);  \n\n  double dt = 0.5 / nsteps;    \n\n\n  \n\n  double r = alpha * dt / (dx * dx);\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  \n\n  std::cout\n    << std::endl\n    << \" MMS heat equation\" << std::endl << std::endl\n    << LINE << std::endl\n    << \"Problem input\" << std::endl << std::endl\n    << \" Grid size: \" << n << \" x \" << n << std::endl\n    << \" Cell width: \" << dx << std::endl\n    << \" Grid length: \" << length << \"x\" << length << std::endl\n    << std::endl\n    << \" Alpha: \" << alpha << std::endl\n    << std::endl\n    << \" Steps: \" <<  nsteps << std::endl\n    << \" Total time: \" << dt*(double)nsteps << std::endl\n    << \" Time step: \" << dt << std::endl\n    << \" GPU device: \" << q.get_device().get_info<sycl::info::device::name>() << std::endl\n    << LINE << std::endl;\n\n  \n\n  std::cout << \"Stability\" << std::endl << std::endl;\n  std::cout << \" r value: \" << r << std::endl;\n  if (r > 0.5)\n    std::cout << \" Warning: unstable\" << std::endl;\n  std::cout << LINE << std::endl;\n\n  const unsigned int grid_size = n * n;\n\n  \n\n  double *u = sycl::malloc_device<double>(grid_size, q);\n  double *u_tmp = sycl::malloc_device<double>(grid_size, q);\n\n  const int block_size = 256;\n  const int n_ceil = (grid_size+block_size-1) / block_size * block_size;\n\n  \n\n  q.submit([&](sycl::handler& cgh) {\n    cgh.parallel_for<class initial_value_kernel>(\n      sycl::nd_range<1>(sycl::range<1>(n_ceil), sycl::range<1>(block_size)),\n      [=](sycl::nd_item<1> item) {\n      int idx = item.get_global_id(0);\n      if (idx < grid_size) {\n        int i = idx % n;\n        int j = idx / n;\n        double y = dx * (j+1); \n\n        double x = dx * (i+1); \n\n        u[i+j*n] = sycl::sin(PI * x / length) * sycl::sin(PI * y / length);\n      }\n    });\n  });\n\n  q.submit([&](sycl::handler& cgh) {\n    cgh.parallel_for<class zero_kernel>(\n      sycl::nd_range<1>(sycl::range<1>(n_ceil), sycl::range<1>(block_size)),\n      [=](sycl::nd_item<1> item) {\n      int idx = item.get_global_id(0);\n      if (idx < grid_size) u_tmp[idx] = 0.0;\n    });\n  });\n\n  \n\n  q.wait();\n\n  \n\n  \n\n  \n\n\n  const double r2 = 1.0 - 4.0*r;\n\n  \n\n  auto tic = std::chrono::high_resolution_clock::now();\n\n  for (int t = 0; t < nsteps; ++t) {\n\n    \n\n    \n\n    \n\n    q.submit([&](sycl::handler& cgh) {\n      \n\n      cgh.parallel_for<class solve_kernel>(\n        sycl::nd_range<1>(sycl::range<1>(n_ceil), sycl::range<1>(block_size)),\n        [=](sycl::nd_item<1> item) {\n        int idx = item.get_global_id(0);\n        if (idx < grid_size) {\n          int i = idx % n;\n          int j = idx / n;\n\n          \n\n          \n\n          u_tmp[i+j*n] = r2 * u[i+j*n] +\n          r * ((i < n-1) ? u[i+1+j*n] : 0.0) +\n          r * ((i > 0)   ? u[i-1+j*n] : 0.0) +\n          r * ((j < n-1) ? u[i+(j+1)*n] : 0.0) +\n          r * ((j > 0)   ? u[i+(j-1)*n] : 0.0);\n        }\n      });\n    });\n\n    \n\n    auto tmp = u;\n    u = u_tmp;\n    u_tmp = tmp;\n  }\n\n  q.wait();\n  auto toc = std::chrono::high_resolution_clock::now();\n\n  double *u_host = new double[grid_size];\n  q.memcpy(u_host, u, sizeof(double) * grid_size).wait();\n\n  \n\n  \n\n  \n\n  \n\n  double norm = l2norm(n, u_host, nsteps, dt, alpha, dx, length);\n\n  \n\n  auto stop = std::chrono::high_resolution_clock::now();\n\n  \n\n  std::cout\n    << \"Results\" << std::endl << std::endl\n    << \"Error (L2norm): \" << norm << std::endl\n    << \"Solve time (s): \" << std::chrono::duration_cast<std::chrono::duration<double>>(toc-tic).count() << std::endl\n    << \"Total time (s): \" << std::chrono::duration_cast<std::chrono::duration<double>>(stop-start).count() << std::endl\n    << \"Bandwidth (GB/s): \" << 1.0E-9*2.0*n*n*nsteps*sizeof(double)/std::chrono::duration_cast<std::chrono::duration<double>>(toc-tic).count() << std::endl\n    << LINE << std::endl;\n\n  sycl::free(u, q);\n  sycl::free(u_tmp, q);\n  delete[] u_host;\n}\n\n\n\n\ndouble solution(const double t, const double x, const double y, const double alpha, const double length) {\n\n  return exp(-2.0*alpha*PI*PI*t/(length*length)) * sin(PI*x/length) * sin(PI*y/length);\n\n}\n\n\n\n\n\n\ndouble l2norm(const unsigned int n, const double * u, const int nsteps, const double dt, const double alpha, const double dx, const double length) {\n  \n\n  double time = dt * (double)nsteps;\n\n  \n\n  double l2norm = 0.0;\n\n  \n\n  double y = dx;\n  for (int j = 0; j < n; ++j) {\n    double x = dx;\n    for (int i = 0; i < n; ++i) {\n      double answer = solution(time, x, y, alpha, length);\n      l2norm += (u[i+j*n] - answer) * (u[i+j*n] - answer);\n\n      x += dx;\n    }\n    y += dx;\n  }\n\n  return sqrt(l2norm);\n}\n"}}
{"kernel_name": "henry", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <map>\n#include <string>\n#include <fstream>\n#include <sstream>\n#include <chrono>\n#include <cuda.h>\n\n#define NUMTHREADS 256  \n\n\n\n\n\n\n\n\nstruct StructureAtom {\n  \n\n  double x;\n  double y;\n  double z;\n  \n\n  double epsilon;  \n\n  \n\n  double sigma;  \n\n};\n\n\n\nconst double T = 298.0; \n\n\n\nconst double R = 8.314; \n\n\n\n__device__\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; \n\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n\n\n\n\n\n\n\n\n__device__ double compute(double x, double y, double z,\n    const StructureAtom * __restrict__ structureAtoms,\n    double natoms, double L) \n{\n  \n\n  \n\n  \n\n  \n\n  \n\n  double E = 0.0;  \n\n\n  \n\n  for (int i = 0; i < natoms; i++) {\n    \n\n    double dx = x - structureAtoms[i].x;\n    double dy = y - structureAtoms[i].y;\n    double dz = z - structureAtoms[i].z;\n\n    \n\n    const double boxupper = 0.5 * L;\n    const double boxlower = -boxupper;\n\n    dx = (dx >  boxupper) ? dx-L : dx;\n    dx = (dx >  boxupper) ? dx-L : dx;\n    dy = (dy >  boxupper) ? dy-L : dy;\n    dy = (dy <= boxlower) ? dy-L : dy;\n    dz = (dz <= boxlower) ? dz-L : dz;\n    dz = (dz <= boxlower) ? dz-L : dz;\n\n    \n\n    double rinv = 1.0 / sqrt(dx*dx + dy*dy + dz*dz);\n\n    \n\n    \n\n    double sig_ovr_r = rinv * structureAtoms[i].sigma;\n    double sig_ovr_r6 = pow(sig_ovr_r, 6);\n    double sig_ovr_r12 = sig_ovr_r6 * sig_ovr_r6;\n    E += 4.0 * structureAtoms[i].epsilon * (sig_ovr_r12 - sig_ovr_r6);\n  }\n  return exp(-E / (R * T));  \n\n}\n\n\n\n\n\n\n\n__global__ void insertions(\n    double *__restrict__ boltzmannFactors, \n    const StructureAtom * __restrict__ structureAtoms, \n    int natoms, double L) \n{\n  \n\n  \n\n  \n\n  \n\n  int id = threadIdx.x + blockIdx.x * NUMTHREADS;\n\n  \n\n  uint64_t seed = id;\n\n  \n\n  double x = L * LCG_random_double(&seed);\n  double y = L * LCG_random_double(&seed);\n  double z = L * LCG_random_double(&seed);\n\n  \n\n  boltzmannFactors[id] = compute(x, y, z, structureAtoms, natoms, L);\n}\n\nint main(int argc, char *argv[]) {\n  \n\n  if (argc != 3) {\n    printf(\"Usage: ./%s <material file> <ninsertions>\\n\", argv[0]);\n    exit(EXIT_FAILURE);\n  }\n\n  \n\n  StructureAtom *structureAtoms;  \n\n  \n\n  std::ifstream materialfile(argv[1]);\n  if (materialfile.fail()) {\n    printf(\"Failed to import file %s.\\n\", argv[1]);\n    exit(EXIT_FAILURE);\n  }\n\n  const int ncycles = atoi(argv[2]);  \n\n\n  \n\n  \n\n\n  \n\n  std::map<std::string, double> epsilons;\n  epsilons[\"Zn\"] = 96.152688;\n  epsilons[\"O\"] = 66.884614;\n  epsilons[\"C\"] = 88.480032;\n  epsilons[\"H\"] = 57.276566;\n\n  \n\n  std::map<std::string, double> sigmas;\n  sigmas[\"Zn\"] = 3.095775;\n  sigmas[\"O\"] = 3.424075;\n  sigmas[\"C\"] = 3.580425;\n  sigmas[\"H\"] = 3.150565;\n\n  \n\n  std::string line;\n  getline(materialfile, line);\n  std::istringstream istream(line);\n\n  double L;  \n\n  istream >> L;\n  printf(\"L = %f\\n\", L);\n\n  \n\n  getline(materialfile, line);\n\n  \n\n  getline(materialfile, line);\n  int natoms;  \n\n  istream.str(line);\n  istream.clear();\n  istream >> natoms;\n  printf(\"%d atoms\\n\", natoms);\n\n  \n\n  getline(materialfile, line);\n\n  \n\n  structureAtoms = (StructureAtom *) malloc(natoms * sizeof(StructureAtom));\n\n  \n\n  for (int i = 0; i < natoms; i++) {\n    \n\n    getline(materialfile, line);\n    istream.str(line);\n    istream.clear();\n\n    int atomno;\n    double xf, yf, zf;  \n\n    std::string element;\n\n    istream >> atomno >> element >> xf >> yf >> zf;\n\n    \n\n    structureAtoms[i].x = L * xf;\n    structureAtoms[i].y = L * yf;\n    structureAtoms[i].z = L * zf;\n\n    \n\n    structureAtoms[i].epsilon = epsilons[element];\n    structureAtoms[i].sigma = sigmas[element];\n  }\n\n  \n\n  StructureAtom *d_structureAtoms;\n  cudaMalloc((void**)&d_structureAtoms, natoms * sizeof(StructureAtom));\n  cudaMemcpy(d_structureAtoms, structureAtoms, natoms * sizeof(StructureAtom), cudaMemcpyHostToDevice);\n\n  \n\n  const int nBlocks = 1024;\n  const int insertionsPerCycle = nBlocks * NUMTHREADS;\n  const int ninsertions = ncycles * insertionsPerCycle;  \n\n  \n\n  double * d_boltzmannFactors;\n  cudaMalloc((void**)&d_boltzmannFactors, insertionsPerCycle * sizeof(double));\n\n  double * boltzmannFactors = (double*) malloc (insertionsPerCycle * sizeof(double));\n\n  \n\n  \n\n  \n\n\n  double total_time = 0.0;\n\n  double KH = 0.0;  \n\n  for (int cycle = 0; cycle < ncycles; cycle++) {\n\n    auto start = std::chrono::steady_clock::now();\n\n    \n\n    insertions<<<nBlocks, NUMTHREADS>>>(d_boltzmannFactors, d_structureAtoms, natoms, L);\n\n    cudaDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n\n    cudaMemcpy(boltzmannFactors, d_boltzmannFactors, insertionsPerCycle * sizeof(double), cudaMemcpyDeviceToHost);\n\n    \n\n    for(int i = 0; i < insertionsPerCycle; i++)\n      KH += boltzmannFactors[i];\n  }\n\n  \n\n  KH = KH / ninsertions;  \n  KH = KH / (R * T);  \n\n  printf(\"Used %d blocks with %d thread each\\n\", nBlocks, NUMTHREADS);\n  printf(\"Henry constant = %e mol/(m3 - Pa)\\n\", KH);\n  printf(\"Number of actual insertions: %d\\n\", ninsertions);\n  printf(\"Number of times we called the device kernel: %d\\n\", ncycles);\n  printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9) / ncycles);\n\n  cudaFree(d_structureAtoms);\n  cudaFree(d_boltzmannFactors);\n  free(structureAtoms);\n  free(boltzmannFactors);\n  return EXIT_SUCCESS;\n}\n"}}
{"kernel_name": "henry", "parallel_api": "hip", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <map>\n#include <string>\n#include <fstream>\n#include <sstream>\n#include <chrono>\n#include <hip/hip_runtime.h>\n\n#define NUMTHREADS 256  \n\n\n\n\n\n\n\n\nstruct StructureAtom {\n  \n\n  double x;\n  double y;\n  double z;\n  \n\n  double epsilon;  \n\n  \n\n  double sigma;  \n\n};\n\n\n\nconst double T = 298.0; \n\n\n\nconst double R = 8.314; \n\n\n\n__device__\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; \n\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n\n\n\n\n\n\n\n\n__device__ double compute(double x, double y, double z,\n    const StructureAtom * __restrict__ structureAtoms,\n    double natoms, double L) \n{\n  \n\n  \n\n  \n\n  \n\n  \n\n  double E = 0.0;  \n\n\n  \n\n  for (int i = 0; i < natoms; i++) {\n    \n\n    double dx = x - structureAtoms[i].x;\n    double dy = y - structureAtoms[i].y;\n    double dz = z - structureAtoms[i].z;\n\n    \n\n    const double boxupper = 0.5 * L;\n    const double boxlower = -boxupper;\n\n    dx = (dx >  boxupper) ? dx-L : dx;\n    dx = (dx >  boxupper) ? dx-L : dx;\n    dy = (dy >  boxupper) ? dy-L : dy;\n    dy = (dy <= boxlower) ? dy-L : dy;\n    dz = (dz <= boxlower) ? dz-L : dz;\n    dz = (dz <= boxlower) ? dz-L : dz;\n\n    \n\n    double rinv = 1.0 / sqrt(dx*dx + dy*dy + dz*dz);\n\n    \n\n    \n\n    double sig_ovr_r = rinv * structureAtoms[i].sigma;\n    double sig_ovr_r6 = pow(sig_ovr_r, 6);\n    double sig_ovr_r12 = sig_ovr_r6 * sig_ovr_r6;\n    E += 4.0 * structureAtoms[i].epsilon * (sig_ovr_r12 - sig_ovr_r6);\n  }\n  return exp(-E / (R * T));  \n\n}\n\n\n\n\n\n\n\n__global__ void insertions(\n    double *__restrict__ boltzmannFactors, \n    const StructureAtom * __restrict__ structureAtoms, \n    int natoms, double L) \n{\n  \n\n  \n\n  \n\n  \n\n  int id = threadIdx.x + blockIdx.x * NUMTHREADS;\n\n  \n\n  uint64_t seed = id;\n\n  \n\n  double x = L * LCG_random_double(&seed);\n  double y = L * LCG_random_double(&seed);\n  double z = L * LCG_random_double(&seed);\n\n  \n\n  boltzmannFactors[id] = compute(x, y, z, structureAtoms, natoms, L);\n}\n\nint main(int argc, char *argv[]) {\n  \n\n  if (argc != 3) {\n    printf(\"Usage: ./%s <material file> <ninsertions>\\n\", argv[0]);\n    exit(EXIT_FAILURE);\n  }\n\n  \n\n  StructureAtom *structureAtoms;  \n\n  \n\n  std::ifstream materialfile(argv[1]);\n  if (materialfile.fail()) {\n    printf(\"Failed to import file %s.\\n\", argv[1]);\n    exit(EXIT_FAILURE);\n  }\n\n  const int ncycles = atoi(argv[2]);  \n\n\n  \n\n  \n\n\n  \n\n  std::map<std::string, double> epsilons;\n  epsilons[\"Zn\"] = 96.152688;\n  epsilons[\"O\"] = 66.884614;\n  epsilons[\"C\"] = 88.480032;\n  epsilons[\"H\"] = 57.276566;\n\n  \n\n  std::map<std::string, double> sigmas;\n  sigmas[\"Zn\"] = 3.095775;\n  sigmas[\"O\"] = 3.424075;\n  sigmas[\"C\"] = 3.580425;\n  sigmas[\"H\"] = 3.150565;\n\n  \n\n  std::string line;\n  getline(materialfile, line);\n  std::istringstream istream(line);\n\n  double L;  \n\n  istream >> L;\n  printf(\"L = %f\\n\", L);\n\n  \n\n  getline(materialfile, line);\n\n  \n\n  getline(materialfile, line);\n  int natoms;  \n\n  istream.str(line);\n  istream.clear();\n  istream >> natoms;\n  printf(\"%d atoms\\n\", natoms);\n\n  \n\n  getline(materialfile, line);\n\n  \n\n  structureAtoms = (StructureAtom *) malloc(natoms * sizeof(StructureAtom));\n\n  \n\n  for (int i = 0; i < natoms; i++) {\n    \n\n    getline(materialfile, line);\n    istream.str(line);\n    istream.clear();\n\n    int atomno;\n    double xf, yf, zf;  \n\n    std::string element;\n\n    istream >> atomno >> element >> xf >> yf >> zf;\n\n    \n\n    structureAtoms[i].x = L * xf;\n    structureAtoms[i].y = L * yf;\n    structureAtoms[i].z = L * zf;\n\n    \n\n    structureAtoms[i].epsilon = epsilons[element];\n    structureAtoms[i].sigma = sigmas[element];\n  }\n\n  \n\n  StructureAtom *d_structureAtoms;\n  hipMalloc((void**)&d_structureAtoms, natoms * sizeof(StructureAtom));\n  hipMemcpy(d_structureAtoms, structureAtoms, natoms * sizeof(StructureAtom), hipMemcpyHostToDevice);\n\n  \n\n  const int nBlocks = 1024;\n  const int insertionsPerCycle = nBlocks * NUMTHREADS;\n  const int ninsertions = ncycles * insertionsPerCycle;  \n\n  \n\n  double * d_boltzmannFactors;\n  hipMalloc((void**)&d_boltzmannFactors, insertionsPerCycle * sizeof(double));\n\n  double * boltzmannFactors = (double*) malloc (insertionsPerCycle * sizeof(double));\n\n  \n\n  \n\n  \n\n\n  double total_time = 0.0;\n\n  double KH = 0.0;  \n\n  for (int cycle = 0; cycle < ncycles; cycle++) {\n\n    auto start = std::chrono::steady_clock::now();\n\n    \n\n    hipLaunchKernelGGL(insertions, nBlocks, NUMTHREADS, 0, 0, d_boltzmannFactors, d_structureAtoms, natoms, L);\n\n    hipDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n\n    hipMemcpy(boltzmannFactors, d_boltzmannFactors, insertionsPerCycle * sizeof(double), hipMemcpyDeviceToHost);\n\n    \n\n    for(int i = 0; i < insertionsPerCycle; i++)\n      KH += boltzmannFactors[i];\n  }\n  \n\n  KH = KH / ninsertions;  \n  KH = KH / (R * T);  \n\n  printf(\"Used %d blocks with %d thread each\\n\", nBlocks, NUMTHREADS);\n  printf(\"Henry constant = %e mol/(m3 - Pa)\\n\", KH);\n  printf(\"Number of actual insertions: %d\\n\", ninsertions);\n  printf(\"Number of times we called the device kernel: %d\\n\", ncycles);\n  printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9) / ncycles);\n\n  hipFree(d_structureAtoms);\n  hipFree(d_boltzmannFactors);\n  free(structureAtoms);\n  free(boltzmannFactors);\n  return EXIT_SUCCESS;\n}\n"}}
{"kernel_name": "henry", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <map>\n#include <string>\n#include <fstream>\n#include <sstream>\n#include <chrono>\n#include <omp.h>\n\n#define NUMTHREADS 256  \n\n\n\n\n\n\n\n\nstruct StructureAtom {\n  \n\n  double x;\n  double y;\n  double z;\n  \n\n  double epsilon;  \n\n  \n\n  double sigma;  \n\n};\n\n#pragma omp declare target\n\n\n\nconst double T = 298.0; \n\n\n\nconst double R = 8.314; \n\n\n\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; \n\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n\n\n\n\n\n\n\n\ndouble compute(double x, double y, double z,\n    const StructureAtom * __restrict__ structureAtoms,\n    double natoms, double L) \n{\n  \n\n  \n\n  \n\n  \n\n  \n\n  double E = 0.0;  \n\n\n  \n\n  for (int i = 0; i < natoms; i++) {\n    \n\n    double dx = x - structureAtoms[i].x;\n    double dy = y - structureAtoms[i].y;\n    double dz = z - structureAtoms[i].z;\n\n    \n\n    const double boxupper = 0.5 * L;\n    const double boxlower = -boxupper;\n\n    dx = (dx >  boxupper) ? dx-L : dx;\n    dx = (dx >  boxupper) ? dx-L : dx;\n    dy = (dy >  boxupper) ? dy-L : dy;\n    dy = (dy <= boxlower) ? dy-L : dy;\n    dz = (dz <= boxlower) ? dz-L : dz;\n    dz = (dz <= boxlower) ? dz-L : dz;\n\n    \n\n    double rinv = 1.0 / sqrt(dx*dx + dy*dy + dz*dz);\n\n    \n\n    \n\n    double sig_ovr_r = rinv * structureAtoms[i].sigma;\n    double sig_ovr_r6 = pow(sig_ovr_r, 6.0);\n    double sig_ovr_r12 = sig_ovr_r6 * sig_ovr_r6;\n    E += 4.0 * structureAtoms[i].epsilon * (sig_ovr_r12 - sig_ovr_r6);\n  }\n  return exp(-E / (R * T));  \n\n}\n#pragma omp end declare target\n\n\nint main(int argc, char *argv[]) {\n  \n\n  if (argc != 3) {\n    printf(\"Usage: ./%s <material file> <ninsertions>\\n\", argv[0]);\n    exit(EXIT_FAILURE);\n  }\n\n  \n\n  StructureAtom *structureAtoms;  \n\n  \n\n  std::ifstream materialfile(argv[1]);\n  if (materialfile.fail()) {\n    printf(\"Failed to import file %s.\\n\", argv[1]);\n    exit(EXIT_FAILURE);\n  }\n\n  const int ncycles = atoi(argv[2]);  \n\n\n  \n\n  \n\n\n  \n\n  std::map<std::string, double> epsilons;\n  epsilons[\"Zn\"] = 96.152688;\n  epsilons[\"O\"] = 66.884614;\n  epsilons[\"C\"] = 88.480032;\n  epsilons[\"H\"] = 57.276566;\n\n  \n\n  std::map<std::string, double> sigmas;\n  sigmas[\"Zn\"] = 3.095775;\n  sigmas[\"O\"] = 3.424075;\n  sigmas[\"C\"] = 3.580425;\n  sigmas[\"H\"] = 3.150565;\n\n  \n\n  std::string line;\n  getline(materialfile, line);\n  std::istringstream istream(line);\n\n  double L;  \n\n  istream >> L;\n  printf(\"L = %f\\n\", L);\n\n  \n\n  getline(materialfile, line);\n\n  \n\n  getline(materialfile, line);\n  int natoms;  \n\n  istream.str(line);\n  istream.clear();\n  istream >> natoms;\n  printf(\"%d atoms\\n\", natoms);\n\n  \n\n  getline(materialfile, line);\n\n  \n\n  structureAtoms = (StructureAtom *) malloc(natoms * sizeof(StructureAtom));\n\n  \n\n  for (int i = 0; i < natoms; i++) {\n    \n\n    getline(materialfile, line);\n    istream.str(line);\n    istream.clear();\n\n    int atomno;\n    double xf, yf, zf;  \n\n    std::string element;\n\n    istream >> atomno >> element >> xf >> yf >> zf;\n\n    \n\n    structureAtoms[i].x = L * xf;\n    structureAtoms[i].y = L * yf;\n    structureAtoms[i].z = L * zf;\n\n    \n\n    structureAtoms[i].epsilon = epsilons[element];\n    structureAtoms[i].sigma = sigmas[element];\n  }\n\n  \n\n  const int nBlocks = 1024;\n  const int insertionsPerCycle = nBlocks * NUMTHREADS;\n  const int ninsertions = ncycles * insertionsPerCycle;  \n\n  double * boltzmannFactors = (double*) malloc (insertionsPerCycle * sizeof(double));\n\n  #pragma omp target data map(to: structureAtoms[0:natoms]) \\\n                          map(alloc: boltzmannFactors[0:insertionsPerCycle])\n  {\n    \n\n    \n\n    \n\n    double total_time = 0.0;\n\n    double KH = 0.0;  \n\n    for (int cycle = 0; cycle < ncycles; cycle++) {\n\n      auto start = std::chrono::steady_clock::now();\n\n      \n\n      \n\n      \n\n      #pragma omp target teams distribute parallel for thread_limit(NUMTHREADS)\n      for (int id = 0; id < insertionsPerCycle; id++) {\n\n        \n\n        uint64_t seed = id;\n\n        \n\n        double x = L * LCG_random_double(&seed);\n        double y = L * LCG_random_double(&seed);\n        double z = L * LCG_random_double(&seed);\n\n        \n\n        boltzmannFactors[id] = compute(x, y, z, structureAtoms, natoms, L);\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      total_time += time;\n\n      #pragma omp target update from (boltzmannFactors[0:insertionsPerCycle])\n\n      \n\n      for(int i = 0; i < insertionsPerCycle; i++)\n        KH += boltzmannFactors[i];\n    }\n\n    \n\n    KH = KH / ninsertions;  \n    KH = KH / (R * T);  \n\n    printf(\"Used %d blocks with %d thread each\\n\", nBlocks, NUMTHREADS);\n    printf(\"Henry constant = %e mol/(m3 - Pa)\\n\", KH);\n    printf(\"Number of actual insertions: %d\\n\", ninsertions);\n    printf(\"Number of times we called the device kernel: %d\\n\", ncycles);\n    printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9) / ncycles);\n  }\n\n  free(structureAtoms);\n  free(boltzmannFactors);\n  return EXIT_SUCCESS;\n}\n"}}
{"kernel_name": "henry", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <map>\n#include <string>\n#include <fstream>\n#include <sstream>\n#include <chrono>\n\n#define NUMTHREADS 256  \n\n\n\n\n\n\n\n\nstruct StructureAtom {\n  \n\n  double x;\n  double y;\n  double z;\n  \n\n  double epsilon;  \n\n  \n\n  double sigma;  \n\n};\n\n\n\n\nconst double T = 298.0; \n\n\n\nconst double R = 8.314; \n\n\n\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; \n\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n\n\n\n\n\n\n\n\ndouble compute(double x, double y, double z,\n    const StructureAtom * __restrict__ structureAtoms,\n    double natoms, double L) \n{\n  \n\n  \n\n  \n\n  \n\n  \n\n  double E = 0.0;  \n\n\n  \n\n  for (int i = 0; i < natoms; i++) {\n    \n\n    double dx = x - structureAtoms[i].x;\n    double dy = y - structureAtoms[i].y;\n    double dz = z - structureAtoms[i].z;\n\n    \n\n    const double boxupper = 0.5 * L;\n    const double boxlower = -boxupper;\n\n    dx = (dx >  boxupper) ? dx-L : dx;\n    dx = (dx >  boxupper) ? dx-L : dx;\n    dy = (dy >  boxupper) ? dy-L : dy;\n    dy = (dy <= boxlower) ? dy-L : dy;\n    dz = (dz <= boxlower) ? dz-L : dz;\n    dz = (dz <= boxlower) ? dz-L : dz;\n\n    \n\n    double rinv = 1.0 / sqrt(dx*dx + dy*dy + dz*dz);\n\n    \n\n    \n\n    double sig_ovr_r = rinv * structureAtoms[i].sigma;\n    double sig_ovr_r6 = pow(sig_ovr_r, 6.0);\n    double sig_ovr_r12 = sig_ovr_r6 * sig_ovr_r6;\n    E += 4.0 * structureAtoms[i].epsilon * (sig_ovr_r12 - sig_ovr_r6);\n  }\n  return exp(-E / (R * T));  \n\n}\n\n\nint main(int argc, char *argv[]) {\n  \n\n  if (argc != 3) {\n    printf(\"Usage: ./%s <material file> <ninsertions>\\n\", argv[0]);\n    exit(EXIT_FAILURE);\n  }\n\n  \n\n  StructureAtom *structureAtoms;  \n\n  \n\n  std::ifstream materialfile(argv[1]);\n  if (materialfile.fail()) {\n    printf(\"Failed to import file %s.\\n\", argv[1]);\n    exit(EXIT_FAILURE);\n  }\n\n  const int ncycles = atoi(argv[2]);  \n\n\n  \n\n  \n\n\n  \n\n  std::map<std::string, double> epsilons;\n  epsilons[\"Zn\"] = 96.152688;\n  epsilons[\"O\"] = 66.884614;\n  epsilons[\"C\"] = 88.480032;\n  epsilons[\"H\"] = 57.276566;\n\n  \n\n  std::map<std::string, double> sigmas;\n  sigmas[\"Zn\"] = 3.095775;\n  sigmas[\"O\"] = 3.424075;\n  sigmas[\"C\"] = 3.580425;\n  sigmas[\"H\"] = 3.150565;\n\n  \n\n  std::string line;\n  getline(materialfile, line);\n  std::istringstream istream(line);\n\n  double L;  \n\n  istream >> L;\n  printf(\"L = %f\\n\", L);\n\n  \n\n  getline(materialfile, line);\n\n  \n\n  getline(materialfile, line);\n  int natoms;  \n\n  istream.str(line);\n  istream.clear();\n  istream >> natoms;\n  printf(\"%d atoms\\n\", natoms);\n\n  \n\n  getline(materialfile, line);\n\n  \n\n  structureAtoms = (StructureAtom *) malloc(natoms * sizeof(StructureAtom));\n\n  \n\n  for (int i = 0; i < natoms; i++) {\n    \n\n    getline(materialfile, line);\n    istream.str(line);\n    istream.clear();\n\n    int atomno;\n    double xf, yf, zf;  \n\n    std::string element;\n\n    istream >> atomno >> element >> xf >> yf >> zf;\n\n    \n\n    structureAtoms[i].x = L * xf;\n    structureAtoms[i].y = L * yf;\n    structureAtoms[i].z = L * zf;\n\n    \n\n    structureAtoms[i].epsilon = epsilons[element];\n    structureAtoms[i].sigma = sigmas[element];\n  }\n\n  \n\n  const int nBlocks = 1024;\n  const int insertionsPerCycle = nBlocks * NUMTHREADS;\n  const int ninsertions = ncycles * insertionsPerCycle;  \n\n  double * boltzmannFactors = (double*) malloc (insertionsPerCycle * sizeof(double));\n\n    {\n    \n\n    \n\n    \n\n    double total_time = 0.0;\n\n    double KH = 0.0;  \n\n    for (int cycle = 0; cycle < ncycles; cycle++) {\n\n      auto start = std::chrono::steady_clock::now();\n\n      \n\n      \n\n      \n\n            for (int id = 0; id < insertionsPerCycle; id++) {\n\n        \n\n        uint64_t seed = id;\n\n        \n\n        double x = L * LCG_random_double(&seed);\n        double y = L * LCG_random_double(&seed);\n        double z = L * LCG_random_double(&seed);\n\n        \n\n        boltzmannFactors[id] = compute(x, y, z, structureAtoms, natoms, L);\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      total_time += time;\n\n      \n      \n\n      for(int i = 0; i < insertionsPerCycle; i++)\n        KH += boltzmannFactors[i];\n    }\n\n    \n\n    KH = KH / ninsertions;  \n    KH = KH / (R * T);  \n\n    printf(\"Used %d blocks with %d thread each\\n\", nBlocks, NUMTHREADS);\n    printf(\"Henry constant = %e mol/(m3 - Pa)\\n\", KH);\n    printf(\"Number of actual insertions: %d\\n\", ninsertions);\n    printf(\"Number of times we called the device kernel: %d\\n\", ncycles);\n    printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9) / ncycles);\n  }\n\n  free(structureAtoms);\n  free(boltzmannFactors);\n  return EXIT_SUCCESS;\n}"}}
{"kernel_name": "henry", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <map>\n#include <string>\n#include <fstream>\n#include <sstream>\n#include <sycl/sycl.hpp>\n\n#define NUMTHREADS 256  \n\n\n\n\n\n\n\n\nstruct StructureAtom {\n  \n\n  double x;\n  double y;\n  double z;\n  \n\n  double epsilon;  \n\n  \n\n  double sigma;  \n\n};\n\n\n\nconst double T = 298.0; \n\n\n\nconst double R = 8.314; \n\n\n\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; \n\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n\n\n\n\n\n\n\n\ndouble compute(double x, double y, double z,\n    const StructureAtom * __restrict structureAtoms,\n    double natoms, double L) \n{\n  \n\n  \n\n  \n\n  \n\n  \n\n  double E = 0.0;  \n\n\n  \n\n  for (int i = 0; i < natoms; i++) {\n    \n\n    double dx = x - structureAtoms[i].x;\n    double dy = y - structureAtoms[i].y;\n    double dz = z - structureAtoms[i].z;\n\n    \n\n    const double boxupper = 0.5 * L;\n    const double boxlower = -boxupper;\n\n    dx = (dx >  boxupper) ? dx-L : dx;\n    dx = (dx >  boxupper) ? dx-L : dx;\n    dy = (dy >  boxupper) ? dy-L : dy;\n    dy = (dy <= boxlower) ? dy-L : dy;\n    dz = (dz <= boxlower) ? dz-L : dz;\n    dz = (dz <= boxlower) ? dz-L : dz;\n\n    \n\n    double rinv = 1.0 / sycl::sqrt(dx*dx + dy*dy + dz*dz);\n\n    \n\n    \n\n    double sig_ovr_r = rinv * structureAtoms[i].sigma;\n    double sig_ovr_r6 = sycl::pow(sig_ovr_r, 6.0);\n    double sig_ovr_r12 = sig_ovr_r6 * sig_ovr_r6;\n    E += 4.0 * structureAtoms[i].epsilon * (sig_ovr_r12 - sig_ovr_r6);\n  }\n  return sycl::exp(-E / (R * T));  \n\n}\n\n\n\n\n\n\n\nvoid insertions(\n    sycl::nd_item<1> &item,\n    double *__restrict boltzmannFactors, \n    const StructureAtom * __restrict structureAtoms, \n    int natoms, double L) \n{\n  \n\n  \n\n  \n\n  \n\n  int id = item.get_global_id(0);\n\n  \n\n  uint64_t seed = id;\n\n  \n\n  double x = L * LCG_random_double(&seed);\n  double y = L * LCG_random_double(&seed);\n  double z = L * LCG_random_double(&seed);\n\n  \n\n  boltzmannFactors[id] = compute(x, y, z, structureAtoms, natoms, L);\n}\n\nint main(int argc, char *argv[]) {\n  \n\n  if (argc != 3) {\n    printf(\"Usage: ./%s <material file> <ninsertions>\\n\", argv[0]);\n    exit(EXIT_FAILURE);\n  }\n\n  \n\n  StructureAtom *structureAtoms;  \n\n  \n\n  std::ifstream materialfile(argv[1]);\n  if (materialfile.fail()) {\n    printf(\"Failed to import file %s.\\n\", argv[1]);\n    exit(EXIT_FAILURE);\n  }\n\n  const int ncycles = atoi(argv[2]);  \n\n\n  \n\n  \n\n\n  \n\n  std::map<std::string, double> epsilons;\n  epsilons[\"Zn\"] = 96.152688;\n  epsilons[\"O\"] = 66.884614;\n  epsilons[\"C\"] = 88.480032;\n  epsilons[\"H\"] = 57.276566;\n\n  \n\n  std::map<std::string, double> sigmas;\n  sigmas[\"Zn\"] = 3.095775;\n  sigmas[\"O\"] = 3.424075;\n  sigmas[\"C\"] = 3.580425;\n  sigmas[\"H\"] = 3.150565;\n\n  \n\n  std::string line;\n  getline(materialfile, line);\n  std::istringstream istream(line);\n\n  double L;  \n\n  istream >> L;\n  printf(\"L = %f\\n\", L);\n\n  \n\n  getline(materialfile, line);\n\n  \n\n  getline(materialfile, line);\n  int natoms;  \n\n  istream.str(line);\n  istream.clear();\n  istream >> natoms;\n  printf(\"%d atoms\\n\", natoms);\n\n  \n\n  getline(materialfile, line);\n\n  \n\n  structureAtoms = (StructureAtom *) malloc(natoms * sizeof(StructureAtom));\n\n  \n\n  for (int i = 0; i < natoms; i++) {\n    \n\n    getline(materialfile, line);\n    istream.str(line);\n    istream.clear();\n\n    int atomno;\n    double xf, yf, zf;  \n\n    std::string element;\n\n    istream >> atomno >> element >> xf >> yf >> zf;\n\n    \n\n    structureAtoms[i].x = L * xf;\n    structureAtoms[i].y = L * yf;\n    structureAtoms[i].z = L * zf;\n\n    \n\n    structureAtoms[i].epsilon = epsilons[element];\n    structureAtoms[i].sigma = sigmas[element];\n  }\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  \n\n  StructureAtom *d_structureAtoms = sycl::malloc_device<StructureAtom>(natoms, q);\n  q.memcpy(d_structureAtoms, structureAtoms, natoms * sizeof(StructureAtom));\n\n  \n\n  const int nBlocks = 1024;\n  const int insertionsPerCycle = nBlocks * NUMTHREADS;\n  const int ninsertions = ncycles * insertionsPerCycle;  \n\n  \n\n  double *d_boltzmannFactors = sycl::malloc_device<double>(insertionsPerCycle, q);\n\n  double * boltzmannFactors = (double*) malloc (insertionsPerCycle * sizeof(double));\n\n  \n\n  \n\n  \n\n\n  sycl::range<1> gws (insertionsPerCycle);\n  sycl::range<1> lws (NUMTHREADS);\n\n  double total_time = 0.0;\n\n  double KH = 0.0;  \n\n  for (int cycle = 0; cycle < ncycles; cycle++) {\n\n    auto start = std::chrono::steady_clock::now();\n\n    \n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class insertations>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        insertions(item, d_boltzmannFactors, d_structureAtoms, natoms, L);\n      });\n    }).wait();\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    total_time += time;\n\n    q.memcpy(boltzmannFactors, d_boltzmannFactors, insertionsPerCycle * sizeof(double)).wait();\n\n    \n\n    for(int i = 0; i < insertionsPerCycle; i++)\n      KH += boltzmannFactors[i];\n  }\n\n  \n\n  KH = KH / ninsertions;  \n  KH = KH / (R * T);  \n\n  printf(\"Used %d blocks with %d thread each\\n\", nBlocks, NUMTHREADS);\n  printf(\"Henry constant = %e mol/(m3 - Pa)\\n\", KH);\n  printf(\"Number of actual insertions: %d\\n\", ninsertions);\n  printf(\"Number of times we called the device kernel: %d\\n\", ncycles);\n  printf(\"Average kernel execution time %f (s)\\n\", (total_time * 1e-9) / ncycles);\n\n  sycl::free(d_structureAtoms, q);\n  sycl::free(d_boltzmannFactors, q);\n  free(structureAtoms);\n  free(boltzmannFactors);\n  return EXIT_SUCCESS;\n}\n"}}
{"kernel_name": "ising", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n#include <chrono>\n#include <fstream>\n#include <getopt.h>\n#include <iostream>\n#include <string>\n#include <cstring>\n\n#ifdef CURAND\n#include <curand.h>\n#endif\n\n#include \"cudamacro.h\"\n\n#define TCRIT 2.26918531421f\n#define THREADS  128\n\n\n\n__global__ void init_spins(signed char* lattice,\n                           const float* __restrict__ randvals,\n                           const long long nx,\n                           const long long ny) {\n  const long long  tid = static_cast<long long>(blockDim.x) * blockIdx.x + threadIdx.x;\n  if (tid >= nx * ny) return;\n\n  float randval = randvals[tid];\n  signed char val = (randval < 0.5f) ? -1 : 1;\n  lattice[tid] = val;\n}\n\ntemplate<bool is_black>\n__global__ void update_lattice(signed char* lattice,\n                               const signed char* __restrict__ op_lattice,\n                               const float* __restrict__ randvals,\n                               const float inv_temp,\n                               const long long nx,\n                               const long long ny) {\n  const long long tid = static_cast<long long>(blockDim.x) * blockIdx.x + threadIdx.x;\n  const int i = tid / ny;\n  const int j = tid % ny;\n\n  if (i >= nx || j >= ny) return;\n\n  \n\n  int ipp = (i + 1 < nx) ? i + 1 : 0;\n  int inn = (i - 1 >= 0) ? i - 1: nx - 1;\n  int jpp = (j + 1 < ny) ? j + 1 : 0;\n  int jnn = (j - 1 >= 0) ? j - 1: ny - 1;\n\n  \n\n  int joff;\n  if (is_black) {\n    joff = (i % 2) ? jpp : jnn;\n  } else {\n    joff = (i % 2) ? jnn : jpp;\n  }\n\n  \n\n  signed char nn_sum = op_lattice[inn * ny + j] + op_lattice[i * ny + j] + op_lattice[ipp * ny + j] + op_lattice[i * ny + joff];\n\n  \n\n  signed char lij = lattice[i * ny + j];\n  float acceptance_ratio = expf(-2.0f * inv_temp * nn_sum * lij);\n  if (randvals[i*ny + j] < acceptance_ratio) {\n    lattice[i * ny + j] = -lij;\n  }\n}\n\nvoid update(signed char *lattice_b, \n    signed char *lattice_w, \n    float* randvals, \n#ifdef CURAND\n    curandGenerator_t rng, \n#endif\n    float inv_temp, \n    long long nx, \n    long long ny) {\n\n  \n\n  int blocks = (nx * ny/2 + THREADS - 1) / THREADS;\n\n  \n\n#ifdef CURAND\n  CHECK_CURAND(curandGenerateUniform(rng, randvals, nx*ny/2));\n#endif\n  update_lattice<true><<<blocks, THREADS>>>(lattice_b, lattice_w, randvals, inv_temp, nx, ny/2);\n\n  \n\n#ifdef CURAND\n  CHECK_CURAND(curandGenerateUniform(rng, randvals, nx*ny/2));\n#endif\n  update_lattice<false><<<blocks, THREADS>>>(lattice_w, lattice_b, randvals, inv_temp, nx, ny/2);\n}\n\nstatic void usage(const char *pname) {\n\n  const char *bname = rindex(pname, '/');\n  if (!bname) {bname = pname;}\n  else        {bname++;}\n\n  fprintf(stdout,\n          \"Usage: %s [options]\\n\"\n          \"options:\\n\"\n          \"\\t-x|--lattice-n <LATTICE_N>\\n\"\n          \"\\t\\tnumber of lattice rows\\n\"\n          \"\\n\"\n          \"\\t-y|--lattice_m <LATTICE_M>\\n\"\n          \"\\t\\tnumber of lattice columns\\n\"\n          \"\\n\"\n          \"\\t-w|--nwarmup <NWARMUP>\\n\"\n          \"\\t\\tnumber of warmup iterations\\n\"\n          \"\\n\"\n          \"\\t-n|--niters <NITERS>\\n\"\n          \"\\t\\tnumber of trial iterations\\n\"\n          \"\\n\"\n          \"\\t-a|--alpha <ALPHA>\\n\"\n          \"\\t\\tcoefficient of critical temperature\\n\"\n          \"\\n\"\n          \"\\t-s|--seed <SEED>\\n\"\n          \"\\t\\tseed for random number generation\\n\\n\",\n          bname);\n  exit(EXIT_SUCCESS);\n}\n\nint main(int argc, char **argv) {\n\n  \n\n  long long nx = 5120;\n  long long ny = 5120;\n  float alpha = 0.1f;\n  int nwarmup = 100;\n  int niters = 1000;\n  bool write = false;\n  unsigned long long seed = 1234ULL;\n\n  while (1) {\n    static struct option long_options[] = {\n        {     \"lattice-n\", required_argument, 0, 'x'},\n        {     \"lattice-m\", required_argument, 0, 'y'},\n        {         \"alpha\", required_argument, 0, 'y'},\n        {          \"seed\", required_argument, 0, 's'},\n        {       \"nwarmup\", required_argument, 0, 'w'},\n        {        \"niters\", required_argument, 0, 'n'},\n        { \"write-lattice\",       no_argument, 0, 'o'},\n        {          \"help\",       no_argument, 0, 'h'},\n        {               0,                 0, 0,   0}\n    };\n\n    int option_index = 0;\n    int ch = getopt_long(argc, argv, \"x:y:a:s:w:n:h\", long_options, &option_index);\n    if (ch == -1) break;\n\n    switch(ch) {\n      case 0:\n        break;\n      case 'x':\n        nx = atoll(optarg); break;\n      case 'y':\n        ny = atoll(optarg); break;\n      case 'a':\n        alpha = atof(optarg); break;\n      case 's':\n        seed = atoll(optarg); break;\n      case 'w':\n        nwarmup = atoi(optarg); break;\n      case 'n':\n        niters = atoi(optarg); break;\n      case 'h':\n        usage(argv[0]); break;\n      case '?':\n        exit(EXIT_FAILURE);\n      default:\n        fprintf(stderr, \"unknown option: %c\\n\", ch);\n        exit(EXIT_FAILURE);\n    }\n  }\n\n  \n\n  if (nx % 2 != 0 || ny % 2 != 0) {\n    fprintf(stderr, \"ERROR: Lattice dimensions must be even values.\\n\");\n    exit(EXIT_FAILURE);\n  }\n\n  float inv_temp = 1.0f / (alpha*TCRIT);\n\n#ifdef CURAND\n  \n\n  curandGenerator_t rng;\n  CHECK_CURAND(curandCreateGenerator(&rng, CURAND_RNG_PSEUDO_PHILOX4_32_10));\n  CHECK_CURAND(curandSetPseudoRandomGeneratorSeed(rng, seed));\n#else\n  \n\n  srand(seed);\n  float* randvals_host = (float*) malloc(nx * ny/2 * sizeof(float));\n  for (int i = 0; i < nx * ny/2; i++)\n    randvals_host[i] = (float)rand() / (float)RAND_MAX;\n#endif\n\n  float *randvals;\n  CHECK_CUDA(cudaMalloc(&randvals, nx * ny/2 * sizeof(*randvals)));\n\n  \n\n  signed char *lattice_b, *lattice_w;\n  CHECK_CUDA(cudaMalloc(&lattice_b, nx * ny/2 * sizeof(*lattice_b)));\n  CHECK_CUDA(cudaMalloc(&lattice_w, nx * ny/2 * sizeof(*lattice_w)));\n\n  int blocks = (nx * ny/2 + THREADS - 1) / THREADS;\n\n#ifndef CURAND\n  cudaMemcpy(randvals, randvals_host, nx*ny/2*sizeof(float), cudaMemcpyHostToDevice);\n#endif\n\n#ifdef CURAND\n  CHECK_CURAND(curandGenerateUniform(rng, randvals, nx*ny/2));\n#endif\n  init_spins<<<blocks, THREADS>>>(lattice_b, randvals, nx, ny/2);\n\n#ifdef CURAND\n  CHECK_CURAND(curandGenerateUniform(rng, randvals, nx*ny/2));\n#endif\n  init_spins<<<blocks, THREADS>>>(lattice_w, randvals, nx, ny/2);\n\n  \n\n  printf(\"Starting warmup...\\n\");\n  for (int i = 0; i < nwarmup; i++) {\n    update(lattice_b, lattice_w, randvals, \n#ifdef CURAND\n        rng, \n#endif\n        inv_temp, nx, ny);\n  }\n\n  CHECK_CUDA(cudaDeviceSynchronize());\n\n  printf(\"Starting trial iterations...\\n\");\n  auto t0 = std::chrono::high_resolution_clock::now();\n  for (int i = 0; i < niters; i++) {\n    update(lattice_b, lattice_w, randvals, \n#ifdef CURAND\n        rng, \n#endif\n        inv_temp, nx, ny);\n  }\n\n  CHECK_CUDA(cudaDeviceSynchronize());\n  auto t1 = std::chrono::high_resolution_clock::now();\n\n  double duration = (double) std::chrono::duration_cast<std::chrono::microseconds>(t1-t0).count();\n  printf(\"REPORT:\\n\");\n  printf(\"\\tnGPUs: %d\\n\", 1);\n  printf(\"\\ttemperature: %f * %f\\n\", alpha, TCRIT);\n  printf(\"\\tseed: %llu\\n\", seed);\n  printf(\"\\twarmup iterations: %d\\n\", nwarmup);\n  printf(\"\\ttrial iterations: %d\\n\", niters);\n  printf(\"\\tlattice dimensions: %lld x %lld\\n\", nx, ny);\n  printf(\"\\telapsed time: %f sec\\n\", duration * 1e-6);\n  printf(\"\\tupdates per ns: %f\\n\", (double) (nx * ny) * niters / duration * 1e-3);\n\n  signed char* lattice_b_h = (signed char*) malloc(nx * ny/2 * sizeof(*lattice_b_h));\n  signed char* lattice_w_h = (signed char*) malloc(nx * ny/2 * sizeof(*lattice_w_h));\n  CHECK_CUDA(cudaMemcpy(lattice_b_h, lattice_b, nx * ny/2 * sizeof(*lattice_b), cudaMemcpyDeviceToHost));\n  CHECK_CUDA(cudaMemcpy(lattice_w_h, lattice_w, nx * ny/2 * sizeof(*lattice_w), cudaMemcpyDeviceToHost));\n  double naivesum = 0.0;\n  for (int i = 0; i < nx*ny/2; i++) {\n    naivesum += lattice_b_h[i];\n    naivesum += lattice_w_h[i];\n  }\n  printf(\"checksum = %lf\\n\", naivesum);\n#ifndef CURAND\n  free(randvals_host);\n#endif\n  free(lattice_b_h);\n  free(lattice_w_h);\n\n  cudaFree(lattice_b);\n  cudaFree(lattice_w);\n  cudaFree(randvals);\n\n  return 0;\n}\n"}}
{"kernel_name": "ising", "parallel_api": "hip", "code": {"main.cu": "\n\n\n#include <chrono>\n#include <fstream>\n#include <getopt.h>\n#include <iostream>\n#include <string>\n#include <cstring>\n\n#ifdef HIPRAND\n#include <hiprand.h>\n#endif\n\n#include \"hipmacro.h\"\n\n#define TCRIT 2.26918531421f\n#define THREADS  128\n\n\n\n__global__ void init_spins(signed char* lattice,\n                           const float* __restrict__ randvals,\n                           const long long nx,\n                           const long long ny) {\n  const long long  tid = static_cast<long long>(blockDim.x) * blockIdx.x + threadIdx.x;\n  if (tid >= nx * ny) return;\n\n  float randval = randvals[tid];\n  signed char val = (randval < 0.5f) ? -1 : 1;\n  lattice[tid] = val;\n}\n\ntemplate<bool is_black>\n__global__ void update_lattice(signed char* lattice,\n                               const signed char* __restrict__ op_lattice,\n                               const float* __restrict__ randvals,\n                               const float inv_temp,\n                               const long long nx,\n                               const long long ny) {\n  const long long tid = static_cast<long long>(blockDim.x) * blockIdx.x + threadIdx.x;\n  const int i = tid / ny;\n  const int j = tid % ny;\n\n  if (i >= nx || j >= ny) return;\n\n  \n\n  int ipp = (i + 1 < nx) ? i + 1 : 0;\n  int inn = (i - 1 >= 0) ? i - 1: nx - 1;\n  int jpp = (j + 1 < ny) ? j + 1 : 0;\n  int jnn = (j - 1 >= 0) ? j - 1: ny - 1;\n\n  \n\n  int joff;\n  if (is_black) {\n    joff = (i % 2) ? jpp : jnn;\n  } else {\n    joff = (i % 2) ? jnn : jpp;\n  }\n\n  \n\n  signed char nn_sum = op_lattice[inn * ny + j] + op_lattice[i * ny + j] + op_lattice[ipp * ny + j] + op_lattice[i * ny + joff];\n\n  \n\n  signed char lij = lattice[i * ny + j];\n  float acceptance_ratio = expf(-2.0f * inv_temp * nn_sum * lij);\n  if (randvals[i*ny + j] < acceptance_ratio) {\n    lattice[i * ny + j] = -lij;\n  }\n}\n\nvoid update(signed char *lattice_b, \n    signed char *lattice_w, \n    float* randvals, \n#ifdef HIPRAND\n    hiprandGenerator_t rng, \n#endif\n    float inv_temp, \n    long long nx, \n    long long ny) {\n\n  \n\n  int blocks = (nx * ny/2 + THREADS - 1) / THREADS;\n\n  \n\n#ifdef HIPRAND\n  CHECK_HIPRAND(hiprandGenerateUniform(rng, randvals, nx*ny/2));\n#endif\n  hipLaunchKernelGGL(HIP_KERNEL_NAME(update_lattice<true>), dim3(blocks), dim3(THREADS), 0, 0, lattice_b, lattice_w, randvals, inv_temp, nx, ny/2);\n\n  \n\n#ifdef HIPRAND\n  CHECK_HIPRAND(hiprandGenerateUniform(rng, randvals, nx*ny/2));\n#endif\n  hipLaunchKernelGGL(HIP_KERNEL_NAME(update_lattice<false>), dim3(blocks), dim3(THREADS), 0, 0, lattice_w, lattice_b, randvals, inv_temp, nx, ny/2);\n}\n\nstatic void usage(const char *pname) {\n\n  const char *bname = rindex(pname, '/');\n  if (!bname) {bname = pname;}\n  else        {bname++;}\n\n  fprintf(stdout,\n          \"Usage: %s [options]\\n\"\n          \"options:\\n\"\n          \"\\t-x|--lattice-n <LATTICE_N>\\n\"\n          \"\\t\\tnumber of lattice rows\\n\"\n          \"\\n\"\n          \"\\t-y|--lattice_m <LATTICE_M>\\n\"\n          \"\\t\\tnumber of lattice columns\\n\"\n          \"\\n\"\n          \"\\t-w|--nwarmup <NWARMUP>\\n\"\n          \"\\t\\tnumber of warmup iterations\\n\"\n          \"\\n\"\n          \"\\t-n|--niters <NITERS>\\n\"\n          \"\\t\\tnumber of trial iterations\\n\"\n          \"\\n\"\n          \"\\t-a|--alpha <ALPHA>\\n\"\n          \"\\t\\tcoefficient of critical temperature\\n\"\n          \"\\n\"\n          \"\\t-s|--seed <SEED>\\n\"\n          \"\\t\\tseed for random number generation\\n\\n\",\n          bname);\n  exit(EXIT_SUCCESS);\n}\n\nint main(int argc, char **argv) {\n\n  \n\n  long long nx = 5120;\n  long long ny = 5120;\n  float alpha = 0.1f;\n  int nwarmup = 100;\n  int niters = 1000;\n  bool write = false;\n  unsigned long long seed = 1234ULL;\n\n  while (1) {\n    static struct option long_options[] = {\n        {     \"lattice-n\", required_argument, 0, 'x'},\n        {     \"lattice-m\", required_argument, 0, 'y'},\n        {         \"alpha\", required_argument, 0, 'y'},\n        {          \"seed\", required_argument, 0, 's'},\n        {       \"nwarmup\", required_argument, 0, 'w'},\n        {        \"niters\", required_argument, 0, 'n'},\n        { \"write-lattice\",       no_argument, 0, 'o'},\n        {          \"help\",       no_argument, 0, 'h'},\n        {               0,                 0, 0,   0}\n    };\n\n    int option_index = 0;\n    int ch = getopt_long(argc, argv, \"x:y:a:s:w:n:h\", long_options, &option_index);\n    if (ch == -1) break;\n\n    switch(ch) {\n      case 0:\n        break;\n      case 'x':\n        nx = atoll(optarg); break;\n      case 'y':\n        ny = atoll(optarg); break;\n      case 'a':\n        alpha = atof(optarg); break;\n      case 's':\n        seed = atoll(optarg); break;\n      case 'w':\n        nwarmup = atoi(optarg); break;\n      case 'n':\n        niters = atoi(optarg); break;\n      case 'h':\n        usage(argv[0]); break;\n      case '?':\n        exit(EXIT_FAILURE);\n      default:\n        fprintf(stderr, \"unknown option: %c\\n\", ch);\n        exit(EXIT_FAILURE);\n    }\n  }\n\n  \n\n  if (nx % 2 != 0 || ny % 2 != 0) {\n    fprintf(stderr, \"ERROR: Lattice dimensions must be even values.\\n\");\n    exit(EXIT_FAILURE);\n  }\n\n  float inv_temp = 1.0f / (alpha*TCRIT);\n\n\n#ifdef HIPRAND\n  \n\n  hiprandGenerator_t rng;\n  CHECK_HIPRAND(hiprandCreateGenerator(&rng, HIPRAND_RNG_PSEUDO_PHILOX4_32_10));\n  CHECK_HIPRAND(hiprandSetPseudoRandomGeneratorSeed(rng, seed));\n#else\n  \n\n  srand(seed);\n  float* randvals_host = (float*) malloc(nx * ny/2 * sizeof(float));\n  for (int i = 0; i < nx * ny/2; i++)\n    randvals_host[i] = (float)rand() / (float)RAND_MAX;\n#endif\n\n  float *randvals;\n  CHECK_HIP(hipMalloc(&randvals, nx * ny/2 * sizeof(*randvals)));\n\n  \n\n  signed char *lattice_b, *lattice_w;\n  CHECK_HIP(hipMalloc(&lattice_b, nx * ny/2 * sizeof(*lattice_b)));\n  CHECK_HIP(hipMalloc(&lattice_w, nx * ny/2 * sizeof(*lattice_w)));\n\n  int blocks = (nx * ny/2 + THREADS - 1) / THREADS;\n\n#ifndef HIPRAND\n  hipMemcpy(randvals, randvals_host, nx*ny/2*sizeof(float), hipMemcpyHostToDevice);\n#endif\n\n#ifdef HIPRAND\n  CHECK_HIPRAND(hiprandGenerateUniform(rng, randvals, nx*ny/2));\n#endif\n  hipLaunchKernelGGL(init_spins, dim3(blocks), dim3(THREADS), 0, 0, lattice_b, randvals, nx, ny/2);\n\n#ifdef HIPRAND\n  CHECK_HIPRAND(hiprandGenerateUniform(rng, randvals, nx*ny/2));\n#endif\n  hipLaunchKernelGGL(init_spins, dim3(blocks), dim3(THREADS), 0, 0, lattice_w, randvals, nx, ny/2);\n\n  \n\n  printf(\"Starting warmup...\\n\");\n  for (int i = 0; i < nwarmup; i++) {\n    update(lattice_b, lattice_w, randvals, \n#ifdef HIPRAND\n        rng, \n#endif\n        inv_temp, nx, ny);\n  }\n\n  CHECK_HIP(hipDeviceSynchronize());\n\n  printf(\"Starting trial iterations...\\n\");\n  auto t0 = std::chrono::high_resolution_clock::now();\n  for (int i = 0; i < niters; i++) {\n    update(lattice_b, lattice_w, randvals, \n#ifdef HIPRAND\n        rng, \n#endif\n        inv_temp, nx, ny);\n  }\n\n  CHECK_HIP(hipDeviceSynchronize());\n  auto t1 = std::chrono::high_resolution_clock::now();\n\n  double duration = (double) std::chrono::duration_cast<std::chrono::microseconds>(t1-t0).count();\n  printf(\"REPORT:\\n\");\n  printf(\"\\tnGPUs: %d\\n\", 1);\n  printf(\"\\ttemperature: %f * %f\\n\", alpha, TCRIT);\n  printf(\"\\tseed: %llu\\n\", seed);\n  printf(\"\\twarmup iterations: %d\\n\", nwarmup);\n  printf(\"\\ttrial iterations: %d\\n\", niters);\n  printf(\"\\tlattice dimensions: %lld x %lld\\n\", nx, ny);\n  printf(\"\\telapsed time: %f sec\\n\", duration * 1e-6);\n  printf(\"\\tupdates per ns: %f\\n\", (double) (nx * ny) * niters / duration * 1e-3);\n\n  signed char* lattice_b_h = (signed char*) malloc(nx * ny/2 * sizeof(*lattice_b_h));\n  signed char* lattice_w_h = (signed char*) malloc(nx * ny/2 * sizeof(*lattice_w_h));\n  CHECK_HIP(hipMemcpy(lattice_b_h, lattice_b, nx * ny/2 * sizeof(*lattice_b), hipMemcpyDeviceToHost));\n  CHECK_HIP(hipMemcpy(lattice_w_h, lattice_w, nx * ny/2 * sizeof(*lattice_w), hipMemcpyDeviceToHost));\n  double naivesum = 0.0;\n  for (int i = 0; i < nx*ny/2; i++) {\n    naivesum += lattice_b_h[i];\n    naivesum += lattice_w_h[i];\n  }\n  printf(\"checksum = %lf\\n\", naivesum);\n#ifndef HIPRAND\n  free(randvals_host);\n#endif\n  free(lattice_b_h);\n  free(lattice_w_h);\n\n  hipFree(lattice_b);\n  hipFree(lattice_w);\n  hipFree(randvals);\n\n  return 0;\n}\n"}}
{"kernel_name": "ising", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n#include <chrono>\n#include <fstream>\n#include <getopt.h>\n#include <iostream>\n#include <cmath>\n#include <cstring>\n\n#define TCRIT 2.26918531421f\n#define THREADS  128\n\n\n\n\nvoid init_spins( signed char* lattice, const float* randvals,\n                           const long long nx,\n                           const long long ny) {\n  #pragma omp target teams distribute parallel for simd thread_limit(THREADS)\n  for (long long tid = 0; tid < nx * ny; tid++) {\n    float randval = randvals[tid];\n    signed char val = (randval < 0.5f) ? -1 : 1;\n    lattice[tid] = val;\n  }\n}\n\ntemplate<bool is_black>\nvoid update_lattice(signed char *lattice,\n    signed char *op_lattice,\n    float* randvals,\n    const float inv_temp,\n    const long long nx,\n    const long long ny) {\n\n  #pragma omp target teams distribute parallel for collapse(2) thread_limit(THREADS)\n  for (int i = 0; i < nx; i++)\n    for (int j = 0; j < ny; j++) {\n      \n\n      int ipp = (i + 1 < nx) ? i + 1 : 0;\n      int inn = (i - 1 >= 0) ? i - 1: nx - 1;\n      int jpp = (j + 1 < ny) ? j + 1 : 0;\n      int jnn = (j - 1 >= 0) ? j - 1: ny - 1;\n\n      \n\n      int joff;\n      if (is_black) {\n        joff = (i % 2) ? jpp : jnn;\n      } else {\n        joff = (i % 2) ? jnn : jpp;\n      }\n\n      \n\n      signed char nn_sum = op_lattice[inn * ny + j] + op_lattice[i * ny + j] + \n                           op_lattice[ipp * ny + j] + op_lattice[i * ny + joff];\n\n      \n\n      signed char lij = lattice[i * ny + j];\n      float acceptance_ratio = expf(-2.0f * inv_temp * nn_sum * lij);\n      if (randvals[i*ny + j] < acceptance_ratio) {\n        lattice[i * ny + j] = -lij;\n      }\n    }\n}\n\n\nvoid update(signed char* lattice_b, signed char* lattice_w, float* randvals,\n\t          const float inv_temp, const long long nx, const long long ny) {\n\n  \n\n  update_lattice<true>(lattice_b, lattice_w, randvals, inv_temp, nx, ny / 2);\n\n  \n\n  update_lattice<false>(lattice_w, lattice_b, randvals, inv_temp, nx, ny / 2);\n\n}\n\nstatic void usage(const char *pname) {\n\n  const char *bname = rindex(pname, '/');\n  if (!bname) {bname = pname;}\n  else        {bname++;}\n\n  fprintf(stdout,\n          \"Usage: %s [options]\\n\"\n          \"options:\\n\"\n          \"\\t-x|--lattice-n <LATTICE_N>\\n\"\n          \"\\t\\tnumber of lattice rows\\n\"\n          \"\\n\"\n          \"\\t-y|--lattice_m <LATTICE_M>\\n\"\n          \"\\t\\tnumber of lattice columns\\n\"\n          \"\\n\"\n          \"\\t-w|--nwarmup <NWARMUP>\\n\"\n          \"\\t\\tnumber of warmup iterations\\n\"\n          \"\\n\"\n          \"\\t-n|--niters <NITERS>\\n\"\n          \"\\t\\tnumber of trial iterations\\n\"\n          \"\\n\"\n          \"\\t-a|--alpha <ALPHA>\\n\"\n          \"\\t\\tcoefficient of critical temperature\\n\"\n          \"\\n\"\n          \"\\t-s|--seed <SEED>\\n\"\n          \"\\t\\tseed for random number generation\\n\\n\",\n          bname);\n  exit(EXIT_SUCCESS);\n}\n\nint main(int argc, char **argv) {\n\n  \n\n  long long nx = 5120;\n  long long ny = 5120;\n  float alpha = 0.1f;\n  int nwarmup = 100;\n  int niters = 1000;\n  unsigned long long seed = 1234ULL;\n  double duration;\n\n  while (1) {\n    static struct option long_options[] = {\n        {     \"lattice-n\", required_argument, 0, 'x'},\n        {     \"lattice-m\", required_argument, 0, 'y'},\n        {         \"alpha\", required_argument, 0, 'y'},\n        {          \"seed\", required_argument, 0, 's'},\n        {       \"nwarmup\", required_argument, 0, 'w'},\n        {        \"niters\", required_argument, 0, 'n'},\n        {          \"help\",       no_argument, 0, 'h'},\n        {               0,                 0, 0,   0}\n    };\n\n    int option_index = 0;\n    int ch = getopt_long(argc, argv, \"x:y:a:s:w:n:h\", long_options, &option_index);\n    if (ch == -1) break;\n\n    switch(ch) {\n      case 0:\n        break;\n      case 'x':\n        nx = atoll(optarg); break;\n      case 'y':\n        ny = atoll(optarg); break;\n      case 'a':\n        alpha = atof(optarg); break;\n      case 's':\n        seed = atoll(optarg); break;\n      case 'w':\n        nwarmup = atoi(optarg); break;\n      case 'n':\n        niters = atoi(optarg); break;\n      case 'h':\n        usage(argv[0]); break;\n      case '?':\n        exit(EXIT_FAILURE);\n      default:\n        fprintf(stderr, \"unknown option: %c\\n\", ch);\n        exit(EXIT_FAILURE);\n    }\n  }\n\n  \n\n  if (nx % 2 != 0 || ny % 2 != 0) {\n    fprintf(stderr, \"ERROR: Lattice dimensions must be even values.\\n\");\n    exit(EXIT_FAILURE);\n  }\n\n  float inv_temp = 1.0f / (alpha*TCRIT);\n\n\n  \n\n  srand(seed);\n  float* randvals = (float*) malloc(nx * ny/2 * sizeof(float));\n  signed char* lattice_b = (signed char*) malloc(nx * ny/2 * sizeof(*lattice_b));\n  signed char* lattice_w = (signed char*) malloc(nx * ny/2 * sizeof(*lattice_w));\n  for (int i = 0; i < nx * ny/2; i++)\n    randvals[i] = (float)rand() / (float)RAND_MAX;\n\n\n#pragma omp target enter data map(to: randvals[0:nx*ny/2]) \\\n                              map(alloc: lattice_b[0:nx*ny/2]) \\\n                              map(alloc: lattice_w[0:nx*ny/2])\n{\n\n  init_spins(lattice_b, randvals, nx, ny / 2);\n  init_spins(lattice_w, randvals, nx, ny / 2);\n\n  \n\n  printf(\"Starting warmup...\\n\");\n  for (int i = 0; i < nwarmup; i++) {\n    update(lattice_b, lattice_w, randvals, inv_temp, nx, ny);\n  }\n\n  printf(\"Starting trial iterations...\\n\");\n  auto t0 = std::chrono::high_resolution_clock::now();\n  for (int i = 0; i < niters; i++) {\n    update( lattice_b, lattice_w, randvals, inv_temp, nx, ny);\n  }\n  auto t1 = std::chrono::high_resolution_clock::now();\n  duration = (double) std::chrono::duration_cast<std::chrono::microseconds>(t1-t0).count();\n}\n#pragma omp target exit data map(from: lattice_b[0:nx*ny/2], lattice_w[0:nx*ny/2]) \\\n\t                     map(delete: randvals[0:nx*ny/2])\n\n\n  printf(\"REPORT:\\n\");\n  printf(\"\\tnGPUs: %d\\n\", 1);\n  printf(\"\\ttemperature: %f * %f\\n\", alpha, TCRIT);\n  printf(\"\\tseed: %llu\\n\", seed);\n  printf(\"\\twarmup iterations: %d\\n\", nwarmup);\n  printf(\"\\ttrial iterations: %d\\n\", niters);\n  printf(\"\\tlattice dimensions: %lld x %lld\\n\", nx, ny);\n  printf(\"\\telapsed time: %f sec\\n\", duration * 1e-6);\n  printf(\"\\tupdates per ns: %f\\n\", (double) (nx * ny) * niters / duration * 1e-3);\n\n  double naivesum = 0.0;\n  for (int i = 0; i < nx*ny/2; i++) {\n    naivesum += lattice_b[i];\n    naivesum += lattice_w[i];\n  }\n  printf(\"checksum = %lf\\n\", naivesum);\n  free(randvals);\n  free(lattice_b);\n  free(lattice_w);\n  return 0;\n}\n"}}
{"kernel_name": "ising", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n#include <chrono>\n#include <fstream>\n#include <getopt.h>\n#include <iostream>\n#include <cmath>\n#include <cstring>\n\n#define TCRIT 2.26918531421f\n#define THREADS  128\n\n\n\n\nvoid init_spins( signed char* lattice, const float* randvals,\n                           const long long nx,\n                           const long long ny) {\n    for (long long tid = 0; tid < nx * ny; tid++) {\n    float randval = randvals[tid];\n    signed char val = (randval < 0.5f) ? -1 : 1;\n    lattice[tid] = val;\n  }\n}\n\ntemplate<bool is_black>\nvoid update_lattice(signed char *lattice,\n    signed char *op_lattice,\n    float* randvals,\n    const float inv_temp,\n    const long long nx,\n    const long long ny) {\n\n    for (int i = 0; i < nx; i++)\n    for (int j = 0; j < ny; j++) {\n      \n\n      int ipp = (i + 1 < nx) ? i + 1 : 0;\n      int inn = (i - 1 >= 0) ? i - 1: nx - 1;\n      int jpp = (j + 1 < ny) ? j + 1 : 0;\n      int jnn = (j - 1 >= 0) ? j - 1: ny - 1;\n\n      \n\n      int joff;\n      if (is_black) {\n        joff = (i % 2) ? jpp : jnn;\n      } else {\n        joff = (i % 2) ? jnn : jpp;\n      }\n\n      \n\n      signed char nn_sum = op_lattice[inn * ny + j] + op_lattice[i * ny + j] + \n                           op_lattice[ipp * ny + j] + op_lattice[i * ny + joff];\n\n      \n\n      signed char lij = lattice[i * ny + j];\n      float acceptance_ratio = expf(-2.0f * inv_temp * nn_sum * lij);\n      if (randvals[i*ny + j] < acceptance_ratio) {\n        lattice[i * ny + j] = -lij;\n      }\n    }\n}\n\n\nvoid update(signed char* lattice_b, signed char* lattice_w, float* randvals,\n\t          const float inv_temp, const long long nx, const long long ny) {\n\n  \n\n  update_lattice<true>(lattice_b, lattice_w, randvals, inv_temp, nx, ny / 2);\n\n  \n\n  update_lattice<false>(lattice_w, lattice_b, randvals, inv_temp, nx, ny / 2);\n\n}\n\nstatic void usage(const char *pname) {\n\n  const char *bname = rindex(pname, '/');\n  if (!bname) {bname = pname;}\n  else        {bname++;}\n\n  fprintf(stdout,\n          \"Usage: %s [options]\\n\"\n          \"options:\\n\"\n          \"\\t-x|--lattice-n <LATTICE_N>\\n\"\n          \"\\t\\tnumber of lattice rows\\n\"\n          \"\\n\"\n          \"\\t-y|--lattice_m <LATTICE_M>\\n\"\n          \"\\t\\tnumber of lattice columns\\n\"\n          \"\\n\"\n          \"\\t-w|--nwarmup <NWARMUP>\\n\"\n          \"\\t\\tnumber of warmup iterations\\n\"\n          \"\\n\"\n          \"\\t-n|--niters <NITERS>\\n\"\n          \"\\t\\tnumber of trial iterations\\n\"\n          \"\\n\"\n          \"\\t-a|--alpha <ALPHA>\\n\"\n          \"\\t\\tcoefficient of critical temperature\\n\"\n          \"\\n\"\n          \"\\t-s|--seed <SEED>\\n\"\n          \"\\t\\tseed for random number generation\\n\\n\",\n          bname);\n  exit(EXIT_SUCCESS);\n}\n\nint main(int argc, char **argv) {\n\n  \n\n  long long nx = 5120;\n  long long ny = 5120;\n  float alpha = 0.1f;\n  int nwarmup = 100;\n  int niters = 1000;\n  unsigned long long seed = 1234ULL;\n  double duration;\n\n  while (1) {\n    static struct option long_options[] = {\n        {     \"lattice-n\", required_argument, 0, 'x'},\n        {     \"lattice-m\", required_argument, 0, 'y'},\n        {         \"alpha\", required_argument, 0, 'y'},\n        {          \"seed\", required_argument, 0, 's'},\n        {       \"nwarmup\", required_argument, 0, 'w'},\n        {        \"niters\", required_argument, 0, 'n'},\n        {          \"help\",       no_argument, 0, 'h'},\n        {               0,                 0, 0,   0}\n    };\n\n    int option_index = 0;\n    int ch = getopt_long(argc, argv, \"x:y:a:s:w:n:h\", long_options, &option_index);\n    if (ch == -1) break;\n\n    switch(ch) {\n      case 0:\n        break;\n      case 'x':\n        nx = atoll(optarg); break;\n      case 'y':\n        ny = atoll(optarg); break;\n      case 'a':\n        alpha = atof(optarg); break;\n      case 's':\n        seed = atoll(optarg); break;\n      case 'w':\n        nwarmup = atoi(optarg); break;\n      case 'n':\n        niters = atoi(optarg); break;\n      case 'h':\n        usage(argv[0]); break;\n      case '?':\n        exit(EXIT_FAILURE);\n      default:\n        fprintf(stderr, \"unknown option: %c\\n\", ch);\n        exit(EXIT_FAILURE);\n    }\n  }\n\n  \n\n  if (nx % 2 != 0 || ny % 2 != 0) {\n    fprintf(stderr, \"ERROR: Lattice dimensions must be even values.\\n\");\n    exit(EXIT_FAILURE);\n  }\n\n  float inv_temp = 1.0f / (alpha*TCRIT);\n\n\n  \n\n  srand(seed);\n  float* randvals = (float*) malloc(nx * ny/2 * sizeof(float));\n  signed char* lattice_b = (signed char*) malloc(nx * ny/2 * sizeof(*lattice_b));\n  signed char* lattice_w = (signed char*) malloc(nx * ny/2 * sizeof(*lattice_w));\n  for (int i = 0; i < nx * ny/2; i++)\n    randvals[i] = (float)rand() / (float)RAND_MAX;\n\n\n{\n\n  init_spins(lattice_b, randvals, nx, ny / 2);\n  init_spins(lattice_w, randvals, nx, ny / 2);\n\n  \n\n  printf(\"Starting warmup...\\n\");\n  for (int i = 0; i < nwarmup; i++) {\n    update(lattice_b, lattice_w, randvals, inv_temp, nx, ny);\n  }\n\n  printf(\"Starting trial iterations...\\n\");\n  auto t0 = std::chrono::high_resolution_clock::now();\n  for (int i = 0; i < niters; i++) {\n    update( lattice_b, lattice_w, randvals, inv_temp, nx, ny);\n  }\n  auto t1 = std::chrono::high_resolution_clock::now();\n  duration = (double) std::chrono::duration_cast<std::chrono::microseconds>(t1-t0).count();\n}\n\n\n  printf(\"REPORT:\\n\");\n  printf(\"\\tnGPUs: %d\\n\", 1);\n  printf(\"\\ttemperature: %f * %f\\n\", alpha, TCRIT);\n  printf(\"\\tseed: %llu\\n\", seed);\n  printf(\"\\twarmup iterations: %d\\n\", nwarmup);\n  printf(\"\\ttrial iterations: %d\\n\", niters);\n  printf(\"\\tlattice dimensions: %lld x %lld\\n\", nx, ny);\n  printf(\"\\telapsed time: %f sec\\n\", duration * 1e-6);\n  printf(\"\\tupdates per ns: %f\\n\", (double) (nx * ny) * niters / duration * 1e-3);\n\n  double naivesum = 0.0;\n  for (int i = 0; i < nx*ny/2; i++) {\n    naivesum += lattice_b[i];\n    naivesum += lattice_w[i];\n  }\n  printf(\"checksum = %lf\\n\", naivesum);\n  free(randvals);\n  free(lattice_b);\n  free(lattice_w);\n  return 0;\n}"}}
{"kernel_name": "ising", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n#include <chrono>\n#include <fstream>\n#include <getopt.h>\n#include <iostream>\n#include <string>\n#include <cstring>\n\n#ifdef MKLRAND\n#include <mkl_rng_sycl.hpp>\n#endif\n#include <sycl/sycl.hpp>\n\n#define TCRIT    2.26918531421f\n#define THREADS  128\n\n\n\n\nvoid init_spins(signed char *lattice,\n                const float *randvals,\n                const long long nx,\n                const long long ny,\n                sycl::nd_item<1> &item)\n{\n  const long long tid = item.get_global_id(0);\n  if (tid >= nx * ny) return;\n\n  float randval = randvals[tid];\n  signed char val = (randval < 0.5f) ? -1 : 1;\n  lattice[tid] = val;\n}\n\ntemplate<bool is_black>\nvoid update_lattice(      signed char *lattice,\n                    const signed char *op_lattice,\n                    const float *randvals,\n                    const float inv_temp,\n                    const long long nx,\n                    const long long ny,\n                    sycl::nd_item<1> &item)\n{\n  const long long tid = item.get_global_id(0);\n  const int i = tid / ny;\n  const int j = tid % ny;\n\n  if (i >= nx || j >= ny) return;\n\n  \n\n  int ipp = (i + 1 < nx) ? i + 1 : 0;\n  int inn = (i - 1 >= 0) ? i - 1: nx - 1;\n  int jpp = (j + 1 < ny) ? j + 1 : 0;\n  int jnn = (j - 1 >= 0) ? j - 1: ny - 1;\n\n  \n\n  int joff;\n  if (is_black) {\n    joff = (i % 2) ? jpp : jnn;\n  } else {\n    joff = (i % 2) ? jnn : jpp;\n  }\n\n  \n\n  signed char nn_sum = op_lattice[inn * ny + j] + op_lattice[i * ny + j] +\n                       op_lattice[ipp * ny + j] + op_lattice[i * ny + joff];\n\n  \n\n  signed char lij = lattice[i * ny + j];\n  float acceptance_ratio = sycl::exp(-2.0f * inv_temp * nn_sum * lij);\n  if (randvals[i*ny + j] < acceptance_ratio) {\n    lattice[i * ny + j] = -lij;\n  }\n}\n\n\nvoid update(sycl::queue &q,\n            signed char *d_lattice_b, \n            signed char *d_lattice_w, \n            float *d_randvals, \n#ifdef MKLRAND\n            mkl::rng::philox4x32x10 &rng,\n            mkl::rng::uniform<float> &distr,\n#endif\n\t    const float inv_temp, const long long nx, const long long ny) {\n\n\n  int blocks = (nx * ny/2 + THREADS - 1) / THREADS * THREADS;\n\n  \n\n#ifdef MKLRAND\n  mkl::rng::generate(distr, rng, nx * ny / 2, d_randvals);\n#endif\n  q.submit([&](sycl::handler &cgh) {\n    cgh.parallel_for<class update_black>(\n      sycl::nd_range<1>(sycl::range<1>(blocks), sycl::range<1>(THREADS)),\n      [=](sycl::nd_item<1> item) {\n      update_lattice<true>(d_lattice_b, d_lattice_w, d_randvals,\n                           inv_temp, nx, ny / 2, item);\n    });\n  });\n\n  \n\n#ifdef MKLRAND\n  mkl::rng::generate(distr, rng, nx * ny / 2, d_randvals);\n#endif\n  q.submit([&](sycl::handler &cgh) {\n    cgh.parallel_for<class update_white>(\n      sycl::nd_range<1>(sycl::range<1>(blocks), sycl::range<1>(THREADS)),\n      [=](sycl::nd_item<1> item) {\n      update_lattice<false>(d_lattice_w, d_lattice_b, d_randvals,\n                            inv_temp, nx, ny / 2, item);\n    });\n  });\n}\n\nstatic void usage(const char *pname) {\n\n  const char *bname = rindex(pname, '/');\n  if (!bname) {bname = pname;}\n  else        {bname++;}\n\n  fprintf(stdout,\n          \"Usage: %s [options]\\n\"\n          \"options:\\n\"\n          \"\\t-x|--lattice-n <LATTICE_N>\\n\"\n          \"\\t\\tnumber of lattice rows\\n\"\n          \"\\n\"\n          \"\\t-y|--lattice_m <LATTICE_M>\\n\"\n          \"\\t\\tnumber of lattice columns\\n\"\n          \"\\n\"\n          \"\\t-w|--nwarmup <NWARMUP>\\n\"\n          \"\\t\\tnumber of warmup iterations\\n\"\n          \"\\n\"\n          \"\\t-n|--niters <NITERS>\\n\"\n          \"\\t\\tnumber of trial iterations\\n\"\n          \"\\n\"\n          \"\\t-a|--alpha <ALPHA>\\n\"\n          \"\\t\\tcoefficient of critical temperature\\n\"\n          \"\\n\"\n          \"\\t-s|--seed <SEED>\\n\"\n          \"\\t\\tseed for random number generation\\n\\n\",\n          bname);\n  exit(EXIT_SUCCESS);\n}\n\nint main(int argc, char **argv) {\n\n  \n\n  long long nx = 5120;\n  long long ny = 5120;\n  float alpha = 0.1f;\n  int nwarmup = 100;\n  int niters = 1000;\n  unsigned long long seed = 1234ULL;\n\n  while (1) {\n    static struct option long_options[] = {\n        {     \"lattice-n\", required_argument, 0, 'x'},\n        {     \"lattice-m\", required_argument, 0, 'y'},\n        {         \"alpha\", required_argument, 0, 'y'},\n        {          \"seed\", required_argument, 0, 's'},\n        {       \"nwarmup\", required_argument, 0, 'w'},\n        {        \"niters\", required_argument, 0, 'n'},\n        {          \"help\",       no_argument, 0, 'h'},\n        {               0,                 0, 0,   0}\n    };\n\n    int option_index = 0;\n    int ch = getopt_long(argc, argv, \"x:y:a:s:w:n:h\", long_options, &option_index);\n    if (ch == -1) break;\n\n    switch(ch) {\n      case 0:\n        break;\n      case 'x':\n        nx = atoll(optarg); break;\n      case 'y':\n        ny = atoll(optarg); break;\n      case 'a':\n        alpha = atof(optarg); break;\n      case 's':\n        seed = atoll(optarg); break;\n      case 'w':\n        nwarmup = atoi(optarg); break;\n      case 'n':\n        niters = atoi(optarg); break;\n      case 'h':\n        usage(argv[0]); break;\n      case '?':\n        exit(EXIT_FAILURE);\n      default:\n        fprintf(stderr, \"unknown option: %c\\n\", ch);\n        exit(EXIT_FAILURE);\n    }\n  }\n\n  \n\n  if (nx % 2 != 0 || ny % 2 != 0) {\n    fprintf(stderr, \"ERROR: Lattice dimensions must be even values.\\n\");\n    exit(EXIT_FAILURE);\n  }\n\n  float inv_temp = 1.0f / (alpha*TCRIT);\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n#ifdef MKLRAND\n  \n\n  mkl::rng::philox4x32x10 rng(q, seed);\n  mkl::rng::uniform<float> distr;\n#else\n  \n\n  srand(seed);\n  float* randvals_host = (float*) malloc(nx * ny/2 * sizeof(float));\n  for (int i = 0; i < nx * ny/2; i++)\n    randvals_host[i] = (float)rand() / (float)RAND_MAX;\n#endif\n\n  float *d_randvals = sycl::malloc_device<float>(nx * ny / 2, q);\n\n#ifndef MKLRAND\n  q.memcpy(d_randvals, randvals_host, nx*ny/2*sizeof(float));\n#endif\n\n  signed char *d_lattice_b = sycl::malloc_device<signed char>(nx * ny / 2, q);\n  signed char *d_lattice_w = sycl::malloc_device<signed char>(nx * ny / 2, q);\n\n  int blocks = (nx * ny/2 + THREADS - 1) / THREADS * THREADS ;\n\n  sycl::range<1> gws (blocks);\n  sycl::range<1> lws (THREADS);\n\n#ifdef MKLRAND\n  mkl::rng::generate(distr, rng, nx * ny / 2, d_randvals);\n#endif\n\n  q.submit([&](sycl::handler &cgh) {\n    cgh.parallel_for<class init_lattice_black>(\n      sycl::nd_range<1>(gws, lws), [=](sycl::nd_item<1> item) {\n      init_spins(d_lattice_b, d_randvals, nx, ny / 2, item);\n    });\n  });\n\n#ifdef MKLRAND\n  mkl::rng::generate(distr, rng, nx * ny / 2, d_randvals);\n#endif\n  q.submit([&](sycl::handler &cgh) {\n    cgh.parallel_for<class init_lattice_white>(\n      sycl::nd_range<1>(gws, lws), [=](sycl::nd_item<1> item) {\n      init_spins(d_lattice_w, d_randvals, nx, ny / 2, item);\n    });\n  });\n\n  \n\n  printf(\"Starting warmup...\\n\");\n  for (int i = 0; i < nwarmup; i++) {\n    update(q, d_lattice_b, d_lattice_w, d_randvals, \n#ifdef MKLRAND\n        rng, \n        distr, \n#endif\n        inv_temp, nx, ny);\n  }\n\n  q.wait();\n\n  printf(\"Starting trial iterations...\\n\");\n  auto t0 = std::chrono::high_resolution_clock::now();\n  for (int i = 0; i < niters; i++) {\n    update(q, d_lattice_b, d_lattice_w, d_randvals, \n#ifdef MKLRAND\n        rng, \n        distr, \n#endif\n        inv_temp, nx, ny);\n  }\n\n  q.wait();\n  auto t1 = std::chrono::high_resolution_clock::now();\n\n  double duration = (double) std::chrono::duration_cast<std::chrono::microseconds>(t1-t0).count();\n  printf(\"REPORT:\\n\");\n  printf(\"\\tnGPUs: %d\\n\", 1);\n  printf(\"\\ttemperature: %f * %f\\n\", alpha, TCRIT);\n  printf(\"\\tseed: %llu\\n\", seed);\n  printf(\"\\twarmup iterations: %d\\n\", nwarmup);\n  printf(\"\\ttrial iterations: %d\\n\", niters);\n  printf(\"\\tlattice dimensions: %lld x %lld\\n\", nx, ny);\n  printf(\"\\telapsed time: %f sec\\n\", duration * 1e-6);\n  printf(\"\\tupdates per ns: %f\\n\", (double) (nx * ny) * niters / duration * 1e-3);\n\n  signed char* lattice_b_h = (signed char*) malloc(nx * ny/2 * sizeof(*lattice_b_h));\n  signed char* lattice_w_h = (signed char*) malloc(nx * ny/2 * sizeof(*lattice_w_h));\n\n  q.memcpy(lattice_b_h, d_lattice_b, nx * ny/2 * sizeof(signed char));\n  q.memcpy(lattice_w_h, d_lattice_w, nx * ny/2 * sizeof(signed char));\n\n  q.wait();\n  double naivesum = 0.0;\n  for (int i = 0; i < nx*ny/2; i++) {\n    naivesum += lattice_b_h[i];\n    naivesum += lattice_w_h[i];\n  }\n  printf(\"checksum = %lf\\n\", naivesum);\n#ifndef MKLRAND\n  free(randvals_host);\n#endif\n  free(lattice_b_h);\n  free(lattice_w_h);\n\n  sycl::free(d_lattice_b, q);\n  sycl::free(d_lattice_w, q);\n  sycl::free(d_randvals, q);\n\n  return 0;\n}\n"}}
{"kernel_name": "iso2dfd", "parallel_api": "cuda", "code": {"iso2dfd.cu": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <fstream>\n#include <iostream>\n#include <cuda.h>\n#include \"iso2dfd.h\"\n\n#define BLOCK_SIZE 16\n#define MIN(a, b) (a) < (b) ? (a) : (b)\n\n\n\nvoid usage(std::string programName) {\n  std::cout << \" Incorrect parameters \" << std::endl;\n  std::cout << \" Usage: \";\n  std::cout << programName << \" n1 n2 Iterations \" << std::endl\n            << std::endl;\n  std::cout << \" n1 n2      : Grid sizes for the stencil \" << std::endl;\n  std::cout << \" Iterations : No. of timesteps. \" << std::endl;\n}\n\n\n\nvoid initialize(float* ptr_prev, float* ptr_next, float* ptr_vel, size_t nRows,\n                size_t nCols) {\n  std::cout << \"Initializing ... \" << std::endl;\n\n  \n\n  float wavelet[12] = {0.016387336, -0.041464937, -0.067372555, 0.386110067,\n                       0.812723635, 0.416998396,  0.076488599,  -0.059434419,\n                       0.023680172, 0.005611435,  0.001823209,  -0.000720549};\n\n  \n\n  for (size_t i = 0; i < nRows; i++) {\n    size_t offset = i * nCols;\n\n    for (size_t k = 0; k < nCols; k++) {\n      ptr_prev[offset + k] = 0.0f;\n      ptr_next[offset + k] = 0.0f;\n      \n\n      ptr_vel[offset + k] = 2250000.0f;\n    }\n  }\n  \n\n  for (int s = 11; s >= 0; s--) {\n    for (size_t i = nRows / 2 - s; i < nRows / 2 + s; i++) {\n      size_t offset = i * nCols;\n      for (size_t k = nCols / 2 - s; k < nCols / 2 + s; k++) {\n        ptr_prev[offset + k] = wavelet[s];\n      }\n    }\n  }\n}\n\n\n\nbool within_epsilon(float* output, float* reference, const size_t dimx,\n                    const size_t dimy, const unsigned int radius,\n                    const float delta = 0.01f) {\n  FILE* fp = fopen(\"./error_diff.txt\", \"w\");\n  if (!fp) fp = stderr;\n\n  bool error = false;\n  \n\n  double norm2 = 0;\n\n  for (size_t iy = 0; iy < dimy; iy++) {\n    for (size_t ix = 0; ix < dimx; ix++) {\n      if (ix >= radius && ix < (dimx - radius) && iy >= radius &&\n          iy < (dimy - radius)) {\n        float difference = fabsf(*reference - *output);\n        norm2 += difference * difference;\n        if (difference > delta) {\n          error = true;\n          fprintf(fp, \" ERROR: (%zu,%zu)\\t%e instead of %e (|e|=%e)\\n\", ix, iy,\n                  *output, *reference, difference);\n        }\n      }\n\n      ++output;\n      ++reference;\n    }\n  }\n\n  if (fp != stderr) fclose(fp);\n  norm2 = sqrt(norm2);\n  if (error) printf(\"error (Euclidean norm): %.9e\\n\", norm2);\n  return error;\n}\n\n\n\nvoid iso_2dfd_iteration_cpu(float* next, float* prev, float* vel,\n                            const float dtDIVdxy, size_t nRows, size_t nCols,\n                            int nIterations) {\n  float* swap;\n  float value = 0.f;\n  for (int k = 0; k < nIterations; k += 1) {\n    for (size_t i = 1; i < nRows - HALF_LENGTH; i += 1) {\n      for (size_t j = 1; j < nCols - HALF_LENGTH; j += 1) {\n        \n\n        size_t gid = j + (i * nCols);\n        value = 0.f;\n        value += prev[gid + 1] - 2.f * prev[gid] + prev[gid - 1];\n        value += prev[gid + nCols] - 2.f * prev[gid] + prev[gid - nCols];\n        value *= dtDIVdxy * vel[gid];\n        next[gid] = 2.f * prev[gid] - next[gid] + value;\n      }\n    }\n\n    \n\n    swap = next;\n    next = prev;\n    prev = swap;\n  }\n}\n\n\n\n__global__ void iso_2dfd_kernel(\n        float*__restrict__ next,\n  const float*__restrict__ prev,\n  const float*__restrict__ vel, \n  const float dtDIVdxy, const size_t nRows, const size_t nCols)\n{\n  \n\n  \n\n  \n\n  \n\n  size_t gidCol = blockDim.x * blockIdx.x + threadIdx.x;\n  size_t gidRow = blockDim.y * blockIdx.y + threadIdx.y;\n\n  if (gidRow < nRows && gidCol < nCols) {\n\n    size_t gid = (gidRow)*nCols + gidCol;\n\n    \n\n    \n\n    if ((gidCol >= HALF_LENGTH && gidCol < nCols - HALF_LENGTH) &&\n        (gidRow >= HALF_LENGTH && gidRow < nRows - HALF_LENGTH)) {\n      \n\n      \n\n      \n\n      \n\n      float value = 0.f;\n      value += prev[gid + 1] - 2.f * prev[gid] + prev[gid - 1];\n      value += prev[gid + nCols] - 2.f * prev[gid] + prev[gid - nCols];\n      value *= dtDIVdxy * vel[gid];\n      next[gid] = 2.f * prev[gid] - next[gid] + value;\n    }\n  }\n}\n\nint main(int argc, char* argv[]) {\n  \n\n  float* prev_base;\n  float* next_base;\n  float* next_cpu;\n  \n\n  float* vel_base;\n\n  bool error = false;\n\n  size_t nRows, nCols;\n  unsigned int nIterations;\n\n  \n\n  try {\n    nRows = std::stoi(argv[1]);\n    nCols = std::stoi(argv[2]);\n    nIterations = std::stoi(argv[3]);\n  }\n\n  catch (...) {\n    usage(argv[0]);\n    return 1;\n  }\n\n  \n\n  size_t nsize = nRows * nCols;\n\n  \n\n  prev_base = new float[nsize];\n  next_base = new float[nsize];\n  next_cpu = new float[nsize];\n  vel_base = new float[nsize];\n\n  \n\n  \n\n  float dtDIVdxy = (DT * DT) / (DXY * DXY);\n\n  \n\n  initialize(prev_base, next_base, vel_base, nRows, nCols);\n\n  std::cout << \"Grid Sizes: \" << nRows << \" \" << nCols << std::endl;\n  std::cout << \"Iterations: \" << nIterations << std::endl;\n  std::cout << std::endl;\n\n  std::cout << \"Computing wavefield in device ..\" << std::endl;\n\n  \n\n  cudaDeviceProp devProp;\n  cudaGetDeviceProperties(&devProp, 0);\n  std::cout << \"Running on:: \" << devProp.name << std::endl;\n  std::cout << \"The Device Max Work Group Size is : \" << devProp.maxThreadsPerBlock << std::endl;\n\n  float* d_next;\n  float* d_prev;\n  float* d_vel;\n  cudaMalloc((void**)&d_next, sizeof(float)*nsize);\n  cudaMemcpy(d_next, next_base, sizeof(float)*nsize, cudaMemcpyHostToDevice); \n  cudaMalloc((void**)&d_prev, sizeof(float)*nsize);\n  cudaMemcpy(d_prev, prev_base, sizeof(float)*nsize, cudaMemcpyHostToDevice); \n  cudaMalloc((void**)&d_vel, sizeof(float)*nsize);\n  cudaMemcpy(d_vel, vel_base, sizeof(float)*nsize, cudaMemcpyHostToDevice); \n\n  unsigned int grid_cols = (nCols + BLOCK_SIZE - 1) / BLOCK_SIZE;\n  unsigned int grid_rows = (nRows + BLOCK_SIZE - 1) / BLOCK_SIZE;\n  dim3 dimGrid(grid_cols, grid_rows);\n  dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n\n  cudaDeviceSynchronize();\n  auto kstart = std::chrono::steady_clock::now();\n\n  \n\n  for (unsigned int k = 0; k < nIterations; k += 1) {\n    \n\n    iso_2dfd_kernel<<<dimGrid, dimBlock>>>((k % 2) ? d_prev : d_next, \n                                           (k % 2) ? d_next : d_prev,\n                                           d_vel, dtDIVdxy, nRows, nCols);\n  }\n\n  cudaDeviceSynchronize();\n  auto kend = std::chrono::steady_clock::now();\n  auto ktime = std::chrono::duration_cast<std::chrono::nanoseconds>(kend - kstart).count();\n  std::cout << \"Total kernel execution time \" << ktime * 1e-6f  << \" (ms)\\n\";\n  std::cout << \"Average kernel execution time \" << (ktime * 1e-3f) / nIterations << \" (us)\\n\";\n\n  cudaMemcpy(next_base, d_next, sizeof(float)*nsize, cudaMemcpyDeviceToHost);\n\n  \n\n  std::ofstream outFile;\n  outFile.open(\"wavefield_snapshot.bin\", std::ios::out | std::ios::binary);\n  outFile.write(reinterpret_cast<char*>(next_base), nsize * sizeof(float));\n  outFile.close();\n\n  \n\n  \n  std::cout << \"Computing wavefield in CPU ..\" << std::endl;\n  \n\n  initialize(prev_base, next_cpu, vel_base, nRows, nCols);\n\n  \n\n  \n\n  auto start = std::chrono::steady_clock::now();\n  iso_2dfd_iteration_cpu(next_cpu, prev_base, vel_base, dtDIVdxy, nRows, nCols,\n                         nIterations);\n\n  \n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::milliseconds>(end - start)\n                  .count();\n  std::cout << \"Host time: \" << time << \" ms\" << std::endl;\n  std::cout << std::endl;\n\n  \n\n  \n\n  error = within_epsilon(next_base, next_cpu, nRows, nCols, HALF_LENGTH, 0.1f);\n\n  \n\n  \n\n  if (error)\n    std::cout << \"Final wavefields from device and CPU are different: Error \"\n              << std::endl;\n  else\n    std::cout << \"Final wavefields from device and CPU are equivalent: Success\"\n              << std::endl;\n\n  \n\n  outFile.open(\"wavefield_snapshot_cpu.bin\", std::ios::out | std::ios::binary);\n  outFile.write(reinterpret_cast<char*>(next_cpu), nsize * sizeof(float));\n  outFile.close();\n\n  std::cout << \"Final wavefields (from device and CPU) written to disk\"\n            << std::endl;\n  std::cout << \"Finished.  \" << std::endl;\n\n  \n\n  delete[] prev_base;\n  delete[] next_base;\n  delete[] vel_base;\n  cudaFree(d_prev);\n  cudaFree(d_next);\n  cudaFree(d_vel);\n\n  return error ? 1 : 0;\n}\n"}}
{"kernel_name": "iso2dfd", "parallel_api": "hip", "code": {"iso2dfd.cu": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <fstream>\n#include <iostream>\n#include <hip/hip_runtime.h>\n#include \"iso2dfd.h\"\n\n#define BLOCK_SIZE 16\n#define MIN(a, b) (a) < (b) ? (a) : (b)\n\n\n\nvoid usage(std::string programName) {\n  std::cout << \" Incorrect parameters \" << std::endl;\n  std::cout << \" Usage: \";\n  std::cout << programName << \" n1 n2 Iterations \" << std::endl\n            << std::endl;\n  std::cout << \" n1 n2      : Grid sizes for the stencil \" << std::endl;\n  std::cout << \" Iterations : No. of timesteps. \" << std::endl;\n}\n\n\n\nvoid initialize(float* ptr_prev, float* ptr_next, float* ptr_vel, size_t nRows,\n                size_t nCols) {\n  std::cout << \"Initializing ... \" << std::endl;\n\n  \n\n  float wavelet[12] = {0.016387336, -0.041464937, -0.067372555, 0.386110067,\n                       0.812723635, 0.416998396,  0.076488599,  -0.059434419,\n                       0.023680172, 0.005611435,  0.001823209,  -0.000720549};\n\n  \n\n  for (size_t i = 0; i < nRows; i++) {\n    size_t offset = i * nCols;\n\n    for (size_t k = 0; k < nCols; k++) {\n      ptr_prev[offset + k] = 0.0f;\n      ptr_next[offset + k] = 0.0f;\n      \n\n      ptr_vel[offset + k] = 2250000.0f;\n    }\n  }\n  \n\n  for (int s = 11; s >= 0; s--) {\n    for (size_t i = nRows / 2 - s; i < nRows / 2 + s; i++) {\n      size_t offset = i * nCols;\n      for (size_t k = nCols / 2 - s; k < nCols / 2 + s; k++) {\n        ptr_prev[offset + k] = wavelet[s];\n      }\n    }\n  }\n}\n\n\n\nbool within_epsilon(float* output, float* reference, const size_t dimx,\n                    const size_t dimy, const unsigned int radius,\n                    const float delta = 0.01f) {\n  FILE* fp = fopen(\"./error_diff.txt\", \"w\");\n  if (!fp) fp = stderr;\n\n  bool error = false;\n  \n\n  double norm2 = 0;\n\n  for (size_t iy = 0; iy < dimy; iy++) {\n    for (size_t ix = 0; ix < dimx; ix++) {\n      if (ix >= radius && ix < (dimx - radius) && iy >= radius &&\n          iy < (dimy - radius)) {\n        float difference = fabsf(*reference - *output);\n        norm2 += difference * difference;\n        if (difference > delta) {\n          error = true;\n          fprintf(fp, \" ERROR: (%zu,%zu)\\t%e instead of %e (|e|=%e)\\n\", ix, iy,\n                  *output, *reference, difference);\n        }\n      }\n\n      ++output;\n      ++reference;\n    }\n  }\n\n  if (fp != stderr) fclose(fp);\n  norm2 = sqrt(norm2);\n  if (error) printf(\"error (Euclidean norm): %.9e\\n\", norm2);\n  return error;\n}\n\n\n\nvoid iso_2dfd_iteration_cpu(float* next, float* prev, float* vel,\n                            const float dtDIVdxy, size_t nRows, size_t nCols,\n                            int nIterations) {\n  float* swap;\n  float value = 0.f;\n  for (int k = 0; k < nIterations; k += 1) {\n    for (size_t i = 1; i < nRows - HALF_LENGTH; i += 1) {\n      for (size_t j = 1; j < nCols - HALF_LENGTH; j += 1) {\n        \n\n        size_t gid = j + (i * nCols);\n        value = 0.f;\n        value += prev[gid + 1] - 2.f * prev[gid] + prev[gid - 1];\n        value += prev[gid + nCols] - 2.f * prev[gid] + prev[gid - nCols];\n        value *= dtDIVdxy * vel[gid];\n        next[gid] = 2.f * prev[gid] - next[gid] + value;\n      }\n    }\n\n    \n\n    swap = next;\n    next = prev;\n    prev = swap;\n  }\n}\n\n\n\n__global__ void iso_2dfd_kernel(\n        float*__restrict__ next,\n  const float*__restrict__ prev,\n  const float*__restrict__ vel, \n  const float dtDIVdxy, const size_t nRows, const size_t nCols)\n{\n  \n\n  \n\n  \n\n  \n\n  size_t gidCol = blockDim.x * blockIdx.x + threadIdx.x;\n  size_t gidRow = blockDim.y * blockIdx.y + threadIdx.y;\n\n  if (gidRow < nRows && gidCol < nCols) {\n\n    size_t gid = (gidRow)*nCols + gidCol;\n\n    \n\n    \n\n    if ((gidCol >= HALF_LENGTH && gidCol < nCols - HALF_LENGTH) &&\n        (gidRow >= HALF_LENGTH && gidRow < nRows - HALF_LENGTH)) {\n      \n\n      \n\n      \n\n      \n\n      float value = 0.f;\n      value += prev[gid + 1] - 2.f * prev[gid] + prev[gid - 1];\n      value += prev[gid + nCols] - 2.f * prev[gid] + prev[gid - nCols];\n      value *= dtDIVdxy * vel[gid];\n      next[gid] = 2.f * prev[gid] - next[gid] + value;\n    }\n  }\n}\n\nint main(int argc, char* argv[]) {\n  \n\n  float* prev_base;\n  float* next_base;\n  float* next_cpu;\n  \n\n  float* vel_base;\n\n  bool error = false;\n\n  size_t nRows, nCols;\n  unsigned int nIterations;\n\n  \n\n  try {\n    nRows = std::stoi(argv[1]);\n    nCols = std::stoi(argv[2]);\n    nIterations = std::stoi(argv[3]);\n  }\n\n  catch (...) {\n    usage(argv[0]);\n    return 1;\n  }\n\n  \n\n  size_t nsize = nRows * nCols;\n\n  \n\n  prev_base = new float[nsize];\n  next_base = new float[nsize];\n  next_cpu = new float[nsize];\n  vel_base = new float[nsize];\n\n  \n\n  \n\n  float dtDIVdxy = (DT * DT) / (DXY * DXY);\n\n  \n\n  initialize(prev_base, next_base, vel_base, nRows, nCols);\n\n  std::cout << \"Grid Sizes: \" << nRows << \" \" << nCols << std::endl;\n  std::cout << \"Iterations: \" << nIterations << std::endl;\n  std::cout << std::endl;\n\n  std::cout << \"Computing wavefield in device ..\" << std::endl;\n\n  \n\n  hipDeviceProp_t devProp;\n  hipGetDeviceProperties(&devProp, 0);\n  std::cout << \"Running on:: \" << devProp.name << std::endl;\n  std::cout << \"The Device Max Work Group Size is : \" << devProp.maxThreadsPerBlock << std::endl;\n\n  float* d_next;\n  float* d_prev;\n  float* d_vel;\n  hipMalloc((void**)&d_next, sizeof(float)*nsize);\n  hipMemcpy(d_next, next_base, sizeof(float)*nsize, hipMemcpyHostToDevice); \n  hipMalloc((void**)&d_prev, sizeof(float)*nsize);\n  hipMemcpy(d_prev, prev_base, sizeof(float)*nsize, hipMemcpyHostToDevice); \n  hipMalloc((void**)&d_vel, sizeof(float)*nsize);\n  hipMemcpy(d_vel, vel_base, sizeof(float)*nsize, hipMemcpyHostToDevice); \n\n  unsigned int grid_cols = (nCols + BLOCK_SIZE - 1) / BLOCK_SIZE;\n  unsigned int grid_rows = (nRows + BLOCK_SIZE - 1) / BLOCK_SIZE;\n  dim3 dimGrid(grid_cols, grid_rows);\n  dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n\n  hipDeviceSynchronize();\n  auto kstart = std::chrono::steady_clock::now();\n\n  \n\n  for (unsigned int k = 0; k < nIterations; k += 1) {\n    \n\n    hipLaunchKernelGGL(iso_2dfd_kernel, dimGrid, dimBlock, 0, 0, (k % 2) ? d_prev : d_next, \n                                           (k % 2) ? d_next : d_prev,\n                                           d_vel, dtDIVdxy, nRows, nCols);\n  }\n\n  hipDeviceSynchronize();\n  auto kend = std::chrono::steady_clock::now();\n  auto ktime = std::chrono::duration_cast<std::chrono::nanoseconds>(kend - kstart).count();\n  std::cout << \"Total kernel execution time \" << ktime * 1e-6f  << \" (ms)\\n\";\n  std::cout << \"Average kernel execution time \" << (ktime * 1e-3f) / nIterations << \" (us)\\n\";\n\n  hipMemcpy(next_base, d_next, sizeof(float)*nsize, hipMemcpyDeviceToHost);\n\n  \n\n  std::ofstream outFile;\n  outFile.open(\"wavefield_snapshot.bin\", std::ios::out | std::ios::binary);\n  outFile.write(reinterpret_cast<char*>(next_base), nsize * sizeof(float));\n  outFile.close();\n\n  \n\n  \n  std::cout << \"Computing wavefield in CPU ..\" << std::endl;\n  \n\n  initialize(prev_base, next_cpu, vel_base, nRows, nCols);\n\n  \n\n  \n\n  auto start = std::chrono::steady_clock::now();\n  iso_2dfd_iteration_cpu(next_cpu, prev_base, vel_base, dtDIVdxy, nRows, nCols,\n                         nIterations);\n\n  \n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::milliseconds>(end - start)\n                  .count();\n  std::cout << \"Host time: \" << time << \" ms\" << std::endl;\n  std::cout << std::endl;\n\n  \n\n  \n\n  error = within_epsilon(next_base, next_cpu, nRows, nCols, HALF_LENGTH, 0.1f);\n\n  \n\n  \n\n  if (error)\n    std::cout << \"Final wavefields from device and CPU are different: Error \"\n              << std::endl;\n  else\n    std::cout << \"Final wavefields from device and CPU are equivalent: Success\"\n              << std::endl;\n\n  \n\n  outFile.open(\"wavefield_snapshot_cpu.bin\", std::ios::out | std::ios::binary);\n  outFile.write(reinterpret_cast<char*>(next_cpu), nsize * sizeof(float));\n  outFile.close();\n\n  std::cout << \"Final wavefields (from device and CPU) written to disk\"\n            << std::endl;\n  std::cout << \"Finished.  \" << std::endl;\n\n  \n\n  delete[] prev_base;\n  delete[] next_base;\n  delete[] vel_base;\n  hipFree(d_prev);\n  hipFree(d_next);\n  hipFree(d_vel);\n\n  return error ? 1 : 0;\n}\n"}}
{"kernel_name": "iso2dfd", "parallel_api": "omp", "code": {"iso2dfd.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <fstream>\n#include <iostream>\n#include \"iso2dfd.h\"\n\n#define MIN(a, b) (a) < (b) ? (a) : (b)\n\n\n\nvoid usage(std::string programName) {\n  std::cout << \" Incorrect parameters \" << std::endl;\n  std::cout << \" Usage: \";\n  std::cout << programName << \" n1 n2 Iterations \" << std::endl\n            << std::endl;\n  std::cout << \" n1 n2      : Grid sizes for the stencil \" << std::endl;\n  std::cout << \" Iterations : No. of timesteps. \" << std::endl;\n}\n\n\n\nvoid initialize(float* ptr_prev, float* ptr_next, float* ptr_vel, size_t nRows,\n                size_t nCols) {\n  std::cout << \"Initializing ... \" << std::endl;\n\n  \n\n  float wavelet[12] = {0.016387336, -0.041464937, -0.067372555, 0.386110067,\n                       0.812723635, 0.416998396,  0.076488599,  -0.059434419,\n                       0.023680172, 0.005611435,  0.001823209,  -0.000720549};\n\n  \n\n  for (size_t i = 0; i < nRows; i++) {\n    size_t offset = i * nCols;\n\n    for (int k = 0; k < nCols; k++) {\n      ptr_prev[offset + k] = 0.0f;\n      ptr_next[offset + k] = 0.0f;\n      \n\n      ptr_vel[offset + k] = 2250000.0f;\n    }\n  }\n  \n\n  for (int s = 11; s >= 0; s--) {\n    for (size_t i = nRows / 2 - s; i < nRows / 2 + s; i++) {\n      size_t offset = i * nCols;\n      for (size_t k = nCols / 2 - s; k < nCols / 2 + s; k++) {\n        ptr_prev[offset + k] = wavelet[s];\n      }\n    }\n  }\n}\n\n\n\nbool within_epsilon(float* output, float* reference, const size_t dimx,\n                    const size_t dimy, const unsigned int radius,\n                    const float delta = 0.01f) {\n  FILE* fp = fopen(\"./error_diff.txt\", \"w\");\n  if (!fp) fp = stderr;\n\n  bool error = false;\n  \n\n  double norm2 = 0;\n\n  for (size_t iy = 0; iy < dimy; iy++) {\n    for (size_t ix = 0; ix < dimx; ix++) {\n      if (ix >= radius && ix < (dimx - radius) && iy >= radius &&\n          iy < (dimy - radius)) {\n        float difference = fabsf(*reference - *output);\n        norm2 += difference * difference;\n        if (difference > delta) {\n          error = true;\n          fprintf(fp, \" ERROR: (%zu,%zu)\\t%e instead of %e (|e|=%e)\\n\", ix, iy,\n                  *output, *reference, difference);\n        }\n      }\n\n      ++output;\n      ++reference;\n    }\n  }\n\n  if (fp != stderr) fclose(fp);\n  norm2 = sqrt(norm2);\n  if (error) printf(\"error (Euclidean norm): %.9e\\n\", norm2);\n  return error;\n}\n\n\n\nvoid iso_2dfd_iteration_cpu(float* next, float* prev, float* vel,\n                            const float dtDIVdxy, int nRows, int nCols,\n                            int nIterations) {\n  for (unsigned int k = 0; k < nIterations; k += 1) {\n    for (size_t i = 1; i < nRows - HALF_LENGTH; i += 1) {\n      for (size_t j = 1; j < nCols - HALF_LENGTH; j += 1) {\n        \n\n        size_t gid = j + (i * nCols);\n        float value = 0.f;\n        value += prev[gid + 1] - 2.f * prev[gid] + prev[gid - 1];\n        value += prev[gid + nCols] - 2.f * prev[gid] + prev[gid - nCols];\n        value *= dtDIVdxy * vel[gid];\n        next[gid] = 2.f * prev[gid] - next[gid] + value;\n      }\n    }\n\n    \n\n    float* swap = next;\n    next = prev;\n    prev = swap;\n  }\n}\n\n\n\nvoid iso_2dfd_kernel(float* next, const float* prev, const float* vel, \n                     const float dtDIVdxy, const size_t nRows, const size_t nCols) {\n  #pragma omp target teams distribute parallel for simd collapse(2) thread_limit(256) \n  for (size_t gidRow = 0; gidRow < nRows ; gidRow++)\n    for (size_t gidCol = 0; gidCol < nCols ; gidCol++) {\n      size_t gid = (gidRow)*nCols + gidCol;\n      \n\n      \n\n      if ((gidCol >= HALF_LENGTH && gidCol < nCols - HALF_LENGTH) &&\n          (gidRow >= HALF_LENGTH && gidRow < nRows - HALF_LENGTH)) {\n        \n\n        \n\n        \n\n        \n\n        float value = 0.f;\n        value += prev[gid + 1] - 2.f * prev[gid] + prev[gid - 1];\n        value += prev[gid + nCols] - 2.f * prev[gid] + prev[gid - nCols];\n        value *= dtDIVdxy * vel[gid];\n        next[gid] = 2.f * prev[gid] - next[gid] + value;\n      }\n  }\n}\n\nint main(int argc, char* argv[]) {\n  \n\n  float* prev_base;\n  float* next_base;\n  float* next_cpu;\n  \n\n  float* vel_base;\n\n  bool error = false;\n\n  size_t nRows, nCols;\n  unsigned int nIterations;\n\n  \n\n  try {\n    nRows = std::stoi(argv[1]);\n    nCols = std::stoi(argv[2]);\n    nIterations = std::stoi(argv[3]);\n  }\n\n  catch (...) {\n    usage(argv[0]);\n    return 1;\n  }\n\n  \n\n  size_t nsize = nRows * nCols;\n\n  \n\n  prev_base = new float[nsize];\n  next_base = new float[nsize];\n  next_cpu = new float[nsize];\n  vel_base = new float[nsize];\n\n  \n\n  \n\n  float dtDIVdxy = (DT * DT) / (DXY * DXY);\n\n  \n\n  initialize(prev_base, next_base, vel_base, nRows, nCols);\n\n  std::cout << \"Grid Sizes: \" << nRows << \" \" << nCols << std::endl;\n  std::cout << \"Iterations: \" << nIterations << std::endl;\n  std::cout << std::endl;\n\n  std::cout << \"Computing wavefield in device ..\" << std::endl;\n\n  #pragma omp target data map(next_base[0:nsize], prev_base[0:nsize]) \\\n                          map(to: vel_base[0:nsize])\n  {\n    auto kstart = std::chrono::steady_clock::now();\n  \n    \n\n    for (unsigned int k = 0; k < nIterations; k += 1) {\n      \n\n      \n\n      iso_2dfd_kernel((k % 2) ? prev_base : next_base,\n                      (k % 2) ? next_base : prev_base,\n                      vel_base, dtDIVdxy, nRows, nCols);\n    }  \n\n  \n    auto kend = std::chrono::steady_clock::now();\n    auto ktime = std::chrono::duration_cast<std::chrono::nanoseconds>(kend - kstart).count();\n    std::cout << \"Total kernel execution time \" << ktime * 1e-6f << \" (ms)\\n\";\n    std::cout << \"Average kernel execution time \" << (ktime * 1e-3f) / nIterations << \" (us)\\n\";\n  }\n\n  \n\n  std::ofstream outFile;\n  outFile.open(\"wavefield_snapshot.bin\", std::ios::out | std::ios::binary);\n  outFile.write(reinterpret_cast<char*>(next_base), nsize * sizeof(float));\n  outFile.close();\n\n  \n\n  \n  std::cout << \"Computing wavefield in CPU ..\" << std::endl;\n  \n\n  initialize(prev_base, next_cpu, vel_base, nRows, nCols);\n\n  \n\n  \n\n  auto start = std::chrono::steady_clock::now();\n  iso_2dfd_iteration_cpu(next_cpu, prev_base, vel_base, dtDIVdxy, nRows, nCols,\n                         nIterations);\n\n  \n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::milliseconds>(end - start)\n                  .count();\n  std::cout << \"CPU time: \" << time << \" ms\" << std::endl;\n  std::cout << std::endl;\n\n  \n\n  \n\n  error = within_epsilon(next_base, next_cpu, nRows, nCols, HALF_LENGTH, 0.1f);\n\n  \n\n  \n\n  if (error)\n    std::cout << \"Final wavefields from device and CPU are different: Error \"\n              << std::endl;\n  else\n    std::cout << \"Final wavefields from device and CPU are equivalent: Success\"\n              << std::endl;\n\n  \n\n  outFile.open(\"wavefield_snapshot_cpu.bin\", std::ios::out | std::ios::binary);\n  outFile.write(reinterpret_cast<char*>(next_cpu), nsize * sizeof(float));\n  outFile.close();\n\n  std::cout << \"Final wavefields (from device and CPU) written to disk\"\n            << std::endl;\n  std::cout << \"Finished.  \" << std::endl;\n\n  \n\n  delete[] prev_base;\n  delete[] next_base;\n  delete[] vel_base;\n\n  return error ? 1 : 0;\n}\n"}}
{"kernel_name": "iso2dfd", "parallel_api": "serial", "code": {"iso2dfd.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <fstream>\n#include <iostream>\n#include \"iso2dfd.h\"\n\n#define MIN(a, b) (a) < (b) ? (a) : (b)\n\n\n\nvoid usage(std::string programName) {\n  std::cout << \" Incorrect parameters \" << std::endl;\n  std::cout << \" Usage: \";\n  std::cout << programName << \" n1 n2 Iterations \" << std::endl\n            << std::endl;\n  std::cout << \" n1 n2      : Grid sizes for the stencil \" << std::endl;\n  std::cout << \" Iterations : No. of timesteps. \" << std::endl;\n}\n\n\n\nvoid initialize(float* ptr_prev, float* ptr_next, float* ptr_vel, size_t nRows,\n                size_t nCols) {\n  std::cout << \"Initializing ... \" << std::endl;\n\n  \n\n  float wavelet[12] = {0.016387336, -0.041464937, -0.067372555, 0.386110067,\n                       0.812723635, 0.416998396,  0.076488599,  -0.059434419,\n                       0.023680172, 0.005611435,  0.001823209,  -0.000720549};\n\n  \n\n  for (size_t i = 0; i < nRows; i++) {\n    size_t offset = i * nCols;\n\n    for (int k = 0; k < nCols; k++) {\n      ptr_prev[offset + k] = 0.0f;\n      ptr_next[offset + k] = 0.0f;\n      \n\n      ptr_vel[offset + k] = 2250000.0f;\n    }\n  }\n  \n\n  for (int s = 11; s >= 0; s--) {\n    for (size_t i = nRows / 2 - s; i < nRows / 2 + s; i++) {\n      size_t offset = i * nCols;\n      for (size_t k = nCols / 2 - s; k < nCols / 2 + s; k++) {\n        ptr_prev[offset + k] = wavelet[s];\n      }\n    }\n  }\n}\n\n\n\nbool within_epsilon(float* output, float* reference, const size_t dimx,\n                    const size_t dimy, const unsigned int radius,\n                    const float delta = 0.01f) {\n  FILE* fp = fopen(\"./error_diff.txt\", \"w\");\n  if (!fp) fp = stderr;\n\n  bool error = false;\n  \n\n  double norm2 = 0;\n\n  for (size_t iy = 0; iy < dimy; iy++) {\n    for (size_t ix = 0; ix < dimx; ix++) {\n      if (ix >= radius && ix < (dimx - radius) && iy >= radius &&\n          iy < (dimy - radius)) {\n        float difference = fabsf(*reference - *output);\n        norm2 += difference * difference;\n        if (difference > delta) {\n          error = true;\n          fprintf(fp, \" ERROR: (%zu,%zu)\\t%e instead of %e (|e|=%e)\\n\", ix, iy,\n                  *output, *reference, difference);\n        }\n      }\n\n      ++output;\n      ++reference;\n    }\n  }\n\n  if (fp != stderr) fclose(fp);\n  norm2 = sqrt(norm2);\n  if (error) printf(\"error (Euclidean norm): %.9e\\n\", norm2);\n  return error;\n}\n\n\n\nvoid iso_2dfd_iteration_cpu(float* next, float* prev, float* vel,\n                            const float dtDIVdxy, int nRows, int nCols,\n                            int nIterations) {\n  for (unsigned int k = 0; k < nIterations; k += 1) {\n    for (size_t i = 1; i < nRows - HALF_LENGTH; i += 1) {\n      for (size_t j = 1; j < nCols - HALF_LENGTH; j += 1) {\n        \n\n        size_t gid = j + (i * nCols);\n        float value = 0.f;\n        value += prev[gid + 1] - 2.f * prev[gid] + prev[gid - 1];\n        value += prev[gid + nCols] - 2.f * prev[gid] + prev[gid - nCols];\n        value *= dtDIVdxy * vel[gid];\n        next[gid] = 2.f * prev[gid] - next[gid] + value;\n      }\n    }\n\n    \n\n    float* swap = next;\n    next = prev;\n    prev = swap;\n  }\n}\n\n\n\nvoid iso_2dfd_kernel(float* next, const float* prev, const float* vel, \n                     const float dtDIVdxy, const size_t nRows, const size_t nCols) {\n    for (size_t gidRow = 0; gidRow < nRows ; gidRow++)\n    for (size_t gidCol = 0; gidCol < nCols ; gidCol++) {\n      size_t gid = (gidRow)*nCols + gidCol;\n      \n\n      \n\n      if ((gidCol >= HALF_LENGTH && gidCol < nCols - HALF_LENGTH) &&\n          (gidRow >= HALF_LENGTH && gidRow < nRows - HALF_LENGTH)) {\n        \n\n        \n\n        \n\n        \n\n        float value = 0.f;\n        value += prev[gid + 1] - 2.f * prev[gid] + prev[gid - 1];\n        value += prev[gid + nCols] - 2.f * prev[gid] + prev[gid - nCols];\n        value *= dtDIVdxy * vel[gid];\n        next[gid] = 2.f * prev[gid] - next[gid] + value;\n      }\n  }\n}\n\nint main(int argc, char* argv[]) {\n  \n\n  float* prev_base;\n  float* next_base;\n  float* next_cpu;\n  \n\n  float* vel_base;\n\n  bool error = false;\n\n  size_t nRows, nCols;\n  unsigned int nIterations;\n\n  \n\n  try {\n    nRows = std::stoi(argv[1]);\n    nCols = std::stoi(argv[2]);\n    nIterations = std::stoi(argv[3]);\n  }\n\n  catch (...) {\n    usage(argv[0]);\n    return 1;\n  }\n\n  \n\n  size_t nsize = nRows * nCols;\n\n  \n\n  prev_base = new float[nsize];\n  next_base = new float[nsize];\n  next_cpu = new float[nsize];\n  vel_base = new float[nsize];\n\n  \n\n  \n\n  float dtDIVdxy = (DT * DT) / (DXY * DXY);\n\n  \n\n  initialize(prev_base, next_base, vel_base, nRows, nCols);\n\n  std::cout << \"Grid Sizes: \" << nRows << \" \" << nCols << std::endl;\n  std::cout << \"Iterations: \" << nIterations << std::endl;\n  std::cout << std::endl;\n\n  std::cout << \"Computing wavefield in device ..\" << std::endl;\n\n    {\n    auto kstart = std::chrono::steady_clock::now();\n  \n    \n\n    for (unsigned int k = 0; k < nIterations; k += 1) {\n      \n\n      \n\n      iso_2dfd_kernel((k % 2) ? prev_base : next_base,\n                      (k % 2) ? next_base : prev_base,\n                      vel_base, dtDIVdxy, nRows, nCols);\n    }  \n\n  \n    auto kend = std::chrono::steady_clock::now();\n    auto ktime = std::chrono::duration_cast<std::chrono::nanoseconds>(kend - kstart).count();\n    std::cout << \"Total kernel execution time \" << ktime * 1e-6f << \" (ms)\\n\";\n    std::cout << \"Average kernel execution time \" << (ktime * 1e-3f) / nIterations << \" (us)\\n\";\n  }\n\n  \n\n  std::ofstream outFile;\n  outFile.open(\"wavefield_snapshot.bin\", std::ios::out | std::ios::binary);\n  outFile.write(reinterpret_cast<char*>(next_base), nsize * sizeof(float));\n  outFile.close();\n\n  \n\n  \n  std::cout << \"Computing wavefield in CPU ..\" << std::endl;\n  \n\n  initialize(prev_base, next_cpu, vel_base, nRows, nCols);\n\n  \n\n  \n\n  auto start = std::chrono::steady_clock::now();\n  iso_2dfd_iteration_cpu(next_cpu, prev_base, vel_base, dtDIVdxy, nRows, nCols,\n                         nIterations);\n\n  \n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::milliseconds>(end - start)\n                  .count();\n  std::cout << \"CPU time: \" << time << \" ms\" << std::endl;\n  std::cout << std::endl;\n\n  \n\n  \n\n  error = within_epsilon(next_base, next_cpu, nRows, nCols, HALF_LENGTH, 0.1f);\n\n  \n\n  \n\n  if (error)\n    std::cout << \"Final wavefields from device and CPU are different: Error \"\n              << std::endl;\n  else\n    std::cout << \"Final wavefields from device and CPU are equivalent: Success\"\n              << std::endl;\n\n  \n\n  outFile.open(\"wavefield_snapshot_cpu.bin\", std::ios::out | std::ios::binary);\n  outFile.write(reinterpret_cast<char*>(next_cpu), nsize * sizeof(float));\n  outFile.close();\n\n  std::cout << \"Final wavefields (from device and CPU) written to disk\"\n            << std::endl;\n  std::cout << \"Finished.  \" << std::endl;\n\n  \n\n  delete[] prev_base;\n  delete[] next_base;\n  delete[] vel_base;\n\n  return error ? 1 : 0;\n}"}}
{"kernel_name": "iso2dfd", "parallel_api": "sycl", "code": {"iso2dfd.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <fstream>\n#include <iostream>\n#include <sycl/sycl.hpp>\n#include \"iso2dfd.h\"\n\n#define MIN(a, b) (a) < (b) ? (a) : (b)\n\n\n\nvoid usage(std::string programName) {\n  std::cout << \" Incorrect parameters \" << std::endl;\n  std::cout << \" Usage: \";\n  std::cout << programName << \" n1 n2 Iterations \" << std::endl\n            << std::endl;\n  std::cout << \" n1 n2      : Grid sizes for the stencil \" << std::endl;\n  std::cout << \" Iterations : No. of timesteps. \" << std::endl;\n}\n\n\n\nvoid initialize(float* ptr_prev, float* ptr_next, float* ptr_vel, size_t nRows,\n                size_t nCols) {\n  std::cout << \"Initializing ... \" << std::endl;\n\n  \n\n  float wavelet[12] = {0.016387336, -0.041464937, -0.067372555, 0.386110067,\n                       0.812723635, 0.416998396,  0.076488599,  -0.059434419,\n                       0.023680172, 0.005611435,  0.001823209,  -0.000720549};\n\n  \n\n  for (size_t i = 0; i < nRows; i++) {\n    size_t offset = i * nCols;\n\n    for (size_t k = 0; k < nCols; k++) {\n      ptr_prev[offset + k] = 0.0f;\n      ptr_next[offset + k] = 0.0f;\n      \n\n      ptr_vel[offset + k] = 2250000.0f;\n    }\n  }\n  \n\n  for (int s = 11; s >= 0; s--) {\n    for (size_t i = nRows / 2 - s; i < nRows / 2 + s; i++) {\n      size_t offset = i * nCols;\n      for (size_t k = nCols / 2 - s; k < nCols / 2 + s; k++) {\n        ptr_prev[offset + k] = wavelet[s];\n      }\n    }\n  }\n}\n\n\n\nbool within_epsilon(float* output, float* reference, const size_t dimx,\n                    const size_t dimy, const unsigned int radius,\n                    const float delta = 0.01f) {\n  FILE* fp = fopen(\"./error_diff.txt\", \"w\");\n  if (!fp) fp = stderr;\n\n  bool error = false;\n  \n\n  double norm2 = 0;\n\n  for (size_t iy = 0; iy < dimy; iy++) {\n    for (size_t ix = 0; ix < dimx; ix++) {\n      if (ix >= radius && ix < (dimx - radius) && iy >= radius &&\n          iy < (dimy - radius)) {\n        float difference = fabsf(*reference - *output);\n        norm2 += difference * difference;\n        if (difference > delta) {\n          error = true;\n          fprintf(fp, \" ERROR: (%zu,%zu)\\t%e instead of %e (|e|=%e)\\n\", ix, iy,\n                  *output, *reference, difference);\n        }\n      }\n\n      ++output;\n      ++reference;\n    }\n  }\n\n  if (fp != stderr) fclose(fp);\n  norm2 = std::sqrt(norm2);\n  if (error) printf(\"error (Euclidean norm): %.9e\\n\", norm2);\n  return error;\n}\n\n\n\nvoid iso_2dfd_iteration_cpu(float* next, float* prev, float* vel,\n                            const float dtDIVdxy, size_t nRows, size_t nCols,\n                            int nIterations) {\n  for (unsigned int k = 0; k < nIterations; k += 1) {\n    for (size_t i = 1; i < nRows - HALF_LENGTH; i += 1) {\n      for (size_t j = 1; j < nCols - HALF_LENGTH; j += 1) {\n        \n\n        size_t gid = j + (i * nCols);\n        float value = 0.0;\n        value += prev[gid + 1] - 2.0 * prev[gid] + prev[gid - 1];\n        value += prev[gid + nCols] - 2.0 * prev[gid] + prev[gid - nCols];\n        value *= dtDIVdxy * vel[gid];\n        next[gid] = 2.0f * prev[gid] - next[gid] + value;\n      }\n    }\n\n    \n\n    float* swap = next;\n    next = prev;\n    prev = swap;\n  }\n}\n\n\n\nvoid iso_2dfd_kernel(sycl::nd_item<2> &item,\n                     float* next, const float* prev, const float* vel, \n\t\t     const float dtDIVdxy, const int nRows, const int nCols) {\n  \n\n  \n\n  \n\n  \n\n  size_t gidRow = item.get_global_id(0);\n  size_t gidCol = item.get_global_id(1);\n\n  if (gidRow < nRows && gidCol < nCols) {\n\n    size_t gid = (gidRow)*nCols + gidCol;\n\n    \n\n    \n\n    if ((gidCol >= HALF_LENGTH && gidCol < nCols - HALF_LENGTH) &&\n        (gidRow >= HALF_LENGTH && gidRow < nRows - HALF_LENGTH)) {\n      \n\n      \n\n      \n\n      \n\n      float value = 0.f;\n      value += prev[gid + 1] - 2.f * prev[gid] + prev[gid - 1];\n      value += prev[gid + nCols] - 2.f * prev[gid] + prev[gid - nCols];\n      value *= dtDIVdxy * vel[gid];\n      next[gid] = 2.0f * prev[gid] - next[gid] + value;\n    }\n  }\n}\n\nint main(int argc, char* argv[]) {\n  \n\n  float* prev_base;\n  float* next_base;\n  float* next_cpu;\n  \n\n  float* vel_base;\n\n  bool error = false;\n\n  size_t nRows, nCols;\n  unsigned int nIterations;\n\n  \n\n  try {\n    nRows = std::stoi(argv[1]);\n    nCols = std::stoi(argv[2]);\n    nIterations = std::stoi(argv[3]);\n  }\n\n  catch (...) {\n    usage(argv[0]);\n    return 1;\n  }\n\n  \n\n  size_t nsize = nRows * nCols;\n\n  \n\n  prev_base = new float[nsize];\n  next_base = new float[nsize];\n  next_cpu = new float[nsize];\n  vel_base = new float[nsize];\n\n  \n\n  \n\n  float dtDIVdxy = (DT * DT) / (DXY * DXY);\n\n  \n\n  initialize(prev_base, next_base, vel_base, nRows, nCols);\n\n  std::cout << \"Grid Sizes: \" << nRows << \" \" << nCols << std::endl;\n  std::cout << \"Iterations: \" << nIterations << std::endl;\n  std::cout << std::endl;\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  std::cout << \"Computing wavefield in device ..\" << std::endl;\n\n  auto device = q.get_device();\n  auto device_name = device.get_info<sycl::info::device::name>();\n  auto device_wgs = device.get_info<sycl::info::device::max_work_group_size>();\n  std::cout << \"Running on:: \" << device_name << std::endl;\n  std::cout << \"The Device Max Work Group Size is : \" << device_wgs << std::endl;\n\n  float *d_next = sycl::malloc_device<float>(nsize, q);\n  q.memcpy(d_next, next_base, sizeof(float)*nsize);\n\n  float *d_prev = sycl::malloc_device<float>(nsize, q);\n  q.memcpy(d_prev, prev_base, sizeof(float)*nsize);\n\n  float *d_vel = sycl::malloc_device<float>(nsize, q);\n  q.memcpy(d_vel, vel_base, sizeof(float)*nsize);\n\n  sycl::range<2> gws ((nRows+15)/16*16, (nCols+15)/16*16);\n  sycl::range<2> lws (16, 16);\n\n  q.wait();\n  auto kstart = std::chrono::steady_clock::now();\n\n  \n\n  for (unsigned int k = 0; k < nIterations; k += 1) {\n\n    \n\n    \n\n    q.submit([&](sycl::handler &h) {\n      \n\n      h.parallel_for<class kernel_next>(\n        sycl::nd_range<2>(gws, lws), [=](sycl::nd_item<2> item) {\n        iso_2dfd_kernel(item, k % 2 ? d_prev : d_next, \n                              k % 2 ? d_next : d_prev,\n                              d_vel, dtDIVdxy, nRows, nCols);\n      });\n    });\n  }  \n\n\n  q.wait();\n  auto kend = std::chrono::steady_clock::now();\n  auto ktime = std::chrono::duration_cast<std::chrono::nanoseconds>(kend - kstart).count();\n  std::cout << \"Total kernel execution time \" << ktime * 1e-6f << \" (ms)\\n\";\n  std::cout << \"Average kernel execution time \" << (ktime * 1e-3f) / nIterations << \" (us)\\n\";\n\n  q.memcpy(next_base, d_next, sizeof(float)*nsize).wait();\n\n  \n\n  std::ofstream outFile;\n  outFile.open(\"wavefield_snapshot.bin\", std::ios::out | std::ios::binary);\n  outFile.write(reinterpret_cast<char*>(next_base), nsize * sizeof(float));\n  outFile.close();\n\n  \n\n  \n  std::cout << \"Computing wavefield in CPU ..\" << std::endl;\n  \n\n  initialize(prev_base, next_cpu, vel_base, nRows, nCols);\n\n  \n\n  \n\n  auto start = std::chrono::steady_clock::now();\n  iso_2dfd_iteration_cpu(next_cpu, prev_base, vel_base, dtDIVdxy, nRows, nCols,\n                         nIterations);\n\n  \n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::milliseconds>(end - start)\n                  .count();\n  std::cout << \"CPU time: \" << time << \" ms\" << std::endl;\n  std::cout << std::endl;\n\n  \n\n  \n\n  error = within_epsilon(next_base, next_cpu, nRows, nCols, HALF_LENGTH, 0.1f);\n\n  \n\n  \n\n  if (error)\n    std::cout << \"Final wavefields from device and CPU are different: Error \"\n              << std::endl;\n  else\n    std::cout << \"Final wavefields from device and CPU are equivalent: Success\"\n              << std::endl;\n\n  \n\n  outFile.open(\"wavefield_snapshot_cpu.bin\", std::ios::out | std::ios::binary);\n  outFile.write(reinterpret_cast<char*>(next_cpu), nsize * sizeof(float));\n  outFile.close();\n\n  std::cout << \"Final wavefields (from device and CPU) written to disk\"\n            << std::endl;\n  std::cout << \"Finished.  \" << std::endl;\n\n  \n\n  delete[] prev_base;\n  delete[] next_base;\n  delete[] vel_base;\n  sycl::free(d_prev, q);\n  sycl::free(d_next, q);\n  sycl::free(d_vel, q);\n\n  return error ? 1 : 0;\n}\n"}}
{"kernel_name": "laplace", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <cuda.h>\n#include <cub/cub.cuh>\n#include \"timer.h\"\n#include \"kernels.h\"\n\n\n\n\nvoid fill_coeffs (int rowmax, int colmax, Real th_cond, Real dx, Real dy,\n    Real width, Real TN, Real * aP, Real * aW, Real * aE,\n    Real * aS, Real * aN, Real * b)\n{\n  int col, row;\n  for (col = 0; col < colmax; ++col) {\n    for (row = 0; row < rowmax; ++row) {\n      int ind = col * rowmax + row;\n\n      b[ind] = ZERO;\n      Real SP = ZERO;\n\n      if (col == 0) {\n        \n\n        aW[ind] = ZERO;\n        SP = -TWO * th_cond * width * dy / dx;\n      } else {\n        aW[ind] = th_cond * width * dy / dx;\n      }\n\n      if (col == colmax - 1) {\n        \n\n        aE[ind] = ZERO;\n        SP = -TWO * th_cond * width * dy / dx;\n      } else {\n        aE[ind] = th_cond * width * dy / dx;\n      }\n\n      if (row == 0) {\n        \n\n        aS[ind] = ZERO;\n        SP = -TWO * th_cond * width * dx / dy;\n      } else {\n        aS[ind] = th_cond * width * dx / dy;\n      }\n\n      if (row == rowmax - 1) {\n        \n\n        aN[ind] = ZERO;\n        b[ind] = TWO * th_cond * width * dx * TN / dy;\n        SP = -TWO * th_cond * width * dx / dy;\n      } else {\n        aN[ind] = th_cond * width * dx / dy;\n      }\n\n      aP[ind] = aW[ind] + aE[ind] + aS[ind] + aN[ind] - SP;\n    } \n\n  } \n\n} \n\n\n\n\n\nint main (void) {\n\n  \n\n  Real L = 1.0;\n  Real H = 1.0;\n  Real width = 0.01;\n\n  \n\n  Real th_cond = 1.0;\n\n  \n\n  Real TN = 1.0;\n\n  \n\n  Real tol = 1.e-6;\n\n  \n\n  \n\n  int num_rows = (NUM / 2) + 2;\n  int num_cols = NUM + 2;\n  int size_temp = num_rows * num_cols;\n  int size = NUM * NUM;\n\n  \n\n  Real dx = L / NUM;\n  Real dy = H / NUM;\n\n  \n\n  int iter;\n  int it_max = 1e6;\n\n  \n\n  Real *aP, *aW, *aE, *aS, *aN, *b;\n  Real *temp_red, *temp_black;\n\n  \n\n  aP = (Real *) calloc (size, sizeof(Real));\n  aW = (Real *) calloc (size, sizeof(Real));\n  aE = (Real *) calloc (size, sizeof(Real));\n  aS = (Real *) calloc (size, sizeof(Real));\n  aN = (Real *) calloc (size, sizeof(Real));\n\n  \n\n  b = (Real *) calloc (size, sizeof(Real));\n\n  \n\n  temp_red = (Real *) calloc (size_temp, sizeof(Real));\n  temp_black = (Real *) calloc (size_temp, sizeof(Real));\n\n  \n\n  fill_coeffs (NUM, NUM, th_cond, dx, dy, width, TN, aP, aW, aE, aS, aN, b);\n\n  int i;\n  for (i = 0; i < size_temp; ++i) {\n    temp_red[i] = ZERO;\n    temp_black[i] = ZERO;\n  }\n\n  \n\n  dim3 dimBlock (BLOCK_SIZE, 2);\n  dim3 dimGrid (NUM / (2 * BLOCK_SIZE), NUM/2);\n\n  \n\n  printf(\"Problem size: %d x %d \\n\", NUM, NUM);\n\n  \n\n  Real *aP_d, *aW_d, *aE_d, *aS_d, *aN_d, *b_d;\n  Real *temp_red_d;\n  Real *temp_black_d;\n  Real *bl_norm_L2_d;\n  Real *norm_L2_d;\n\n  \n\n  \n\n  int size_norm = size_temp;\n  cudaMalloc ((void**) &bl_norm_L2_d, size_norm * sizeof(Real));\n  cudaMemset (bl_norm_L2_d, 0, size_norm * sizeof(Real));\n\n  \n\n  cudaMalloc ((void**) &norm_L2_d, sizeof(Real));\n\n  cudaMalloc ((void**) &aP_d, size * sizeof(Real));\n  cudaMalloc ((void**) &aW_d, size * sizeof(Real));\n  cudaMalloc ((void**) &aE_d, size * sizeof(Real));\n  cudaMalloc ((void**) &aS_d, size * sizeof(Real));\n  cudaMalloc ((void**) &aN_d, size * sizeof(Real));\n  cudaMalloc ((void**) &b_d, size * sizeof(Real));\n  cudaMalloc ((void**) &temp_red_d, size_temp * sizeof(Real));\n  cudaMalloc ((void**) &temp_black_d, size_temp * sizeof(Real));\n\n  \n\n  cudaMemcpy (aP_d, aP, size * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (aW_d, aW, size * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (aE_d, aE, size * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (aS_d, aS, size * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (aN_d, aN, size * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (b_d, b, size * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (temp_red_d, temp_red, size_temp * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (temp_black_d, temp_black, size_temp * sizeof(Real), cudaMemcpyHostToDevice);\n\n  cudaDeviceSynchronize();\n  StartTimer();\n\n  \n\n  void     *d_temp_storage = nullptr;\n  size_t   temp_storage_bytes = 0;\n  cub::DeviceReduce::Sum(d_temp_storage, temp_storage_bytes,\n                         bl_norm_L2_d, norm_L2_d, size_norm);\n\n  \n\n  if (temp_storage_bytes != 0)\n    cudaMalloc(&d_temp_storage, temp_storage_bytes);\n\n  \n\n  for (iter = 1; iter <= it_max; ++iter) {\n\n    Real norm_L2 = ZERO;\n\n    red_kernel <<<dimGrid, dimBlock>>> (aP_d, aW_d, aE_d, aS_d, aN_d, b_d, temp_black_d, temp_red_d, bl_norm_L2_d);\n    cub::DeviceReduce::Sum(d_temp_storage, temp_storage_bytes, bl_norm_L2_d, norm_L2_d, size_norm);\n    cudaMemcpy(&norm_L2, norm_L2_d, sizeof(Real), cudaMemcpyDeviceToHost);\n\n    black_kernel <<<dimGrid, dimBlock>>> (aP_d, aW_d, aE_d, aS_d, aN_d, b_d, temp_red_d, temp_black_d, bl_norm_L2_d);\n    cub::DeviceReduce::Sum(d_temp_storage, temp_storage_bytes, bl_norm_L2_d, norm_L2_d, size_norm);\n    Real temp = norm_L2;\n    cudaMemcpy(&norm_L2, norm_L2_d, sizeof(Real), cudaMemcpyDeviceToHost);\n    norm_L2 += temp;\n\n    \n\n    norm_L2 = sqrt(norm_L2 / ((Real)size));\n\n    if (iter % 1000 == 0) printf(\"%5d, %0.6f\\n\", iter, norm_L2);\n\n    \n\n    if (norm_L2 < tol) break;\n  }\n\n  if (d_temp_storage != nullptr) cudaFree(d_temp_storage);\n\n  double runtime = GetTimer();\n  printf(\"Total time for %i iterations: %f s\\n\", iter, runtime / 1000.0);\n\n  \n\n  cudaMemcpy (temp_red, temp_red_d, size_temp * sizeof(Real), cudaMemcpyDeviceToHost);\n  cudaMemcpy (temp_black, temp_black_d, size_temp * sizeof(Real), cudaMemcpyDeviceToHost);\n\n  \n\n  FILE * pfile;\n  pfile = fopen(\"temperature.dat\", \"w\");\n\n  if (pfile != NULL) {\n    fprintf(pfile, \"#x\\ty\\ttemp(K)\\n\");\n\n    int row, col;\n    for (row = 1; row < NUM + 1; ++row) {\n      for (col = 1; col < NUM + 1; ++col) {\n        Real x_pos = (col - 1) * dx + (dx / 2);\n        Real y_pos = (row - 1) * dy + (dy / 2);\n\n        if ((row + col) % 2 == 0) {\n          \n\n          int ind = col * num_rows + (row + (col % 2)) / 2;\n          fprintf(pfile, \"%f\\t%f\\t%f\\n\", x_pos, y_pos, temp_red[ind]);\n        } else {\n          \n\n          int ind = col * num_rows + (row + ((col + 1) % 2)) / 2;\n          fprintf(pfile, \"%f\\t%f\\t%f\\n\", x_pos, y_pos, temp_black[ind]);\n        }\n      }\n      fprintf(pfile, \"\\n\");\n    }\n  }\n\n  fclose(pfile);\n\n  cudaFree(aP_d);\n  cudaFree(aW_d);\n  cudaFree(aE_d);\n  cudaFree(aS_d);\n  cudaFree(aN_d);\n  cudaFree(b_d);\n  cudaFree(temp_red_d);\n  cudaFree(temp_black_d);\n  cudaFree(bl_norm_L2_d);\n  cudaFree(norm_L2_d);\n\n  free(aP);\n  free(aW);\n  free(aE);\n  free(aS);\n  free(aN);\n  free(b);\n  free(temp_red);\n  free(temp_black);\n\n  return 0;\n}\n"}}
{"kernel_name": "laplace", "parallel_api": "hip", "code": {"main.cu": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <hip/hip_runtime.h>\n#include <hipcub/hipcub.hpp>\n#include \"timer.h\"\n#include \"kernels.h\"\n\n\n\n\nvoid fill_coeffs (int rowmax, int colmax, Real th_cond, Real dx, Real dy,\n    Real width, Real TN, Real * aP, Real * aW, Real * aE,\n    Real * aS, Real * aN, Real * b)\n{\n  int col, row;\n  for (col = 0; col < colmax; ++col) {\n    for (row = 0; row < rowmax; ++row) {\n      int ind = col * rowmax + row;\n\n      b[ind] = ZERO;\n      Real SP = ZERO;\n\n      if (col == 0) {\n        \n\n        aW[ind] = ZERO;\n        SP = -TWO * th_cond * width * dy / dx;\n      } else {\n        aW[ind] = th_cond * width * dy / dx;\n      }\n\n      if (col == colmax - 1) {\n        \n\n        aE[ind] = ZERO;\n        SP = -TWO * th_cond * width * dy / dx;\n      } else {\n        aE[ind] = th_cond * width * dy / dx;\n      }\n\n      if (row == 0) {\n        \n\n        aS[ind] = ZERO;\n        SP = -TWO * th_cond * width * dx / dy;\n      } else {\n        aS[ind] = th_cond * width * dx / dy;\n      }\n\n      if (row == rowmax - 1) {\n        \n\n        aN[ind] = ZERO;\n        b[ind] = TWO * th_cond * width * dx * TN / dy;\n        SP = -TWO * th_cond * width * dx / dy;\n      } else {\n        aN[ind] = th_cond * width * dx / dy;\n      }\n\n      aP[ind] = aW[ind] + aE[ind] + aS[ind] + aN[ind] - SP;\n    } \n\n  } \n\n} \n\n\n\n\n\nint main (void) {\n\n  \n\n  Real L = 1.0;\n  Real H = 1.0;\n  Real width = 0.01;\n\n  \n\n  Real th_cond = 1.0;\n\n  \n\n  Real TN = 1.0;\n\n  \n\n  Real tol = 1.e-6;\n\n  \n\n  \n\n  int num_rows = (NUM / 2) + 2;\n  int num_cols = NUM + 2;\n  int size_temp = num_rows * num_cols;\n  int size = NUM * NUM;\n\n  \n\n  Real dx = L / NUM;\n  Real dy = H / NUM;\n\n  \n\n  int iter;\n  int it_max = 1e6;\n\n  \n\n  Real *aP, *aW, *aE, *aS, *aN, *b;\n  Real *temp_red, *temp_black;\n\n  \n\n  aP = (Real *) calloc (size, sizeof(Real));\n  aW = (Real *) calloc (size, sizeof(Real));\n  aE = (Real *) calloc (size, sizeof(Real));\n  aS = (Real *) calloc (size, sizeof(Real));\n  aN = (Real *) calloc (size, sizeof(Real));\n\n  \n\n  b = (Real *) calloc (size, sizeof(Real));\n\n  \n\n  temp_red = (Real *) calloc (size_temp, sizeof(Real));\n  temp_black = (Real *) calloc (size_temp, sizeof(Real));\n\n  \n\n  fill_coeffs (NUM, NUM, th_cond, dx, dy, width, TN, aP, aW, aE, aS, aN, b);\n\n  int i;\n  for (i = 0; i < size_temp; ++i) {\n    temp_red[i] = ZERO;\n    temp_black[i] = ZERO;\n  }\n\n  \n\n  dim3 dimBlock (BLOCK_SIZE, 2);\n  dim3 dimGrid (NUM / (2 * BLOCK_SIZE), NUM/2);\n\n  \n\n  printf(\"Problem size: %d x %d \\n\", NUM, NUM);\n\n  \n\n  Real *aP_d, *aW_d, *aE_d, *aS_d, *aN_d, *b_d;\n  Real *temp_red_d;\n  Real *temp_black_d;\n  Real *bl_norm_L2_d;\n  Real *norm_L2_d;\n\n  \n\n  \n\n  int size_norm = size_temp;\n  hipMalloc ((void**) &bl_norm_L2_d, size_norm * sizeof(Real));\n  hipMemset (bl_norm_L2_d, 0, size_norm * sizeof(Real));\n\n  \n\n  hipMalloc ((void**) &norm_L2_d, sizeof(Real));\n\n  hipMalloc ((void**) &aP_d, size * sizeof(Real));\n  hipMalloc ((void**) &aW_d, size * sizeof(Real));\n  hipMalloc ((void**) &aE_d, size * sizeof(Real));\n  hipMalloc ((void**) &aS_d, size * sizeof(Real));\n  hipMalloc ((void**) &aN_d, size * sizeof(Real));\n  hipMalloc ((void**) &b_d, size * sizeof(Real));\n  hipMalloc ((void**) &temp_red_d, size_temp * sizeof(Real));\n  hipMalloc ((void**) &temp_black_d, size_temp * sizeof(Real));\n\n  \n\n  hipMemcpy (aP_d, aP, size * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (aW_d, aW, size * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (aE_d, aE, size * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (aS_d, aS, size * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (aN_d, aN, size * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (b_d, b, size * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (temp_red_d, temp_red, size_temp * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (temp_black_d, temp_black, size_temp * sizeof(Real), hipMemcpyHostToDevice);\n\n  hipDeviceSynchronize();\n  StartTimer();\n\n  \n\n  void     *d_temp_storage = nullptr;\n  size_t   temp_storage_bytes = 0;\n  hipcub::DeviceReduce::Sum(d_temp_storage, temp_storage_bytes,\n                         bl_norm_L2_d, norm_L2_d, size_norm);\n\n  \n\n  if (temp_storage_bytes != 0)\n    hipMalloc(&d_temp_storage, temp_storage_bytes);\n\n  \n\n  for (iter = 1; iter <= it_max; ++iter) {\n\n    Real norm_L2 = ZERO;\n\n    red_kernel <<<dimGrid, dimBlock>>> (aP_d, aW_d, aE_d, aS_d, aN_d, b_d, temp_black_d, temp_red_d, bl_norm_L2_d);\n    hipcub::DeviceReduce::Sum(d_temp_storage, temp_storage_bytes, bl_norm_L2_d, norm_L2_d, size_norm);\n    hipMemcpy(&norm_L2, norm_L2_d, sizeof(Real), hipMemcpyDeviceToHost);\n\n    black_kernel <<<dimGrid, dimBlock>>> (aP_d, aW_d, aE_d, aS_d, aN_d, b_d, temp_red_d, temp_black_d, bl_norm_L2_d);\n    hipcub::DeviceReduce::Sum(d_temp_storage, temp_storage_bytes, bl_norm_L2_d, norm_L2_d, size_norm);\n    Real temp = norm_L2;\n    hipMemcpy(&norm_L2, norm_L2_d, sizeof(Real), hipMemcpyDeviceToHost);\n    norm_L2 += temp;\n\n    \n\n    norm_L2 = sqrt(norm_L2 / ((Real)size));\n\n    if (iter % 1000 == 0) printf(\"%5d, %0.6f\\n\", iter, norm_L2);\n\n    \n\n    if (norm_L2 < tol) break;\n  }\n\n  if (d_temp_storage != nullptr) hipFree(d_temp_storage);\n\n  double runtime = GetTimer();\n  printf(\"Total time for %i iterations: %f s\\n\", iter, runtime / 1000.0);\n\n  \n\n  hipMemcpy (temp_red, temp_red_d, size_temp * sizeof(Real), hipMemcpyDeviceToHost);\n  hipMemcpy (temp_black, temp_black_d, size_temp * sizeof(Real), hipMemcpyDeviceToHost);\n\n  \n\n  FILE * pfile;\n  pfile = fopen(\"temperature.dat\", \"w\");\n\n  if (pfile != NULL) {\n    fprintf(pfile, \"#x\\ty\\ttemp(K)\\n\");\n\n    int row, col;\n    for (row = 1; row < NUM + 1; ++row) {\n      for (col = 1; col < NUM + 1; ++col) {\n        Real x_pos = (col - 1) * dx + (dx / 2);\n        Real y_pos = (row - 1) * dy + (dy / 2);\n\n        if ((row + col) % 2 == 0) {\n          \n\n          int ind = col * num_rows + (row + (col % 2)) / 2;\n          fprintf(pfile, \"%f\\t%f\\t%f\\n\", x_pos, y_pos, temp_red[ind]);\n        } else {\n          \n\n          int ind = col * num_rows + (row + ((col + 1) % 2)) / 2;\n          fprintf(pfile, \"%f\\t%f\\t%f\\n\", x_pos, y_pos, temp_black[ind]);\n        }\n      }\n      fprintf(pfile, \"\\n\");\n    }\n  }\n\n  fclose(pfile);\n\n  hipFree(aP_d);\n  hipFree(aW_d);\n  hipFree(aE_d);\n  hipFree(aS_d);\n  hipFree(aN_d);\n  hipFree(b_d);\n  hipFree(temp_red_d);\n  hipFree(temp_black_d);\n  hipFree(bl_norm_L2_d);\n  hipFree(norm_L2_d);\n\n  free(aP);\n  free(aW);\n  free(aE);\n  free(aS);\n  free(aN);\n  free(b);\n  free(temp_red);\n  free(temp_black);\n\n  return 0;\n}\n"}}
{"kernel_name": "laplace", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include \"timer.h\"\n\n\n\n#define NUM 1024\n\n\n\n#define BLOCK_SIZE 256\n\n#define Real float\n#define ZERO 0.0f\n#define ONE 1.0f\n#define TWO 2.0f\n\n\n\nconst Real omega = 1.85f;\n\n\n\nvoid fill_coeffs (int rowmax, int colmax, Real th_cond, Real dx, Real dy,\n    Real width, Real TN, Real * aP, Real * aW, Real * aE, \n    Real * aS, Real * aN, Real * b)\n{\n  int col, row;\n  for (col = 0; col < colmax; ++col) {\n    for (row = 0; row < rowmax; ++row) {\n      int ind = col * rowmax + row;\n\n      b[ind] = ZERO;\n      Real SP = ZERO;\n\n      if (col == 0) {\n        \n\n        aW[ind] = ZERO;\n        SP = -TWO * th_cond * width * dy / dx;\n      } else {\n        aW[ind] = th_cond * width * dy / dx;\n      }\n\n      if (col == colmax - 1) {\n        \n\n        aE[ind] = ZERO;\n        SP = -TWO * th_cond * width * dy / dx;\n      } else {\n        aE[ind] = th_cond * width * dy / dx;\n      }\n\n      if (row == 0) {\n        \n\n        aS[ind] = ZERO;\n        SP = -TWO * th_cond * width * dx / dy;\n      } else {\n        aS[ind] = th_cond * width * dx / dy;\n      }\n\n      if (row == rowmax - 1) {\n        \n\n        aN[ind] = ZERO;\n        b[ind] = TWO * th_cond * width * dx * TN / dy;\n        SP = -TWO * th_cond * width * dx / dy;\n      } else {\n        aN[ind] = th_cond * width * dx / dy;\n      }\n\n      aP[ind] = aW[ind] + aE[ind] + aS[ind] + aN[ind] - SP;\n    } \n\n  } \n\n} \n\n\n\n\nint main (void) {\n\n  \n\n  Real L = 1.0;\n  Real H = 1.0;\n  Real width = 0.01;\n\n  \n\n  Real th_cond = 1.0;\n\n  \n\n  Real TN = 1.0;\n\n  \n\n  Real tol = 1.e-6;\n\n  \n\n  \n\n  int num_rows = (NUM / 2) + 2;\n  int num_cols = NUM + 2;\n  int size_temp = num_rows * num_cols;\n  int size = NUM * NUM;\n\n  \n\n  Real dx = L / NUM;\n  Real dy = H / NUM;\n\n  \n\n  int iter;\n  int it_max = 1e6;\n\n  \n\n  Real *aP, *aW, *aE, *aS, *aN, *b;\n  Real *temp_red, *temp_black;\n\n  \n\n  aP = (Real *) calloc (size, sizeof(Real));\n  aW = (Real *) calloc (size, sizeof(Real));\n  aE = (Real *) calloc (size, sizeof(Real));\n  aS = (Real *) calloc (size, sizeof(Real));\n  aN = (Real *) calloc (size, sizeof(Real));\n\n  \n\n  b = (Real *) calloc (size, sizeof(Real));\n\n  \n\n  temp_red = (Real *) calloc (size_temp, sizeof(Real));\n  temp_black = (Real *) calloc (size_temp, sizeof(Real));\n\n  \n\n  fill_coeffs (NUM, NUM, th_cond, dx, dy, width, TN, aP, aW, aE, aS, aN, b);\n\n  int i;\n  for (i = 0; i < size_temp; ++i) {\n    temp_red[i] = ZERO;\n    temp_black[i] = ZERO;\n  }\n\n  \n\n  Real *bl_norm_L2;\n\n  \n\n  int size_norm = size_temp;\n  bl_norm_L2 = (Real *) calloc (size_norm, sizeof(Real));\n  for (i = 0; i < size_norm; ++i) {\n    bl_norm_L2[i] = ZERO;\n  }\n\n  \n\n  printf(\"Problem size: %d x %d \\n\", NUM, NUM);\n\n  \n\n  #pragma omp target data map(to: aP[0:size], aW[0:size], aE[0:size], aS[0:size], aN[0:size], \\\n                                  b[0:size], bl_norm_L2[0:size_norm]) \\\n                          map(tofrom: temp_red[0:size_temp], temp_black[0:size_temp])\n  {\n    StartTimer();\n  \n    for (iter = 1; iter <= it_max; ++iter) {\n  \n      Real norm_L2 = ZERO;\n  \n      #pragma omp target teams distribute parallel for collapse(2) num_threads(BLOCK_SIZE)\n      for (int row = 1; row <= NUM/2; row++) {\n        for (int col = 1; col <= NUM; col++) {\n          int ind_red = col * ((NUM >> 1) + 2) + row;  \t\t\t\t\t\n\n          int ind = 2 * row - (col & 1) - 1 + NUM * (col - 1);\t\n\n  \n          Real temp_old = temp_red[ind_red];\n  \n          Real res = b[ind] + (aW[ind] * temp_black[row + (col - 1) * ((NUM >> 1) + 2)]\n                + aE[ind] * temp_black[row + (col + 1) * ((NUM >> 1) + 2)]\n                + aS[ind] * temp_black[row - (col & 1) + col * ((NUM >> 1) + 2)]\n                + aN[ind] * temp_black[row + ((col + 1) & 1) + col * ((NUM >> 1) + 2)]);\n  \n          Real temp_new = temp_old * (ONE - omega) + omega * (res / aP[ind]);\n  \n          temp_red[ind_red] = temp_new;\n          res = temp_new - temp_old;\n  \n          bl_norm_L2[ind_red] = res * res;\n        }\n      }\n      \n\n      #pragma omp target teams distribute parallel for reduction(+:norm_L2)\n      for (int i = 0; i < size_norm; ++i) {\n        norm_L2 += bl_norm_L2[i];\n      }\n  \n      #pragma omp target teams distribute parallel for collapse(2) num_threads(BLOCK_SIZE)\n      for (int row = 1; row <= NUM/2; row++) {\n        for (int col = 1; col <= NUM; col++) {\n          int ind_black = col * ((NUM >> 1) + 2) + row; \n\n          int ind = 2 * row - ((col + 1) & 1) - 1 + NUM * (col - 1); \n\n  \n          Real temp_old = temp_black[ind_black];\n  \n          Real res = b[ind] + (aW[ind] * temp_red[row + (col - 1) * ((NUM >> 1) + 2)]\n                + aE[ind] * temp_red[row + (col + 1) * ((NUM >> 1) + 2)]\n                + aS[ind] * temp_red[row - ((col + 1) & 1) + col * ((NUM >> 1) + 2)]\n                + aN[ind] * temp_red[row + (col & 1) + col * ((NUM >> 1) + 2)]);\n  \n          Real temp_new = temp_old * (ONE - omega) + omega * (res / aP[ind]);\n  \n          temp_black[ind_black] = temp_new;\n          res = temp_new - temp_old;\n  \n          bl_norm_L2[ind_black] = res * res;\n        }\n      }\n      \n\n      #pragma omp target teams distribute parallel for reduction(+:norm_L2)\n      for (int i = 0; i < size_norm; ++i)\n        norm_L2 += bl_norm_L2[i];\n  \n      \n\n      norm_L2 = sqrt(norm_L2 / ((Real)size));\n  \n      if (iter % 1000 == 0) printf(\"%5d, %0.6f\\n\", iter, norm_L2);\n  \n      \n\n      if (norm_L2 < tol) break;\n    }\n  \n    double runtime = GetTimer();\n    printf(\"Total time for %i iterations: %f s\\n\", iter, runtime / 1000.0);\n  }\n\n  \n\n  FILE * pfile;\n  pfile = fopen(\"temperature.dat\", \"w\");\n\n  if (pfile != NULL) {\n    fprintf(pfile, \"#x\\ty\\ttemp(K)\\n\");\n\n    int row, col;\n    for (row = 1; row < NUM + 1; ++row) {\n      for (col = 1; col < NUM + 1; ++col) {\n        Real x_pos = (col - 1) * dx + (dx / 2);\n        Real y_pos = (row - 1) * dy + (dy / 2);\n\n        if ((row + col) % 2 == 0) {\n          \n\n          int ind = col * num_rows + (row + (col % 2)) / 2;\n          fprintf(pfile, \"%f\\t%f\\t%f\\n\", x_pos, y_pos, temp_red[ind]);\n        } else {\n          \n\n          int ind = col * num_rows + (row + ((col + 1) % 2)) / 2;\n          fprintf(pfile, \"%f\\t%f\\t%f\\n\", x_pos, y_pos, temp_black[ind]);\n        }\t\n      }\n      fprintf(pfile, \"\\n\");\n    }\n  }\n  fclose(pfile);\n\n  free(aP);\n  free(aW);\n  free(aE);\n  free(aS);\n  free(aN);\n  free(b);\n  free(temp_red);\n  free(temp_black);\n  free(bl_norm_L2);\n\n  return 0;\n}\n"}}
{"kernel_name": "laplace", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include \"timer.h\"\n\n\n\n#define NUM 1024\n\n\n\n#define BLOCK_SIZE 256\n\n#define Real float\n#define ZERO 0.0f\n#define ONE 1.0f\n#define TWO 2.0f\n\n\n\nconst Real omega = 1.85f;\n\n\n\nvoid fill_coeffs (int rowmax, int colmax, Real th_cond, Real dx, Real dy,\n    Real width, Real TN, Real * aP, Real * aW, Real * aE, \n    Real * aS, Real * aN, Real * b)\n{\n  int col, row;\n  for (col = 0; col < colmax; ++col) {\n    for (row = 0; row < rowmax; ++row) {\n      int ind = col * rowmax + row;\n\n      b[ind] = ZERO;\n      Real SP = ZERO;\n\n      if (col == 0) {\n        \n\n        aW[ind] = ZERO;\n        SP = -TWO * th_cond * width * dy / dx;\n      } else {\n        aW[ind] = th_cond * width * dy / dx;\n      }\n\n      if (col == colmax - 1) {\n        \n\n        aE[ind] = ZERO;\n        SP = -TWO * th_cond * width * dy / dx;\n      } else {\n        aE[ind] = th_cond * width * dy / dx;\n      }\n\n      if (row == 0) {\n        \n\n        aS[ind] = ZERO;\n        SP = -TWO * th_cond * width * dx / dy;\n      } else {\n        aS[ind] = th_cond * width * dx / dy;\n      }\n\n      if (row == rowmax - 1) {\n        \n\n        aN[ind] = ZERO;\n        b[ind] = TWO * th_cond * width * dx * TN / dy;\n        SP = -TWO * th_cond * width * dx / dy;\n      } else {\n        aN[ind] = th_cond * width * dx / dy;\n      }\n\n      aP[ind] = aW[ind] + aE[ind] + aS[ind] + aN[ind] - SP;\n    } \n\n  } \n\n} \n\n\n\n\nint main (void) {\n\n  \n\n  Real L = 1.0;\n  Real H = 1.0;\n  Real width = 0.01;\n\n  \n\n  Real th_cond = 1.0;\n\n  \n\n  Real TN = 1.0;\n\n  \n\n  Real tol = 1.e-6;\n\n  \n\n  \n\n  int num_rows = (NUM / 2) + 2;\n  int num_cols = NUM + 2;\n  int size_temp = num_rows * num_cols;\n  int size = NUM * NUM;\n\n  \n\n  Real dx = L / NUM;\n  Real dy = H / NUM;\n\n  \n\n  int iter;\n  int it_max = 1e6;\n\n  \n\n  Real *aP, *aW, *aE, *aS, *aN, *b;\n  Real *temp_red, *temp_black;\n\n  \n\n  aP = (Real *) calloc (size, sizeof(Real));\n  aW = (Real *) calloc (size, sizeof(Real));\n  aE = (Real *) calloc (size, sizeof(Real));\n  aS = (Real *) calloc (size, sizeof(Real));\n  aN = (Real *) calloc (size, sizeof(Real));\n\n  \n\n  b = (Real *) calloc (size, sizeof(Real));\n\n  \n\n  temp_red = (Real *) calloc (size_temp, sizeof(Real));\n  temp_black = (Real *) calloc (size_temp, sizeof(Real));\n\n  \n\n  fill_coeffs (NUM, NUM, th_cond, dx, dy, width, TN, aP, aW, aE, aS, aN, b);\n\n  int i;\n  for (i = 0; i < size_temp; ++i) {\n    temp_red[i] = ZERO;\n    temp_black[i] = ZERO;\n  }\n\n  \n\n  Real *bl_norm_L2;\n\n  \n\n  int size_norm = size_temp;\n  bl_norm_L2 = (Real *) calloc (size_norm, sizeof(Real));\n  for (i = 0; i < size_norm; ++i) {\n    bl_norm_L2[i] = ZERO;\n  }\n\n  \n\n  printf(\"Problem size: %d x %d \\n\", NUM, NUM);\n\n  \n\n    {\n    StartTimer();\n  \n    for (iter = 1; iter <= it_max; ++iter) {\n  \n      Real norm_L2 = ZERO;\n  \n            for (int row = 1; row <= NUM/2; row++) {\n        for (int col = 1; col <= NUM; col++) {\n          int ind_red = col * ((NUM >> 1) + 2) + row;  \t\t\t\t\t\n\n          int ind = 2 * row - (col & 1) - 1 + NUM * (col - 1);\t\n\n  \n          Real temp_old = temp_red[ind_red];\n  \n          Real res = b[ind] + (aW[ind] * temp_black[row + (col - 1) * ((NUM >> 1) + 2)]\n                + aE[ind] * temp_black[row + (col + 1) * ((NUM >> 1) + 2)]\n                + aS[ind] * temp_black[row - (col & 1) + col * ((NUM >> 1) + 2)]\n                + aN[ind] * temp_black[row + ((col + 1) & 1) + col * ((NUM >> 1) + 2)]);\n  \n          Real temp_new = temp_old * (ONE - omega) + omega * (res / aP[ind]);\n  \n          temp_red[ind_red] = temp_new;\n          res = temp_new - temp_old;\n  \n          bl_norm_L2[ind_red] = res * res;\n        }\n      }\n      \n\n            for (int i = 0; i < size_norm; ++i) {\n        norm_L2 += bl_norm_L2[i];\n      }\n  \n            for (int row = 1; row <= NUM/2; row++) {\n        for (int col = 1; col <= NUM; col++) {\n          int ind_black = col * ((NUM >> 1) + 2) + row; \n\n          int ind = 2 * row - ((col + 1) & 1) - 1 + NUM * (col - 1); \n\n  \n          Real temp_old = temp_black[ind_black];\n  \n          Real res = b[ind] + (aW[ind] * temp_red[row + (col - 1) * ((NUM >> 1) + 2)]\n                + aE[ind] * temp_red[row + (col + 1) * ((NUM >> 1) + 2)]\n                + aS[ind] * temp_red[row - ((col + 1) & 1) + col * ((NUM >> 1) + 2)]\n                + aN[ind] * temp_red[row + (col & 1) + col * ((NUM >> 1) + 2)]);\n  \n          Real temp_new = temp_old * (ONE - omega) + omega * (res / aP[ind]);\n  \n          temp_black[ind_black] = temp_new;\n          res = temp_new - temp_old;\n  \n          bl_norm_L2[ind_black] = res * res;\n        }\n      }\n      \n\n            for (int i = 0; i < size_norm; ++i)\n        norm_L2 += bl_norm_L2[i];\n  \n      \n\n      norm_L2 = sqrt(norm_L2 / ((Real)size));\n  \n      if (iter % 1000 == 0) printf(\"%5d, %0.6f\\n\", iter, norm_L2);\n  \n      \n\n      if (norm_L2 < tol) break;\n    }\n  \n    double runtime = GetTimer();\n    printf(\"Total time for %i iterations: %f s\\n\", iter, runtime / 1000.0);\n  }\n\n  \n\n  FILE * pfile;\n  pfile = fopen(\"temperature.dat\", \"w\");\n\n  if (pfile != NULL) {\n    fprintf(pfile, \"#x\\ty\\ttemp(K)\\n\");\n\n    int row, col;\n    for (row = 1; row < NUM + 1; ++row) {\n      for (col = 1; col < NUM + 1; ++col) {\n        Real x_pos = (col - 1) * dx + (dx / 2);\n        Real y_pos = (row - 1) * dy + (dy / 2);\n\n        if ((row + col) % 2 == 0) {\n          \n\n          int ind = col * num_rows + (row + (col % 2)) / 2;\n          fprintf(pfile, \"%f\\t%f\\t%f\\n\", x_pos, y_pos, temp_red[ind]);\n        } else {\n          \n\n          int ind = col * num_rows + (row + ((col + 1) % 2)) / 2;\n          fprintf(pfile, \"%f\\t%f\\t%f\\n\", x_pos, y_pos, temp_black[ind]);\n        }\t\n      }\n      fprintf(pfile, \"\\n\");\n    }\n  }\n  fclose(pfile);\n\n  free(aP);\n  free(aW);\n  free(aE);\n  free(aS);\n  free(aN);\n  free(b);\n  free(temp_red);\n  free(temp_black);\n  free(bl_norm_L2);\n\n  return 0;\n}"}}
{"kernel_name": "laplace", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n#include <oneapi/dpl/execution>\n#include <oneapi/dpl/numeric>\n#include <sycl/sycl.hpp>\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\n#include \"timer.h\"\n#include \"kernels.h\"\n\n\n\n\nvoid fill_coeffs (int rowmax, int colmax, Real th_cond, Real dx, Real dy,\n    Real width, Real TN, Real * aP, Real * aW, Real * aE,\n    Real * aS, Real * aN, Real * b)\n{\n  int col, row;\n  for (col = 0; col < colmax; ++col) {\n    for (row = 0; row < rowmax; ++row) {\n      int ind = col * rowmax + row;\n\n      b[ind] = ZERO;\n      Real SP = ZERO;\n\n      if (col == 0) {\n        \n\n        aW[ind] = ZERO;\n        SP = -TWO * th_cond * width * dy / dx;\n      } else {\n        aW[ind] = th_cond * width * dy / dx;\n      }\n\n      if (col == colmax - 1) {\n        \n\n        aE[ind] = ZERO;\n        SP = -TWO * th_cond * width * dy / dx;\n      } else {\n        aE[ind] = th_cond * width * dy / dx;\n      }\n\n      if (row == 0) {\n        \n\n        aS[ind] = ZERO;\n        SP = -TWO * th_cond * width * dx / dy;\n      } else {\n        aS[ind] = th_cond * width * dx / dy;\n      }\n\n      if (row == rowmax - 1) {\n        \n\n        aN[ind] = ZERO;\n        b[ind] = TWO * th_cond * width * dx * TN / dy;\n        SP = -TWO * th_cond * width * dx / dy;\n      } else {\n        aN[ind] = th_cond * width * dx / dy;\n      }\n\n      aP[ind] = aW[ind] + aE[ind] + aS[ind] + aN[ind] - SP;\n    } \n\n  } \n\n} \n\n\n\n\n\nint main(void) {\n\n  \n\n  Real L = 1.0;\n  Real H = 1.0;\n  Real width = 0.01;\n\n  \n\n  Real th_cond = 1.0;\n\n  \n\n  Real TN = 1.0;\n\n  \n\n  Real tol = 1.e-6;\n\n  \n\n  \n\n  int num_rows = (NUM / 2) + 2;\n  int num_cols = NUM + 2;\n  int size_temp = num_rows * num_cols;\n  int size = NUM * NUM;\n\n  \n\n  Real dx = L / NUM;\n  Real dy = H / NUM;\n\n  \n\n  int iter;\n  int it_max = 1e6;\n\n  \n\n  Real *aP, *aW, *aE, *aS, *aN, *b;\n  Real *temp_red, *temp_black;\n\n  \n\n  aP = (Real *) calloc (size, sizeof(Real));\n  aW = (Real *) calloc (size, sizeof(Real));\n  aE = (Real *) calloc (size, sizeof(Real));\n  aS = (Real *) calloc (size, sizeof(Real));\n  aN = (Real *) calloc (size, sizeof(Real));\n\n  \n\n  b = (Real *) calloc (size, sizeof(Real));\n\n  \n\n  temp_red = (Real *) calloc (size_temp, sizeof(Real));\n  temp_black = (Real *) calloc (size_temp, sizeof(Real));\n\n  \n\n  fill_coeffs (NUM, NUM, th_cond, dx, dy, width, TN, aP, aW, aE, aS, aN, b);\n\n  int i;\n  for (i = 0; i < size_temp; ++i) {\n    temp_red[i] = ZERO;\n    temp_black[i] = ZERO;\n  }\n\n  \n\n  printf(\"Problem size: %d x %d \\n\", NUM, NUM);\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  \n\n  sycl::range<2> lws (2, BLOCK_SIZE);\n  sycl::range<2> gws (NUM, NUM / 2);\n\n  \n\n  Real *aP_d, *aW_d, *aE_d, *aS_d, *aN_d, *b_d;\n  Real *temp_red_d;\n  Real *temp_black_d;\n  Real *bl_norm_L2_d;\n\n  \n\n  \n\n  int size_norm = size_temp;\n  bl_norm_L2_d = sycl::malloc_device<Real>(size_norm, q);\n  q.memset(bl_norm_L2_d, 0, size_norm * sizeof(Real));\n\n  aP_d = sycl::malloc_device<Real>(size, q);\n  aW_d = sycl::malloc_device<Real>(size, q);\n  aE_d = sycl::malloc_device<Real>(size, q);\n  aS_d = sycl::malloc_device<Real>(size, q);\n  aN_d = sycl::malloc_device<Real>(size, q);\n  b_d = sycl::malloc_device<Real>(size, q);\n  temp_red_d = sycl::malloc_device<Real>(size_temp, q);\n  temp_black_d = sycl::malloc_device<Real>(size_temp, q);\n\n  \n\n  q.memcpy(aP_d, aP, size * sizeof(Real));\n  q.memcpy(aW_d, aW, size * sizeof(Real));\n  q.memcpy(aE_d, aE, size * sizeof(Real));\n  q.memcpy(aS_d, aS, size * sizeof(Real));\n  q.memcpy(aN_d, aN, size * sizeof(Real));\n  q.memcpy(b_d, b, size * sizeof(Real));\n  q.memcpy(temp_red_d, temp_red, size_temp * sizeof(Real));\n  q.memcpy(temp_black_d, temp_black, size_temp * sizeof(Real));\n\n  q.wait();\n  StartTimer();\n\n  auto policy = oneapi::dpl::execution::device_policy(q);\n\n  \n\n  for (iter = 1; iter <= it_max; ++iter) {\n\n    Real norm_L2 = ZERO;\n\n    q.parallel_for(sycl::nd_range<2>(gws, lws),\n      [=](sycl::nd_item<2> item) {\n      red_kernel(aP_d, aW_d, aE_d, aS_d, aN_d, b_d,\n                 temp_black_d, temp_red_d, bl_norm_L2_d, item);\n    });\n\n    norm_L2 = oneapi::dpl::reduce(policy, bl_norm_L2_d, bl_norm_L2_d + size_norm, Real(0));\n\n    q.parallel_for(sycl::nd_range<2>(gws, lws),\n      [=](sycl::nd_item<2> item) {\n      black_kernel(aP_d, aW_d, aE_d, aS_d, aN_d, b_d,\n                   temp_red_d, temp_black_d, bl_norm_L2_d, item);\n    });\n\n    norm_L2 += oneapi::dpl::reduce(policy, bl_norm_L2_d, bl_norm_L2_d + size_norm, Real(0));\n\n    \n\n    norm_L2 = sqrt(norm_L2 / ((Real)size));\n\n    if (iter % 1000 == 0) printf(\"%5d, %0.6f\\n\", iter, norm_L2);\n\n    \n\n    if (norm_L2 < tol) break;\n  }\n\n  double runtime = GetTimer();\n  printf(\"Total time for %i iterations: %f s\\n\", iter, runtime / 1000.0);\n\n  \n\n  q.memcpy(temp_red, temp_red_d, size_temp * sizeof(Real));\n  q.memcpy(temp_black, temp_black_d, size_temp * sizeof(Real));\n  q.wait();\n\n  \n\n  FILE * pfile;\n  pfile = fopen(\"temperature.dat\", \"w\");\n\n  if (pfile != NULL) {\n    fprintf(pfile, \"#x\\ty\\ttemp(K)\\n\");\n\n    int row, col;\n    for (row = 1; row < NUM + 1; ++row) {\n      for (col = 1; col < NUM + 1; ++col) {\n        Real x_pos = (col - 1) * dx + (dx / 2);\n        Real y_pos = (row - 1) * dy + (dy / 2);\n\n        if ((row + col) % 2 == 0) {\n          \n\n          int ind = col * num_rows + (row + (col % 2)) / 2;\n          fprintf(pfile, \"%f\\t%f\\t%f\\n\", x_pos, y_pos, temp_red[ind]);\n        } else {\n          \n\n          int ind = col * num_rows + (row + ((col + 1) % 2)) / 2;\n          fprintf(pfile, \"%f\\t%f\\t%f\\n\", x_pos, y_pos, temp_black[ind]);\n        }\n      }\n      fprintf(pfile, \"\\n\");\n    }\n  }\n\n  fclose(pfile);\n\n  sycl::free(aP_d, q);\n  sycl::free(aW_d, q);\n  sycl::free(aE_d, q);\n  sycl::free(aS_d, q);\n  sycl::free(aN_d, q);\n  sycl::free(b_d, q);\n  sycl::free(temp_red_d, q);\n  sycl::free(temp_black_d, q);\n  sycl::free(bl_norm_L2_d, q);\n\n  free(aP);\n  free(aW);\n  free(aE);\n  free(aS);\n  free(aN);\n  free(b);\n  free(temp_red);\n  free(temp_black);\n\n  return 0;\n}\n"}}
{"kernel_name": "laplace3d", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n\n\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <math.h>\n#include <algorithm>\n#include <chrono>\n#include <cuda.h>\n#include \"kernel.h\"\n#include \"reference.h\"\n\n\n\n\nvoid reference(int NX, int NY, int NZ, float* h_u1, float* h_u2);\n\nvoid printHelp(void);\n\n\n\n\nint main(int argc, char **argv){\n\n  \n\n  if(argc != 6) {\n    printHelp();\n    return 1;\n  }\n\n  const int NX = atoi(argv[1]);\n  const int NY = atoi(argv[2]);\n  const int NZ = atoi(argv[3]);\n  const int REPEAT = atoi(argv[4]);\n  const int verify = atoi(argv[5]);\n\n  \n\n  if (NX <= 0 || NX % 32 != 0 || NY <= 0 || NZ <= 0 || REPEAT <= 0) return 1;\n\n  printf(\"\\nGrid dimensions: %d x %d x %d\\n\", NX, NY, NZ);\n  printf(\"Result verification %s \\n\", verify ? \"enabled\" : \"disabled\");\n \n  \n\n\n  const size_t grid3D_size = NX * NY * NZ ;\n  const size_t grid3D_bytes = grid3D_size * sizeof(float);\n\n  float *h_u1 = (float *) malloc (grid3D_bytes);\n  float *h_u2 = (float *) malloc (grid3D_bytes);\n  float *h_u3 = (float *) malloc (grid3D_bytes);\n\n  float *d_u1, *d_u2;\n  cudaMalloc((void **)&d_u1, grid3D_bytes);\n  cudaMalloc((void **)&d_u2, grid3D_bytes);\n\n  const int pitch = NX;\n\n  \n\n  int i, j, k;\n    \n  for (k=0; k<NZ; k++) {\n    for (j=0; j<NY; j++) {\n      for (i=0; i<NX; i++) {\n        int ind = i + j*NX + k*NX*NY;\n        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n          h_u1[ind] = 1.0f;           \n\n        else\n          h_u1[ind] = 0.0f;\n      }\n    }\n  }\n\n  \n\n  cudaMemcpy(d_u1, h_u1, grid3D_bytes, cudaMemcpyHostToDevice);\n\n  \n\n  const int bx = 1 + (NX-1)/BLOCK_X;\n  const int by = 1 + (NY-1)/BLOCK_Y;\n\n  dim3 dimGrid(bx,by);\n  dim3 dimBlock(BLOCK_X,BLOCK_Y);\n\n  printf(\"\\ndimGrid  = %d %d %d \\n\", bx, by, 1);\n  printf(\"dimBlock = %d %d %d \\n\", BLOCK_X, BLOCK_Y, 1);\n\n  \n\n  laplace3d<<<dimGrid, dimBlock>>>(NX, NY, NZ, pitch, d_u1, d_u2);\n  cudaDeviceSynchronize();\n\n  \n\n  auto start = std::chrono::steady_clock::now();\n\n  for (i = 1; i <= REPEAT; ++i) {\n    laplace3d<<<dimGrid, dimBlock>>>(NX, NY, NZ, pitch, d_u1, d_u2);\n    std::swap(d_u1, d_u2);\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / REPEAT);\n\n  \n\n  cudaMemcpy(h_u2, d_u1, grid3D_bytes, cudaMemcpyDeviceToHost);\n\n  if (verify) {\n    \n\n    for (i = 1; i <= REPEAT; ++i) {\n      reference(NX, NY, NZ, h_u1, h_u3);\n      std::swap(h_u1, h_u3);\n    }\n\n    \n\n    float err = 0.f;\n    for (k=0; k<NZ; k++) {\n      for (j=0; j<NY; j++) {\n        for (i=0; i<NX; i++) {\n          int ind = i + j*NX + k*NX*NY;\n          err += (h_u1[ind]-h_u2[ind])*(h_u1[ind]-h_u2[ind]);\n        }\n      }\n    }\n    printf(\"\\n rms error = %f \\n\", sqrtf(err/ NX*NY*NZ));\n  }\n\n \n\n  cudaFree(d_u1);\n  cudaFree(d_u2);\n  free(h_u1);\n  free(h_u2);\n  free(h_u3);\n\n  return 0;\n}\n\n\n\nvoid printHelp(void)\n{\n  printf(\"Usage:  laplace3d [OPTION]...\\n\");\n  printf(\"6-point stencil 3D Laplace test \\n\");\n  printf(\"\\n\");\n  printf(\"Example: run 100 iterations on a 256x128x128 grid\\n\");\n  printf(\"./main 256 128 128 100 1\\n\");\n\n  printf(\"\\n\");\n  printf(\"Options:\\n\");\n  printf(\"Grid width\\n\");\n  printf(\"Grid height\\n\");\n  printf(\"Grid depth\\n\");\n  printf(\"Number of repetitions\\n\");\n  printf(\"verify the result\\n\");\n}\n"}}
{"kernel_name": "laplace3d", "parallel_api": "hip", "code": {"main.cu": "\n\n\n\n\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <math.h>\n#include <algorithm>\n#include <chrono>\n#include <hip/hip_runtime.h>\n#include \"kernel.h\"\n#include \"reference.h\"\n\n\n\n\nvoid reference(int NX, int NY, int NZ, float* h_u1, float* h_u2);\n\nvoid printHelp(void);\n\n\n\n\nint main(int argc, char **argv){\n\n  \n\n  if(argc != 6) {\n    printHelp();\n    return 1;\n  }\n\n  const int NX = atoi(argv[1]);\n  const int NY = atoi(argv[2]);\n  const int NZ = atoi(argv[3]);\n  const int REPEAT = atoi(argv[4]);\n  const int verify = atoi(argv[5]);\n\n  \n\n  if (NX <= 0 || NX % 32 != 0 || NY <= 0 || NZ <= 0 || REPEAT <= 0) return 1;\n\n  printf(\"\\nGrid dimensions: %d x %d x %d\\n\", NX, NY, NZ);\n  printf(\"Result verification %s \\n\", verify ? \"enabled\" : \"disabled\");\n \n  \n\n\n  const size_t grid3D_size = NX * NY * NZ ;\n  const size_t grid3D_bytes = grid3D_size * sizeof(float);\n\n  float *h_u1 = (float *) malloc (grid3D_bytes);\n  float *h_u2 = (float *) malloc (grid3D_bytes);\n  float *h_u3 = (float *) malloc (grid3D_bytes);\n\n  float *d_u1, *d_u2;\n  hipMalloc((void **)&d_u1, grid3D_bytes);\n  hipMalloc((void **)&d_u2, grid3D_bytes);\n\n  const int pitch = NX;\n\n  \n\n  int i, j, k;\n    \n  for (k=0; k<NZ; k++) {\n    for (j=0; j<NY; j++) {\n      for (i=0; i<NX; i++) {\n        int ind = i + j*NX + k*NX*NY;\n        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n          h_u1[ind] = 1.0f;           \n\n        else\n          h_u1[ind] = 0.0f;\n      }\n    }\n  }\n\n  \n\n  hipMemcpy(d_u1, h_u1, grid3D_bytes, hipMemcpyHostToDevice);\n\n  \n\n  const int bx = 1 + (NX-1)/BLOCK_X;\n  const int by = 1 + (NY-1)/BLOCK_Y;\n\n  dim3 dimGrid(bx,by);\n  dim3 dimBlock(BLOCK_X,BLOCK_Y);\n\n  printf(\"\\ndimGrid  = %d %d %d \\n\", bx, by, 1);\n  printf(\"dimBlock = %d %d %d \\n\", BLOCK_X, BLOCK_Y, 1);\n\n  \n\n  hipLaunchKernelGGL(laplace3d, dimGrid, dimBlock, 0, 0, NX, NY, NZ, pitch, d_u1, d_u2);\n  hipDeviceSynchronize();\n\n  \n\n  auto start = std::chrono::steady_clock::now();\n\n  for (i = 1; i <= REPEAT; ++i) {\n    hipLaunchKernelGGL(laplace3d, dimGrid, dimBlock, 0, 0, NX, NY, NZ, pitch, d_u1, d_u2);\n    std::swap(d_u1, d_u2);\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / REPEAT);\n\n  \n\n  hipMemcpy(h_u2, d_u1, grid3D_bytes, hipMemcpyDeviceToHost);\n\n  if (verify) {\n    \n\n    for (i = 1; i <= REPEAT; ++i) {\n      reference(NX, NY, NZ, h_u1, h_u3);\n      std::swap(h_u1, h_u3);\n    }\n\n    \n\n    float err = 0.f;\n    for (k=0; k<NZ; k++) {\n      for (j=0; j<NY; j++) {\n        for (i=0; i<NX; i++) {\n          int ind = i + j*NX + k*NX*NY;\n          err += (h_u1[ind]-h_u2[ind])*(h_u1[ind]-h_u2[ind]);\n        }\n      }\n    }\n    printf(\"\\n rms error = %f \\n\", sqrtf(err/ NX*NY*NZ));\n  }\n\n \n\n  hipFree(d_u1);\n  hipFree(d_u2);\n  free(h_u1);\n  free(h_u2);\n  free(h_u3);\n\n  return 0;\n}\n\n\n\nvoid printHelp(void)\n{\n  printf(\"Usage:  laplace3d [OPTION]...\\n\");\n  printf(\"6-point stencil 3D Laplace test \\n\");\n  printf(\"\\n\");\n  printf(\"Example: run 100 iterations on a 256x128x128 grid\\n\");\n  printf(\"./main 256 128 128 100 1\\n\");\n\n  printf(\"\\n\");\n  printf(\"Options:\\n\");\n  printf(\"Grid width\\n\");\n  printf(\"Grid height\\n\");\n  printf(\"Grid depth\\n\");\n  printf(\"Number of repetitions\\n\");\n  printf(\"verify the result\\n\");\n}\n"}}
{"kernel_name": "laplace3d", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n\n\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <math.h>\n#include <algorithm>\n#include <chrono>\n#include <omp.h>\n#include \"kernel.h\"\n#include \"reference.h\"\n\n\n\n\nvoid reference(int NX, int NY, int NZ, float* h_u1, float* h_u2);\n\nvoid printHelp(void);\n\n\n\n\nint main(int argc, char **argv){\n\n  \n\n\n  if(argc != 6) {\n    printHelp();\n    return 1;\n  }\n\n  const int NX = atoi(argv[1]);\n  const int NY = atoi(argv[2]);\n  const int NZ = atoi(argv[3]);\n  const int REPEAT = atoi(argv[4]);\n  const int verify = atoi(argv[5]);\n\n  \n\n  if (NX <= 0 || NX % 32 != 0 || NY <= 0 || NZ <= 0 || REPEAT <= 0) return 1;\n\n  printf(\"\\nGrid dimensions: %d x %d x %d\\n\", NX, NY, NZ);\n  printf(\"Result verification %s \\n\", verify ? \"enabled\" : \"disabled\");\n \n  \n\n\n  const size_t grid3D_size = NX * NY * NZ ;\n  const size_t grid3D_bytes = grid3D_size * sizeof(float);\n\n  float *h_u1 = (float *) malloc (grid3D_bytes);\n  float *h_u2 = (float *) malloc (grid3D_bytes);\n  float *h_u3 = (float *) malloc (grid3D_bytes);\n\n  const int pitch = NX;\n\n  \n\n  int i, j, k;\n    \n  for (k=0; k<NZ; k++) {\n    for (j=0; j<NY; j++) {\n      for (i=0; i<NX; i++) {\n        int ind = i + j*NX + k*NX*NY;\n        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n          h_u2[ind] = h_u1[ind] = 1.0f;           \n\n        else\n          h_u2[ind] = h_u1[ind] = 0.0f;\n      }\n    }\n  }\n\n  #pragma omp target data map (tofrom: h_u1[0:grid3D_size]) \\\n                          map (alloc: h_u2[0:grid3D_size])\n  {\n    \n\n    laplace3d(NX, NY, NZ, pitch, h_u1, h_u2);\n\n    \n\n    auto start = std::chrono::steady_clock::now();\n\n    for (i = 1; i <= REPEAT; ++i) {\n      laplace3d(NX, NY, NZ, pitch, h_u1, h_u2);\n      std::swap(h_u1, h_u2);\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / REPEAT);\n  }\n  \n\n\n  if (verify) {\n    \n\n    for (i = 1; i <= REPEAT; ++i) {\n      reference(NX, NY, NZ, h_u2, h_u3);\n      std::swap(h_u2, h_u3);\n    }\n\n    \n\n    float err = 0.f;\n    for (k=0; k<NZ; k++) {\n      for (j=0; j<NY; j++) {\n        for (i=0; i<NX; i++) {\n          int ind = i + j*NX + k*NX*NY;\n          err += (h_u1[ind]-h_u2[ind])*(h_u1[ind]-h_u2[ind]);\n        }\n      }\n    }\n    printf(\"\\n rms error = %f \\n\",sqrtf(err/ NX*NY*NZ));\n  }\n\n \n\n  free(h_u1);\n  free(h_u2);\n  free(h_u3);\n\n  return 0;\n}\n\n\n\n\nvoid printHelp(void)\n{\n  printf(\"Usage:  laplace3d [OPTION]...\\n\");\n  printf(\"6-point stencil 3D Laplace test \\n\");\n  printf(\"\\n\");\n  printf(\"Example: run 100 iterations on a 256x128x128 grid\\n\");\n  printf(\"./main 256 128 128 100 1\\n\");\n\n  printf(\"\\n\");\n  printf(\"Options:\\n\");\n  printf(\"Grid width\\n\");\n  printf(\"Grid height\\n\");\n  printf(\"Grid depth\\n\");\n  printf(\"Number of repetitions\\n\");\n  printf(\"verify the result\\n\");\n}\n"}}
{"kernel_name": "laplace3d", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n\n\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <math.h>\n#include <algorithm>\n#include <chrono>\n#include \"kernel.h\"\n#include \"reference.h\"\n\n\n\n\nvoid reference(int NX, int NY, int NZ, float* h_u1, float* h_u2);\n\nvoid printHelp(void);\n\n\n\n\nint main(int argc, char **argv){\n\n  \n\n\n  if(argc != 6) {\n    printHelp();\n    return 1;\n  }\n\n  const int NX = atoi(argv[1]);\n  const int NY = atoi(argv[2]);\n  const int NZ = atoi(argv[3]);\n  const int REPEAT = atoi(argv[4]);\n  const int verify = atoi(argv[5]);\n\n  \n\n  if (NX <= 0 || NX % 32 != 0 || NY <= 0 || NZ <= 0 || REPEAT <= 0) return 1;\n\n  printf(\"\\nGrid dimensions: %d x %d x %d\\n\", NX, NY, NZ);\n  printf(\"Result verification %s \\n\", verify ? \"enabled\" : \"disabled\");\n \n  \n\n\n  const size_t grid3D_size = NX * NY * NZ ;\n  const size_t grid3D_bytes = grid3D_size * sizeof(float);\n\n  float *h_u1 = (float *) malloc (grid3D_bytes);\n  float *h_u2 = (float *) malloc (grid3D_bytes);\n  float *h_u3 = (float *) malloc (grid3D_bytes);\n\n  const int pitch = NX;\n\n  \n\n  int i, j, k;\n    \n  for (k=0; k<NZ; k++) {\n    for (j=0; j<NY; j++) {\n      for (i=0; i<NX; i++) {\n        int ind = i + j*NX + k*NX*NY;\n        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n          h_u2[ind] = h_u1[ind] = 1.0f;           \n\n        else\n          h_u2[ind] = h_u1[ind] = 0.0f;\n      }\n    }\n  }\n\n    {\n    \n\n    laplace3d(NX, NY, NZ, pitch, h_u1, h_u2);\n\n    \n\n    auto start = std::chrono::steady_clock::now();\n\n    for (i = 1; i <= REPEAT; ++i) {\n      laplace3d(NX, NY, NZ, pitch, h_u1, h_u2);\n      std::swap(h_u1, h_u2);\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / REPEAT);\n  }\n  \n\n\n  if (verify) {\n    \n\n    for (i = 1; i <= REPEAT; ++i) {\n      reference(NX, NY, NZ, h_u2, h_u3);\n      std::swap(h_u2, h_u3);\n    }\n\n    \n\n    float err = 0.f;\n    for (k=0; k<NZ; k++) {\n      for (j=0; j<NY; j++) {\n        for (i=0; i<NX; i++) {\n          int ind = i + j*NX + k*NX*NY;\n          err += (h_u1[ind]-h_u2[ind])*(h_u1[ind]-h_u2[ind]);\n        }\n      }\n    }\n    printf(\"\\n rms error = %f \\n\",sqrtf(err/ NX*NY*NZ));\n  }\n\n \n\n  free(h_u1);\n  free(h_u2);\n  free(h_u3);\n\n  return 0;\n}\n\n\n\n\nvoid printHelp(void)\n{\n  printf(\"Usage:  laplace3d [OPTION]...\\n\");\n  printf(\"6-point stencil 3D Laplace test \\n\");\n  printf(\"\\n\");\n  printf(\"Example: run 100 iterations on a 256x128x128 grid\\n\");\n  printf(\"./main 256 128 128 100 1\\n\");\n\n  printf(\"\\n\");\n  printf(\"Options:\\n\");\n  printf(\"Grid width\\n\");\n  printf(\"Grid height\\n\");\n  printf(\"Grid depth\\n\");\n  printf(\"Number of repetitions\\n\");\n  printf(\"verify the result\\n\");\n}"}}
{"kernel_name": "laplace3d", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n\n\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <math.h>\n#include <algorithm>\n#include <chrono>\n#include <sycl/sycl.hpp>\n#include \"kernel.h\"\n#include \"reference.h\"\n\n\n\n\nvoid reference(int NX, int NY, int NZ, float* h_u1, float* h_u2);\n\nvoid printHelp(void);\n\n\n\n\nint main(int argc, char **argv){\n\n  \n\n  if(argc != 6) {\n    printHelp();\n    return 1;\n  }\n\n  const int NX = atoi(argv[1]);\n  const int NY = atoi(argv[2]);\n  const int NZ = atoi(argv[3]);\n  const int REPEAT = atoi(argv[4]);\n  const int verify = atoi(argv[5]);\n\n  \n\n  if (NX <= 0 || NX % 32 != 0 || NY <= 0 || NZ <= 0 || REPEAT <= 0) return 1;\n\n  printf(\"\\nGrid dimensions: %d x %d x %d\\n\", NX, NY, NZ);\n  printf(\"Result verification %s \\n\", verify ? \"enabled\" : \"disabled\");\n \n  \n\n\n  const size_t grid3D_size = NX * NY * NZ ;\n  const size_t grid3D_bytes = grid3D_size * sizeof(float);\n\n  float *h_u1 = (float *) malloc (grid3D_bytes);\n  float *h_u2 = (float *) malloc (grid3D_bytes);\n  float *h_u3 = (float *) malloc (grid3D_bytes);\n\n  const int pitch = NX;\n\n  \n\n  int i, j, k;\n    \n  for (k=0; k<NZ; k++) {\n    for (j=0; j<NY; j++) {\n      for (i=0; i<NX; i++) {\n        int ind = i + j*NX + k*NX*NY;\n        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n          h_u1[ind] = 1.0f;           \n\n        else\n          h_u1[ind] = 0.0f;\n      }\n    }\n  }\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  float *d_u1 = sycl::malloc_device<float>(grid3D_size, q);\n  q.memcpy(d_u1, h_u1, grid3D_bytes);\n\n  float *d_u2 = sycl::malloc_device<float>(grid3D_size, q);\n\n  \n\n  const int bx = 1 + (NX-1)/BLOCK_X;\n  const int by = 1 + (NY-1)/BLOCK_Y;\n\n  sycl::range<2> lws (BLOCK_Y, BLOCK_X);\n  sycl::range<2> gws (by * BLOCK_Y, bx * BLOCK_X);\n\n  printf(\"\\nglobal work size  = %d %d %d \\n\", bx * BLOCK_X, by * BLOCK_Y, 1);\n  printf(\"local work size = %d %d %d \\n\", BLOCK_X, BLOCK_Y, 1);\n\n  \n\n  q.submit([&] (sycl::handler &cgh) {\n    sycl::local_accessor<float, 1> sm (3*KOFF, cgh);\n    cgh.parallel_for<class warmup>(\n      sycl::nd_range<2>(gws, lws), [=] (sycl::nd_item<2> item) {\n      laplace3d(item, NX, NY, NZ, pitch,\n                d_u1, d_u2, sm.get_pointer());\n    });\n  }).wait();\n\n  \n\n  auto start = std::chrono::steady_clock::now();\n\n  for (i = 1; i <= REPEAT; ++i) {\n    q.submit([&] (sycl::handler &cgh) {\n      sycl::local_accessor<float, 1> sm (3*KOFF, cgh);\n      cgh.parallel_for<class eval>(\n        sycl::nd_range<2>(gws, lws), [=] (sycl::nd_item<2> item) {\n        laplace3d(item, NX, NY, NZ, pitch,\n                  d_u1, d_u2, sm.get_pointer());\n      });\n    });\n    std::swap(d_u1, d_u2);\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / REPEAT);\n\n  q.memcpy(h_u2, d_u1, grid3D_bytes).wait();\n\n  if (verify) {\n    \n\n    for (i = 1; i <= REPEAT; ++i) {\n      reference(NX, NY, NZ, h_u1, h_u3);\n      std::swap(h_u1, h_u3);\n    }\n\n    \n\n    float err = 0.f;\n    for (k=0; k<NZ; k++) {\n      for (j=0; j<NY; j++) {\n        for (i=0; i<NX; i++) {\n          int ind = i + j*NX + k*NX*NY;\n          err += (h_u1[ind]-h_u2[ind])*(h_u1[ind]-h_u2[ind]);\n        }\n      }\n    }\n    printf(\"\\n rms error = %f \\n\",sqrtf(err/ NX*NY*NZ));\n  }\n\n \n\n  sycl::free(d_u1, q);\n  sycl::free(d_u2, q);\n  free(h_u1);\n  free(h_u2);\n  free(h_u3);\n\n  return 0;\n}\n\n\n\n\nvoid printHelp(void)\n{\n  printf(\"Usage:  laplace3d [OPTION]...\\n\");\n  printf(\"6-point stencil 3D Laplace test \\n\");\n  printf(\"\\n\");\n  printf(\"Example: run 100 iterations on a 256x128x128 grid\\n\");\n  printf(\"./main 256 128 128 100 1\\n\");\n\n  printf(\"\\n\");\n  printf(\"Options:\\n\");\n  printf(\"Grid width\\n\");\n  printf(\"Grid height\\n\");\n  printf(\"Grid depth\\n\");\n  printf(\"Number of repetitions\\n\");\n  printf(\"verify the result\\n\");\n}\n"}}
{"kernel_name": "lavaMD", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <stdbool.h>\n#include <string.h>\n#include <cuda.h>\n#include \"./util/timer/timer.h\"\n#include \"./util/num/num.h\"\n#include \"./main.h\"\n#include \"kernel.h\"\n\nint main(  int argc, char *argv [])\n{\n  \n\n  int i, j, k, l, m, n;\n\n  \n\n  par_str par_cpu;\n  dim_str dim_cpu;\n  box_str* box_cpu;\n  FOUR_VECTOR* rv_cpu;\n  fp* qv_cpu;\n  FOUR_VECTOR* fv_cpu;\n  int nh;\n\n  printf(\"WG size of kernel = %d \\n\", NUMBER_THREADS);\n\n  \n\n  dim_cpu.arch_arg = 0;\n  dim_cpu.cores_arg = 1;\n  dim_cpu.boxes1d_arg = 1;\n\n  \n\n  if(argc==3){\n    for(dim_cpu.cur_arg=1; dim_cpu.cur_arg<argc; dim_cpu.cur_arg++){\n      \n\n      if(strcmp(argv[dim_cpu.cur_arg], \"-boxes1d\")==0){\n        \n\n        if(argc>=dim_cpu.cur_arg+1){\n          \n\n          if(isInteger(argv[dim_cpu.cur_arg+1])==1){\n            dim_cpu.boxes1d_arg = atoi(argv[dim_cpu.cur_arg+1]);\n            if(dim_cpu.boxes1d_arg<0){\n              printf(\"ERROR: Wrong value to -boxes1d argument, cannot be <=0\\n\");\n              return 0;\n            }\n            dim_cpu.cur_arg = dim_cpu.cur_arg+1;\n          }\n          \n\n          else{\n            printf(\"ERROR: Value to -boxes1d argument in not a number\\n\");\n            return 0;\n          }\n        }\n        \n\n        else{\n          printf(\"ERROR: Missing value to -boxes1d argument\\n\");\n          return 0;\n        }\n      }\n      \n\n      else{\n        printf(\"ERROR: Unknown argument\\n\");\n        return 0;\n      }\n    }\n    \n\n    printf(\"Configuration used: arch = %d, cores = %d, boxes1d = %d\\n\", dim_cpu.arch_arg, dim_cpu.cores_arg, dim_cpu.boxes1d_arg);\n  }\n  else{\n    printf(\"Provide boxes1d argument, example: -boxes1d 16\");\n    return 0;\n  }\n\n  par_cpu.alpha = 0.5;\n\n  \n\n  dim_cpu.number_boxes = dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg; \n\n  \n\n  dim_cpu.space_elem = dim_cpu.number_boxes * NUMBER_PAR_PER_BOX;\n  dim_cpu.space_mem = dim_cpu.space_elem * sizeof(FOUR_VECTOR);\n  dim_cpu.space_mem2 = dim_cpu.space_elem * sizeof(fp);\n\n  \n\n  dim_cpu.box_mem = dim_cpu.number_boxes * sizeof(box_str);\n\n  \n\n  box_cpu = (box_str*)malloc(dim_cpu.box_mem);\n\n  \n\n  nh = 0;\n\n  \n\n  for(i=0; i<dim_cpu.boxes1d_arg; i++){\n    \n\n    for(j=0; j<dim_cpu.boxes1d_arg; j++){\n      \n\n      for(k=0; k<dim_cpu.boxes1d_arg; k++){\n\n        \n\n        box_cpu[nh].x = k;\n        box_cpu[nh].y = j;\n        box_cpu[nh].z = i;\n        box_cpu[nh].number = nh;\n        box_cpu[nh].offset = nh * NUMBER_PAR_PER_BOX;\n\n        \n\n        box_cpu[nh].nn = 0;\n\n        \n\n        for(l=-1; l<2; l++){\n          \n\n          for(m=-1; m<2; m++){\n            \n\n            for(n=-1; n<2; n++){\n\n              \n\n              if(    (((i+l)>=0 && (j+m)>=0 && (k+n)>=0)==true && ((i+l)<dim_cpu.boxes1d_arg && (j+m)<dim_cpu.boxes1d_arg && (k+n)<dim_cpu.boxes1d_arg)==true)  &&\n                  (l==0 && m==0 && n==0)==false  ){\n\n                \n\n                box_cpu[nh].nei[box_cpu[nh].nn].x = (k+n);\n                box_cpu[nh].nei[box_cpu[nh].nn].y = (j+m);\n                box_cpu[nh].nei[box_cpu[nh].nn].z = (i+l);\n                box_cpu[nh].nei[box_cpu[nh].nn].number =  (box_cpu[nh].nei[box_cpu[nh].nn].z * dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg) + \n                  (box_cpu[nh].nei[box_cpu[nh].nn].y * dim_cpu.boxes1d_arg) + \n                  box_cpu[nh].nei[box_cpu[nh].nn].x;\n                box_cpu[nh].nei[box_cpu[nh].nn].offset = box_cpu[nh].nei[box_cpu[nh].nn].number * NUMBER_PAR_PER_BOX;\n\n                \n\n                box_cpu[nh].nn = box_cpu[nh].nn + 1;\n\n              }\n\n            } \n\n          } \n\n        } \n\n\n        \n\n        nh = nh + 1;\n\n      } \n\n    } \n\n  } \n\n\n  \n\n\n  \n\n  srand(2);\n\n  \n\n  rv_cpu = (FOUR_VECTOR*)malloc(dim_cpu.space_mem);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    rv_cpu[i].v = (rand()%10 + 1) / 10.0;      \n\n    rv_cpu[i].x = (rand()%10 + 1) / 10.0;      \n\n    rv_cpu[i].y = (rand()%10 + 1) / 10.0;      \n\n    rv_cpu[i].z = (rand()%10 + 1) / 10.0;      \n\n  }\n\n  \n\n  qv_cpu = (fp*)malloc(dim_cpu.space_mem2);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    qv_cpu[i] = (rand()%10 + 1) / 10.0;      \n\n  }\n\n  \n\n  fv_cpu = (FOUR_VECTOR*)malloc(dim_cpu.space_mem);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    fv_cpu[i].v = 0;                \n\n    fv_cpu[i].x = 0;                \n\n    fv_cpu[i].y = 0;                \n\n    fv_cpu[i].z = 0;                \n\n  }\n\n  long long start = get_time();\n\n  int dim_cpu_number_boxes = dim_cpu.number_boxes;\n\n  dim_cpu.space_elem = dim_cpu.number_boxes * NUMBER_PAR_PER_BOX;\n  dim_cpu.space_mem = dim_cpu.space_elem * sizeof(FOUR_VECTOR);\n  dim_cpu.space_mem2 = dim_cpu.space_elem * sizeof(fp);\n\n  box_str* d_box_gpu;\n  FOUR_VECTOR* d_rv_gpu;\n  fp* d_qv_gpu;\n  FOUR_VECTOR* d_fv_gpu;\n  \n  cudaMalloc ((void**)&d_box_gpu, dim_cpu.box_mem);\n  cudaMalloc ((void**)&d_rv_gpu, dim_cpu.space_mem);\n  cudaMalloc ((void**)&d_qv_gpu, dim_cpu.space_mem2);\n  cudaMalloc ((void**)&d_fv_gpu, dim_cpu.space_mem);\n\n  cudaMemcpy(d_box_gpu, box_cpu, dim_cpu.box_mem, cudaMemcpyHostToDevice); \n  cudaMemcpy(d_rv_gpu, rv_cpu, dim_cpu.space_mem, cudaMemcpyHostToDevice); \n  cudaMemcpy(d_qv_gpu, qv_cpu, dim_cpu.space_mem2, cudaMemcpyHostToDevice); \n  cudaMemcpy(d_fv_gpu, fv_cpu, dim_cpu.space_mem, cudaMemcpyHostToDevice); \n\n  cudaDeviceSynchronize();\n  long long kstart = get_time();\n\n  md<<<dim_cpu_number_boxes, NUMBER_THREADS>>>(\n    d_box_gpu, d_rv_gpu, d_qv_gpu, d_fv_gpu, par_cpu.alpha, dim_cpu_number_boxes);\n\n  cudaDeviceSynchronize();\n  long long kend = get_time();\n\n  cudaMemcpy(fv_cpu, d_fv_gpu, dim_cpu.space_mem, cudaMemcpyDeviceToHost); \n\n  cudaFree(d_box_gpu);\n  cudaFree(d_rv_gpu);\n  cudaFree(d_qv_gpu);\n  cudaFree(d_fv_gpu);\n\n  long long end = get_time();\n  printf(\"Device offloading time:\\n\"); \n  printf(\"%.12f s\\n\", (float) (end-start) / 1000000); \n\n  printf(\"Kernel execution time:\\n\"); \n  printf(\"%.12f s\\n\", (float) (kend-kstart) / 1000000); \n\n  \n\n#ifdef OUTPUT\n  FILE *fptr;\n  fptr = fopen(\"result.txt\", \"w\");  \n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    fprintf(fptr, \"%f, %f, %f, %f\\n\", fv_cpu[i].v, fv_cpu[i].x, fv_cpu[i].y, fv_cpu[i].z);\n  }\n  fclose(fptr);\n#endif         \n\n  free(rv_cpu);\n  free(qv_cpu);\n  free(fv_cpu);\n  free(box_cpu);\n\n  return 0; \n}\n"}}
{"kernel_name": "lavaMD", "parallel_api": "hip", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <stdbool.h>\n#include <string.h>\n#include <hip/hip_runtime.h>\n#include \"./util/timer/timer.h\"\n#include \"./util/num/num.h\"\n#include \"./main.h\"\n#include \"kernel.h\"\n\nint main(  int argc, char *argv [])\n{\n  \n\n  int i, j, k, l, m, n;\n\n  \n\n  par_str par_cpu;\n  dim_str dim_cpu;\n  box_str* box_cpu;\n  FOUR_VECTOR* rv_cpu;\n  fp* qv_cpu;\n  FOUR_VECTOR* fv_cpu;\n  int nh;\n\n  printf(\"WG size of kernel = %d \\n\", NUMBER_THREADS);\n\n  \n\n  dim_cpu.arch_arg = 0;\n  dim_cpu.cores_arg = 1;\n  dim_cpu.boxes1d_arg = 1;\n\n  \n\n  if(argc==3){\n    for(dim_cpu.cur_arg=1; dim_cpu.cur_arg<argc; dim_cpu.cur_arg++){\n      \n\n      if(strcmp(argv[dim_cpu.cur_arg], \"-boxes1d\")==0){\n        \n\n        if(argc>=dim_cpu.cur_arg+1){\n          \n\n          if(isInteger(argv[dim_cpu.cur_arg+1])==1){\n            dim_cpu.boxes1d_arg = atoi(argv[dim_cpu.cur_arg+1]);\n            if(dim_cpu.boxes1d_arg<0){\n              printf(\"ERROR: Wrong value to -boxes1d argument, cannot be <=0\\n\");\n              return 0;\n            }\n            dim_cpu.cur_arg = dim_cpu.cur_arg+1;\n          }\n          \n\n          else{\n            printf(\"ERROR: Value to -boxes1d argument in not a number\\n\");\n            return 0;\n          }\n        }\n        \n\n        else{\n          printf(\"ERROR: Missing value to -boxes1d argument\\n\");\n          return 0;\n        }\n      }\n      \n\n      else{\n        printf(\"ERROR: Unknown argument\\n\");\n        return 0;\n      }\n    }\n    \n\n    printf(\"Configuration used: arch = %d, cores = %d, boxes1d = %d\\n\", dim_cpu.arch_arg, dim_cpu.cores_arg, dim_cpu.boxes1d_arg);\n  }\n  else{\n    printf(\"Provide boxes1d argument, example: -boxes1d 16\");\n    return 0;\n  }\n\n  par_cpu.alpha = 0.5;\n\n  \n\n  dim_cpu.number_boxes = dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg; \n\n  \n\n  dim_cpu.space_elem = dim_cpu.number_boxes * NUMBER_PAR_PER_BOX;\n  dim_cpu.space_mem = dim_cpu.space_elem * sizeof(FOUR_VECTOR);\n  dim_cpu.space_mem2 = dim_cpu.space_elem * sizeof(fp);\n\n  \n\n  dim_cpu.box_mem = dim_cpu.number_boxes * sizeof(box_str);\n\n  \n\n  box_cpu = (box_str*)malloc(dim_cpu.box_mem);\n\n  \n\n  nh = 0;\n\n  \n\n  for(i=0; i<dim_cpu.boxes1d_arg; i++){\n    \n\n    for(j=0; j<dim_cpu.boxes1d_arg; j++){\n      \n\n      for(k=0; k<dim_cpu.boxes1d_arg; k++){\n\n        \n\n        box_cpu[nh].x = k;\n        box_cpu[nh].y = j;\n        box_cpu[nh].z = i;\n        box_cpu[nh].number = nh;\n        box_cpu[nh].offset = nh * NUMBER_PAR_PER_BOX;\n\n        \n\n        box_cpu[nh].nn = 0;\n\n        \n\n        for(l=-1; l<2; l++){\n          \n\n          for(m=-1; m<2; m++){\n            \n\n            for(n=-1; n<2; n++){\n\n              \n\n              if(    (((i+l)>=0 && (j+m)>=0 && (k+n)>=0)==true && ((i+l)<dim_cpu.boxes1d_arg && (j+m)<dim_cpu.boxes1d_arg && (k+n)<dim_cpu.boxes1d_arg)==true)  &&\n                  (l==0 && m==0 && n==0)==false  ){\n\n                \n\n                box_cpu[nh].nei[box_cpu[nh].nn].x = (k+n);\n                box_cpu[nh].nei[box_cpu[nh].nn].y = (j+m);\n                box_cpu[nh].nei[box_cpu[nh].nn].z = (i+l);\n                box_cpu[nh].nei[box_cpu[nh].nn].number =  (box_cpu[nh].nei[box_cpu[nh].nn].z * dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg) + \n                  (box_cpu[nh].nei[box_cpu[nh].nn].y * dim_cpu.boxes1d_arg) + \n                  box_cpu[nh].nei[box_cpu[nh].nn].x;\n                box_cpu[nh].nei[box_cpu[nh].nn].offset = box_cpu[nh].nei[box_cpu[nh].nn].number * NUMBER_PAR_PER_BOX;\n\n                \n\n                box_cpu[nh].nn = box_cpu[nh].nn + 1;\n\n              }\n\n            } \n\n          } \n\n        } \n\n\n        \n\n        nh = nh + 1;\n\n      } \n\n    } \n\n  } \n\n\n  \n\n\n  \n\n  srand(2);\n\n  \n\n  rv_cpu = (FOUR_VECTOR*)malloc(dim_cpu.space_mem);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    rv_cpu[i].v = (rand()%10 + 1) / 10.0;      \n\n    rv_cpu[i].x = (rand()%10 + 1) / 10.0;      \n\n    rv_cpu[i].y = (rand()%10 + 1) / 10.0;      \n\n    rv_cpu[i].z = (rand()%10 + 1) / 10.0;      \n\n  }\n\n  \n\n  qv_cpu = (fp*)malloc(dim_cpu.space_mem2);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    qv_cpu[i] = (rand()%10 + 1) / 10.0;      \n\n  }\n\n  \n\n  fv_cpu = (FOUR_VECTOR*)malloc(dim_cpu.space_mem);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    fv_cpu[i].v = 0;                \n\n    fv_cpu[i].x = 0;                \n\n    fv_cpu[i].y = 0;                \n\n    fv_cpu[i].z = 0;                \n\n  }\n\n  long long start = get_time();\n\n  int dim_cpu_number_boxes = dim_cpu.number_boxes;\n\n  dim_cpu.space_elem = dim_cpu.number_boxes * NUMBER_PAR_PER_BOX;\n  dim_cpu.space_mem = dim_cpu.space_elem * sizeof(FOUR_VECTOR);\n  dim_cpu.space_mem2 = dim_cpu.space_elem * sizeof(fp);\n\n  box_str* d_box_gpu;\n  FOUR_VECTOR* d_rv_gpu;\n  fp* d_qv_gpu;\n  FOUR_VECTOR* d_fv_gpu;\n  \n  hipMalloc ((void**)&d_box_gpu, dim_cpu.box_mem);\n  hipMalloc ((void**)&d_rv_gpu, dim_cpu.space_mem);\n  hipMalloc ((void**)&d_qv_gpu, dim_cpu.space_mem2);\n  hipMalloc ((void**)&d_fv_gpu, dim_cpu.space_mem);\n\n  hipMemcpy(d_box_gpu, box_cpu, dim_cpu.box_mem, hipMemcpyHostToDevice); \n  hipMemcpy(d_rv_gpu, rv_cpu, dim_cpu.space_mem, hipMemcpyHostToDevice); \n  hipMemcpy(d_qv_gpu, qv_cpu, dim_cpu.space_mem2, hipMemcpyHostToDevice); \n  hipMemcpy(d_fv_gpu, fv_cpu, dim_cpu.space_mem, hipMemcpyHostToDevice); \n\n  hipDeviceSynchronize();\n  long long kstart = get_time();\n\n  hipLaunchKernelGGL(md, dim_cpu_number_boxes, NUMBER_THREADS, 0, 0, \n    d_box_gpu, d_rv_gpu, d_qv_gpu, d_fv_gpu, par_cpu.alpha, dim_cpu_number_boxes);\n\n  hipDeviceSynchronize();\n  long long kend = get_time();\n\n  hipMemcpy(fv_cpu, d_fv_gpu, dim_cpu.space_mem, hipMemcpyDeviceToHost); \n\n  hipFree(d_box_gpu);\n  hipFree(d_rv_gpu);\n  hipFree(d_qv_gpu);\n  hipFree(d_fv_gpu);\n\n  long long end = get_time();\n  printf(\"Device offloading time:\\n\"); \n  printf(\"%.12f s\\n\", (float) (end-start) / 1000000); \n\n  printf(\"Kernel execution time:\\n\"); \n  printf(\"%.12f s\\n\", (float) (kend-kstart) / 1000000); \n\n  \n\n#ifdef OUTPUT\n  FILE *fptr;\n  fptr = fopen(\"result.txt\", \"w\");  \n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    fprintf(fptr, \"%f, %f, %f, %f\\n\", fv_cpu[i].v, fv_cpu[i].x, fv_cpu[i].y, fv_cpu[i].z);\n  }\n  fclose(fptr);\n#endif         \n\n  free(rv_cpu);\n  free(qv_cpu);\n  free(fv_cpu);\n  free(box_cpu);\n\n  return 0; \n}\n"}}
{"kernel_name": "lavaMD", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <stdbool.h>\n#include <string.h>\n#include <omp.h>\n#include <math.h>\n#include \"./util/timer/timer.h\"\n#include \"./util/num/num.h\"\n#include \"main.h\"\n\nint main(int argc, char *argv [])\n{\n  \n\n  int i, j, k, l, m, n;\n\n  \n\n  par_str par_cpu;\n  dim_str dim_cpu;\n  box_str* box_cpu;\n  FOUR_VECTOR* rv_cpu;\n  fp* qv_cpu;\n  FOUR_VECTOR* fv_cpu;\n  int nh;\n\n  printf(\"WG size of kernel = %d \\n\", NUMBER_THREADS);\n\n  \n\n  dim_cpu.arch_arg = 0;\n  dim_cpu.cores_arg = 1;\n  dim_cpu.boxes1d_arg = 1;\n\n  \n\n  if(argc==3){\n    for(dim_cpu.cur_arg=1; dim_cpu.cur_arg<argc; dim_cpu.cur_arg++){\n      \n\n      if(strcmp(argv[dim_cpu.cur_arg], \"-boxes1d\")==0){\n        \n\n        if(argc>=dim_cpu.cur_arg+1){\n          \n\n          if(isInteger(argv[dim_cpu.cur_arg+1])==1){\n            dim_cpu.boxes1d_arg = atoi(argv[dim_cpu.cur_arg+1]);\n            if(dim_cpu.boxes1d_arg<0){\n              printf(\"ERROR: Wrong value to -boxes1d argument, cannot be <=0\\n\");\n              return 0;\n            }\n            dim_cpu.cur_arg = dim_cpu.cur_arg+1;\n          }\n          \n\n          else{\n            printf(\"ERROR: Value to -boxes1d argument in not a number\\n\");\n            return 0;\n          }\n        }\n        \n\n        else{\n          printf(\"ERROR: Missing value to -boxes1d argument\\n\");\n          return 0;\n        }\n      }\n      \n\n      else{\n        printf(\"ERROR: Unknown argument\\n\");\n        return 0;\n      }\n    }\n    \n\n    printf(\"Configuration used: arch = %d, cores = %d, boxes1d = %d\\n\", dim_cpu.arch_arg, dim_cpu.cores_arg, dim_cpu.boxes1d_arg);\n  }\n  else{\n    printf(\"Provide boxes1d argument, example: -boxes1d 16\");\n    return 0;\n  }\n\n  par_cpu.alpha = 0.5;\n\n  \n\n  dim_cpu.number_boxes = dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg; \n\n\n  \n\n  dim_cpu.space_elem = dim_cpu.number_boxes * NUMBER_PAR_PER_BOX;              \n\n  dim_cpu.space_mem = dim_cpu.space_elem * sizeof(FOUR_VECTOR);\n  dim_cpu.space_mem2 = dim_cpu.space_elem * sizeof(fp);\n\n  \n\n  dim_cpu.box_mem = dim_cpu.number_boxes * sizeof(box_str);\n\n  \n\n  box_cpu = (box_str*)malloc(dim_cpu.box_mem);\n\n  \n\n  nh = 0;\n\n  \n\n  for(i=0; i<dim_cpu.boxes1d_arg; i++){\n    \n\n    for(j=0; j<dim_cpu.boxes1d_arg; j++){\n      \n\n      for(k=0; k<dim_cpu.boxes1d_arg; k++){\n\n        \n\n        box_cpu[nh].x = k;\n        box_cpu[nh].y = j;\n        box_cpu[nh].z = i;\n        box_cpu[nh].number = nh;\n        box_cpu[nh].offset = nh * NUMBER_PAR_PER_BOX;\n\n        \n\n        box_cpu[nh].nn = 0;\n\n        \n\n        for(l=-1; l<2; l++){\n          \n\n          for(m=-1; m<2; m++){\n            \n\n            for(n=-1; n<2; n++){\n\n              \n\n              if(    (((i+l)>=0 && (j+m)>=0 && (k+n)>=0)==true && ((i+l)<dim_cpu.boxes1d_arg && (j+m)<dim_cpu.boxes1d_arg && (k+n)<dim_cpu.boxes1d_arg)==true)  &&\n                  (l==0 && m==0 && n==0)==false  ){\n\n                \n\n                box_cpu[nh].nei[box_cpu[nh].nn].x = (k+n);\n                box_cpu[nh].nei[box_cpu[nh].nn].y = (j+m);\n                box_cpu[nh].nei[box_cpu[nh].nn].z = (i+l);\n                box_cpu[nh].nei[box_cpu[nh].nn].number =  (box_cpu[nh].nei[box_cpu[nh].nn].z * dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg) + \n                  (box_cpu[nh].nei[box_cpu[nh].nn].y * dim_cpu.boxes1d_arg) + \n                  box_cpu[nh].nei[box_cpu[nh].nn].x;\n                box_cpu[nh].nei[box_cpu[nh].nn].offset = box_cpu[nh].nei[box_cpu[nh].nn].number * NUMBER_PAR_PER_BOX;\n\n                \n\n                box_cpu[nh].nn = box_cpu[nh].nn + 1;\n\n              }\n\n            } \n\n          } \n\n        } \n\n\n        \n\n        nh = nh + 1;\n\n      } \n\n    } \n\n  } \n\n\n  \n\n  \n\n  \n\n\n  \n\n  srand(2);\n\n  \n\n  rv_cpu = (FOUR_VECTOR*)malloc(dim_cpu.space_mem);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    rv_cpu[i].v = (rand()%10 + 1) / 10.0;      \n\n    \n\n    rv_cpu[i].x = (rand()%10 + 1) / 10.0;      \n\n    \n\n    rv_cpu[i].y = (rand()%10 + 1) / 10.0;      \n\n    \n\n    rv_cpu[i].z = (rand()%10 + 1) / 10.0;      \n\n    \n\n  }\n\n  \n\n  qv_cpu = (fp*)malloc(dim_cpu.space_mem2);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    qv_cpu[i] = (rand()%10 + 1) / 10.0;      \n\n    \n\n  }\n\n  \n\n  fv_cpu = (FOUR_VECTOR*)malloc(dim_cpu.space_mem);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    fv_cpu[i].v = 0;                \n\n    fv_cpu[i].x = 0;                \n\n    fv_cpu[i].y = 0;                \n\n    fv_cpu[i].z = 0;                \n\n  }\n\n  long long kstart, kend;\n  long long start = get_time();\n\n  \n\n  int dim_cpu_number_boxes = dim_cpu.number_boxes;\n\n#pragma omp target data map(to: box_cpu[0:dim_cpu.number_boxes], \\\n                                rv_cpu[0:dim_cpu.space_elem], \\\n                                qv_cpu[0:dim_cpu.space_elem]) \\\n                        map(tofrom: fv_cpu[0:dim_cpu.space_elem])\n{\n  kstart = get_time();\n  #pragma omp target teams num_teams(dim_cpu_number_boxes) thread_limit(NUMBER_THREADS)\n  {\n    FOUR_VECTOR rA_shared[100];\n    FOUR_VECTOR rB_shared[100];\n    fp qB_shared[100];\n\n    #pragma omp parallel\n    {\n      int bx = omp_get_team_num();\n      int tx = omp_get_thread_num();\n      int wtx = tx;\n\n      \n\n\n      if(bx<dim_cpu_number_boxes){\n\n        \n\n\n        \n\n        fp a2 = 2*par_cpu.alpha*par_cpu.alpha;\n\n        \n\n        int first_i;\n        \n\n\n        \n\n        int pointer;\n        int k = 0;\n        int first_j;\n        int j = 0;\n        \n\n\n        \n\n        fp r2;\n        fp u2;\n        fp vij;\n        fp fs;\n        fp fxij;\n        fp fyij;\n        fp fzij;\n        THREE_VECTOR d;\n\n        \n\n\n        \n\n\n        \n\n        first_i = box_cpu[bx].offset;\n\n        \n\n\n        \n\n        \n\n        while(wtx<NUMBER_PAR_PER_BOX){\n          rA_shared[wtx] = rv_cpu[first_i+wtx];\n          wtx = wtx + NUMBER_THREADS;\n        }\n        wtx = tx;\n\n        \n\n        \n\n#pragma omp barrier\n\n        \n\n\n        \n\n        for (k=0; k<(1+box_cpu[bx].nn); k++){\n\n          \n\n          \n\n          \n\n\n          if(k==0){\n            pointer = bx;                          \n\n          }\n          else{\n            pointer = box_cpu[bx].nei[k-1].number;              \n\n          }\n\n          \n\n\n          \n\n          first_j = box_cpu[pointer].offset;\n\n          \n\n          \n\n          while(wtx<NUMBER_PAR_PER_BOX){\n            rB_shared[wtx] = rv_cpu[first_j+wtx];\n            qB_shared[wtx] = qv_cpu[first_j+wtx];\n            wtx = wtx + NUMBER_THREADS;\n          }\n          wtx = tx;\n\n          \n\n          \n\n#pragma omp barrier\n\n          \n\n\n          \n\n          while(wtx<NUMBER_PAR_PER_BOX){\n\n            \n\n            for (j=0; j<NUMBER_PAR_PER_BOX; j++){\n\n              r2 = rA_shared[wtx].v + rB_shared[j].v - DOT(rA_shared[wtx],rB_shared[j]); \n              u2 = a2*r2;\n              vij= exp(-u2);\n              fs = 2*vij;\n              d.x = rA_shared[wtx].x  - rB_shared[j].x;\n              fxij=fs*d.x;\n              d.y = rA_shared[wtx].y  - rB_shared[j].y;\n              fyij=fs*d.y;\n              d.z = rA_shared[wtx].z  - rB_shared[j].z;\n              fzij=fs*d.z;\n              fv_cpu[first_i+wtx].v +=  qB_shared[j]*vij;\n              fv_cpu[first_i+wtx].x +=  qB_shared[j]*fxij;\n              fv_cpu[first_i+wtx].y +=  qB_shared[j]*fyij;\n              fv_cpu[first_i+wtx].z +=  qB_shared[j]*fzij;\n\n            }\n\n            \n\n            wtx = wtx + NUMBER_THREADS;\n\n          }\n\n          \n\n          wtx = tx;\n\n          \n\n#pragma omp barrier\n\n        }\n      }\n    }\n  }\n  kend = get_time();\n}\n\n  long long end = get_time();\n  printf(\"Device offloading time:\\n\"); \n  printf(\"%.12f s\\n\", (float) (end-start) / 1000000);\n\n  printf(\"Kernel execution time:\\n\"); \n  printf(\"%.12f s\\n\", (float) (kend-kstart) / 1000000); \n\n#ifdef DEBUG\n  int offset = 395;\n  for(int g=0; g<10; g++){\n    printf(\"g=%d %f, %f, %f, %f\\n\", \\\n        g, fv_cpu[offset+g].v, fv_cpu[offset+g].x, fv_cpu[offset+g].y, fv_cpu[offset+g].z);\n  }\n#endif\n\n  \n\n#ifdef OUTPUT\n  FILE *fptr;\n  fptr = fopen(\"result.txt\", \"w\");  \n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    fprintf(fptr, \"%f, %f, %f, %f\\n\", fv_cpu[i].v, fv_cpu[i].x, fv_cpu[i].y, fv_cpu[i].z);\n  }\n  fclose(fptr);\n#endif         \n\n  free(rv_cpu);\n  free(qv_cpu);\n  free(fv_cpu);\n  free(box_cpu);\n\n  return 0;\n}\n\n"}}
{"kernel_name": "lavaMD", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <stdbool.h>\n#include <string.h>\n#include <math.h>\n#include \"./util/timer/timer.h\"\n#include \"./util/num/num.h\"\n#include \"main.h\"\n\nint main(int argc, char *argv [])\n{\n  \n\n  int i, j, k, l, m, n;\n\n  \n\n  par_str par_cpu;\n  dim_str dim_cpu;\n  box_str* box_cpu;\n  FOUR_VECTOR* rv_cpu;\n  fp* qv_cpu;\n  FOUR_VECTOR* fv_cpu;\n  int nh;\n\n  printf(\"WG size of kernel = %d \\n\", NUMBER_THREADS);\n\n  \n\n  dim_cpu.arch_arg = 0;\n  dim_cpu.cores_arg = 1;\n  dim_cpu.boxes1d_arg = 1;\n\n  \n\n  if(argc==3){\n    for(dim_cpu.cur_arg=1; dim_cpu.cur_arg<argc; dim_cpu.cur_arg++){\n      \n\n      if(strcmp(argv[dim_cpu.cur_arg], \"-boxes1d\")==0){\n        \n\n        if(argc>=dim_cpu.cur_arg+1){\n          \n\n          if(isInteger(argv[dim_cpu.cur_arg+1])==1){\n            dim_cpu.boxes1d_arg = atoi(argv[dim_cpu.cur_arg+1]);\n            if(dim_cpu.boxes1d_arg<0){\n              printf(\"ERROR: Wrong value to -boxes1d argument, cannot be <=0\\n\");\n              return 0;\n            }\n            dim_cpu.cur_arg = dim_cpu.cur_arg+1;\n          }\n          \n\n          else{\n            printf(\"ERROR: Value to -boxes1d argument in not a number\\n\");\n            return 0;\n          }\n        }\n        \n\n        else{\n          printf(\"ERROR: Missing value to -boxes1d argument\\n\");\n          return 0;\n        }\n      }\n      \n\n      else{\n        printf(\"ERROR: Unknown argument\\n\");\n        return 0;\n      }\n    }\n    \n\n    printf(\"Configuration used: arch = %d, cores = %d, boxes1d = %d\\n\", dim_cpu.arch_arg, dim_cpu.cores_arg, dim_cpu.boxes1d_arg);\n  }\n  else{\n    printf(\"Provide boxes1d argument, example: -boxes1d 16\");\n    return 0;\n  }\n\n  par_cpu.alpha = 0.5;\n\n  \n\n  dim_cpu.number_boxes = dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg; \n\n\n  \n\n  dim_cpu.space_elem = dim_cpu.number_boxes * NUMBER_PAR_PER_BOX;              \n\n  dim_cpu.space_mem = dim_cpu.space_elem * sizeof(FOUR_VECTOR);\n  dim_cpu.space_mem2 = dim_cpu.space_elem * sizeof(fp);\n\n  \n\n  dim_cpu.box_mem = dim_cpu.number_boxes * sizeof(box_str);\n\n  \n\n  box_cpu = (box_str*)malloc(dim_cpu.box_mem);\n\n  \n\n  nh = 0;\n\n  \n\n  for(i=0; i<dim_cpu.boxes1d_arg; i++){\n    \n\n    for(j=0; j<dim_cpu.boxes1d_arg; j++){\n      \n\n      for(k=0; k<dim_cpu.boxes1d_arg; k++){\n\n        \n\n        box_cpu[nh].x = k;\n        box_cpu[nh].y = j;\n        box_cpu[nh].z = i;\n        box_cpu[nh].number = nh;\n        box_cpu[nh].offset = nh * NUMBER_PAR_PER_BOX;\n\n        \n\n        box_cpu[nh].nn = 0;\n\n        \n\n        for(l=-1; l<2; l++){\n          \n\n          for(m=-1; m<2; m++){\n            \n\n            for(n=-1; n<2; n++){\n\n              \n\n              if(    (((i+l)>=0 && (j+m)>=0 && (k+n)>=0)==true && ((i+l)<dim_cpu.boxes1d_arg && (j+m)<dim_cpu.boxes1d_arg && (k+n)<dim_cpu.boxes1d_arg)==true)  &&\n                  (l==0 && m==0 && n==0)==false  ){\n\n                \n\n                box_cpu[nh].nei[box_cpu[nh].nn].x = (k+n);\n                box_cpu[nh].nei[box_cpu[nh].nn].y = (j+m);\n                box_cpu[nh].nei[box_cpu[nh].nn].z = (i+l);\n                box_cpu[nh].nei[box_cpu[nh].nn].number =  (box_cpu[nh].nei[box_cpu[nh].nn].z * dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg) + \n                  (box_cpu[nh].nei[box_cpu[nh].nn].y * dim_cpu.boxes1d_arg) + \n                  box_cpu[nh].nei[box_cpu[nh].nn].x;\n                box_cpu[nh].nei[box_cpu[nh].nn].offset = box_cpu[nh].nei[box_cpu[nh].nn].number * NUMBER_PAR_PER_BOX;\n\n                \n\n                box_cpu[nh].nn = box_cpu[nh].nn + 1;\n\n              }\n\n            } \n\n          } \n\n        } \n\n\n        \n\n        nh = nh + 1;\n\n      } \n\n    } \n\n  } \n\n\n  \n\n  \n\n  \n\n\n  \n\n  srand(2);\n\n  \n\n  rv_cpu = (FOUR_VECTOR*)malloc(dim_cpu.space_mem);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    rv_cpu[i].v = (rand()%10 + 1) / 10.0;      \n\n    \n\n    rv_cpu[i].x = (rand()%10 + 1) / 10.0;      \n\n    \n\n    rv_cpu[i].y = (rand()%10 + 1) / 10.0;      \n\n    \n\n    rv_cpu[i].z = (rand()%10 + 1) / 10.0;      \n\n    \n\n  }\n\n  \n\n  qv_cpu = (fp*)malloc(dim_cpu.space_mem2);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    qv_cpu[i] = (rand()%10 + 1) / 10.0;      \n\n    \n\n  }\n\n  \n\n  fv_cpu = (FOUR_VECTOR*)malloc(dim_cpu.space_mem);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    fv_cpu[i].v = 0;                \n\n    fv_cpu[i].x = 0;                \n\n    fv_cpu[i].y = 0;                \n\n    fv_cpu[i].z = 0;                \n\n  }\n\n  long long kstart, kend;\n  long long start = get_time();\n\n  \n\n  int dim_cpu_number_boxes = dim_cpu.number_boxes;\n\n{\n  kstart = get_time();\n    {\n    FOUR_VECTOR rA_shared[100];\n    FOUR_VECTOR rB_shared[100];\n    fp qB_shared[100];\n\n        {\n      int bx = omp_get_team_num();\n      int tx = omp_get_thread_num();\n      int wtx = tx;\n\n      \n\n\n      if(bx<dim_cpu_number_boxes){\n\n        \n\n\n        \n\n        fp a2 = 2*par_cpu.alpha*par_cpu.alpha;\n\n        \n\n        int first_i;\n        \n\n\n        \n\n        int pointer;\n        int k = 0;\n        int first_j;\n        int j = 0;\n        \n\n\n        \n\n        fp r2;\n        fp u2;\n        fp vij;\n        fp fs;\n        fp fxij;\n        fp fyij;\n        fp fzij;\n        THREE_VECTOR d;\n\n        \n\n\n        \n\n\n        \n\n        first_i = box_cpu[bx].offset;\n\n        \n\n\n        \n\n        \n\n        while(wtx<NUMBER_PAR_PER_BOX){\n          rA_shared[wtx] = rv_cpu[first_i+wtx];\n          wtx = wtx + NUMBER_THREADS;\n        }\n        wtx = tx;\n\n        \n\n        \n\n\n        \n\n\n        \n\n        for (k=0; k<(1+box_cpu[bx].nn); k++){\n\n          \n\n          \n\n          \n\n\n          if(k==0){\n            pointer = bx;                          \n\n          }\n          else{\n            pointer = box_cpu[bx].nei[k-1].number;              \n\n          }\n\n          \n\n\n          \n\n          first_j = box_cpu[pointer].offset;\n\n          \n\n          \n\n          while(wtx<NUMBER_PAR_PER_BOX){\n            rB_shared[wtx] = rv_cpu[first_j+wtx];\n            qB_shared[wtx] = qv_cpu[first_j+wtx];\n            wtx = wtx + NUMBER_THREADS;\n          }\n          wtx = tx;\n\n          \n\n          \n\n\n          \n\n\n          \n\n          while(wtx<NUMBER_PAR_PER_BOX){\n\n            \n\n            for (j=0; j<NUMBER_PAR_PER_BOX; j++){\n\n              r2 = rA_shared[wtx].v + rB_shared[j].v - DOT(rA_shared[wtx],rB_shared[j]); \n              u2 = a2*r2;\n              vij= exp(-u2);\n              fs = 2*vij;\n              d.x = rA_shared[wtx].x  - rB_shared[j].x;\n              fxij=fs*d.x;\n              d.y = rA_shared[wtx].y  - rB_shared[j].y;\n              fyij=fs*d.y;\n              d.z = rA_shared[wtx].z  - rB_shared[j].z;\n              fzij=fs*d.z;\n              fv_cpu[first_i+wtx].v +=  qB_shared[j]*vij;\n              fv_cpu[first_i+wtx].x +=  qB_shared[j]*fxij;\n              fv_cpu[first_i+wtx].y +=  qB_shared[j]*fyij;\n              fv_cpu[first_i+wtx].z +=  qB_shared[j]*fzij;\n\n            }\n\n            \n\n            wtx = wtx + NUMBER_THREADS;\n\n          }\n\n          \n\n          wtx = tx;\n\n          \n\n\n        }\n      }\n    }\n  }\n  kend = get_time();\n}\n\n  long long end = get_time();\n  printf(\"Device offloading time:\\n\"); \n  printf(\"%.12f s\\n\", (float) (end-start) / 1000000);\n\n  printf(\"Kernel execution time:\\n\"); \n  printf(\"%.12f s\\n\", (float) (kend-kstart) / 1000000); \n\n#ifdef DEBUG\n  int offset = 395;\n  for(int g=0; g<10; g++){\n    printf(\"g=%d %f, %f, %f, %f\\n\", \\\n        g, fv_cpu[offset+g].v, fv_cpu[offset+g].x, fv_cpu[offset+g].y, fv_cpu[offset+g].z);\n  }\n#endif\n\n  \n\n#ifdef OUTPUT\n  FILE *fptr;\n  fptr = fopen(\"result.txt\", \"w\");  \n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    fprintf(fptr, \"%f, %f, %f, %f\\n\", fv_cpu[i].v, fv_cpu[i].x, fv_cpu[i].y, fv_cpu[i].z);\n  }\n  fclose(fptr);\n#endif         \n\n  free(rv_cpu);\n  free(qv_cpu);\n  free(fv_cpu);\n  free(box_cpu);\n\n  return 0;\n}\n"}}
{"kernel_name": "lavaMD", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <stdbool.h>\n#include <string.h>\n#include <sycl/sycl.hpp>\n#include \"./util/timer/timer.h\"\n#include \"./util/num/num.h\"\n#include \"./main.h\"\n\nint main(  int argc, char *argv [])\n{\n  \n\n  int i, j, k, l, m, n;\n\n  \n\n  par_str par_cpu;\n  dim_str dim_cpu;\n  box_str* box_cpu;\n  FOUR_VECTOR* rv_cpu;\n  fp* qv_cpu;\n  FOUR_VECTOR* fv_cpu;\n  int nh;\n\n  printf(\"WG size of kernel = %d \\n\", NUMBER_THREADS);\n\n  \n\n  dim_cpu.arch_arg = 0;\n  dim_cpu.cores_arg = 1;\n  dim_cpu.boxes1d_arg = 1;\n\n  \n\n  if(argc==3){\n    for(dim_cpu.cur_arg=1; dim_cpu.cur_arg<argc; dim_cpu.cur_arg++){\n      \n\n      if(strcmp(argv[dim_cpu.cur_arg], \"-boxes1d\")==0){\n        \n\n        if(argc>=dim_cpu.cur_arg+1){\n          \n\n          if(isInteger(argv[dim_cpu.cur_arg+1])==1){\n            dim_cpu.boxes1d_arg = atoi(argv[dim_cpu.cur_arg+1]);\n            if(dim_cpu.boxes1d_arg<0){\n              printf(\"ERROR: Wrong value to -boxes1d argument, cannot be <=0\\n\");\n              return 0;\n            }\n            dim_cpu.cur_arg = dim_cpu.cur_arg+1;\n          }\n          \n\n          else{\n            printf(\"ERROR: Value to -boxes1d argument in not a number\\n\");\n            return 0;\n          }\n        }\n        \n\n        else{\n          printf(\"ERROR: Missing value to -boxes1d argument\\n\");\n          return 0;\n        }\n      }\n      \n\n      else{\n        printf(\"ERROR: Unknown argument\\n\");\n        return 0;\n      }\n    }\n    \n\n    printf(\"Configuration used: arch = %d, cores = %d, boxes1d = %d\\n\", dim_cpu.arch_arg, dim_cpu.cores_arg, dim_cpu.boxes1d_arg);\n  }\n  else{\n    printf(\"Provide boxes1d argument, example: -boxes1d 16\");\n    return 0;\n  }\n\n  par_cpu.alpha = 0.5;\n\n  \n\n  dim_cpu.number_boxes = dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg;\n\n  \n\n  dim_cpu.space_elem = dim_cpu.number_boxes * NUMBER_PAR_PER_BOX;\n  dim_cpu.space_mem = dim_cpu.space_elem * sizeof(FOUR_VECTOR);\n  dim_cpu.space_mem2 = dim_cpu.space_elem * sizeof(fp);\n\n  \n\n  dim_cpu.box_mem = dim_cpu.number_boxes * sizeof(box_str);\n\n  \n\n  box_cpu = (box_str*)malloc(dim_cpu.box_mem);\n\n  \n\n  nh = 0;\n\n  \n\n  for(i=0; i<dim_cpu.boxes1d_arg; i++){\n    \n\n    for(j=0; j<dim_cpu.boxes1d_arg; j++){\n      \n\n      for(k=0; k<dim_cpu.boxes1d_arg; k++){\n\n        \n\n        box_cpu[nh].x = k;\n        box_cpu[nh].y = j;\n        box_cpu[nh].z = i;\n        box_cpu[nh].number = nh;\n        box_cpu[nh].offset = nh * NUMBER_PAR_PER_BOX;\n\n        \n\n        box_cpu[nh].nn = 0;\n\n        \n\n        for(l=-1; l<2; l++){\n          \n\n          for(m=-1; m<2; m++){\n            \n\n            for(n=-1; n<2; n++){\n\n              \n\n              if(    (((i+l)>=0 && (j+m)>=0 && (k+n)>=0)==true && ((i+l)<dim_cpu.boxes1d_arg && (j+m)<dim_cpu.boxes1d_arg && (k+n)<dim_cpu.boxes1d_arg)==true)  &&\n                  (l==0 && m==0 && n==0)==false  ){\n\n                \n\n                box_cpu[nh].nei[box_cpu[nh].nn].x = (k+n);\n                box_cpu[nh].nei[box_cpu[nh].nn].y = (j+m);\n                box_cpu[nh].nei[box_cpu[nh].nn].z = (i+l);\n                box_cpu[nh].nei[box_cpu[nh].nn].number =  (box_cpu[nh].nei[box_cpu[nh].nn].z * dim_cpu.boxes1d_arg * dim_cpu.boxes1d_arg) +\n                  (box_cpu[nh].nei[box_cpu[nh].nn].y * dim_cpu.boxes1d_arg) +\n                  box_cpu[nh].nei[box_cpu[nh].nn].x;\n                box_cpu[nh].nei[box_cpu[nh].nn].offset = box_cpu[nh].nei[box_cpu[nh].nn].number * NUMBER_PAR_PER_BOX;\n\n                \n\n                box_cpu[nh].nn = box_cpu[nh].nn + 1;\n\n              }\n\n            } \n\n          } \n\n        } \n\n\n        \n\n        nh = nh + 1;\n\n      } \n\n    } \n\n  } \n\n\n  \n\n\n  \n\n  srand(2);\n\n  \n\n  rv_cpu = (FOUR_VECTOR*)malloc(dim_cpu.space_mem);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    rv_cpu[i].v = (rand()%10 + 1) / 10.0;      \n\n    rv_cpu[i].x = (rand()%10 + 1) / 10.0;      \n\n    rv_cpu[i].y = (rand()%10 + 1) / 10.0;      \n\n    rv_cpu[i].z = (rand()%10 + 1) / 10.0;      \n\n  }\n\n  \n\n  qv_cpu = (fp*)malloc(dim_cpu.space_mem2);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    qv_cpu[i] = (rand()%10 + 1) / 10.0;      \n\n  }\n\n  \n\n  fv_cpu = (FOUR_VECTOR*)malloc(dim_cpu.space_mem);\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    fv_cpu[i].v = 0;                \n\n    fv_cpu[i].x = 0;                \n\n    fv_cpu[i].y = 0;                \n\n    fv_cpu[i].z = 0;                \n\n  }\n\n  long long kstart, kend;\n  long long start = get_time();\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  \n\n  size_t local_work_size = NUMBER_THREADS;\n  size_t global_work_size = dim_cpu.number_boxes * local_work_size;\n\n#ifdef DEBUG\n  printf(\"# of blocks = %lu, # of threads/block = %lu (ensure that device can handle)\\n\",\n         global_work_size/local_work_size, local_work_size);\n#endif\n\n  int dim_cpu_number_boxes = dim_cpu.number_boxes;\n\n  box_str* d_box_gpu = sycl::malloc_device<box_str>(dim_cpu.number_boxes, q);\n\n  FOUR_VECTOR* d_rv_gpu = sycl::malloc_device<FOUR_VECTOR>(dim_cpu.space_elem, q);\n\n  fp* d_qv_gpu = sycl::malloc_device<fp>(dim_cpu.space_elem, q);\n\n  FOUR_VECTOR* d_fv_gpu = sycl::malloc_device<FOUR_VECTOR>(dim_cpu.space_elem, q);\n\n  q.memcpy(d_box_gpu, box_cpu, dim_cpu.box_mem);\n  q.memcpy(d_rv_gpu, rv_cpu, dim_cpu.space_mem);\n  q.memcpy(d_qv_gpu, qv_cpu, dim_cpu.space_mem2);\n  q.memcpy(d_fv_gpu, fv_cpu, dim_cpu.space_mem);\n\n  sycl::range<1> gws (global_work_size);\n  sycl::range<1> lws (local_work_size);\n\n  q.wait();\n  kstart = get_time();\n\n  q.submit([&](sycl::handler& cgh) {\n    sycl::local_accessor <FOUR_VECTOR> rA_shared (100, cgh);\n    sycl::local_accessor <FOUR_VECTOR> rB_shared (100, cgh);\n    sycl::local_accessor <fp, 1> qB_shared (100, cgh);\n    cgh.parallel_for<class lavamd>(\n      sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n      #include \"kernel.sycl\"\n    });\n  }).wait();\n\n  kend = get_time();\n\n  q.memcpy(fv_cpu, d_fv_gpu, dim_cpu.space_mem).wait();\n\n  sycl::free(d_box_gpu, q);\n  sycl::free(d_rv_gpu, q);\n  sycl::free(d_qv_gpu, q);\n  sycl::free(d_fv_gpu, q);\n\n  long long end = get_time();\n  printf(\"Device offloading time:\\n\");\n  printf(\"%.12f s\\n\", (float) (end-start) / 1000000);\n\n  printf(\"Kernel execution time:\\n\");\n  printf(\"%.12f s\\n\", (float) (kend-kstart) / 1000000);\n\n  \n\n#ifdef OUTPUT\n  FILE *fptr;\n  fptr = fopen(\"result.txt\", \"w\");\n  for(i=0; i<dim_cpu.space_elem; i=i+1){\n    fprintf(fptr, \"%f, %f, %f, %f\\n\", fv_cpu[i].v, fv_cpu[i].x, fv_cpu[i].y, fv_cpu[i].z);\n  }\n  fclose(fptr);\n#endif\n\n  free(rv_cpu);\n  free(qv_cpu);\n  free(fv_cpu);\n  free(box_cpu);\n\n  return 0;\n}\n", "kernel.sycl": "\n\n\n\nint bx = item.get_group(0);    \n\nint tx = item.get_local_id(0); \n\nint wtx = tx;\n\n\n\n\nif(bx<dim_cpu_number_boxes) {\n\n  \n\n\n  \n\n  fp a2 = 2*par_cpu.alpha*par_cpu.alpha;\n\n  \n\n  int first_i;\n  \n\n\n  \n\n  int pointer;\n  int k = 0;\n  int first_j;\n  int j = 0;\n  \n\n\n  \n\n  fp r2;\n  fp u2;\n  fp vij;\n  fp fs;\n  fp fxij;\n  fp fyij;\n  fp fzij;\n  THREE_VECTOR d;\n\n  \n\n\n  \n\n\n  \n\n  first_i = d_box_gpu[bx].offset;\n\n  \n\n\n  \n\n  \n\n  while(wtx<NUMBER_PAR_PER_BOX){\n    rA_shared[wtx] = d_rv_gpu[first_i+wtx];\n    wtx = wtx + NUMBER_THREADS;\n  }\n  wtx = tx;\n\n  \n\n  \n\n  item.barrier(sycl::access::fence_space::local_space);\n\n  \n\n\n  \n\n  for (k=0; k<(1+d_box_gpu[bx].nn); k++){\n\n    \n\n\n    if(k==0){\n      pointer = bx;                          \n\n    }\n    else{\n      pointer = d_box_gpu[bx].nei[k-1].number;              \n\n    }\n\n    \n\n\n    \n\n    first_j = d_box_gpu[pointer].offset;\n\n    \n\n    \n\n    while(wtx<NUMBER_PAR_PER_BOX){\n      rB_shared[wtx] = d_rv_gpu[first_j+wtx];\n      qB_shared[wtx] = d_qv_gpu[first_j+wtx];\n      wtx = wtx + NUMBER_THREADS;\n    }\n    wtx = tx;\n\n    \n\n    \n\n    item.barrier(sycl::access::fence_space::local_space);\n\n    \n\n\n    \n\n    while(wtx<NUMBER_PAR_PER_BOX){\n\n      auto fv = d_fv_gpu[first_i+wtx];\n      auto rA = rA_shared[wtx];\n\n      \n\n      for (j=0; j<NUMBER_PAR_PER_BOX; j++){\n        auto rB = rB_shared[j];\n        auto qB = qB_shared[j];\n\n        r2 = rA.v + rB.v - DOT(rA, rB); \n        u2 = a2*r2;\n        vij= sycl::exp(-u2);\n        fs = 2*vij;\n        d.x = rA.x  - rB.x;\n        fxij=fs*d.x;\n        d.y = rA.y  - rB.y;\n        fyij=fs*d.y;\n        d.z = rA.z  - rB.z;\n        fzij=fs*d.z;\n\n        fv.v += qB * vij;\n        fv.x += qB * fxij;\n        fv.y += qB * fyij;\n        fv.z += qB * fzij;\n      }\n\n      d_fv_gpu[first_i+wtx] = fv;\n\n      \n\n      wtx = wtx + NUMBER_THREADS;\n    }\n\n    \n\n    wtx = tx;\n\n    \n\n    item.barrier(sycl::access::fence_space::local_space);\n\n    \n\n  }\n  \n\n}\n"}}
{"kernel_name": "lid-driven-cavity", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <chrono>\n#include <cuda.h>\n\n\n\n#define NUM 512\n\n\n\n#define BLOCK_SIZE 128\n\n\n\n#define DOUBLE\n\n#ifdef DOUBLE\n#define Real double\n\n#define ZERO 0.0\n#define ONE 1.0\n#define TWO 2.0\n#define FOUR 4.0\n\n#define SMALL 1.0e-10;\n\n\n\nconst Real Re_num = 1000.0;\n\n\n\nconst Real omega = 1.7;\n\n\n\nconst Real mix_param = 0.9;\n\n\n\nconst Real tau = 0.5;\n\n\n\nconst Real gx = 0.0;\nconst Real gy = 0.0;\n\n\n\n#define xLength 1.0\n#define yLength 1.0\n#else\n#define Real float\n\n\n\n#undef fmin\n#define fmin fminf\n#undef fmax\n#define fmax fmaxf\n#undef fabs\n#define fabs fabsf\n#undef sqrt\n#define sqrt sqrtf\n\n#define ZERO 0.0f\n#define ONE 1.0f\n#define TWO 2.0f\n#define FOUR 4.0f\n#define SMALL 1.0e-10f;\n\n\n\nconst Real Re_num = 1000.0f;\n\n\n\nconst Real omega = 1.7f;\n\n\n\nconst Real mix_param = 0.9f;\n\n\n\nconst Real tau = 0.5f;\n\n\n\nconst Real gx = 0.0f;\nconst Real gy = 0.0f;\n\n\n\n#define xLength 1.0f\n#define yLength 1.0f\n#endif\n\n\n\nconst Real dx = xLength / NUM;\nconst Real dy = yLength / NUM;\n\n\n\n\n\n\n\n\n\n\n\n\n\n#define u(I, J) u[((I) * ((NUM) + 2)) + (J)]\n#define v(I, J) v[((I) * ((NUM) + 2)) + (J)]\n#define F(I, J) F[((I) * ((NUM) + 2)) + (J)]\n#define G(I, J) G[((I) * ((NUM) + 2)) + (J)]\n#define pres_red(I, J) pres_red[((I) * ((NUM_2) + 2)) + (J)]\n#define pres_black(I, J) pres_black[((I) * ((NUM_2) + 2)) + (J)]\n\n\n\n__host__\nvoid set_BCs_host (Real* u, Real* v) \n{\n  int ind;\n\n  \n\n  for (ind = 0; ind < NUM + 2; ++ind) {\n\n    \n\n    u(0, ind) = ZERO;\n    v(0, ind) = -v(1, ind);\n\n    \n\n    u(NUM, ind) = ZERO;\n    v(NUM + 1, ind) = -v(NUM, ind);\n\n    \n\n    u(ind, 0) = -u(ind, 1);\n    v(ind, 0) = ZERO;\n\n    \n\n    u(ind, NUM + 1) = TWO - u(ind, NUM);\n    v(ind, NUM) = ZERO;\n\n    if (ind == NUM) {\n      \n\n      u(0, 0) = ZERO;\n      v(0, 0) = -v(1, 0);\n      u(0, NUM + 1) = ZERO;\n      v(0, NUM + 1) = -v(1, NUM + 1);\n\n      \n\n      u(NUM, 0) = ZERO;\n      v(NUM + 1, 0) = -v(NUM, 0);\n      u(NUM, NUM + 1) = ZERO;\n      v(NUM + 1, NUM + 1) = -v(NUM, NUM + 1);\n\n      \n\n      u(0, 0) = -u(0, 1);\n      v(0, 0) = ZERO;\n      u(NUM + 1, 0) = -u(NUM + 1, 1);\n      v(NUM + 1, 0) = ZERO;\n\n      \n\n      u(0, NUM + 1) = TWO - u(0, NUM);\n      v(0, NUM) = ZERO;\n      u(NUM + 1, NUM + 1) = TWO - u(NUM + 1, NUM);\n      v(ind, NUM + 1) = ZERO;\n    } \n\n\n  } \n\n\n} \n\n\n\n\n__global__\nvoid set_BCs (Real*__restrict__ u, Real*__restrict__ v) \n{\n  int ind = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n\n  \n\n  u(0, ind) = ZERO;\n  v(0, ind) = -v(1, ind);\n\n  \n\n  u(NUM, ind) = ZERO;\n  v(NUM + 1, ind) = -v(NUM, ind);\n\n  \n\n  u(ind, 0) = -u(ind, 1);\n  v(ind, 0) = ZERO;\n\n  \n\n  u(ind, NUM + 1) = TWO - u(ind, NUM);\n  v(ind, NUM) = ZERO;\n\n  if (ind == NUM) {\n    \n\n    u(0, 0) = ZERO;\n    v(0, 0) = -v(1, 0);\n    u(0, NUM + 1) = ZERO;\n    v(0, NUM + 1) = -v(1, NUM + 1);\n\n    \n\n    u(NUM, 0) = ZERO;\n    v(NUM + 1, 0) = -v(NUM, 0);\n    u(NUM, NUM + 1) = ZERO;\n    v(NUM + 1, NUM + 1) = -v(NUM, NUM + 1);\n\n    \n\n    u(0, 0) = -u(0, 1);\n    v(0, 0) = ZERO;\n    u(NUM + 1, 0) = -u(NUM + 1, 1);\n    v(NUM + 1, 0) = ZERO;\n\n    \n\n    u(0, NUM + 1) = TWO - u(0, NUM);\n    v(0, NUM) = ZERO;\n    u(NUM + 1, NUM + 1) = TWO - u(NUM + 1, NUM);\n    v(ind, NUM + 1) = ZERO;\n  } \n\n\n} \n\n\n\n\n\n__global__ \nvoid calculate_F (const Real dt,\n                  const Real*__restrict__ u,\n                  const Real*__restrict__ v,\n                        Real*__restrict__ F) \n{  \n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  if (col == NUM) {\n    \n\n    \n\n    F(0, row) = u(0, row);\n    F(NUM, row) = u(NUM, row);\n  } else {\n\n    \n\n    Real u_ij = u(col, row);\n    Real u_ip1j = u(col + 1, row);\n    Real u_ijp1 = u(col, row + 1);\n    Real u_im1j = u(col - 1, row);\n    Real u_ijm1 = u(col, row - 1);\n\n    \n\n    Real v_ij = v(col, row);\n    Real v_ip1j = v(col + 1, row);\n    Real v_ijm1 = v(col, row - 1);\n    Real v_ip1jm1 = v(col + 1, row - 1);\n\n    \n\n    Real du2dx, duvdy, d2udx2, d2udy2;\n\n    du2dx = (((u_ij + u_ip1j) * (u_ij + u_ip1j) - (u_im1j + u_ij) * (u_im1j + u_ij))\n        + mix_param * (fabs(u_ij + u_ip1j) * (u_ij - u_ip1j)\n          - fabs(u_im1j + u_ij) * (u_im1j - u_ij))) / (FOUR * dx);\n    duvdy = ((v_ij + v_ip1j) * (u_ij + u_ijp1) - (v_ijm1 + v_ip1jm1) * (u_ijm1 + u_ij)\n        + mix_param * (fabs(v_ij + v_ip1j) * (u_ij - u_ijp1)\n          - fabs(v_ijm1 + v_ip1jm1) * (u_ijm1 - u_ij))) / (FOUR * dy);\n    d2udx2 = (u_ip1j - (TWO * u_ij) + u_im1j) / (dx * dx);\n    d2udy2 = (u_ijp1 - (TWO * u_ij) + u_ijm1) / (dy * dy);\n\n    F(col, row) = u_ij + dt * (((d2udx2 + d2udy2) / Re_num) - du2dx - duvdy + gx);\n\n  } \n\n\n} \n\n\n\n\n\n__global__ \nvoid calculate_G (const Real dt,\n                  const Real*__restrict__ u,\n                  const Real*__restrict__ v,\n                        Real*__restrict__ G) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  if (row == NUM) {\n    \n\n    G(col, 0) = v(col, 0);\n    G(col, NUM) = v(col, NUM);\n\n  } else {\n\n    \n\n    Real u_ij = u(col, row);\n    Real u_ijp1 = u(col, row + 1);\n    Real u_im1j = u(col - 1, row);\n    Real u_im1jp1 = u(col - 1, row + 1);\n\n    \n\n    Real v_ij = v(col, row);\n    Real v_ijp1 = v(col, row + 1);\n    Real v_ip1j = v(col + 1, row);\n    Real v_ijm1 = v(col, row - 1);\n    Real v_im1j = v(col - 1, row);\n\n    \n\n    Real dv2dy, duvdx, d2vdx2, d2vdy2;\n\n    dv2dy = ((v_ij + v_ijp1) * (v_ij + v_ijp1) - (v_ijm1 + v_ij) * (v_ijm1 + v_ij)\n        + mix_param * (fabs(v_ij + v_ijp1) * (v_ij - v_ijp1)\n          - fabs(v_ijm1 + v_ij) * (v_ijm1 - v_ij))) / (FOUR * dy);\n    duvdx = ((u_ij + u_ijp1) * (v_ij + v_ip1j) - (u_im1j + u_im1jp1) * (v_im1j + v_ij)\n        + mix_param * (fabs(u_ij + u_ijp1) * (v_ij - v_ip1j) \n          - fabs(u_im1j + u_im1jp1) * (v_im1j - v_ij))) / (FOUR * dx);\n    d2vdx2 = (v_ip1j - (TWO * v_ij) + v_im1j) / (dx * dx);\n    d2vdy2 = (v_ijp1 - (TWO * v_ij) + v_ijm1) / (dy * dy);\n\n    G(col, row) = v_ij + dt * (((d2vdx2 + d2vdy2) / Re_num) - dv2dy - duvdx + gy);\n\n  } \n\n\n} \n\n\n\n\n\n__global__ \nvoid sum_pressure (const Real*__restrict__ pres_red,\n                   const Real*__restrict__ pres_black, \n                         Real*__restrict__ pres_sum) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  \n\n  __shared__ Real sum_cache[BLOCK_SIZE];\n\n  int NUM_2 = NUM >> 1;\n\n  Real pres_r = pres_red(col, row);\n  Real pres_b = pres_black(col, row);\n\n  \n\n  sum_cache[threadIdx.x] = (pres_r * pres_r) + (pres_b * pres_b);\n\n  \n\n  __syncthreads();\n\n  \n\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      sum_cache[threadIdx.x] += sum_cache[threadIdx.x + i];\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  \n\n  if (threadIdx.x == 0) {\n    pres_sum[blockIdx.y + (gridDim.y * blockIdx.x)] = sum_cache[0];\n  }\n\n} \n\n\n\n\n__global__ \nvoid set_horz_pres_BCs (Real*__restrict__ pres_red, Real*__restrict__ pres_black) \n{\n  int col = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  col = (col * 2) - 1;\n\n  int NUM_2 = NUM >> 1;\n\n  \n\n  pres_black(col, 0) = pres_red(col, 1);\n  pres_red(col + 1, 0) = pres_black(col + 1, 1);\n\n  \n\n  pres_red(col, NUM_2 + 1) = pres_black(col, NUM_2);\n  pres_black(col + 1, NUM_2 + 1) = pres_red(col + 1, NUM_2);\n\n} \n\n\n\n\n\n__global__\nvoid set_vert_pres_BCs (Real*__restrict__ pres_red, Real*__restrict__ pres_black) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n\n  int NUM_2 = NUM >> 1;\n\n  \n\n  pres_black(0, row) = pres_red(1, row);\n  pres_red(0, row) = pres_black(1, row);\n\n  \n\n  pres_black(NUM + 1, row) = pres_red(NUM, row);\n  pres_red(NUM + 1, row) = pres_black(NUM, row);\n\n} \n\n\n\n\n\n\n\n__global__\nvoid red_kernel (const Real dt,\n                 const Real*__restrict__ F, \n                 const Real*__restrict__ G,\n                 const Real*__restrict__ pres_black,\n                       Real*__restrict__ pres_red) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  int NUM_2 = NUM >> 1;      \n\n  Real p_ij = pres_red(col, row);\n\n  Real p_im1j = pres_black(col - 1, row);\n  Real p_ip1j = pres_black(col + 1, row);\n  Real p_ijm1 = pres_black(col, row - (col & 1));\n  Real p_ijp1 = pres_black(col, row + ((col + 1) & 1));\n\n  \n\n  Real rhs = (((F(col, (2 * row) - (col & 1))\n          - F(col - 1, (2 * row) - (col & 1))) / dx)\n      + ((G(col, (2 * row) - (col & 1))\n          - G(col, (2 * row) - (col & 1) - 1)) / dy)) / dt;\n\n  pres_red(col, row) = p_ij * (ONE - omega) + omega * \n    (((p_ip1j + p_im1j) / (dx * dx)) + ((p_ijp1 + p_ijm1) / (dy * dy)) - \n     rhs) / ((TWO / (dx * dx)) + (TWO / (dy * dy)));\n\n} \n\n\n\n\n\n\n\n__global__ \nvoid black_kernel (const Real dt,\n                   const Real*__restrict__ F, \n                   const Real*__restrict__ G,\n                   const Real*__restrict__ pres_red, \n                         Real*__restrict__ pres_black) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  int NUM_2 = NUM >> 1;\n\n  Real p_ij = pres_black(col, row);\n\n  Real p_im1j = pres_red(col - 1, row);\n  Real p_ip1j = pres_red(col + 1, row);\n  Real p_ijm1 = pres_red(col, row - ((col + 1) & 1));\n  Real p_ijp1 = pres_red(col, row + (col & 1));\n\n  \n\n  Real rhs = (((F(col, (2 * row) - ((col + 1) & 1))\n          - F(col - 1, (2 * row) - ((col + 1) & 1))) / dx)\n      + ((G(col, (2 * row) - ((col + 1) & 1))\n          - G(col, (2 * row) - ((col + 1) & 1) - 1)) / dy)) / dt;\n\n  pres_black(col, row) = p_ij * (ONE - omega) + omega * \n    (((p_ip1j + p_im1j) / (dx * dx)) + ((p_ijp1 + p_ijm1) / (dy * dy)) - \n     rhs) / ((TWO / (dx * dx)) + (TWO / (dy * dy)));\n\n} \n\n\n\n\n\n__global__\nvoid calc_residual (const Real dt,\n                    const Real*__restrict__ F,\n                    const Real*__restrict__ G, \n                    const Real*__restrict__ pres_red,\n                    const Real*__restrict__ pres_black,\n                          Real*__restrict__ res_array)\n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  int NUM_2 = NUM >> 1;\n\n  Real p_ij, p_im1j, p_ip1j, p_ijm1, p_ijp1, rhs, res, res2;\n\n  \n\n  p_ij = pres_red(col, row);\n\n  p_im1j = pres_black(col - 1, row);\n  p_ip1j = pres_black(col + 1, row);\n  p_ijm1 = pres_black(col, row - (col & 1));\n  p_ijp1 = pres_black(col, row + ((col + 1) & 1));\n\n  rhs = (((F(col, (2 * row) - (col & 1)) - F(col - 1, (2 * row) - (col & 1))) / dx)\n      +  ((G(col, (2 * row) - (col & 1)) - G(col, (2 * row) - (col & 1) - 1)) / dy)) / dt;\n\n  \n\n  res = ((p_ip1j - (TWO * p_ij) + p_im1j) / (dx * dx))\n    + ((p_ijp1 - (TWO * p_ij) + p_ijm1) / (dy * dy)) - rhs;\n\n  \n\n  p_ij = pres_black(col, row);\n\n  p_im1j = pres_red(col - 1, row);\n  p_ip1j = pres_red(col + 1, row);\n  p_ijm1 = pres_red(col, row - ((col + 1) & 1));\n  p_ijp1 = pres_red(col, row + (col & 1));\n\n  \n\n  rhs = (((F(col, (2 * row) - ((col + 1) & 1)) - F(col - 1, (2 * row) - ((col + 1) & 1))) / dx)\n      +  ((G(col, (2 * row) - ((col + 1) & 1)) - G(col, (2 * row) - ((col + 1) & 1) - 1)) / dy)) / dt;\n\n  \n\n  res2 = ((p_ip1j - (TWO * p_ij) + p_im1j) / (dx * dx))\n    + ((p_ijp1 - (TWO * p_ij) + p_ijm1) / (dy * dy)) - rhs;\n\n  \n\n  __shared__ Real sum_cache[BLOCK_SIZE];\n\n  sum_cache[threadIdx.x] = (res * res) + (res2 * res2);\n\n  \n\n  __syncthreads();\n\n  \n\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      sum_cache[threadIdx.x] += sum_cache[threadIdx.x + i];\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  \n\n  if (threadIdx.x == 0) {\n    res_array[blockIdx.y + (gridDim.y * blockIdx.x)] = sum_cache[0];\n  }\n} \n\n\n\n\n__global__ \nvoid calculate_u (const Real dt,\n                  const Real*__restrict__ F, \n                  const Real*__restrict__ pres_red,\n                  const Real*__restrict__ pres_black, \n                        Real*__restrict__ u,\n                        Real*__restrict__ max_u)\n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  \n\n  __shared__ Real max_cache[BLOCK_SIZE];\n  max_cache[threadIdx.x] = ZERO;\n\n  int NUM_2 = NUM >> 1;\n  Real new_u = ZERO;\n\n  if (col != NUM) {\n\n    Real p_ij, p_ip1j, new_u2;\n\n    \n\n    p_ij = pres_red(col, row);\n    p_ip1j = pres_black(col + 1, row);\n\n    new_u = F(col, (2 * row) - (col & 1)) - (dt * (p_ip1j - p_ij) / dx);\n    u(col, (2 * row) - (col & 1)) = new_u;\n\n    \n\n    p_ij = pres_black(col, row);\n    p_ip1j = pres_red(col + 1, row);\n\n    new_u2 = F(col, (2 * row) - ((col + 1) & 1)) - (dt * (p_ip1j - p_ij) / dx);\n    u(col, (2 * row) - ((col + 1) & 1)) = new_u2;\n\n    \n\n    new_u = fmax(fabs(new_u), fabs(new_u2));\n\n    if ((2 * row) == NUM) {\n      \n\n      new_u = fmax(new_u, fabs( u(col, NUM + 1) ));\n    }\n  } else {\n    \n\n    new_u = fmax(fabs( u(NUM, (2 * row)) ), fabs( u(0, (2 * row)) ));\n    new_u = fmax(fabs( u(NUM, (2 * row) - 1) ), new_u);\n    new_u = fmax(fabs( u(0, (2 * row) - 1) ), new_u);\n\n    new_u = fmax(fabs( u(NUM + 1, (2 * row)) ), new_u);\n    new_u = fmax(fabs( u(NUM + 1, (2 * row) - 1) ), new_u);\n\n  } \n\n\n  \n\n  max_cache[threadIdx.x] = new_u;\n\n  \n\n  __syncthreads();\n\n  \n\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      max_cache[threadIdx.x] = fmax(max_cache[threadIdx.x], max_cache[threadIdx.x + i]);\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  \n\n  if (threadIdx.x == 0) {\n    max_u[blockIdx.y + (gridDim.y * blockIdx.x)] = max_cache[0];\n  }\n} \n\n\n\n\n\n__global__ \nvoid calculate_v (const Real dt,\n                  const Real*__restrict__ G, \n                  const Real*__restrict__ pres_red,\n                  const Real*__restrict__ pres_black, \n                        Real*__restrict__ v,\n                        Real*__restrict__ max_v)\n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  \n\n  __shared__ Real max_cache[BLOCK_SIZE];\n  max_cache[threadIdx.x] = ZERO;\n\n  int NUM_2 = NUM >> 1;\n  Real new_v = ZERO;\n\n  if (row != NUM_2) {\n    Real p_ij, p_ijp1, new_v2;\n\n    \n\n    p_ij = pres_red(col, row);\n    p_ijp1 = pres_black(col, row + ((col + 1) & 1));\n\n    new_v = G(col, (2 * row) - (col & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n    v(col, (2 * row) - (col & 1)) = new_v;\n\n    \n\n    p_ij = pres_black(col, row);\n    p_ijp1 = pres_red(col, row + (col & 1));\n\n    new_v2 = G(col, (2 * row) - ((col + 1) & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n    v(col, (2 * row) - ((col + 1) & 1)) = new_v2;\n\n    \n\n    new_v = fmax(fabs(new_v), fabs(new_v2));\n\n    if (col == NUM) {\n      \n\n      new_v = fmax(new_v, fabs( v(NUM + 1, (2 * row)) ));\n    }\n\n  } else {\n\n    if ((col & 1) == 1) {\n      \n\n      Real p_ij = pres_red(col, row);\n      Real p_ijp1 = pres_black(col, row + ((col + 1) & 1));\n\n      new_v = G(col, (2 * row) - (col & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n      v(col, (2 * row) - (col & 1)) = new_v;\n    } else {\n      \n\n      Real p_ij = pres_black(col, row);\n      Real p_ijp1 = pres_red(col, row + (col & 1));\n\n      new_v = G(col, (2 * row) - ((col + 1) & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n      v(col, (2 * row) - ((col + 1) & 1)) = new_v;\n    }\n\n    \n\n    new_v = fabs(new_v);\n\n    \n\n    new_v = fmax(fabs( v(col, NUM) ), new_v);\n    new_v = fmax(fabs( v(col, 0) ), new_v);\n\n    new_v = fmax(fabs( v(col, NUM + 1) ), new_v);\n  } \n\n\n  \n\n  max_cache[threadIdx.x] = new_v;\n\n  \n\n  __syncthreads();\n\n  \n\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      max_cache[threadIdx.x] = fmax(max_cache[threadIdx.x], max_cache[threadIdx.x + i]);\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  \n\n  if (threadIdx.x == 0) {\n    max_v[blockIdx.y + (gridDim.y * blockIdx.x)] = max_cache[0];\n  }\n} \n\n\n\n\n\nint main (int argc, char *argv[])\n{\n  \n\n  int iter = 0;\n  const int it_max = 1000000;\n\n  \n\n  const Real tol = 0.001;\n\n  \n\n  const Real time_start = 0.0;\n  const Real time_end = 0.001; \n\n\n  \n\n  Real dt = 0.02;\n\n  int size = (NUM + 2) * (NUM + 2);\n  int size_pres = ((NUM / 2) + 2) * (NUM + 2);\n\n  \n\n  Real* F;\n  Real* u;\n  Real* G;\n  Real* v;\n\n  F = (Real *) calloc (size, sizeof(Real));\n  u = (Real *) calloc (size, sizeof(Real));\n  G = (Real *) calloc (size, sizeof(Real));\n  v = (Real *) calloc (size, sizeof(Real));\n\n  for (int i = 0; i < size; ++i) {\n    F[i] = ZERO;\n    u[i] = ZERO;\n    G[i] = ZERO;\n    v[i] = ZERO;\n  }\n\n  \n\n  Real* pres_red;\n  Real* pres_black;\n\n  pres_red = (Real *) calloc (size_pres, sizeof(Real));\n  pres_black = (Real *) calloc (size_pres, sizeof(Real));\n\n  for (int i = 0; i < size_pres; ++i) {\n    pres_red[i] = ZERO;\n    pres_black[i] = ZERO;\n  }\n\n  \n\n  printf(\"Problem size: %d x %d \\n\", NUM, NUM);\n\n  \n\n\n  \n\n  dim3 block_bcs (BLOCK_SIZE, 1);\n  dim3 grid_bcs (NUM / BLOCK_SIZE, 1);\n\n  \n\n  dim3 block_pr (BLOCK_SIZE, 1);\n  dim3 grid_pr (NUM / (2 * BLOCK_SIZE), NUM);\n\n  \n\n  dim3 block_F (BLOCK_SIZE, 1);\n  dim3 grid_F (NUM / BLOCK_SIZE, NUM);\n\n  \n\n  dim3 block_G (BLOCK_SIZE, 1);\n  dim3 grid_G (NUM / BLOCK_SIZE, NUM);\n\n  \n\n  dim3 block_hpbc (BLOCK_SIZE, 1);\n  dim3 grid_hpbc (NUM / (2 * BLOCK_SIZE), 1);\n\n  \n\n  dim3 block_vpbc (BLOCK_SIZE, 1);\n  dim3 grid_vpbc (NUM / (2 * BLOCK_SIZE), 1);\n\n  \n\n  Real* res;\n\n  int size_res = grid_pr.x * grid_pr.y;\n  res = (Real *) calloc (size_res, sizeof(Real));\n\n  \n\n  Real* max_u_arr;\n  Real* max_v_arr;\n  int size_max = grid_pr.x * grid_pr.y;\n\n  max_u_arr = (Real *) calloc (size_max, sizeof(Real));\n  max_v_arr = (Real *) calloc (size_max, sizeof(Real));\n\n  \n\n  Real* pres_sum;\n  pres_sum = (Real *) calloc (size_res, sizeof(Real));\n\n  \n\n  set_BCs_host (u, v);\n\n  Real max_u = SMALL;\n  Real max_v = SMALL;\n  \n\n  #pragma unroll\n  for (int col = 0; col < NUM + 2; ++col) {\n    #pragma unroll\n    for (int row = 1; row < NUM + 2; ++row) {\n      max_u = fmax(max_u, fabs( u(col, row) ));\n    }\n  }\n\n  #pragma unroll\n  for (int col = 1; col < NUM + 2; ++col) {\n    #pragma unroll\n    for (int row = 0; row < NUM + 2; ++row) {\n      max_v = fmax(max_v, fabs( v(col, row) ));\n    }\n  }\n\n  \n\n  Real* u_d;\n  Real* F_d;\n  Real* v_d;\n  Real* G_d;\n\n  Real* pres_red_d;\n  Real* pres_black_d;\n  Real* pres_sum_d;\n  Real* res_d;\n\n  Real* max_u_d;\n  Real* max_v_d;\n\n  cudaMalloc ((void**) &u_d, size * sizeof(Real));\n  cudaMalloc ((void**) &F_d, size * sizeof(Real));\n  cudaMalloc ((void**) &v_d, size * sizeof(Real));\n  cudaMalloc ((void**) &G_d, size * sizeof(Real));\n\n  cudaMalloc ((void**) &pres_red_d, size_pres * sizeof(Real));\n  cudaMalloc ((void**) &pres_black_d, size_pres * sizeof(Real));\n\n  cudaMalloc ((void**) &pres_sum_d, size_res * sizeof(Real));\n  cudaMalloc ((void**) &res_d, size_res * sizeof(Real));\n  cudaMalloc ((void**) &max_u_d, size_max * sizeof(Real));\n  cudaMalloc ((void**) &max_v_d, size_max * sizeof(Real));\n\n  \n\n  cudaMemcpy (u_d, u, size * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (F_d, F, size * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (v_d, v, size * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (G_d, G, size * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (pres_red_d, pres_red, size_pres * sizeof(Real), cudaMemcpyHostToDevice);\n  cudaMemcpy (pres_black_d, pres_black, size_pres * sizeof(Real), cudaMemcpyHostToDevice);\n\n  Real time = time_start;\n\n  \n\n  Real dt_Re = 0.5 * Re_num / ((1.0 / (dx * dx)) + (1.0 / (dy * dy)));\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  while (time < time_end) {\n\n    \n\n    dt = fmin((dx / max_u), (dy / max_v));\n    dt = tau * fmin(dt_Re, dt);\n\n    if ((time + dt) >= time_end) {\n      dt = time_end - time;\n    }\n\n    \n\n    calculate_F <<<grid_F, block_F>>> (dt, u_d, v_d, F_d);\n    calculate_G <<<grid_G, block_G>>> (dt, u_d, v_d, G_d);\n\n    \n\n    sum_pressure <<<grid_pr, block_pr>>> (pres_red_d, pres_black_d, pres_sum_d);\n    cudaMemcpy (pres_sum, pres_sum_d, size_res * sizeof(Real), cudaMemcpyDeviceToHost);\n\n    Real p0_norm = ZERO;\n    #pragma unroll\n    for (int i = 0; i < size_res; ++i) {\n      p0_norm += pres_sum[i];\n    }\n\n    p0_norm = sqrt(p0_norm / ((Real)(NUM * NUM)));\n    if (p0_norm < 0.0001) {\n      p0_norm = 1.0;\n    }\n\n    \n\n    \n\n\n    Real norm_L2;\n\n    \n\n    \n\n    for (iter = 1; iter <= it_max; ++iter) {\n\n      \n\n      set_horz_pres_BCs <<<grid_hpbc, block_hpbc>>> (pres_red_d, pres_black_d);\n      set_vert_pres_BCs <<<grid_vpbc, block_vpbc>>> (pres_red_d, pres_black_d);\n\n      \n\n      \n\n\n      \n\n      red_kernel <<<grid_pr, block_pr>>> (dt, F_d, G_d, pres_black_d, pres_red_d);\n\n      \n\n      \n\n\n      \n\n      black_kernel <<<grid_pr, block_pr>>> (dt, F_d, G_d, pres_red_d, pres_black_d);\n\n      \n\n      \n\n\n      \n\n      calc_residual <<<grid_pr, block_pr>>> (dt, F_d, G_d, pres_red_d, pres_black_d, res_d);\n\n      \n\n      cudaMemcpy (res, res_d, size_res * sizeof(Real), cudaMemcpyDeviceToHost);\n\n      norm_L2 = ZERO;\n      #pragma unroll\n      for (int i = 0; i < size_res; ++i) {\n        norm_L2 += res[i];\n      }\n\n      \n\n      norm_L2 = sqrt(norm_L2 / ((Real)(NUM * NUM))) / p0_norm;\n\n      \n\n      if (norm_L2 < tol) {\n        break;\n      }  \n    } \n\n\n    printf(\"Time = %f, delt = %e, iter = %i, res = %e\\n\", time + dt, dt, iter, norm_L2);\n\n    \n\n\n    calculate_u <<<grid_pr, block_pr>>> (dt, F_d, pres_red_d, pres_black_d, u_d, max_u_d);\n    cudaMemcpy (max_u_arr, max_u_d, size_max * sizeof(Real), cudaMemcpyDeviceToHost);\n\n    calculate_v <<<grid_pr, block_pr>>> (dt, G_d, pres_red_d, pres_black_d, v_d, max_v_d);\n    cudaMemcpy (max_v_arr, max_v_d, size_max * sizeof(Real), cudaMemcpyDeviceToHost);\n\n    \n\n    max_v = SMALL;\n    max_u = SMALL;\n\n    #pragma unroll\n    for (int i = 0; i < size_max; ++i) {\n      Real test_u = max_u_arr[i];\n      max_u = fmax(max_u, test_u);\n\n      Real test_v = max_v_arr[i];\n      max_v = fmax(max_v, test_v);\n    }\n\n    \n\n    set_BCs <<<grid_bcs, block_bcs>>> (u_d, v_d);\n\n    \n\n    time += dt;\n\n    \n\n    \n\n\n  } \n\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto elapsed_time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"\\nTotal execution time of the iteration loop: %f (s)\\n\", elapsed_time * 1e-9f);\n\n  \n\n  cudaMemcpy (u, u_d, size * sizeof(Real), cudaMemcpyDeviceToHost);\n  cudaMemcpy (v, v_d, size * sizeof(Real), cudaMemcpyDeviceToHost);\n  cudaMemcpy (pres_red, pres_red_d, size_pres * sizeof(Real), cudaMemcpyDeviceToHost);\n  cudaMemcpy (pres_black, pres_black_d, size_pres * sizeof(Real), cudaMemcpyDeviceToHost);\n\n  \n\n  FILE * pfile;\n  pfile = fopen(\"velocity_gpu.dat\", \"w\");\n  fprintf(pfile, \"#x\\ty\\tu\\tv\\n\");\n  if (pfile != NULL) {\n    for (int row = 0; row < NUM; ++row) {\n      for (int col = 0; col < NUM; ++col) {\n\n        Real u_ij = u[(col * NUM) + row];\n        Real u_im1j;\n        if (col == 0) {\n          u_im1j = 0.0;\n        } else {\n          u_im1j = u[(col - 1) * NUM + row];\n        }\n\n        u_ij = (u_ij + u_im1j) / 2.0;\n\n        Real v_ij = v[(col * NUM) + row];\n        Real v_ijm1;\n        if (row == 0) {\n          v_ijm1 = 0.0;\n        } else {\n          v_ijm1 = v[(col * NUM) + row - 1];\n        }\n\n        v_ij = (v_ij + v_ijm1) / 2.0;\n\n        fprintf(pfile, \"%f\\t%f\\t%f\\t%f\\n\", ((Real)col + 0.5) * dx, ((Real)row + 0.5) * dy, u_ij, v_ij);\n      }\n    }\n  }\n\n  fclose(pfile);\n\n  \n\n  cudaFree(u_d);\n  cudaFree(v_d);\n  cudaFree(F_d);\n  cudaFree(G_d);\n  cudaFree(pres_red_d);\n  cudaFree(pres_black_d);\n  cudaFree(max_u_d);\n  cudaFree(max_v_d);\n  cudaFree(pres_sum_d);\n  cudaFree(res_d);\n\n  free(pres_red);\n  free(pres_black);\n  free(u);\n  free(v);\n  free(F);\n  free(G);\n  free(max_u_arr);\n  free(max_v_arr);\n  free(res);\n  free(pres_sum);\n\n  return 0;\n}\n"}}
{"kernel_name": "lid-driven-cavity", "parallel_api": "hip", "code": {"main.cu": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n\n\n\n#define NUM 512\n\n\n\n#define BLOCK_SIZE 128\n\n\n\n#define DOUBLE\n\n#ifdef DOUBLE\n#define Real double\n\n#define ZERO 0.0\n#define ONE 1.0\n#define TWO 2.0\n#define FOUR 4.0\n\n#define SMALL 1.0e-10;\n\n\n\nconst Real Re_num = 1000.0;\n\n\n\nconst Real omega = 1.7;\n\n\n\nconst Real mix_param = 0.9;\n\n\n\nconst Real tau = 0.5;\n\n\n\nconst Real gx = 0.0;\nconst Real gy = 0.0;\n\n\n\n#define xLength 1.0\n#define yLength 1.0\n#else\n#define Real float\n\n\n\n#undef fmin\n#define fmin fminf\n#undef fmax\n#define fmax fmaxf\n#undef fabs\n#define fabs fabsf\n#undef sqrt\n#define sqrt sqrtf\n\n#define ZERO 0.0f\n#define ONE 1.0f\n#define TWO 2.0f\n#define FOUR 4.0f\n#define SMALL 1.0e-10f;\n\n\n\nconst Real Re_num = 1000.0f;\n\n\n\nconst Real omega = 1.7f;\n\n\n\nconst Real mix_param = 0.9f;\n\n\n\nconst Real tau = 0.5f;\n\n\n\nconst Real gx = 0.0f;\nconst Real gy = 0.0f;\n\n\n\n#define xLength 1.0f\n#define yLength 1.0f\n#endif\n\n\n\nconst Real dx = xLength / NUM;\nconst Real dy = yLength / NUM;\n\n\n\n\n\n\n\n\n\n\n\n\n\n#define u(I, J) u[((I) * ((NUM) + 2)) + (J)]\n#define v(I, J) v[((I) * ((NUM) + 2)) + (J)]\n#define F(I, J) F[((I) * ((NUM) + 2)) + (J)]\n#define G(I, J) G[((I) * ((NUM) + 2)) + (J)]\n#define pres_red(I, J) pres_red[((I) * ((NUM_2) + 2)) + (J)]\n#define pres_black(I, J) pres_black[((I) * ((NUM_2) + 2)) + (J)]\n\n\n\n__host__\nvoid set_BCs_host (Real* u, Real* v) \n{\n  int ind;\n\n  \n\n  for (ind = 0; ind < NUM + 2; ++ind) {\n\n    \n\n    u(0, ind) = ZERO;\n    v(0, ind) = -v(1, ind);\n\n    \n\n    u(NUM, ind) = ZERO;\n    v(NUM + 1, ind) = -v(NUM, ind);\n\n    \n\n    u(ind, 0) = -u(ind, 1);\n    v(ind, 0) = ZERO;\n\n    \n\n    u(ind, NUM + 1) = TWO - u(ind, NUM);\n    v(ind, NUM) = ZERO;\n\n    if (ind == NUM) {\n      \n\n      u(0, 0) = ZERO;\n      v(0, 0) = -v(1, 0);\n      u(0, NUM + 1) = ZERO;\n      v(0, NUM + 1) = -v(1, NUM + 1);\n\n      \n\n      u(NUM, 0) = ZERO;\n      v(NUM + 1, 0) = -v(NUM, 0);\n      u(NUM, NUM + 1) = ZERO;\n      v(NUM + 1, NUM + 1) = -v(NUM, NUM + 1);\n\n      \n\n      u(0, 0) = -u(0, 1);\n      v(0, 0) = ZERO;\n      u(NUM + 1, 0) = -u(NUM + 1, 1);\n      v(NUM + 1, 0) = ZERO;\n\n      \n\n      u(0, NUM + 1) = TWO - u(0, NUM);\n      v(0, NUM) = ZERO;\n      u(NUM + 1, NUM + 1) = TWO - u(NUM + 1, NUM);\n      v(ind, NUM + 1) = ZERO;\n    } \n\n\n  } \n\n\n} \n\n\n\n\n__global__\nvoid set_BCs (Real*__restrict__ u, Real*__restrict__ v) \n{\n  int ind = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n\n  \n\n  u(0, ind) = ZERO;\n  v(0, ind) = -v(1, ind);\n\n  \n\n  u(NUM, ind) = ZERO;\n  v(NUM + 1, ind) = -v(NUM, ind);\n\n  \n\n  u(ind, 0) = -u(ind, 1);\n  v(ind, 0) = ZERO;\n\n  \n\n  u(ind, NUM + 1) = TWO - u(ind, NUM);\n  v(ind, NUM) = ZERO;\n\n  if (ind == NUM) {\n    \n\n    u(0, 0) = ZERO;\n    v(0, 0) = -v(1, 0);\n    u(0, NUM + 1) = ZERO;\n    v(0, NUM + 1) = -v(1, NUM + 1);\n\n    \n\n    u(NUM, 0) = ZERO;\n    v(NUM + 1, 0) = -v(NUM, 0);\n    u(NUM, NUM + 1) = ZERO;\n    v(NUM + 1, NUM + 1) = -v(NUM, NUM + 1);\n\n    \n\n    u(0, 0) = -u(0, 1);\n    v(0, 0) = ZERO;\n    u(NUM + 1, 0) = -u(NUM + 1, 1);\n    v(NUM + 1, 0) = ZERO;\n\n    \n\n    u(0, NUM + 1) = TWO - u(0, NUM);\n    v(0, NUM) = ZERO;\n    u(NUM + 1, NUM + 1) = TWO - u(NUM + 1, NUM);\n    v(ind, NUM + 1) = ZERO;\n  } \n\n\n} \n\n\n\n\n\n__global__ \nvoid calculate_F (const Real dt,\n                  const Real*__restrict__ u,\n                  const Real*__restrict__ v,\n                        Real*__restrict__ F) \n{  \n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  if (col == NUM) {\n    \n\n    \n\n    F(0, row) = u(0, row);\n    F(NUM, row) = u(NUM, row);\n  } else {\n\n    \n\n    Real u_ij = u(col, row);\n    Real u_ip1j = u(col + 1, row);\n    Real u_ijp1 = u(col, row + 1);\n    Real u_im1j = u(col - 1, row);\n    Real u_ijm1 = u(col, row - 1);\n\n    \n\n    Real v_ij = v(col, row);\n    Real v_ip1j = v(col + 1, row);\n    Real v_ijm1 = v(col, row - 1);\n    Real v_ip1jm1 = v(col + 1, row - 1);\n\n    \n\n    Real du2dx, duvdy, d2udx2, d2udy2;\n\n    du2dx = (((u_ij + u_ip1j) * (u_ij + u_ip1j) - (u_im1j + u_ij) * (u_im1j + u_ij))\n        + mix_param * (fabs(u_ij + u_ip1j) * (u_ij - u_ip1j)\n          - fabs(u_im1j + u_ij) * (u_im1j - u_ij))) / (FOUR * dx);\n    duvdy = ((v_ij + v_ip1j) * (u_ij + u_ijp1) - (v_ijm1 + v_ip1jm1) * (u_ijm1 + u_ij)\n        + mix_param * (fabs(v_ij + v_ip1j) * (u_ij - u_ijp1)\n          - fabs(v_ijm1 + v_ip1jm1) * (u_ijm1 - u_ij))) / (FOUR * dy);\n    d2udx2 = (u_ip1j - (TWO * u_ij) + u_im1j) / (dx * dx);\n    d2udy2 = (u_ijp1 - (TWO * u_ij) + u_ijm1) / (dy * dy);\n\n    F(col, row) = u_ij + dt * (((d2udx2 + d2udy2) / Re_num) - du2dx - duvdy + gx);\n\n  } \n\n\n} \n\n\n\n\n\n__global__ \nvoid calculate_G (const Real dt,\n                  const Real*__restrict__ u,\n                  const Real*__restrict__ v,\n                        Real*__restrict__ G) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  if (row == NUM) {\n    \n\n    G(col, 0) = v(col, 0);\n    G(col, NUM) = v(col, NUM);\n\n  } else {\n\n    \n\n    Real u_ij = u(col, row);\n    Real u_ijp1 = u(col, row + 1);\n    Real u_im1j = u(col - 1, row);\n    Real u_im1jp1 = u(col - 1, row + 1);\n\n    \n\n    Real v_ij = v(col, row);\n    Real v_ijp1 = v(col, row + 1);\n    Real v_ip1j = v(col + 1, row);\n    Real v_ijm1 = v(col, row - 1);\n    Real v_im1j = v(col - 1, row);\n\n    \n\n    Real dv2dy, duvdx, d2vdx2, d2vdy2;\n\n    dv2dy = ((v_ij + v_ijp1) * (v_ij + v_ijp1) - (v_ijm1 + v_ij) * (v_ijm1 + v_ij)\n        + mix_param * (fabs(v_ij + v_ijp1) * (v_ij - v_ijp1)\n          - fabs(v_ijm1 + v_ij) * (v_ijm1 - v_ij))) / (FOUR * dy);\n    duvdx = ((u_ij + u_ijp1) * (v_ij + v_ip1j) - (u_im1j + u_im1jp1) * (v_im1j + v_ij)\n        + mix_param * (fabs(u_ij + u_ijp1) * (v_ij - v_ip1j) \n          - fabs(u_im1j + u_im1jp1) * (v_im1j - v_ij))) / (FOUR * dx);\n    d2vdx2 = (v_ip1j - (TWO * v_ij) + v_im1j) / (dx * dx);\n    d2vdy2 = (v_ijp1 - (TWO * v_ij) + v_ijm1) / (dy * dy);\n\n    G(col, row) = v_ij + dt * (((d2vdx2 + d2vdy2) / Re_num) - dv2dy - duvdx + gy);\n\n  } \n\n\n} \n\n\n\n\n\n__global__ \nvoid sum_pressure (const Real*__restrict__ pres_red,\n                   const Real*__restrict__ pres_black, \n                         Real*__restrict__ pres_sum) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  \n\n  __shared__ Real sum_cache[BLOCK_SIZE];\n\n  int NUM_2 = NUM >> 1;\n\n  Real pres_r = pres_red(col, row);\n  Real pres_b = pres_black(col, row);\n\n  \n\n  sum_cache[threadIdx.x] = (pres_r * pres_r) + (pres_b * pres_b);\n\n  \n\n  __syncthreads();\n\n  \n\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      sum_cache[threadIdx.x] += sum_cache[threadIdx.x + i];\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  \n\n  if (threadIdx.x == 0) {\n    pres_sum[blockIdx.y + (gridDim.y * blockIdx.x)] = sum_cache[0];\n  }\n\n} \n\n\n\n\n__global__ \nvoid set_horz_pres_BCs (Real*__restrict__ pres_red, Real*__restrict__ pres_black) \n{\n  int col = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  col = (col * 2) - 1;\n\n  int NUM_2 = NUM >> 1;\n\n  \n\n  pres_black(col, 0) = pres_red(col, 1);\n  pres_red(col + 1, 0) = pres_black(col + 1, 1);\n\n  \n\n  pres_red(col, NUM_2 + 1) = pres_black(col, NUM_2);\n  pres_black(col + 1, NUM_2 + 1) = pres_red(col + 1, NUM_2);\n\n} \n\n\n\n\n\n__global__\nvoid set_vert_pres_BCs (Real*__restrict__ pres_red, Real*__restrict__ pres_black) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n\n  int NUM_2 = NUM >> 1;\n\n  \n\n  pres_black(0, row) = pres_red(1, row);\n  pres_red(0, row) = pres_black(1, row);\n\n  \n\n  pres_black(NUM + 1, row) = pres_red(NUM, row);\n  pres_red(NUM + 1, row) = pres_black(NUM, row);\n\n} \n\n\n\n\n\n\n\n__global__\nvoid red_kernel (const Real dt,\n                 const Real*__restrict__ F, \n                 const Real*__restrict__ G,\n                 const Real*__restrict__ pres_black,\n                       Real*__restrict__ pres_red) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  int NUM_2 = NUM >> 1;      \n\n  Real p_ij = pres_red(col, row);\n\n  Real p_im1j = pres_black(col - 1, row);\n  Real p_ip1j = pres_black(col + 1, row);\n  Real p_ijm1 = pres_black(col, row - (col & 1));\n  Real p_ijp1 = pres_black(col, row + ((col + 1) & 1));\n\n  \n\n  Real rhs = (((F(col, (2 * row) - (col & 1))\n          - F(col - 1, (2 * row) - (col & 1))) / dx)\n      + ((G(col, (2 * row) - (col & 1))\n          - G(col, (2 * row) - (col & 1) - 1)) / dy)) / dt;\n\n  pres_red(col, row) = p_ij * (ONE - omega) + omega * \n    (((p_ip1j + p_im1j) / (dx * dx)) + ((p_ijp1 + p_ijm1) / (dy * dy)) - \n     rhs) / ((TWO / (dx * dx)) + (TWO / (dy * dy)));\n\n} \n\n\n\n\n\n\n\n__global__ \nvoid black_kernel (const Real dt,\n                   const Real*__restrict__ F, \n                   const Real*__restrict__ G,\n                   const Real*__restrict__ pres_red, \n                         Real*__restrict__ pres_black) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  int NUM_2 = NUM >> 1;\n\n  Real p_ij = pres_black(col, row);\n\n  Real p_im1j = pres_red(col - 1, row);\n  Real p_ip1j = pres_red(col + 1, row);\n  Real p_ijm1 = pres_red(col, row - ((col + 1) & 1));\n  Real p_ijp1 = pres_red(col, row + (col & 1));\n\n  \n\n  Real rhs = (((F(col, (2 * row) - ((col + 1) & 1))\n          - F(col - 1, (2 * row) - ((col + 1) & 1))) / dx)\n      + ((G(col, (2 * row) - ((col + 1) & 1))\n          - G(col, (2 * row) - ((col + 1) & 1) - 1)) / dy)) / dt;\n\n  pres_black(col, row) = p_ij * (ONE - omega) + omega * \n    (((p_ip1j + p_im1j) / (dx * dx)) + ((p_ijp1 + p_ijm1) / (dy * dy)) - \n     rhs) / ((TWO / (dx * dx)) + (TWO / (dy * dy)));\n\n} \n\n\n\n\n\n__global__\nvoid calc_residual (const Real dt,\n                    const Real*__restrict__ F,\n                    const Real*__restrict__ G, \n                    const Real*__restrict__ pres_red,\n                    const Real*__restrict__ pres_black,\n                          Real*__restrict__ res_array)\n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  int NUM_2 = NUM >> 1;\n\n  Real p_ij, p_im1j, p_ip1j, p_ijm1, p_ijp1, rhs, res, res2;\n\n  \n\n  p_ij = pres_red(col, row);\n\n  p_im1j = pres_black(col - 1, row);\n  p_ip1j = pres_black(col + 1, row);\n  p_ijm1 = pres_black(col, row - (col & 1));\n  p_ijp1 = pres_black(col, row + ((col + 1) & 1));\n\n  rhs = (((F(col, (2 * row) - (col & 1)) - F(col - 1, (2 * row) - (col & 1))) / dx)\n      +  ((G(col, (2 * row) - (col & 1)) - G(col, (2 * row) - (col & 1) - 1)) / dy)) / dt;\n\n  \n\n  res = ((p_ip1j - (TWO * p_ij) + p_im1j) / (dx * dx))\n    + ((p_ijp1 - (TWO * p_ij) + p_ijm1) / (dy * dy)) - rhs;\n\n  \n\n  p_ij = pres_black(col, row);\n\n  p_im1j = pres_red(col - 1, row);\n  p_ip1j = pres_red(col + 1, row);\n  p_ijm1 = pres_red(col, row - ((col + 1) & 1));\n  p_ijp1 = pres_red(col, row + (col & 1));\n\n  \n\n  rhs = (((F(col, (2 * row) - ((col + 1) & 1)) - F(col - 1, (2 * row) - ((col + 1) & 1))) / dx)\n      +  ((G(col, (2 * row) - ((col + 1) & 1)) - G(col, (2 * row) - ((col + 1) & 1) - 1)) / dy)) / dt;\n\n  \n\n  res2 = ((p_ip1j - (TWO * p_ij) + p_im1j) / (dx * dx))\n    + ((p_ijp1 - (TWO * p_ij) + p_ijm1) / (dy * dy)) - rhs;\n\n  \n\n  __shared__ Real sum_cache[BLOCK_SIZE];\n\n  sum_cache[threadIdx.x] = (res * res) + (res2 * res2);\n\n  \n\n  __syncthreads();\n\n  \n\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      sum_cache[threadIdx.x] += sum_cache[threadIdx.x + i];\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  \n\n  if (threadIdx.x == 0) {\n    res_array[blockIdx.y + (gridDim.y * blockIdx.x)] = sum_cache[0];\n  }\n} \n\n\n\n\n__global__ \nvoid calculate_u (const Real dt,\n                  const Real*__restrict__ F, \n                  const Real*__restrict__ pres_red,\n                  const Real*__restrict__ pres_black, \n                        Real*__restrict__ u,\n                        Real*__restrict__ max_u)\n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  \n\n  __shared__ Real max_cache[BLOCK_SIZE];\n  max_cache[threadIdx.x] = ZERO;\n\n  int NUM_2 = NUM >> 1;\n  Real new_u = ZERO;\n\n  if (col != NUM) {\n\n    Real p_ij, p_ip1j, new_u2;\n\n    \n\n    p_ij = pres_red(col, row);\n    p_ip1j = pres_black(col + 1, row);\n\n    new_u = F(col, (2 * row) - (col & 1)) - (dt * (p_ip1j - p_ij) / dx);\n    u(col, (2 * row) - (col & 1)) = new_u;\n\n    \n\n    p_ij = pres_black(col, row);\n    p_ip1j = pres_red(col + 1, row);\n\n    new_u2 = F(col, (2 * row) - ((col + 1) & 1)) - (dt * (p_ip1j - p_ij) / dx);\n    u(col, (2 * row) - ((col + 1) & 1)) = new_u2;\n\n    \n\n    new_u = fmax(fabs(new_u), fabs(new_u2));\n\n    if ((2 * row) == NUM) {\n      \n\n      new_u = fmax(new_u, fabs( u(col, NUM + 1) ));\n    }\n  } else {\n    \n\n    new_u = fmax(fabs( u(NUM, (2 * row)) ), fabs( u(0, (2 * row)) ));\n    new_u = fmax(fabs( u(NUM, (2 * row) - 1) ), new_u);\n    new_u = fmax(fabs( u(0, (2 * row) - 1) ), new_u);\n\n    new_u = fmax(fabs( u(NUM + 1, (2 * row)) ), new_u);\n    new_u = fmax(fabs( u(NUM + 1, (2 * row) - 1) ), new_u);\n\n  } \n\n\n  \n\n  max_cache[threadIdx.x] = new_u;\n\n  \n\n  __syncthreads();\n\n  \n\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      max_cache[threadIdx.x] = fmax(max_cache[threadIdx.x], max_cache[threadIdx.x + i]);\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  \n\n  if (threadIdx.x == 0) {\n    max_u[blockIdx.y + (gridDim.y * blockIdx.x)] = max_cache[0];\n  }\n} \n\n\n\n\n\n__global__ \nvoid calculate_v (const Real dt,\n                  const Real*__restrict__ G, \n                  const Real*__restrict__ pres_red,\n                  const Real*__restrict__ pres_black, \n                        Real*__restrict__ v,\n                        Real*__restrict__ max_v)\n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  \n\n  __shared__ Real max_cache[BLOCK_SIZE];\n  max_cache[threadIdx.x] = ZERO;\n\n  int NUM_2 = NUM >> 1;\n  Real new_v = ZERO;\n\n  if (row != NUM_2) {\n    Real p_ij, p_ijp1, new_v2;\n\n    \n\n    p_ij = pres_red(col, row);\n    p_ijp1 = pres_black(col, row + ((col + 1) & 1));\n\n    new_v = G(col, (2 * row) - (col & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n    v(col, (2 * row) - (col & 1)) = new_v;\n\n    \n\n    p_ij = pres_black(col, row);\n    p_ijp1 = pres_red(col, row + (col & 1));\n\n    new_v2 = G(col, (2 * row) - ((col + 1) & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n    v(col, (2 * row) - ((col + 1) & 1)) = new_v2;\n\n    \n\n    new_v = fmax(fabs(new_v), fabs(new_v2));\n\n    if (col == NUM) {\n      \n\n      new_v = fmax(new_v, fabs( v(NUM + 1, (2 * row)) ));\n    }\n\n  } else {\n\n    if ((col & 1) == 1) {\n      \n\n      Real p_ij = pres_red(col, row);\n      Real p_ijp1 = pres_black(col, row + ((col + 1) & 1));\n\n      new_v = G(col, (2 * row) - (col & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n      v(col, (2 * row) - (col & 1)) = new_v;\n    } else {\n      \n\n      Real p_ij = pres_black(col, row);\n      Real p_ijp1 = pres_red(col, row + (col & 1));\n\n      new_v = G(col, (2 * row) - ((col + 1) & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n      v(col, (2 * row) - ((col + 1) & 1)) = new_v;\n    }\n\n    \n\n    new_v = fabs(new_v);\n\n    \n\n    new_v = fmax(fabs( v(col, NUM) ), new_v);\n    new_v = fmax(fabs( v(col, 0) ), new_v);\n\n    new_v = fmax(fabs( v(col, NUM + 1) ), new_v);\n  } \n\n\n  \n\n  max_cache[threadIdx.x] = new_v;\n\n  \n\n  __syncthreads();\n\n  \n\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      max_cache[threadIdx.x] = fmax(max_cache[threadIdx.x], max_cache[threadIdx.x + i]);\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  \n\n  if (threadIdx.x == 0) {\n    max_v[blockIdx.y + (gridDim.y * blockIdx.x)] = max_cache[0];\n  }\n} \n\n\n\n\n\nint main (int argc, char *argv[])\n{\n  \n\n  int iter = 0;\n  const int it_max = 1000000;\n\n  \n\n  const Real tol = 0.001;\n\n  \n\n  const Real time_start = 0.0;\n  const Real time_end = 0.001; \n\n\n  \n\n  Real dt = 0.02;\n\n  int size = (NUM + 2) * (NUM + 2);\n  int size_pres = ((NUM / 2) + 2) * (NUM + 2);\n\n  \n\n  Real* F;\n  Real* u;\n  Real* G;\n  Real* v;\n\n  F = (Real *) calloc (size, sizeof(Real));\n  u = (Real *) calloc (size, sizeof(Real));\n  G = (Real *) calloc (size, sizeof(Real));\n  v = (Real *) calloc (size, sizeof(Real));\n\n  for (int i = 0; i < size; ++i) {\n    F[i] = ZERO;\n    u[i] = ZERO;\n    G[i] = ZERO;\n    v[i] = ZERO;\n  }\n\n  \n\n  Real* pres_red;\n  Real* pres_black;\n\n  pres_red = (Real *) calloc (size_pres, sizeof(Real));\n  pres_black = (Real *) calloc (size_pres, sizeof(Real));\n\n  for (int i = 0; i < size_pres; ++i) {\n    pres_red[i] = ZERO;\n    pres_black[i] = ZERO;\n  }\n\n  \n\n  printf(\"Problem size: %d x %d \\n\", NUM, NUM);\n\n  \n\n\n  \n\n  dim3 block_bcs (BLOCK_SIZE, 1);\n  dim3 grid_bcs (NUM / BLOCK_SIZE, 1);\n\n  \n\n  dim3 block_pr (BLOCK_SIZE, 1);\n  dim3 grid_pr (NUM / (2 * BLOCK_SIZE), NUM);\n\n  \n\n  dim3 block_F (BLOCK_SIZE, 1);\n  dim3 grid_F (NUM / BLOCK_SIZE, NUM);\n\n  \n\n  dim3 block_G (BLOCK_SIZE, 1);\n  dim3 grid_G (NUM / BLOCK_SIZE, NUM);\n\n  \n\n  dim3 block_hpbc (BLOCK_SIZE, 1);\n  dim3 grid_hpbc (NUM / (2 * BLOCK_SIZE), 1);\n\n  \n\n  dim3 block_vpbc (BLOCK_SIZE, 1);\n  dim3 grid_vpbc (NUM / (2 * BLOCK_SIZE), 1);\n\n  \n\n  Real* res;\n\n  int size_res = grid_pr.x * grid_pr.y;\n  res = (Real *) calloc (size_res, sizeof(Real));\n\n  \n\n  Real* max_u_arr;\n  Real* max_v_arr;\n  int size_max = grid_pr.x * grid_pr.y;\n\n  max_u_arr = (Real *) calloc (size_max, sizeof(Real));\n  max_v_arr = (Real *) calloc (size_max, sizeof(Real));\n\n  \n\n  Real* pres_sum;\n  pres_sum = (Real *) calloc (size_res, sizeof(Real));\n\n  \n\n  set_BCs_host (u, v);\n\n  Real max_u = SMALL;\n  Real max_v = SMALL;\n  \n\n  #pragma unroll\n  for (int col = 0; col < NUM + 2; ++col) {\n    #pragma unroll\n    for (int row = 1; row < NUM + 2; ++row) {\n      max_u = fmax(max_u, fabs( u(col, row) ));\n    }\n  }\n\n  #pragma unroll\n  for (int col = 1; col < NUM + 2; ++col) {\n    #pragma unroll\n    for (int row = 0; row < NUM + 2; ++row) {\n      max_v = fmax(max_v, fabs( v(col, row) ));\n    }\n  }\n\n  \n\n  Real* u_d;\n  Real* F_d;\n  Real* v_d;\n  Real* G_d;\n\n  Real* pres_red_d;\n  Real* pres_black_d;\n  Real* pres_sum_d;\n  Real* res_d;\n\n  Real* max_u_d;\n  Real* max_v_d;\n\n  hipMalloc ((void**) &u_d, size * sizeof(Real));\n  hipMalloc ((void**) &F_d, size * sizeof(Real));\n  hipMalloc ((void**) &v_d, size * sizeof(Real));\n  hipMalloc ((void**) &G_d, size * sizeof(Real));\n\n  hipMalloc ((void**) &pres_red_d, size_pres * sizeof(Real));\n  hipMalloc ((void**) &pres_black_d, size_pres * sizeof(Real));\n\n  hipMalloc ((void**) &pres_sum_d, size_res * sizeof(Real));\n  hipMalloc ((void**) &res_d, size_res * sizeof(Real));\n  hipMalloc ((void**) &max_u_d, size_max * sizeof(Real));\n  hipMalloc ((void**) &max_v_d, size_max * sizeof(Real));\n\n  \n\n  hipMemcpy (u_d, u, size * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (F_d, F, size * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (v_d, v, size * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (G_d, G, size * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (pres_red_d, pres_red, size_pres * sizeof(Real), hipMemcpyHostToDevice);\n  hipMemcpy (pres_black_d, pres_black, size_pres * sizeof(Real), hipMemcpyHostToDevice);\n\n  Real time = time_start;\n\n  \n\n  Real dt_Re = 0.5 * Re_num / ((1.0 / (dx * dx)) + (1.0 / (dy * dy)));\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  while (time < time_end) {\n\n    \n\n    dt = fmin((dx / max_u), (dy / max_v));\n    dt = tau * fmin(dt_Re, dt);\n\n    if ((time + dt) >= time_end) {\n      dt = time_end - time;\n    }\n\n    \n\n    hipLaunchKernelGGL(calculate_F, grid_F, block_F, 0, 0, dt, u_d, v_d, F_d);\n    hipLaunchKernelGGL(calculate_G, grid_G, block_G, 0, 0, dt, u_d, v_d, G_d);\n\n    \n\n    hipLaunchKernelGGL(sum_pressure, grid_pr, block_pr, 0, 0, pres_red_d, pres_black_d, pres_sum_d);\n    hipMemcpy (pres_sum, pres_sum_d, size_res * sizeof(Real), hipMemcpyDeviceToHost);\n\n    Real p0_norm = ZERO;\n    #pragma unroll\n    for (int i = 0; i < size_res; ++i) {\n      p0_norm += pres_sum[i];\n    }\n\n    p0_norm = sqrt(p0_norm / ((Real)(NUM * NUM)));\n    if (p0_norm < 0.0001) {\n      p0_norm = 1.0;\n    }\n\n    \n\n    \n\n\n    Real norm_L2;\n\n    \n\n    \n\n    for (iter = 1; iter <= it_max; ++iter) {\n\n      \n\n      hipLaunchKernelGGL(set_horz_pres_BCs, grid_hpbc, block_hpbc, 0, 0, pres_red_d, pres_black_d);\n      hipLaunchKernelGGL(set_vert_pres_BCs, grid_vpbc, block_vpbc, 0, 0, pres_red_d, pres_black_d);\n\n      \n\n      \n\n\n      \n\n      hipLaunchKernelGGL(red_kernel, grid_pr, block_pr, 0, 0, dt, F_d, G_d, pres_black_d, pres_red_d);\n\n      \n\n      \n\n\n      \n\n      hipLaunchKernelGGL(black_kernel, grid_pr, block_pr, 0, 0, dt, F_d, G_d, pres_red_d, pres_black_d);\n\n      \n\n      \n\n\n      \n\n      hipLaunchKernelGGL(calc_residual, grid_pr, block_pr, 0, 0, dt, F_d, G_d, pres_red_d, pres_black_d, res_d);\n\n      \n\n      hipMemcpy (res, res_d, size_res * sizeof(Real), hipMemcpyDeviceToHost);\n\n      norm_L2 = ZERO;\n      #pragma unroll\n      for (int i = 0; i < size_res; ++i) {\n        norm_L2 += res[i];\n      }\n\n      \n\n      norm_L2 = sqrt(norm_L2 / ((Real)(NUM * NUM))) / p0_norm;\n\n      \n\n      if (norm_L2 < tol) {\n        break;\n      }  \n    } \n\n\n    printf(\"Time = %f, delt = %e, iter = %i, res = %e\\n\", time + dt, dt, iter, norm_L2);\n\n    \n\n\n    hipLaunchKernelGGL(calculate_u, grid_pr, block_pr, 0, 0, dt, F_d, pres_red_d, pres_black_d, u_d, max_u_d);\n    hipMemcpy (max_u_arr, max_u_d, size_max * sizeof(Real), hipMemcpyDeviceToHost);\n\n    hipLaunchKernelGGL(calculate_v, grid_pr, block_pr, 0, 0, dt, G_d, pres_red_d, pres_black_d, v_d, max_v_d);\n    hipMemcpy (max_v_arr, max_v_d, size_max * sizeof(Real), hipMemcpyDeviceToHost);\n\n    \n\n    max_v = SMALL;\n    max_u = SMALL;\n\n    #pragma unroll\n    for (int i = 0; i < size_max; ++i) {\n      Real test_u = max_u_arr[i];\n      max_u = fmax(max_u, test_u);\n\n      Real test_v = max_v_arr[i];\n      max_v = fmax(max_v, test_v);\n    }\n\n    \n\n    hipLaunchKernelGGL(set_BCs, grid_bcs, block_bcs, 0, 0, u_d, v_d);\n\n    \n\n    time += dt;\n\n    \n\n    \n\n\n  } \n\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto elapsed_time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"\\nTotal execution time of the iteration loop: %f (s)\\n\", elapsed_time * 1e-9f);\n\n  \n\n  hipMemcpy (u, u_d, size * sizeof(Real), hipMemcpyDeviceToHost);\n  hipMemcpy (v, v_d, size * sizeof(Real), hipMemcpyDeviceToHost);\n  hipMemcpy (pres_red, pres_red_d, size_pres * sizeof(Real), hipMemcpyDeviceToHost);\n  hipMemcpy (pres_black, pres_black_d, size_pres * sizeof(Real), hipMemcpyDeviceToHost);\n\n  \n\n  FILE * pfile;\n  pfile = fopen(\"velocity_gpu.dat\", \"w\");\n  fprintf(pfile, \"#x\\ty\\tu\\tv\\n\");\n  if (pfile != NULL) {\n    for (int row = 0; row < NUM; ++row) {\n      for (int col = 0; col < NUM; ++col) {\n\n        Real u_ij = u[(col * NUM) + row];\n        Real u_im1j;\n        if (col == 0) {\n          u_im1j = 0.0;\n        } else {\n          u_im1j = u[(col - 1) * NUM + row];\n        }\n\n        u_ij = (u_ij + u_im1j) / 2.0;\n\n        Real v_ij = v[(col * NUM) + row];\n        Real v_ijm1;\n        if (row == 0) {\n          v_ijm1 = 0.0;\n        } else {\n          v_ijm1 = v[(col * NUM) + row - 1];\n        }\n\n        v_ij = (v_ij + v_ijm1) / 2.0;\n\n        fprintf(pfile, \"%f\\t%f\\t%f\\t%f\\n\", ((Real)col + 0.5) * dx, ((Real)row + 0.5) * dy, u_ij, v_ij);\n      }\n    }\n  }\n\n  fclose(pfile);\n\n  \n\n  hipFree(u_d);\n  hipFree(v_d);\n  hipFree(F_d);\n  hipFree(G_d);\n  hipFree(pres_red_d);\n  hipFree(pres_black_d);\n  hipFree(max_u_d);\n  hipFree(max_v_d);\n  hipFree(pres_sum_d);\n  hipFree(res_d);\n\n  free(pres_red);\n  free(pres_black);\n  free(u);\n  free(v);\n  free(F);\n  free(G);\n  free(max_u_arr);\n  free(max_v_arr);\n  free(res);\n  free(pres_sum);\n\n  return 0;\n}\n"}}
{"kernel_name": "lid-driven-cavity", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <chrono>\n#include <omp.h>\n\n\n\n#define NUM 512\n\n\n\n#define BLOCK_SIZE 128\n\n\n\n#define DOUBLE\n\n#ifdef DOUBLE\n#define Real double\n\n#define ZERO 0.0\n#define ONE 1.0\n#define TWO 2.0\n#define FOUR 4.0\n\n#define SMALL 1.0e-10;\n\n\n\nconst Real Re_num = 1000.0;\n\n\n\nconst Real omega = 1.7;\n\n\n\nconst Real mix_param = 0.9;\n\n\n\nconst Real tau = 0.5;\n\n\n\nconst Real gx = 0.0;\nconst Real gy = 0.0;\n\n\n\n#define xLength 1.0\n#define yLength 1.0\n\n#else\n\n#define Real float\n\n\n\n#undef fmin\n#define fmin fminf\n#undef fmax\n#define fmax fmaxf\n#undef fabs\n#define fabs fabsf\n#undef sqrt\n#define sqrt sqrtf\n\n#define ZERO 0.0f\n#define ONE 1.0f\n#define TWO 2.0f\n#define FOUR 4.0f\n#define SMALL 1.0e-10f;\n\n\n\nconst Real Re_num = 1000.0f;\n\n\n\nconst Real omega = 1.7f;\n\n\n\nconst Real mix_param = 0.9f;\n\n\n\nconst Real tau = 0.5f;\n\n\n\nconst Real gx = 0.0f;\nconst Real gy = 0.0f;\n\n\n\n#define xLength 1.0f\n#define yLength 1.0f\n#endif\n\n\n\nconst Real dx = xLength / NUM;\nconst Real dy = yLength / NUM;\n\n\n\n\n\n\n\n\n\n\n\n\n\n#define u(I, J) u[((I) * ((NUM) + 2)) + (J)]\n#define v(I, J) v[((I) * ((NUM) + 2)) + (J)]\n#define F(I, J) F[((I) * ((NUM) + 2)) + (J)]\n#define G(I, J) G[((I) * ((NUM) + 2)) + (J)]\n#define pres_red(I, J) pres_red[((I) * ((NUM_2) + 2)) + (J)]\n#define pres_black(I, J) pres_black[((I) * ((NUM_2) + 2)) + (J)]\n\n\n\nvoid set_BCs_host (Real* u, Real* v) \n{\n  int ind;\n\n  \n\n  for (ind = 0; ind < NUM + 2; ++ind) {\n\n    \n\n    u(0, ind) = ZERO;\n    v(0, ind) = -v(1, ind);\n\n    \n\n    u(NUM, ind) = ZERO;\n    v(NUM + 1, ind) = -v(NUM, ind);\n\n    \n\n    u(ind, 0) = -u(ind, 1);\n    v(ind, 0) = ZERO;\n\n    \n\n    u(ind, NUM + 1) = TWO - u(ind, NUM);\n    v(ind, NUM) = ZERO;\n\n    if (ind == NUM) {\n      \n\n      u(0, 0) = ZERO;\n      v(0, 0) = -v(1, 0);\n      u(0, NUM + 1) = ZERO;\n      v(0, NUM + 1) = -v(1, NUM + 1);\n\n      \n\n      u(NUM, 0) = ZERO;\n      v(NUM + 1, 0) = -v(NUM, 0);\n      u(NUM, NUM + 1) = ZERO;\n      v(NUM + 1, NUM + 1) = -v(NUM, NUM + 1);\n\n      \n\n      u(0, 0) = -u(0, 1);\n      v(0, 0) = ZERO;\n      u(NUM + 1, 0) = -u(NUM + 1, 1);\n      v(NUM + 1, 0) = ZERO;\n\n      \n\n      u(0, NUM + 1) = TWO - u(0, NUM);\n      v(0, NUM) = ZERO;\n      u(NUM + 1, NUM + 1) = TWO - u(NUM + 1, NUM);\n      v(ind, NUM + 1) = ZERO;\n    } \n\n\n  } \n\n\n} \n\n\n\n\n\nint main (int argc, char *argv[])\n{\n  \n\n  int iter = 0;\n\n  const int it_max = 1000000;\n\n  \n\n  const Real tol = 0.001;\n\n  \n\n  const Real time_start = 0.0;\n  const Real time_end = 0.001; \n\n\n  \n\n  Real dt = 0.02;\n\n  int size = (NUM + 2) * (NUM + 2);\n  int size_pres = ((NUM / 2) + 2) * (NUM + 2);\n\n  \n\n  Real* F;\n  Real* u;\n  Real* G;\n  Real* v;\n\n  F = (Real *) calloc (size, sizeof(Real));\n  u = (Real *) calloc (size, sizeof(Real));\n  G = (Real *) calloc (size, sizeof(Real));\n  v = (Real *) calloc (size, sizeof(Real));\n\n  for (int i = 0; i < size; ++i) {\n    F[i] = ZERO;\n    u[i] = ZERO;\n    G[i] = ZERO;\n    v[i] = ZERO;\n  }\n\n  \n\n  Real* pres_red;\n  Real* pres_black;\n\n  pres_red = (Real *) calloc (size_pres, sizeof(Real));\n  pres_black = (Real *) calloc (size_pres, sizeof(Real));\n\n  for (int i = 0; i < size_pres; ++i) {\n    pres_red[i] = ZERO;\n    pres_black[i] = ZERO;\n  }\n\n  \n\n  printf(\"Problem size: %d x %d \\n\", NUM, NUM);\n\n  \n\n  Real* res_arr;\n\n  int size_res = NUM / (2 * BLOCK_SIZE) * NUM;\n  res_arr = (Real *) calloc (size_res, sizeof(Real));\n\n  \n\n  Real* max_u_arr;\n  Real* max_v_arr;\n  int size_max = size_res;\n\n  max_u_arr = (Real *) calloc (size_max, sizeof(Real));\n  max_v_arr = (Real *) calloc (size_max, sizeof(Real));\n\n  \n\n  Real* pres_sum;\n  pres_sum = (Real *) calloc (size_res, sizeof(Real));\n\n  \n\n  set_BCs_host (u, v);\n\n  Real max_u = SMALL;\n  Real max_v = SMALL;\n  \n\n  #pragma unroll\n  for (int col = 0; col < NUM + 2; ++col) {\n    #pragma unroll\n    for (int row = 1; row < NUM + 2; ++row) {\n      max_u = fmax(max_u, fabs( u(col, row) ));\n    }\n  }\n\n  #pragma unroll\n  for (int col = 1; col < NUM + 2; ++col) {\n    #pragma unroll\n    for (int row = 0; row < NUM + 2; ++row) {\n      max_v = fmax(max_v, fabs( v(col, row) ));\n    }\n  }\n\n#pragma omp target data map(tofrom: u[0:size], \\\n                                    v[0:size], \\\n                                    pres_red[0:size_pres], \\\n                                    pres_black[0:size_pres]) \\\n                        map(to: F[0:size], G[0:size]) \\\n                        map(alloc: pres_sum[0:size_res], \\\n                                   res_arr[0:size_res], \\\n                                   max_u_arr[0:size_max],\\\n                                   max_v_arr[0:size_max])\n  {\n    Real time = time_start;\n\n    \n\n    Real dt_Re = 0.5 * Re_num / ((1.0 / (dx * dx)) + (1.0 / (dy * dy)));\n\n    auto start = std::chrono::steady_clock::now();\n\n    \n\n    while (time < time_end) {\n\n      \n\n      dt = fmin((dx / max_u), (dy / max_v));\n      dt = tau * fmin(dt_Re, dt);\n\n      if ((time + dt) >= time_end) {\n        dt = time_end - time;\n      }\n\n      \n\n      \n\n      #include \"calculate_F.h\"\n\n      \n\n      #include \"calculate_G.h\"\n\n      \n\n      \n\n      #include \"sum_pressure.h\"\n\n      \n\n      #pragma omp target update from(pres_sum[0:size_res])\n\n      Real p0_norm = ZERO;\n      #pragma unroll\n      for (int i = 0; i < size_res; ++i) {\n        p0_norm += pres_sum[i];\n      }\n      \n\n\n      p0_norm = sqrt(p0_norm / ((Real)(NUM * NUM)));\n      if (p0_norm < 0.0001) {\n        p0_norm = 1.0;\n      }\n\n      Real norm_L2;\n\n      \n\n      \n\n      for (iter = 1; iter <= it_max; ++iter) {\n\n        \n\n        \n\n        #include \"set_horz_pres_BCs.h\"\n\n        \n\n        #include \"set_vert_pres_BCs.h\"\n\n        \n\n        \n\n        #include \"red_kernel.h\"\n\n        \n\n        \n\n        #include \"black_kernel.h\"\n\n        \n\n        \n\n        #include \"calc_residual.h\"\n\n        #pragma omp target update from (res_arr[0:size_res])\n        \n\n        \n\n\n        norm_L2 = ZERO;\n\n        #pragma unroll\n        for (int i = 0; i < size_res; ++i) {\n          norm_L2 += res_arr[i];\n        }\n\n      \n\n\n        \n\n        norm_L2 = sqrt(norm_L2 / ((Real)(NUM * NUM))) / p0_norm;\n\n        \n\n        if (norm_L2 < tol) {\n          break;\n        }  \n      } \n\n\n      printf(\"Time = %f, delt = %e, iter = %i, res = %e\\n\", time + dt, dt, iter, norm_L2);\n\n      \n\n\n      \n\n      #include \"calculate_u.h\"\n\n      \n\n      #pragma omp target update from(max_u_arr[0:size_max])\n\n      \n\n      #include \"calculate_v.h\"\n\n      \n\n      #pragma omp target update from(max_v_arr[0:size_max])\n\n      \n\n      max_v = SMALL;\n      max_u = SMALL;\n\n      #pragma unroll\n      for (int i = 0; i < size_max; ++i) {\n        Real test_u = max_u_arr[i];\n        max_u = fmax(max_u, test_u);\n\n        Real test_v = max_v_arr[i];\n        max_v = fmax(max_v, test_v);\n      }\n\n      \n\n      \n\n      #include \"set_BCs.h\"\n\n      \n\n      time += dt;\n\n      \n\n      \n\n\n    } \n\n\n    auto end = std::chrono::steady_clock::now();\n    auto elapsed_time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"\\nTotal execution time of the iteration loop: %f (s)\\n\", elapsed_time * 1e-9f);\n  }\n  \n\n\n  \n\n  FILE * pfile;\n  pfile = fopen(\"velocity_gpu.dat\", \"w\");\n  fprintf(pfile, \"#x\\ty\\tu\\tv\\n\");\n  if (pfile != NULL) {\n    for (int row = 0; row < NUM; ++row) {\n      for (int col = 0; col < NUM; ++col) {\n\n        Real u_ij = u[(col * NUM) + row];\n        Real u_im1j;\n        if (col == 0) {\n          u_im1j = 0.0;\n        } else {\n          u_im1j = u[(col - 1) * NUM + row];\n        }\n\n        u_ij = (u_ij + u_im1j) / 2.0;\n\n        Real v_ij = v[(col * NUM) + row];\n        Real v_ijm1;\n        if (row == 0) {\n          v_ijm1 = 0.0;\n        } else {\n          v_ijm1 = v[(col * NUM) + row - 1];\n        }\n\n        v_ij = (v_ij + v_ijm1) / 2.0;\n\n        fprintf(pfile, \"%f\\t%f\\t%f\\t%f\\n\", ((Real)col + 0.5) * dx, ((Real)row + 0.5) * dy, u_ij, v_ij);\n      }\n    }\n  }\n\n  fclose(pfile);\n\n  free(pres_red);\n  free(pres_black);\n  free(u);\n  free(v);\n  free(F);\n  free(G);\n  free(max_u_arr);\n  free(max_v_arr);\n  free(res_arr);\n  free(pres_sum);\n  return 0;\n}\n"}}
{"kernel_name": "lid-driven-cavity", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <chrono>\n\n\n\n#define NUM 512\n\n\n\n#define BLOCK_SIZE 128\n\n\n\n#define DOUBLE\n\n#ifdef DOUBLE\n#define Real double\n\n#define ZERO 0.0\n#define ONE 1.0\n#define TWO 2.0\n#define FOUR 4.0\n\n#define SMALL 1.0e-10;\n\n\n\nconst Real Re_num = 1000.0;\n\n\n\nconst Real omega = 1.7;\n\n\n\nconst Real mix_param = 0.9;\n\n\n\nconst Real tau = 0.5;\n\n\n\nconst Real gx = 0.0;\nconst Real gy = 0.0;\n\n\n\n#define xLength 1.0\n#define yLength 1.0\n\n#else\n\n#define Real float\n\n\n\n#define fmin fminf\n#define fmax fmaxf\n#define fabs fabsf\n#define sqrt sqrtf\n\n#define ZERO 0.0f\n#define ONE 1.0f\n#define TWO 2.0f\n#define FOUR 4.0f\n#define SMALL 1.0e-10f;\n\n\n\nconst Real Re_num = 1000.0f;\n\n\n\nconst Real omega = 1.7f;\n\n\n\nconst Real mix_param = 0.9f;\n\n\n\nconst Real tau = 0.5f;\n\n\n\nconst Real gx = 0.0f;\nconst Real gy = 0.0f;\n\n\n\n#define xLength 1.0f\n#define yLength 1.0f\n#endif\n\n\n\nconst Real dx = xLength / NUM;\nconst Real dy = yLength / NUM;\n\n\n\n\n\n\n\n\n\n\n\n\n\n#define u(I, J) u[((I) * ((NUM) + 2)) + (J)]\n#define v(I, J) v[((I) * ((NUM) + 2)) + (J)]\n#define F(I, J) F[((I) * ((NUM) + 2)) + (J)]\n#define G(I, J) G[((I) * ((NUM) + 2)) + (J)]\n#define pres_red(I, J) pres_red[((I) * ((NUM_2) + 2)) + (J)]\n#define pres_black(I, J) pres_black[((I) * ((NUM_2) + 2)) + (J)]\n\n\n\nvoid set_BCs_host (Real* u, Real* v) \n{\n  int ind;\n\n  \n\n  for (ind = 0; ind < NUM + 2; ++ind) {\n\n    \n\n    u(0, ind) = ZERO;\n    v(0, ind) = -v(1, ind);\n\n    \n\n    u(NUM, ind) = ZERO;\n    v(NUM + 1, ind) = -v(NUM, ind);\n\n    \n\n    u(ind, 0) = -u(ind, 1);\n    v(ind, 0) = ZERO;\n\n    \n\n    u(ind, NUM + 1) = TWO - u(ind, NUM);\n    v(ind, NUM) = ZERO;\n\n    if (ind == NUM) {\n      \n\n      u(0, 0) = ZERO;\n      v(0, 0) = -v(1, 0);\n      u(0, NUM + 1) = ZERO;\n      v(0, NUM + 1) = -v(1, NUM + 1);\n\n      \n\n      u(NUM, 0) = ZERO;\n      v(NUM + 1, 0) = -v(NUM, 0);\n      u(NUM, NUM + 1) = ZERO;\n      v(NUM + 1, NUM + 1) = -v(NUM, NUM + 1);\n\n      \n\n      u(0, 0) = -u(0, 1);\n      v(0, 0) = ZERO;\n      u(NUM + 1, 0) = -u(NUM + 1, 1);\n      v(NUM + 1, 0) = ZERO;\n\n      \n\n      u(0, NUM + 1) = TWO - u(0, NUM);\n      v(0, NUM) = ZERO;\n      u(NUM + 1, NUM + 1) = TWO - u(NUM + 1, NUM);\n      v(ind, NUM + 1) = ZERO;\n    } \n\n\n  } \n\n\n} \n\n\n\n\n\nint main (int argc, char *argv[])\n{\n  \n\n  int iter = 0;\n\n  const int it_max = 1000000;\n\n  \n\n  const Real tol = 0.001;\n\n  \n\n  const Real time_start = 0.0;\n  const Real time_end = 0.001; \n\n\n  \n\n  Real dt = 0.02;\n\n  int size = (NUM + 2) * (NUM + 2);\n  int size_pres = ((NUM / 2) + 2) * (NUM + 2);\n\n  \n\n  Real* F;\n  Real* u;\n  Real* G;\n  Real* v;\n\n  F = (Real *) calloc (size, sizeof(Real));\n  u = (Real *) calloc (size, sizeof(Real));\n  G = (Real *) calloc (size, sizeof(Real));\n  v = (Real *) calloc (size, sizeof(Real));\n\n  for (int i = 0; i < size; ++i) {\n    F[i] = ZERO;\n    u[i] = ZERO;\n    G[i] = ZERO;\n    v[i] = ZERO;\n  }\n\n  \n\n  Real* pres_red;\n  Real* pres_black;\n\n  pres_red = (Real *) calloc (size_pres, sizeof(Real));\n  pres_black = (Real *) calloc (size_pres, sizeof(Real));\n\n  for (int i = 0; i < size_pres; ++i) {\n    pres_red[i] = ZERO;\n    pres_black[i] = ZERO;\n  }\n\n  \n\n  printf(\"Problem size: %d x %d \\n\", NUM, NUM);\n\n  \n\n  Real* res_arr;\n\n  int size_res = NUM / (2 * BLOCK_SIZE) * NUM;\n  res_arr = (Real *) calloc (size_res, sizeof(Real));\n\n  \n\n  Real* max_u_arr;\n  Real* max_v_arr;\n  int size_max = size_res;\n\n  max_u_arr = (Real *) calloc (size_max, sizeof(Real));\n  max_v_arr = (Real *) calloc (size_max, sizeof(Real));\n\n  \n\n  Real* pres_sum;\n  pres_sum = (Real *) calloc (size_res, sizeof(Real));\n\n  \n\n  set_BCs_host (u, v);\n\n  Real max_u = SMALL;\n  Real max_v = SMALL;\n  \n\n    for (int col = 0; col < NUM + 2; ++col) {\n        for (int row = 1; row < NUM + 2; ++row) {\n      max_u = fmax(max_u, fabs( u(col, row) ));\n    }\n  }\n\n    for (int col = 1; col < NUM + 2; ++col) {\n        for (int row = 0; row < NUM + 2; ++row) {\n      max_v = fmax(max_v, fabs( v(col, row) ));\n    }\n  }\n\n  {\n    Real time = time_start;\n\n    \n\n    Real dt_Re = 0.5 * Re_num / ((1.0 / (dx * dx)) + (1.0 / (dy * dy)));\n\n    auto start = std::chrono::steady_clock::now();\n\n    \n\n    while (time < time_end) {\n\n      \n\n      dt = fmin((dx / max_u), (dy / max_v));\n      dt = tau * fmin(dt_Re, dt);\n\n      if ((time + dt) >= time_end) {\n        dt = time_end - time;\n      }\n\n      \n\n      \n\n      #include \"calculate_F.h\"\n\n      \n\n      #include \"calculate_G.h\"\n\n      \n\n      \n\n      #include \"sum_pressure.h\"\n\n      \n\n      \n      Real p0_norm = ZERO;\n            for (int i = 0; i < size_res; ++i) {\n        p0_norm += pres_sum[i];\n      }\n      \n\n\n      p0_norm = sqrt(p0_norm / ((Real)(NUM * NUM)));\n      if (p0_norm < 0.0001) {\n        p0_norm = 1.0;\n      }\n\n      Real norm_L2;\n\n      \n\n      \n\n      for (iter = 1; iter <= it_max; ++iter) {\n\n        \n\n        \n\n        #include \"set_horz_pres_BCs.h\"\n\n        \n\n        #include \"set_vert_pres_BCs.h\"\n\n        \n\n        \n\n        #include \"red_kernel.h\"\n\n        \n\n        \n\n        #include \"black_kernel.h\"\n\n        \n\n        \n\n        #include \"calc_residual.h\"\n\n                \n\n        \n\n\n        norm_L2 = ZERO;\n\n                for (int i = 0; i < size_res; ++i) {\n          norm_L2 += res_arr[i];\n        }\n\n      \n\n\n        \n\n        norm_L2 = sqrt(norm_L2 / ((Real)(NUM * NUM))) / p0_norm;\n\n        \n\n        if (norm_L2 < tol) {\n          break;\n        }  \n      } \n\n\n      printf(\"Time = %f, delt = %e, iter = %i, res = %e\\n\", time + dt, dt, iter, norm_L2);\n\n      \n\n\n      \n\n      #include \"calculate_u.h\"\n\n      \n\n      \n      \n\n      #include \"calculate_v.h\"\n\n      \n\n      \n      \n\n      max_v = SMALL;\n      max_u = SMALL;\n\n            for (int i = 0; i < size_max; ++i) {\n        Real test_u = max_u_arr[i];\n        max_u = fmax(max_u, test_u);\n\n        Real test_v = max_v_arr[i];\n        max_v = fmax(max_v, test_v);\n      }\n\n      \n\n      \n\n      #include \"set_BCs.h\"\n\n      \n\n      time += dt;\n\n      \n\n      \n\n\n    } \n\n\n    auto end = std::chrono::steady_clock::now();\n    auto elapsed_time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"\\nTotal execution time of the iteration loop: %f (s)\\n\", elapsed_time * 1e-9f);\n  }\n  \n\n\n  \n\n  FILE * pfile;\n  pfile = fopen(\"velocity_gpu.dat\", \"w\");\n  fprintf(pfile, \"#x\\ty\\tu\\tv\\n\");\n  if (pfile != NULL) {\n    for (int row = 0; row < NUM; ++row) {\n      for (int col = 0; col < NUM; ++col) {\n\n        Real u_ij = u[(col * NUM) + row];\n        Real u_im1j;\n        if (col == 0) {\n          u_im1j = 0.0;\n        } else {\n          u_im1j = u[(col - 1) * NUM + row];\n        }\n\n        u_ij = (u_ij + u_im1j) / 2.0;\n\n        Real v_ij = v[(col * NUM) + row];\n        Real v_ijm1;\n        if (row == 0) {\n          v_ijm1 = 0.0;\n        } else {\n          v_ijm1 = v[(col * NUM) + row - 1];\n        }\n\n        v_ij = (v_ij + v_ijm1) / 2.0;\n\n        fprintf(pfile, \"%f\\t%f\\t%f\\t%f\\n\", ((Real)col + 0.5) * dx, ((Real)row + 0.5) * dy, u_ij, v_ij);\n      }\n    }\n  }\n\n  fclose(pfile);\n\n  free(pres_red);\n  free(pres_black);\n  free(u);\n  free(v);\n  free(F);\n  free(G);\n  free(max_u_arr);\n  free(max_v_arr);\n  free(res_arr);\n  free(pres_sum);\n  return 0;\n}"}}
{"kernel_name": "md", "parallel_api": "cuda", "code": {"main.cu": "#include <cassert>\n#include <chrono>\n#include <cfloat>\n#include <cmath>\n#include <cstdlib>\n#include <list>\n#include <iostream>\n#include <cuda.h>\n#include \"MD.h\"\n#include \"reference.h\"\n#include \"utils.h\"\n\n__global__ void md (\n  const POSVECTYPE* __restrict__ position,\n        FORCEVECTYPE* __restrict__ force,\n  const int* __restrict__ neighborList, \n  const int nAtom,\n  const int maxNeighbors, \n  const FPTYPE lj1_t,\n  const FPTYPE lj2_t,\n  const FPTYPE cutsq_t )\n{\n  const uint idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx >= nAtom) return;\n\n  POSVECTYPE ipos = position[idx];\n  FORCEVECTYPE f = zero;\n\n  int j = 0;\n  while (j < maxNeighbors)\n  {\n    int jidx = neighborList[j*nAtom + idx];\n\n    \n\n    POSVECTYPE jpos = position[jidx];\n\n    \n\n    FPTYPE delx = ipos.x - jpos.x;\n    FPTYPE dely = ipos.y - jpos.y;\n    FPTYPE delz = ipos.z - jpos.z;\n    FPTYPE r2inv = delx*delx + dely*dely + delz*delz;\n\n    \n\n    if (r2inv > 0 && r2inv < cutsq_t)\n    {\n      r2inv = (FPTYPE)1.0 / r2inv;\n      FPTYPE r6inv = r2inv * r2inv * r2inv;\n      FPTYPE forceC = r2inv * r6inv * (lj1_t * r6inv - lj2_t);\n\n      f.x += delx * forceC;\n      f.y += dely * forceC;\n      f.z += delz * forceC;\n    }\n    j++;\n  }\n  force[idx] = f;\n}\n\nint main(int argc, char** argv)\n{\n  if (argc != 3) {\n    std::cout << \"Usage: ./\" << argv[0] << \" <class size> <iteration>\\n\";\n    return 1;\n  }\n\n  \n\n  int sizeClass = atoi(argv[1]);\n  int iteration = atoi(argv[2]);\n  const int probSizes[] = { 12288, 24576, 36864, 73728 };\n  assert(sizeClass >= 0 && sizeClass < 4);\n  assert(iteration >= 0);\n\n  int nAtom = probSizes[sizeClass];\n\n  \n\n  POSVECTYPE* position = (POSVECTYPE*) malloc(nAtom * sizeof(POSVECTYPE));\n  FORCEVECTYPE* h_force = (FORCEVECTYPE*) malloc(nAtom * sizeof(FORCEVECTYPE));\n  int *neighborList = (int*) malloc(maxNeighbors * nAtom * sizeof(int));\n\n  std::cout << \"Initializing test problem (this can take several minutes for large problems).\\n\";\n\n  \n\n  srand(123);\n\n  \n\n  \n\n  \n\n  for (int i = 0; i < nAtom; i++)\n  {\n    position[i].x = rand() % domainEdge;\n    position[i].y = rand() % domainEdge;\n    position[i].z = rand() % domainEdge;\n  }\n\n  std::cout << \"Finished.\\n\";\n  int totalPairs = buildNeighborList<FPTYPE, POSVECTYPE>(nAtom, position, neighborList);\n  std::cout << totalPairs << \" of \" << nAtom*maxNeighbors << \" pairs within cutoff distance = \" \n       << 100.0 * ((double)totalPairs / (nAtom*maxNeighbors)) << \" %\\n\";\n\n  POSVECTYPE*   d_position;\n  FORCEVECTYPE* d_force;\n  int* d_neighborList;\n  cudaMalloc((void**)&d_force, nAtom * sizeof(FORCEVECTYPE));\n  cudaMalloc((void**)&d_position, nAtom * sizeof(POSVECTYPE));\n  cudaMalloc((void**)&d_neighborList, nAtom * maxNeighbors * sizeof(int));\n\n  cudaMemcpy(d_position, position, nAtom * sizeof(POSVECTYPE), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_neighborList, neighborList, nAtom * maxNeighbors * sizeof(int), cudaMemcpyHostToDevice);\n\n  dim3 grids ((nAtom+255) / 256);\n  dim3 block (256);\n\n  \n\n  md <<< grids, block >>> (d_position, d_force, d_neighborList,\n      nAtom, maxNeighbors, lj1, lj2, cutsq);\n\n  cudaMemcpy(h_force, d_force, nAtom * sizeof(FORCEVECTYPE), cudaMemcpyDeviceToHost);\n\n  std::cout << \"Performing Correctness Check (may take several minutes)\\n\";\n\n  checkResults<FPTYPE, FORCEVECTYPE, POSVECTYPE>(h_force, position, neighborList, nAtom);\n\n  auto start = std::chrono::steady_clock::now();\n\n  for (int j = 0; j < iteration; j++)\n  {\n    md <<< grids, block >>> (d_position, d_force, d_neighborList,\n      nAtom, maxNeighbors, lj1, lj2, cutsq);\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  std::cout << \"Average kernel execution time \" << (time * 1e-9f) / iteration << \" (s)\\n\";\n\n  cudaFree(d_position);\n  cudaFree(d_force);\n  cudaFree(d_neighborList);\n\n  free(position);\n  free(h_force);\n  free(neighborList);\n\n  return 0;\n}\n"}}
{"kernel_name": "md", "parallel_api": "hip", "code": {"main.cu": "#include <cassert>\n#include <chrono>\n#include <cfloat>\n#include <cmath>\n#include <cstdlib>\n#include <list>\n#include <iostream>\n#include <hip/hip_runtime.h>\n#include \"MD.h\"\n#include \"reference.h\"\n#include \"utils.h\"\n\n__global__ void md (\n  const POSVECTYPE* __restrict__ position,\n        FORCEVECTYPE* __restrict__ force,\n  const int* __restrict__ neighborList, \n  const int nAtom,\n  const int maxNeighbors, \n  const FPTYPE lj1_t,\n  const FPTYPE lj2_t,\n  const FPTYPE cutsq_t )\n{\n  const uint idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx >= nAtom) return;\n\n  POSVECTYPE ipos = position[idx];\n  FORCEVECTYPE f = zero;\n\n  int j = 0;\n  while (j < maxNeighbors)\n  {\n    int jidx = neighborList[j*nAtom + idx];\n\n    \n\n    POSVECTYPE jpos = position[jidx];\n\n    \n\n    FPTYPE delx = ipos.x - jpos.x;\n    FPTYPE dely = ipos.y - jpos.y;\n    FPTYPE delz = ipos.z - jpos.z;\n    FPTYPE r2inv = delx*delx + dely*dely + delz*delz;\n\n    \n\n    if (r2inv > 0 && r2inv < cutsq_t)\n    {\n      r2inv = (FPTYPE)1.0 / r2inv;\n      FPTYPE r6inv = r2inv * r2inv * r2inv;\n      FPTYPE forceC = r2inv * r6inv * (lj1_t * r6inv - lj2_t);\n\n      f.x += delx * forceC;\n      f.y += dely * forceC;\n      f.z += delz * forceC;\n    }\n    j++;\n  }\n  force[idx] = f;\n}\n\nint main(int argc, char** argv)\n{\n  if (argc != 3) {\n    std::cout << \"Usage: ./\" << argv[0] << \" <class size> <iteration>\\n\";\n    return 1;\n  }\n\n  \n\n  int sizeClass = atoi(argv[1]);\n  int iteration = atoi(argv[2]);\n  const int probSizes[] = { 12288, 24576, 36864, 73728 };\n  assert(sizeClass >= 0 && sizeClass < 4);\n  assert(iteration >= 0);\n\n  int nAtom = probSizes[sizeClass];\n\n  \n\n  POSVECTYPE* position = (POSVECTYPE*) malloc(nAtom * sizeof(POSVECTYPE));\n  FORCEVECTYPE* h_force = (FORCEVECTYPE*) malloc(nAtom * sizeof(FORCEVECTYPE));\n  int *neighborList = (int*) malloc(maxNeighbors * nAtom * sizeof(int));\n\n  std::cout << \"Initializing test problem (this can take several minutes for large problems).\\n\";\n\n  \n\n  srand(123);\n\n  \n\n  \n\n  \n\n  for (int i = 0; i < nAtom; i++)\n  {\n    position[i].x = rand() % domainEdge;\n    position[i].y = rand() % domainEdge;\n    position[i].z = rand() % domainEdge;\n  }\n\n  std::cout << \"Finished.\\n\";\n  int totalPairs = buildNeighborList<FPTYPE, POSVECTYPE>(nAtom, position, neighborList);\n  std::cout << totalPairs << \" of \" << nAtom*maxNeighbors << \" pairs within cutoff distance = \" \n       << 100.0 * ((double)totalPairs / (nAtom*maxNeighbors)) << \" %\\n\";\n\n  POSVECTYPE*   d_position;\n  FORCEVECTYPE* d_force;\n  int* d_neighborList;\n  hipMalloc((void**)&d_force, nAtom * sizeof(FORCEVECTYPE));\n  hipMalloc((void**)&d_position, nAtom * sizeof(POSVECTYPE));\n  hipMalloc((void**)&d_neighborList, nAtom * maxNeighbors * sizeof(int));\n\n  hipMemcpy(d_position, position, nAtom * sizeof(POSVECTYPE), hipMemcpyHostToDevice);\n  hipMemcpy(d_neighborList, neighborList, nAtom * maxNeighbors * sizeof(int), hipMemcpyHostToDevice);\n\n  dim3 grids ((nAtom+255) / 256);\n  dim3 block (256);\n\n  \n\n  hipLaunchKernelGGL(md, grids, block, 0, 0, d_position, d_force, d_neighborList,\n      nAtom, maxNeighbors, lj1, lj2, cutsq);\n\n  hipMemcpy(h_force, d_force, nAtom * sizeof(FORCEVECTYPE), hipMemcpyDeviceToHost);\n\n  std::cout << \"Performing Correctness Check (may take several minutes)\\n\";\n\n  checkResults<FPTYPE, FORCEVECTYPE, POSVECTYPE>(h_force, position, neighborList, nAtom);\n\n  auto start = std::chrono::steady_clock::now();\n\n  for (int j = 0; j < iteration; j++)\n  {\n    hipLaunchKernelGGL(md, grids, block, 0, 0, d_position, d_force, d_neighborList,\n      nAtom, maxNeighbors, lj1, lj2, cutsq);\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  std::cout << \"Average kernel execution time \" << (time * 1e-9f) / iteration << \" (s)\\n\";\n\n  hipFree(d_position);\n  hipFree(d_force);\n  hipFree(d_neighborList);\n\n  free(position);\n  free(h_force);\n  free(neighborList);\n\n  return 0;\n}\n"}}
{"kernel_name": "md", "parallel_api": "omp", "code": {"main.cpp": "#include <cassert>\n#include <chrono>\n#include <cfloat>\n#include <cmath>\n#include <cstdlib>\n#include <list>\n#include <iostream>\n#include <omp.h>\n#include \"MD.h\"\n#include \"reference.h\"\n#include \"utils.h\"\n\nvoid md (\n    const POSVECTYPE* __restrict position,\n    FORCEVECTYPE* __restrict force,\n    const int* __restrict neighborList, \n    const int nAtom,\n    const int maxNeighbors, \n    const FPTYPE lj1_t,\n    const FPTYPE lj2_t,\n    const FPTYPE cutsq_t )\n{\n  #pragma omp target teams distribute parallel for thread_limit(256) \n  for (uint idx = 0; idx < nAtom; idx++) {\n    POSVECTYPE ipos = position[idx];\n    FORCEVECTYPE f = zero;\n\n    int j = 0;\n    while (j < maxNeighbors)\n    {\n      int jidx = neighborList[j*nAtom + idx];\n\n      \n\n      POSVECTYPE jpos = position[jidx];\n\n      \n\n      FPTYPE delx = ipos.x - jpos.x;\n      FPTYPE dely = ipos.y - jpos.y;\n      FPTYPE delz = ipos.z - jpos.z;\n      FPTYPE r2inv = delx*delx + dely*dely + delz*delz;\n\n      \n\n      if (r2inv > 0 && r2inv < cutsq_t)\n      {\n        r2inv = (FPTYPE)1.0 / r2inv;\n        FPTYPE r6inv = r2inv * r2inv * r2inv;\n        FPTYPE forceC = r2inv*r6inv*(lj1_t*r6inv - lj2_t);\n\n        f.x += delx * forceC;\n        f.y += dely * forceC;\n        f.z += delz * forceC;\n      }\n      j++;\n    }\n    force[idx] = f;\n  }\n}\n\nint main(int argc, char** argv)\n{\n  if (argc != 3) {\n    printf(\"usage: %s <class size> <iteration>\", argv[0]);\n    return 1;\n  }\n\n  \n\n  int sizeClass = atoi(argv[1]);\n  int iteration = atoi(argv[2]);\n  const int probSizes[] = { 12288, 24576, 36864, 73728 };\n  assert(sizeClass >= 0 && sizeClass < 4);\n  assert(iteration >= 0);\n\n  int nAtom = probSizes[sizeClass];\n\n  \n\n  POSVECTYPE* position = (POSVECTYPE*) malloc(nAtom * sizeof(POSVECTYPE));\n  FORCEVECTYPE* force = (FORCEVECTYPE*) malloc(nAtom * sizeof(FORCEVECTYPE));\n  int *neighborList = (int*) malloc(maxNeighbors * nAtom * sizeof(int));\n\n  std::cout << \"Initializing test problem (this can take several minutes for large problems).\\n\";\n\n  \n\n  srand(123);\n\n  \n\n  \n\n  \n\n  for (int i = 0; i < nAtom; i++)\n  {\n    position[i].x = rand() % domainEdge;\n    position[i].y = rand() % domainEdge;\n    position[i].z = rand() % domainEdge;\n  }\n\n  std::cout << \"Finished.\\n\";\n  int totalPairs = buildNeighborList<FPTYPE, POSVECTYPE>(nAtom, position, neighborList);\n  std::cout << totalPairs << \" of \" << nAtom*maxNeighbors\n            << \" pairs within cutoff distance = \"\n            << 100.0 * ((double)totalPairs / (nAtom*maxNeighbors)) << \" %\\n\";\n\n  #pragma omp target data map(to: position[0:nAtom], \\\n                                  neighborList[0:nAtom * maxNeighbors]) \\\n                          map(alloc: force[0:nAtom])\n  {\n    \n\n    md(position,\n       force,\n       neighborList,\n       nAtom,\n       maxNeighbors,\n       lj1,\n       lj2,\n       cutsq);\n\n    #pragma omp target update from(force[0:nAtom])\n\n    std::cout << \"Performing Correctness Check (may take several minutes)\\n\";\n\n    checkResults<FPTYPE, FORCEVECTYPE, POSVECTYPE>(force, position, neighborList, nAtom);\n\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < iteration; i++) {\n      md(position,\n         force,\n         neighborList,\n         nAtom,\n         maxNeighbors,\n         lj1,\n         lj2,\n         cutsq);\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    std::cout << \"Average kernel execution time \" << (time * 1e-9f) / iteration << \" (s)\\n\";\n  }\n\n  free(position);\n  free(force);\n  free(neighborList);\n\n  return 0;\n}\n"}}
{"kernel_name": "md", "parallel_api": "serial", "code": {"main.cpp": "#include <cassert>\n#include <chrono>\n#include <cfloat>\n#include <cmath>\n#include <cstdlib>\n#include <list>\n#include <iostream>\n#include \"MD.h\"\n#include \"reference.h\"\n#include \"utils.h\"\n\nvoid md (\n    const POSVECTYPE* __restrict position,\n    FORCEVECTYPE* __restrict force,\n    const int* __restrict neighborList, \n    const int nAtom,\n    const int maxNeighbors, \n    const FPTYPE lj1_t,\n    const FPTYPE lj2_t,\n    const FPTYPE cutsq_t )\n{\n    for (uint idx = 0; idx < nAtom; idx++) {\n    POSVECTYPE ipos = position[idx];\n    FORCEVECTYPE f = zero;\n\n    int j = 0;\n    while (j < maxNeighbors)\n    {\n      int jidx = neighborList[j*nAtom + idx];\n\n      \n\n      POSVECTYPE jpos = position[jidx];\n\n      \n\n      FPTYPE delx = ipos.x - jpos.x;\n      FPTYPE dely = ipos.y - jpos.y;\n      FPTYPE delz = ipos.z - jpos.z;\n      FPTYPE r2inv = delx*delx + dely*dely + delz*delz;\n\n      \n\n      if (r2inv > 0 && r2inv < cutsq_t)\n      {\n        r2inv = (FPTYPE)1.0 / r2inv;\n        FPTYPE r6inv = r2inv * r2inv * r2inv;\n        FPTYPE forceC = r2inv*r6inv*(lj1_t*r6inv - lj2_t);\n\n        f.x += delx * forceC;\n        f.y += dely * forceC;\n        f.z += delz * forceC;\n      }\n      j++;\n    }\n    force[idx] = f;\n  }\n}\n\nint main(int argc, char** argv)\n{\n  if (argc != 3) {\n    printf(\"usage: %s <class size> <iteration>\", argv[0]);\n    return 1;\n  }\n\n  \n\n  int sizeClass = atoi(argv[1]);\n  int iteration = atoi(argv[2]);\n  const int probSizes[] = { 12288, 24576, 36864, 73728 };\n  assert(sizeClass >= 0 && sizeClass < 4);\n  assert(iteration >= 0);\n\n  int nAtom = probSizes[sizeClass];\n\n  \n\n  POSVECTYPE* position = (POSVECTYPE*) malloc(nAtom * sizeof(POSVECTYPE));\n  FORCEVECTYPE* force = (FORCEVECTYPE*) malloc(nAtom * sizeof(FORCEVECTYPE));\n  int *neighborList = (int*) malloc(maxNeighbors * nAtom * sizeof(int));\n\n  std::cout << \"Initializing test problem (this can take several minutes for large problems).\\n\";\n\n  \n\n  srand(123);\n\n  \n\n  \n\n  \n\n  for (int i = 0; i < nAtom; i++)\n  {\n    position[i].x = rand() % domainEdge;\n    position[i].y = rand() % domainEdge;\n    position[i].z = rand() % domainEdge;\n  }\n\n  std::cout << \"Finished.\\n\";\n  int totalPairs = buildNeighborList<FPTYPE, POSVECTYPE>(nAtom, position, neighborList);\n  std::cout << totalPairs << \" of \" << nAtom*maxNeighbors\n            << \" pairs within cutoff distance = \"\n            << 100.0 * ((double)totalPairs / (nAtom*maxNeighbors)) << \" %\\n\";\n\n    {\n    \n\n    md(position,\n       force,\n       neighborList,\n       nAtom,\n       maxNeighbors,\n       lj1,\n       lj2,\n       cutsq);\n\n    \n    std::cout << \"Performing Correctness Check (may take several minutes)\\n\";\n\n    checkResults<FPTYPE, FORCEVECTYPE, POSVECTYPE>(force, position, neighborList, nAtom);\n\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < iteration; i++) {\n      md(position,\n         force,\n         neighborList,\n         nAtom,\n         maxNeighbors,\n         lj1,\n         lj2,\n         cutsq);\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    std::cout << \"Average kernel execution time \" << (time * 1e-9f) / iteration << \" (s)\\n\";\n  }\n\n  free(position);\n  free(force);\n  free(neighborList);\n\n  return 0;\n}"}}
{"kernel_name": "md", "parallel_api": "sycl", "code": {"main.cpp": "#include <cassert>\n#include <chrono>\n#include <cfloat>\n#include <cmath>\n#include <cstdlib>\n#include <list>\n#include <iostream>\n#include <sycl/sycl.hpp>\n#include \"MD.h\"\n#include \"reference.h\"\n#include \"utils.h\"\n\nvoid md (\n  sycl::nd_item<1> &item,\n  const POSVECTYPE* __restrict position,\n        FORCEVECTYPE* __restrict force,\n  const int* __restrict neighborList,\n  const int nAtom,\n  const int maxNeighbors,\n  const FPTYPE lj1_t,\n  const FPTYPE lj2_t,\n  const FPTYPE cutsq_t )\n{\n  const uint idx = item.get_global_id(0);\n  if (idx >= nAtom) return;\n\n  POSVECTYPE ipos = position[idx];\n  FORCEVECTYPE f = FORCEVECTYPE(0);\n\n  int j = 0;\n  while (j < maxNeighbors)\n  {\n    int jidx = neighborList[j*nAtom + idx];\n\n    \n\n    POSVECTYPE jpos = position[jidx];\n\n    \n\n    FPTYPE delx = ipos.x() - jpos.x();\n    FPTYPE dely = ipos.y() - jpos.y();\n    FPTYPE delz = ipos.z() - jpos.z();\n    FPTYPE r2inv = delx*delx + dely*dely + delz*delz;\n\n    \n\n    if (r2inv > 0 && r2inv < cutsq_t)\n    {\n      r2inv = (FPTYPE)1.0 / r2inv;\n      FPTYPE r6inv = r2inv * r2inv * r2inv;\n      FPTYPE forceC = r2inv * r6inv * (lj1_t * r6inv - lj2_t);\n\n      f.x() += delx * forceC;\n      f.y() += dely * forceC;\n      f.z() += delz * forceC;\n    }\n    j++;\n  }\n  force[idx] = f;\n}\n\nint main(int argc, char** argv)\n{\n  if (argc != 3) {\n    std::cout << \"Usage: ./\" << argv[0] << \" <class size> <iteration>\\n\";\n    return 1;\n  }\n\n  \n\n  int sizeClass = atoi(argv[1]);\n  int iteration = atoi(argv[2]);\n  const int probSizes[] = { 12288, 24576, 36864, 73728 };\n  assert(sizeClass >= 0 && sizeClass < 4);\n  assert(iteration >= 0);\n\n  int nAtom = probSizes[sizeClass];\n\n  \n\n  POSVECTYPE* position = (POSVECTYPE*) malloc(nAtom * sizeof(POSVECTYPE));\n  FORCEVECTYPE* force = (FORCEVECTYPE*) malloc(nAtom * sizeof(FORCEVECTYPE));\n  int *neighborList = (int*) malloc(maxNeighbors * nAtom * sizeof(int));\n\n  std::cout << \"Initializing test problem (this can take several minutes for large problems).\\n\";\n\n  \n\n  srand(123);\n\n  \n\n  \n\n  \n\n  for (int i = 0; i < nAtom; i++)\n  {\n    position[i].x() = rand() % domainEdge;\n    position[i].y() = rand() % domainEdge;\n    position[i].z() = rand() % domainEdge;\n  }\n\n  std::cout << \"Finished.\\n\";\n  int totalPairs = buildNeighborList<FPTYPE, POSVECTYPE>(nAtom, position, neighborList);\n  std::cout << totalPairs << \" of \" << nAtom*maxNeighbors\n            << \" pairs within cutoff distance = \"\n            << 100.0 * ((double)totalPairs / (nAtom*maxNeighbors)) << \" %\\n\";\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  FORCEVECTYPE *d_force = sycl::malloc_device<FORCEVECTYPE>(nAtom, q);\n\n  POSVECTYPE *d_position = sycl::malloc_device<POSVECTYPE>(nAtom, q);\n  q.memcpy(d_position, position, sizeof(POSVECTYPE) * nAtom);\n\n  int *d_neighborList = sycl::malloc_device<int>(nAtom * maxNeighbors, q);\n  q.memcpy(d_neighborList, neighborList, sizeof(int) * nAtom * maxNeighbors);\n\n  sycl::range<1> lws (256);\n  sycl::range<1> gws ((nAtom + 255) / 256 * 256);\n\n  \n\n  q.submit([&](sycl::handler& cgh) {\n    cgh.parallel_for<class warmup>(\n      sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n      md(item,\n         d_position,\n         d_force,\n         d_neighborList,\n         nAtom,\n         maxNeighbors,\n         lj1,\n         lj2,\n         cutsq);\n    });\n  });\n\n  q.memcpy(force, d_force, nAtom * sizeof(FORCEVECTYPE)).wait();\n\n  std::cout << \"Performing Correctness Check (may take several minutes)\\n\";\n\n  checkResults<FPTYPE, FORCEVECTYPE, POSVECTYPE>(force, position, neighborList, nAtom);\n\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < iteration; i++)\n  {\n    q.submit([&](sycl::handler& cgh) {\n      cgh.parallel_for<class run>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        md(item,\n           d_position,\n           d_force,\n           d_neighborList,\n           nAtom,\n           maxNeighbors,\n           lj1,\n           lj2,\n           cutsq);\n      });\n    });\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  std::cout << \"Average kernel execution time \" << (time * 1e-9f) / iteration << \" (s)\\n\";\n\n  sycl::free(d_position, q);\n  sycl::free(d_force, q);\n  sycl::free(d_neighborList, q);\n\n  free(position);\n  free(force);\n  free(neighborList);\n\n  return 0;\n}\n"}}
{"kernel_name": "miniWeather", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n\n\n\n\n\n\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <time.h>\n#include <mpi.h>\n#include <cuda.h>\n\nconst double pi        = 3.14159265358979323846264338327;   \n\nconst double grav      = 9.8;                               \n\nconst double cp        = 1004.;                             \n\nconst double cv        = 717.;                              \n\nconst double rd        = 287.;                              \n\nconst double p0        = 1.e5;                              \n\nconst double C0        = 27.5629410929725921310572974482;   \n\nconst double gamm      = 1.40027894002789400278940027894;   \n\n\n\nconst double xlen      = 2.e4;    \n\nconst double zlen      = 1.e4;    \n\nconst double hv_beta   = 0.25;     \n\nconst double cfl       = 1.50;    \n\nconst double max_speed = 450;        \n\nconst int hs        = 2;          \n\nconst int sten_size = 4;          \n\n\n\n\nconst int NUM_VARS = 4;           \n\nconst int ID_DENS  = 0;           \n\nconst int ID_UMOM  = 1;           \n\nconst int ID_WMOM  = 2;           \n\nconst int ID_RHOT  = 3;           \n\nconst int DIR_X = 1;              \n\nconst int DIR_Z = 2;              \n\nconst int DATA_SPEC_COLLISION       = 1;\nconst int DATA_SPEC_THERMAL         = 2;\nconst int DATA_SPEC_MOUNTAIN        = 3;\nconst int DATA_SPEC_TURBULENCE      = 4;\nconst int DATA_SPEC_DENSITY_CURRENT = 5;\nconst int DATA_SPEC_INJECTION       = 6;\n\nconst int nqpoints = 3;\ndouble qpoints [] = { 0.112701665379258311482073460022E0 , 0.500000000000000000000000000000E0 , 0.887298334620741688517926539980E0 };\ndouble qweights[] = { 0.277777777777777777777777777779E0 , 0.444444444444444444444444444444E0 , 0.277777777777777777777777777779E0 };\n\n\n\n\n\n\n\ndouble sim_time;              \n\ndouble dt;                    \n\nint    nx, nz;                \n\ndouble dx, dz;                \n\nint    nx_glob, nz_glob;      \n\nint    i_beg, k_beg;          \n\nint    nranks, myrank;        \n\nint    left_rank, right_rank; \n\nint    masterproc;            \n\ndouble data_spec_int;         \n\ndouble *hy_dens_cell;         \n\ndouble *hy_dens_theta_cell;   \n\ndouble *hy_dens_int;          \n\ndouble *hy_dens_theta_int;    \n\ndouble *hy_pressure_int;      \n\n\n\n\n\n\n\n\ndouble etime;                 \n\ndouble output_counter;        \n\n\n\ndouble *state;                \n\ndouble *state_tmp;            \n\ndouble *flux;                 \n\ndouble *tend;                 \n\ndouble *sendbuf_l;            \n\ndouble *sendbuf_r;            \n\ndouble *recvbuf_l;            \n\ndouble *recvbuf_r;            \n\nint    num_out = 0;           \n\nint    direction_switch = 1;\ndouble mass0, te0;            \n\ndouble mass , te ;            \n\n\n#include \"kernels.h\"\n\n\n\n\n\n\n\nvoid hydro_const_theta( double z , double &r , double &t ) {\n  const double theta0 = 300.;  \n\n  const double exner0 = 1.;    \n\n  double       p,exner,rt;\n  \n\n  t = theta0;                                  \n\n  exner = exner0 - grav * z / (cp * theta0);   \n\n  p = p0 * pow(exner,(cp/rd));                 \n\n  rt = pow((p / C0),(1. / gamm));             \n\n  r = rt / t;                                  \n\n}\n\n\n\n\n\n\n\n\n\nvoid hydro_const_bvfreq( double z , double bv_freq0 , double &r , double &t ) {\n  const double theta0 = 300.;  \n\n  const double exner0 = 1.;    \n\n  double       p, exner, rt;\n  t = theta0 * exp( bv_freq0*bv_freq0 / grav * z );                                    \n\n  exner = exner0 - grav*grav / (cp * bv_freq0*bv_freq0) * (t - theta0) / (t * theta0); \n\n  p = p0 * pow(exner,(cp/rd));                                                         \n\n  rt = pow((p / C0),(1. / gamm));                                                  \n\n  r = rt / t;                                                                          \n\n}\n\n\n\n\n\n\n\ndouble sample_ellipse_cosine( double x , double z , double amp , double x0 , double z0 , double xrad , double zrad ) {\n  double dist;\n  \n\n  dist = sqrt( ((x-x0)/xrad)*((x-x0)/xrad) + ((z-z0)/zrad)*((z-z0)/zrad) ) * pi / 2.;\n  \n\n  if (dist <= pi / 2.) {\n    return amp * pow(cos(dist),2.);\n  } else {\n    return 0.;\n  }\n}\n\n\n\n\n\n\n\n\n\nvoid injection( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n}\n\n\n\n\n\n\n\n\n\nvoid density_current( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z,-20. ,xlen/2,5000.,4000.,2000.);\n}\n\n\n\n\n\n\n\nvoid turbulence( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  \n\n  \n\n  \n\n  \n\n}\n\n\n\n\n\n\n\nvoid mountain_waves( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_bvfreq(z,0.02,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 15.;\n  w = 0.;\n}\n\n\n\n\n\n\n\n\n\nvoid thermal( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z, 3. ,xlen/2,2000.,2000.,2000.);\n}\n\n\n\n\n\n\n\n\n\nvoid collision( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z, 20.,xlen/2,2000.,2000.,2000.);\n  t = t + sample_ellipse_cosine(x,z,-20.,xlen/2,8000.,2000.,2000.);\n}\n\n\n\n\n\n\n\n\n\nvoid compute_tendencies_x(\n    const int hs, \n    const int nx, \n    const int nz, \n    const double dx,\n    double *d_state, \n    double *d_flux, \n    double *d_tend, \n    double *d_hy_dens_cell, \n    double *d_hy_dens_theta_cell)\n{\n  dim3 flux_gws ((nx+16)/16, (nz+15)/16, 1);\n  dim3 flux_lws (16, 16, 1);\n\n  \n\n  double hv_coef = -hv_beta * dx / (16*dt);\n\n  \n\n  compute_flux_x <<<flux_gws, flux_lws>>> (\n    d_state, d_flux, d_hy_dens_cell, d_hy_dens_theta_cell,\n    hv_coef, nx, nz, hs);\n\n  \n\n  dim3 tend_gws ((nx+15)/16, (nz+15)/16, NUM_VARS);\n  dim3 tend_lws (16, 16, 1);\n\n  compute_tend_x <<< tend_gws, tend_lws >>> (d_flux, d_tend, nx, nz, dx);\n}\n\n\n\n\n\n\n\n\n\nvoid compute_tendencies_z(\n    const int hs, \n    const int nx, \n    const int nz,  \n    const double dz,\n    double *d_state, \n    double *d_flux, \n    double *d_tend, \n    double *d_hy_dens_int, \n    double *d_hy_dens_theta_int, \n    double *d_hy_pressure_int)\n{\n  \n\n  double hv_coef = -hv_beta * dz / (16*dt);\n\n  \n\n  dim3 flux_gws ((nx+15)/16, (nz+16)/16, 1);\n  dim3 flux_lws (16, 16, 1);\n\n  compute_flux_z <<<flux_gws, flux_lws>>> (\n    d_state, d_flux, d_hy_dens_int, d_hy_pressure_int, d_hy_dens_theta_int,\n    hv_coef, nx, nz, hs);\n\n  \n\n  dim3 tend_gws ((nx+15)/16, (nz+15)/16, NUM_VARS);\n  dim3 tend_lws (16, 16, 1);\n\n  compute_tend_z <<< tend_gws, tend_lws >>> (d_state, d_flux, d_tend, nx, nz, dz);\n}\n\n\n\nvoid set_halo_values_x(\n    const int hs, \n    const int nx,\n    const int nz,\n    const int k_beg,\n    const double dz,\n    double *d_state, \n    double *d_hy_dens_cell, \n    double *d_hy_dens_theta_cell, \n    double *d_sendbuf_l,\n    double *d_sendbuf_r,\n    double *d_recvbuf_l,\n    double *d_recvbuf_r)\n{\n  int ierr;\n  MPI_Request req_r[2], req_s[2];\n\n  \n\n  ierr = MPI_Irecv(recvbuf_l,hs*nz*NUM_VARS,MPI_DOUBLE, left_rank,0,MPI_COMM_WORLD,&req_r[0]);\n  ierr = MPI_Irecv(recvbuf_r,hs*nz*NUM_VARS,MPI_DOUBLE,right_rank,1,MPI_COMM_WORLD,&req_r[1]);\n\n  \n\n  dim3 buffer_gws ((hs+15)/16, (nz+15)/16, NUM_VARS);\n  dim3 buffer_lws (16, 16, 1);\n\n  pack_send_buf <<< buffer_gws, buffer_lws >>> (\n    d_state, d_sendbuf_l, d_sendbuf_r, nx, nz, hs);\n\n  cudaMemcpy(sendbuf_l, d_sendbuf_l, sizeof(double)*hs*nz*NUM_VARS, cudaMemcpyDeviceToHost);\n  cudaMemcpy(sendbuf_r, d_sendbuf_r, sizeof(double)*hs*nz*NUM_VARS, cudaMemcpyDeviceToHost);\n\n  \n\n  ierr = MPI_Isend(sendbuf_l,hs*nz*NUM_VARS,MPI_DOUBLE, left_rank,1,MPI_COMM_WORLD,&req_s[0]);\n  ierr = MPI_Isend(sendbuf_r,hs*nz*NUM_VARS,MPI_DOUBLE,right_rank,0,MPI_COMM_WORLD,&req_s[1]);\n\n  \n\n  ierr = MPI_Waitall(2,req_r,MPI_STATUSES_IGNORE);\n\n  cudaMemcpy(d_recvbuf_l, recvbuf_l, sizeof(double)*hs*nz*NUM_VARS, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_recvbuf_r, recvbuf_r, sizeof(double)*hs*nz*NUM_VARS, cudaMemcpyHostToDevice);\n\n  \n\n  unpack_recv_buf <<< buffer_gws, buffer_lws >>> (\n    d_state, d_recvbuf_l, d_recvbuf_r, nx, nz, hs);\n\n  \n\n  ierr = MPI_Waitall(2,req_s,MPI_STATUSES_IGNORE);\n\n  if (data_spec_int == DATA_SPEC_INJECTION) {\n    if (myrank == 0) {\n      dim3 inj_gws ((hs+15)/16, (nz+15)/16, 1);\n      dim3 inj_lws (16, 16, 1);\n\n      update_state_x <<< inj_gws, inj_lws >>> (\n        d_state, d_hy_dens_cell, d_hy_dens_theta_cell, nx, nz, hs, k_beg, dz);\n    }\n  }\n}\n\n\n\n\n\nvoid set_halo_values_z(\n    const int hs, const int nx, const int nz, \n    const int i_beg,\n    const double dx,\n    const int data_spec_int,\n    double *d_state)\n{\n  const double mnt_width = xlen/8;\n\n  dim3 gws ((nx+2*hs+15)/16, (NUM_VARS+15)/16);\n  dim3 lws (16, 16, 1);\n\n  update_state_z <<< gws, lws >>> (d_state, data_spec_int, i_beg, nx, nz, hs, dx, mnt_width);\n}\n\n\nvoid init( int *argc , char ***argv ) {\n  int    i, k, ii, kk, ll, ierr, inds, i_end;\n  double x, z, r, u, w, t, hr, ht, nper;\n\n  ierr = MPI_Init(argc,argv);\n\n  \n\n  dx = xlen / nx_glob;\n  dz = zlen / nz_glob;\n\n  ierr = MPI_Comm_size(MPI_COMM_WORLD,&nranks);\n  ierr = MPI_Comm_rank(MPI_COMM_WORLD,&myrank);\n  nper = ( (double) nx_glob ) / nranks;\n  i_beg = round( nper* (myrank)    );\n  i_end = round( nper*((myrank)+1) )-1;\n  nx = i_end - i_beg + 1;\n  left_rank  = myrank - 1;\n  if (left_rank == -1) left_rank = nranks-1;\n  right_rank = myrank + 1;\n  if (right_rank == nranks) right_rank = 0;\n\n  \n\n  \n\n  \n\n\n  \n\n  k_beg = 0;\n  nz = nz_glob;\n  masterproc = (myrank == 0);\n\n  \n\n  state              = (double *) malloc( (nx+2*hs)*(nz+2*hs)*NUM_VARS*sizeof(double) );\n  state_tmp          = (double *) malloc( (nx+2*hs)*(nz+2*hs)*NUM_VARS*sizeof(double) );\n  flux               = (double *) malloc( (nx+1)*(nz+1)*NUM_VARS*sizeof(double) );\n  tend               = (double *) malloc( nx*nz*NUM_VARS*sizeof(double) );\n  hy_dens_cell       = (double *) malloc( (nz+2*hs)*sizeof(double) );\n  hy_dens_theta_cell = (double *) malloc( (nz+2*hs)*sizeof(double) );\n  hy_dens_int        = (double *) malloc( (nz+1)*sizeof(double) );\n  hy_dens_theta_int  = (double *) malloc( (nz+1)*sizeof(double) );\n  hy_pressure_int    = (double *) malloc( (nz+1)*sizeof(double) );\n  sendbuf_l          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  sendbuf_r          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  recvbuf_l          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  recvbuf_r          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n\n  \n\n  dt = fmin(dx,dz) / max_speed * cfl;\n  \n\n  etime = 0.;\n  output_counter = 0.;\n\n  \n\n  if (masterproc) {\n    printf( \"nx_glob, nz_glob: %d %d\\n\", nx_glob, nz_glob);\n    printf( \"dx,dz: %lf %lf\\n\",dx,dz);\n    printf( \"dt: %lf\\n\",dt);\n  }\n  \n\n  ierr = MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  \n\n  \n\n  for (k=0; k<nz+2*hs; k++) {\n    for (i=0; i<nx+2*hs; i++) {\n      \n\n      for (ll=0; ll<NUM_VARS; ll++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n        state[inds] = 0.;\n      }\n      \n\n      for (kk=0; kk<nqpoints; kk++) {\n        for (ii=0; ii<nqpoints; ii++) {\n          \n\n          x = (i_beg + i-hs+0.5)*dx + (qpoints[ii]-0.5)*dx;\n          z = (k_beg + k-hs+0.5)*dz + (qpoints[kk]-0.5)*dz;\n\n          \n\n          if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (x,z,r,u,w,t,hr,ht); }\n\n          \n\n          inds = ID_DENS*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + r                         * qweights[ii]*qweights[kk];\n          inds = ID_UMOM*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + (r+hr)*u                  * qweights[ii]*qweights[kk];\n          inds = ID_WMOM*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + (r+hr)*w                  * qweights[ii]*qweights[kk];\n          inds = ID_RHOT*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + ( (r+hr)*(t+ht) - hr*ht ) * qweights[ii]*qweights[kk];\n        }\n      }\n      for (ll=0; ll<NUM_VARS; ll++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n        state_tmp[inds] = state[inds];\n      }\n    }\n  }\n  \n\n  for (k=0; k<nz+2*hs; k++) {\n    hy_dens_cell      [k] = 0.;\n    hy_dens_theta_cell[k] = 0.;\n    for (kk=0; kk<nqpoints; kk++) {\n      z = (k_beg + k-hs+0.5)*dz;\n      \n\n      if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (0.,z,r,u,w,t,hr,ht); }\n      hy_dens_cell      [k] = hy_dens_cell      [k] + hr    * qweights[kk];\n      hy_dens_theta_cell[k] = hy_dens_theta_cell[k] + hr*ht * qweights[kk];\n    }\n  }\n  \n\n  for (k=0; k<nz+1; k++) {\n    z = (k_beg + k)*dz;\n    if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (0.,z,r,u,w,t,hr,ht); }\n    hy_dens_int      [k] = hr;\n    hy_dens_theta_int[k] = hr*ht;\n    hy_pressure_int  [k] = C0*pow((hr*ht),gamm);\n  }\n}\n\nvoid finalize() {\n  int ierr;\n  free( state );\n  free( state_tmp );\n  free( flux );\n  free( tend );\n  free( hy_dens_cell );\n  free( hy_dens_theta_cell );\n  free( hy_dens_int );\n  free( hy_dens_theta_int );\n  free( hy_pressure_int );\n  free( sendbuf_l );\n  free( sendbuf_r );\n  free( recvbuf_l );\n  free( recvbuf_r );\n  ierr = MPI_Finalize();\n}\n\n\n\n\n\nvoid reductions( \n    double &mass, \n    double &te, \n    const int hs,\n    const int nx,\n    const int nz,\n    const double dx,\n    const double dz,\n    const double *d_state, \n    const double *d_hy_dens_cell, \n    const double *d_hy_dens_theta_cell)\n{\n  double* d_mass, *d_te;\n  cudaMalloc((void**)&d_mass, sizeof(double));\n  cudaMalloc((void**)&d_te, sizeof(double));\n\n  dim3 gws ((nx+15)/16, (nz+15)/16, 1);\n  dim3 lws (16, 16, 1);\n\n  cudaMemset(d_mass, 0, sizeof(double));\n  cudaMemset(d_te, 0, sizeof(double));\n\n  acc_mass_te <<< gws, lws >>> (\n    d_mass, d_te, d_state, d_hy_dens_cell, d_hy_dens_theta_cell, nx, nz, dx, dz);\n\n  double glob[2], loc[2];\n\n  cudaMemcpy(loc, d_mass, sizeof(double), cudaMemcpyDeviceToHost);\n  cudaMemcpy(loc+1, d_te, sizeof(double), cudaMemcpyDeviceToHost);\n\n  int ierr = MPI_Allreduce(loc,glob,2,MPI_DOUBLE,MPI_SUM,MPI_COMM_WORLD);\n\n  cudaFree(d_mass);\n  cudaFree(d_te);\n\n  mass = glob[0];\n  te   = glob[1];\n}\n\n\n\n\n\n\n\nvoid semi_discrete_step( \n    const int hs, \n    const int nx, \n    const int nz, \n    const int k_beg, \n    const int i_beg, \n    const double dx, \n    const double dz, \n    const double dt, \n    int dir, \n    const int data_spec_int,\n    double *d_state_init , \n    double *d_state_forcing , \n    double *d_state_out, \n    double *d_flux , \n    double *d_tend,\n    double *d_hy_dens_cell , \n    double *d_hy_dens_theta_cell , \n    double *d_hy_dens_int , \n    double *d_hy_dens_theta_int , \n    double *d_hy_pressure_int , \n    double *d_sendbuf_l , \n    double *d_sendbuf_r , \n    double *d_recvbuf_l , \n    double *d_recvbuf_r)\n{\n  if (dir == DIR_X) {\n    \n\n    set_halo_values_x(\n        hs, \n        nx, \n        nz, \n        k_beg, \n        dz, \n        d_state_forcing, \n        d_hy_dens_cell , \n        d_hy_dens_theta_cell , \n        d_sendbuf_l,\n        d_sendbuf_r,\n        d_recvbuf_l,\n        d_recvbuf_r);\n\n    \n\n    compute_tendencies_x(hs, nx, nz, dx, d_state_forcing, d_flux, d_tend,\n                         d_hy_dens_cell, d_hy_dens_theta_cell);\n\n  } else if (dir == DIR_Z) {\n    \n\n    set_halo_values_z(hs, nx, nz, i_beg, dx, data_spec_int, d_state_forcing);\n    \n\n    compute_tendencies_z(hs, nx, nz, dz, d_state_forcing, d_flux, d_tend,\n                         d_hy_dens_int,d_hy_dens_theta_int,d_hy_pressure_int);\n  }\n\n  \n\n  dim3 tend_gws ((nx+15)/16, (nz+15)/16, NUM_VARS);\n  dim3 tend_lws (16, 16, 1);\n\n  update_fluid_state <<< tend_gws, tend_lws >>> (d_state_init, d_state_out, d_tend,\n    nx, nz, hs, dt);\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvoid perform_timestep( \n    double *d_state , \n    double *d_state_tmp , \n    double *d_flux , \n    double *d_tend ,\n    double *d_hy_dens_cell , \n    double *d_hy_dens_theta_cell , \n    double *d_hy_dens_int , \n    double *d_hy_dens_theta_int , \n    double *d_hy_pressure_int , \n    double *d_sendbuf_l , \n    double *d_sendbuf_r , \n    double *d_recvbuf_l , \n    double *d_recvbuf_r , \n    const double dt)\n{\n\n\n#define SEMI_DSTEP(dt, dir, state, next_state) \\\n    semi_discrete_step(hs, nx, nz, k_beg, i_beg, dx, dz, dt, dir ,\\\n\t\t    data_spec_int, d_state, state, \\\n\t\t    next_state, d_flux, d_tend, \\\n\t\t    d_hy_dens_cell, d_hy_dens_theta_cell,\\\n\t\t    d_hy_dens_int, d_hy_dens_theta_int,\\\n\t\t    d_hy_pressure_int, d_sendbuf_l,\\\n\t\t    d_sendbuf_r, d_recvbuf_l, d_recvbuf_r);\n\n  if (direction_switch) {\n    SEMI_DSTEP(dt/3, DIR_X, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_X, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_X, d_state_tmp, d_state)\n    SEMI_DSTEP(dt/3, DIR_Z, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_Z, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_Z, d_state_tmp, d_state)\n  } else {\n    SEMI_DSTEP(dt/3, DIR_Z, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_Z, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_Z, d_state_tmp, d_state)\n    SEMI_DSTEP(dt/3, DIR_X, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_X, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_X, d_state_tmp, d_state)\n  }\n\n  if (direction_switch) { direction_switch = 0; } else { direction_switch = 1; }\n}\n\n\n\nint main(int argc, char **argv) {\n\n  \n\n  \n\n  \n\n  nx_glob = NX;               \n\n  nz_glob = NZ;               \n\n  sim_time = SIM_TIME;        \n\n  data_spec_int = DATA_SPEC;  \n\n\n  \n\n\n  init( &argc , &argv );\n\n  double *d_state_tmp;\n  const int state_size = (nz+2*hs)*(nx+2*hs)*NUM_VARS;\n  const int state_size_byte = (nz+2*hs)*(nx+2*hs)*NUM_VARS*sizeof(double);\n  cudaMalloc((void**)&d_state_tmp, state_size_byte);\n  cudaMemcpy(d_state_tmp, state, state_size_byte, cudaMemcpyHostToDevice);\n\n  double *d_state;\n  cudaMalloc((void**)&d_state, state_size_byte);\n  cudaMemcpy(d_state, state, state_size_byte, cudaMemcpyHostToDevice);\n\n  double *d_hy_dens_cell;\n  cudaMalloc((void**)&d_hy_dens_cell, (nz+2*hs)*sizeof(double));\n  cudaMemcpy(d_hy_dens_cell, hy_dens_cell, (nz+2*hs)*sizeof(double), cudaMemcpyHostToDevice);\n\n  double *d_hy_dens_theta_cell;\n  cudaMalloc((void**)&d_hy_dens_theta_cell, (nz+2*hs)*sizeof(double));\n  cudaMemcpy(d_hy_dens_theta_cell, hy_dens_theta_cell, (nz+2*hs)*sizeof(double), cudaMemcpyHostToDevice);\n\n  double *d_hy_dens_int;\n  cudaMalloc((void**)&d_hy_dens_int, (nz+1)*sizeof(double));\n  cudaMemcpy(d_hy_dens_int, hy_dens_int, (nz+1)*sizeof(double), cudaMemcpyHostToDevice);\n  \n  double *d_hy_dens_theta_int;\n  cudaMalloc((void**)&d_hy_dens_theta_int, (nz+1)*sizeof(double));\n  cudaMemcpy(d_hy_dens_theta_int, hy_dens_theta_int, (nz+1)*sizeof(double), cudaMemcpyHostToDevice);\n\n  double *d_hy_pressure_int;\n  cudaMalloc((void**)&d_hy_pressure_int, (nz+1)*sizeof(double));\n  cudaMemcpy(d_hy_pressure_int, hy_pressure_int, (nz+1)*sizeof(double), cudaMemcpyHostToDevice);\n\n  double *d_flux;\n  cudaMalloc((void**)&d_flux, (nz+1)*(nx+1)*NUM_VARS*sizeof(double));\n\n  double *d_tend;\n  cudaMalloc((void**)&d_tend, nz*nx*NUM_VARS*sizeof(double));\n\n  double *d_sendbuf_l;\n  cudaMalloc((void**)&d_sendbuf_l, hs*nz*NUM_VARS*sizeof(double));\n\n  double *d_sendbuf_r;\n  cudaMalloc((void**)&d_sendbuf_r, hs*nz*NUM_VARS*sizeof(double));\n\n  double *d_recvbuf_l;\n  cudaMalloc((void**)&d_recvbuf_l, hs*nz*NUM_VARS*sizeof(double));\n\n  double *d_recvbuf_r;\n  cudaMalloc((void**)&d_recvbuf_r, hs*nz*NUM_VARS*sizeof(double));\n\n  \n\n  reductions(mass0, te0, hs, nx, nz, dx, dz, d_state, d_hy_dens_cell, d_hy_dens_theta_cell);\n\n  \n\n  auto c_start = clock();\n\n  while (etime < sim_time) {\n    \n\n    if (etime + dt > sim_time) { dt = sim_time - etime; }\n    \n\n    perform_timestep(\n        d_state,\n        d_state_tmp,\n        d_flux,\n        d_tend,\n        d_hy_dens_cell, \n        d_hy_dens_theta_cell, \n        d_hy_dens_int, \n        d_hy_dens_theta_int, \n        d_hy_pressure_int, \n        d_sendbuf_l, \n        d_sendbuf_r, \n        d_recvbuf_l, \n        d_recvbuf_r, \n        dt);\n\n    \n\n    etime = etime + dt;\n  }\n\n  auto c_end = clock();\n  if (masterproc)\n    printf(\"Total main time step loop: %lf sec\\n\", ( (double) (c_end-c_start) ) / CLOCKS_PER_SEC);\n\n  \n\n  reductions(mass, te, hs, nx, nz, dx, dz, d_state, d_hy_dens_cell, d_hy_dens_theta_cell);\n\n  printf( \"d_mass: %le\\n\" , (mass - mass0) / mass0 );\n  printf( \"d_te:   %le\\n\" , (te   - te0  ) / te0   );\n\n  finalize();\n\n  cudaFree(d_state);\n  cudaFree(d_state_tmp);\n  cudaFree(d_flux);\n  cudaFree(d_tend);\n  cudaFree(d_hy_dens_cell); \n  cudaFree(d_hy_dens_theta_cell); \n  cudaFree(d_hy_dens_int); \n  cudaFree(d_hy_dens_theta_int); \n  cudaFree(d_hy_pressure_int); \n  cudaFree(d_sendbuf_l); \n  cudaFree(d_sendbuf_r); \n  cudaFree(d_recvbuf_l); \n  cudaFree(d_recvbuf_r); \n  \n  return 0;\n}\n"}}
{"kernel_name": "miniWeather", "parallel_api": "hip", "code": {"main.cu": "\n\n\n\n\n\n\n\n\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <time.h>\n#include <mpi.h>\n#include <hip/hip_runtime.h>\n\nconst double pi        = 3.14159265358979323846264338327;   \n\nconst double grav      = 9.8;                               \n\nconst double cp        = 1004.;                             \n\nconst double cv        = 717.;                              \n\nconst double rd        = 287.;                              \n\nconst double p0        = 1.e5;                              \n\nconst double C0        = 27.5629410929725921310572974482;   \n\nconst double gamm      = 1.40027894002789400278940027894;   \n\n\n\nconst double xlen      = 2.e4;    \n\nconst double zlen      = 1.e4;    \n\nconst double hv_beta   = 0.25;     \n\nconst double cfl       = 1.50;    \n\nconst double max_speed = 450;        \n\nconst int hs        = 2;          \n\nconst int sten_size = 4;          \n\n\n\n\nconst int NUM_VARS = 4;           \n\nconst int ID_DENS  = 0;           \n\nconst int ID_UMOM  = 1;           \n\nconst int ID_WMOM  = 2;           \n\nconst int ID_RHOT  = 3;           \n\nconst int DIR_X = 1;              \n\nconst int DIR_Z = 2;              \n\nconst int DATA_SPEC_COLLISION       = 1;\nconst int DATA_SPEC_THERMAL         = 2;\nconst int DATA_SPEC_MOUNTAIN        = 3;\nconst int DATA_SPEC_TURBULENCE      = 4;\nconst int DATA_SPEC_DENSITY_CURRENT = 5;\nconst int DATA_SPEC_INJECTION       = 6;\n\nconst int nqpoints = 3;\ndouble qpoints [] = { 0.112701665379258311482073460022E0 , 0.500000000000000000000000000000E0 , 0.887298334620741688517926539980E0 };\ndouble qweights[] = { 0.277777777777777777777777777779E0 , 0.444444444444444444444444444444E0 , 0.277777777777777777777777777779E0 };\n\n\n\n\n\n\n\ndouble sim_time;              \n\ndouble dt;                    \n\nint    nx, nz;                \n\ndouble dx, dz;                \n\nint    nx_glob, nz_glob;      \n\nint    i_beg, k_beg;          \n\nint    nranks, myrank;        \n\nint    left_rank, right_rank; \n\nint    masterproc;            \n\ndouble data_spec_int;         \n\ndouble *hy_dens_cell;         \n\ndouble *hy_dens_theta_cell;   \n\ndouble *hy_dens_int;          \n\ndouble *hy_dens_theta_int;    \n\ndouble *hy_pressure_int;      \n\n\n\n\n\n\n\n\ndouble etime;                 \n\ndouble output_counter;        \n\n\n\ndouble *state;                \n\ndouble *state_tmp;            \n\ndouble *flux;                 \n\ndouble *tend;                 \n\ndouble *sendbuf_l;            \n\ndouble *sendbuf_r;            \n\ndouble *recvbuf_l;            \n\ndouble *recvbuf_r;            \n\nint    num_out = 0;           \n\nint    direction_switch = 1;\ndouble mass0, te0;            \n\ndouble mass , te ;            \n\n\n#include \"kernels.h\"\n\n\n\n\n\n\n\nvoid hydro_const_theta( double z , double &r , double &t ) {\n  const double theta0 = 300.;  \n\n  const double exner0 = 1.;    \n\n  double       p,exner,rt;\n  \n\n  t = theta0;                                  \n\n  exner = exner0 - grav * z / (cp * theta0);   \n\n  p = p0 * pow(exner,(cp/rd));                 \n\n  rt = pow((p / C0),(1. / gamm));             \n\n  r = rt / t;                                  \n\n}\n\n\n\n\n\n\n\n\n\nvoid hydro_const_bvfreq( double z , double bv_freq0 , double &r , double &t ) {\n  const double theta0 = 300.;  \n\n  const double exner0 = 1.;    \n\n  double       p, exner, rt;\n  t = theta0 * exp( bv_freq0*bv_freq0 / grav * z );                                    \n\n  exner = exner0 - grav*grav / (cp * bv_freq0*bv_freq0) * (t - theta0) / (t * theta0); \n\n  p = p0 * pow(exner,(cp/rd));                                                         \n\n  rt = pow((p / C0),(1. / gamm));                                                  \n\n  r = rt / t;                                                                          \n\n}\n\n\n\n\n\n\n\ndouble sample_ellipse_cosine( double x , double z , double amp , double x0 , double z0 , double xrad , double zrad ) {\n  double dist;\n  \n\n  dist = sqrt( ((x-x0)/xrad)*((x-x0)/xrad) + ((z-z0)/zrad)*((z-z0)/zrad) ) * pi / 2.;\n  \n\n  if (dist <= pi / 2.) {\n    return amp * pow(cos(dist),2.);\n  } else {\n    return 0.;\n  }\n}\n\n\n\n\n\n\n\n\n\nvoid injection( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n}\n\n\n\n\n\n\n\n\n\nvoid density_current( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z,-20. ,xlen/2,5000.,4000.,2000.);\n}\n\n\n\n\n\n\n\nvoid turbulence( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  \n\n  \n\n  \n\n  \n\n}\n\n\n\n\n\n\n\nvoid mountain_waves( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_bvfreq(z,0.02,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 15.;\n  w = 0.;\n}\n\n\n\n\n\n\n\n\n\nvoid thermal( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z, 3. ,xlen/2,2000.,2000.,2000.);\n}\n\n\n\n\n\n\n\n\n\nvoid collision( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z, 20.,xlen/2,2000.,2000.,2000.);\n  t = t + sample_ellipse_cosine(x,z,-20.,xlen/2,8000.,2000.,2000.);\n}\n\n\n\n\n\n\n\n\n\nvoid compute_tendencies_x(\n    const int hs, \n    const int nx, \n    const int nz, \n    const double dx,\n    double *d_state, \n    double *d_flux, \n    double *d_tend, \n    double *d_hy_dens_cell, \n    double *d_hy_dens_theta_cell)\n{\n  dim3 flux_gws ((nx+16)/16, (nz+15)/16, 1);\n  dim3 flux_lws (16, 16, 1);\n\n  \n\n  double hv_coef = -hv_beta * dx / (16*dt);\n\n  \n\n  compute_flux_x <<<flux_gws, flux_lws>>> (\n    d_state, d_flux, d_hy_dens_cell, d_hy_dens_theta_cell,\n    hv_coef, nx, nz, hs);\n\n  \n\n  dim3 tend_gws ((nx+15)/16, (nz+15)/16, NUM_VARS);\n  dim3 tend_lws (16, 16, 1);\n\n  compute_tend_x <<< tend_gws, tend_lws >>> (d_flux, d_tend, nx, nz, dx);\n}\n\n\n\n\n\n\n\n\n\nvoid compute_tendencies_z(\n    const int hs, \n    const int nx, \n    const int nz,  \n    const double dz,\n    double *d_state, \n    double *d_flux, \n    double *d_tend, \n    double *d_hy_dens_int, \n    double *d_hy_dens_theta_int, \n    double *d_hy_pressure_int)\n{\n  \n\n  double hv_coef = -hv_beta * dz / (16*dt);\n\n  \n\n  dim3 flux_gws ((nx+15)/16, (nz+16)/16, 1);\n  dim3 flux_lws (16, 16, 1);\n\n  compute_flux_z <<<flux_gws, flux_lws>>> (\n    d_state, d_flux, d_hy_dens_int, d_hy_pressure_int, d_hy_dens_theta_int,\n    hv_coef, nx, nz, hs);\n\n  \n\n  dim3 tend_gws ((nx+15)/16, (nz+15)/16, NUM_VARS);\n  dim3 tend_lws (16, 16, 1);\n\n  compute_tend_z <<< tend_gws, tend_lws >>> (d_state, d_flux, d_tend, nx, nz, dz);\n}\n\n\n\nvoid set_halo_values_x(\n    const int hs, \n    const int nx,\n    const int nz,\n    const int k_beg,\n    const double dz,\n    double *d_state, \n    double *d_hy_dens_cell, \n    double *d_hy_dens_theta_cell, \n    double *d_sendbuf_l,\n    double *d_sendbuf_r,\n    double *d_recvbuf_l,\n    double *d_recvbuf_r)\n{\n  int ierr;\n  MPI_Request req_r[2], req_s[2];\n\n  \n\n  ierr = MPI_Irecv(recvbuf_l,hs*nz*NUM_VARS,MPI_DOUBLE, left_rank,0,MPI_COMM_WORLD,&req_r[0]);\n  ierr = MPI_Irecv(recvbuf_r,hs*nz*NUM_VARS,MPI_DOUBLE,right_rank,1,MPI_COMM_WORLD,&req_r[1]);\n\n  \n\n  dim3 buffer_gws ((hs+15)/16, (nz+15)/16, NUM_VARS);\n  dim3 buffer_lws (16, 16, 1);\n\n  pack_send_buf <<< buffer_gws, buffer_lws >>> (\n    d_state, d_sendbuf_l, d_sendbuf_r, nx, nz, hs);\n\n  hipMemcpy(sendbuf_l, d_sendbuf_l, sizeof(double)*hs*nz*NUM_VARS, hipMemcpyDeviceToHost);\n  hipMemcpy(sendbuf_r, d_sendbuf_r, sizeof(double)*hs*nz*NUM_VARS, hipMemcpyDeviceToHost);\n\n  \n\n  ierr = MPI_Isend(sendbuf_l,hs*nz*NUM_VARS,MPI_DOUBLE, left_rank,1,MPI_COMM_WORLD,&req_s[0]);\n  ierr = MPI_Isend(sendbuf_r,hs*nz*NUM_VARS,MPI_DOUBLE,right_rank,0,MPI_COMM_WORLD,&req_s[1]);\n\n  \n\n  ierr = MPI_Waitall(2,req_r,MPI_STATUSES_IGNORE);\n\n  hipMemcpy(d_recvbuf_l, recvbuf_l, sizeof(double)*hs*nz*NUM_VARS, hipMemcpyHostToDevice);\n  hipMemcpy(d_recvbuf_r, recvbuf_r, sizeof(double)*hs*nz*NUM_VARS, hipMemcpyHostToDevice);\n\n  \n\n  unpack_recv_buf <<< buffer_gws, buffer_lws >>> (\n    d_state, d_recvbuf_l, d_recvbuf_r, nx, nz, hs);\n\n  \n\n  ierr = MPI_Waitall(2,req_s,MPI_STATUSES_IGNORE);\n\n  if (data_spec_int == DATA_SPEC_INJECTION) {\n    if (myrank == 0) {\n      dim3 inj_gws ((hs+15)/16, (nz+15)/16, 1);\n      dim3 inj_lws (16, 16, 1);\n\n      update_state_x <<< inj_gws, inj_lws >>> (\n        d_state, d_hy_dens_cell, d_hy_dens_theta_cell, nx, nz, hs, k_beg, dz);\n    }\n  }\n}\n\n\n\n\n\nvoid set_halo_values_z(\n    const int hs, const int nx, const int nz, \n    const int i_beg,\n    const double dx,\n    const int data_spec_int,\n    double *d_state)\n{\n  const double mnt_width = xlen/8;\n\n  dim3 gws ((nx+2*hs+15)/16, (NUM_VARS+15)/16);\n  dim3 lws (16, 16, 1);\n\n  update_state_z <<< gws, lws >>> (d_state, data_spec_int, i_beg, nx, nz, hs, dx, mnt_width);\n}\n\n\nvoid init( int *argc , char ***argv ) {\n  int    i, k, ii, kk, ll, ierr, inds, i_end;\n  double x, z, r, u, w, t, hr, ht, nper;\n\n  ierr = MPI_Init(argc,argv);\n\n  \n\n  dx = xlen / nx_glob;\n  dz = zlen / nz_glob;\n\n  ierr = MPI_Comm_size(MPI_COMM_WORLD,&nranks);\n  ierr = MPI_Comm_rank(MPI_COMM_WORLD,&myrank);\n  nper = ( (double) nx_glob ) / nranks;\n  i_beg = round( nper* (myrank)    );\n  i_end = round( nper*((myrank)+1) )-1;\n  nx = i_end - i_beg + 1;\n  left_rank  = myrank - 1;\n  if (left_rank == -1) left_rank = nranks-1;\n  right_rank = myrank + 1;\n  if (right_rank == nranks) right_rank = 0;\n\n  \n\n  \n\n  \n\n\n  \n\n  k_beg = 0;\n  nz = nz_glob;\n  masterproc = (myrank == 0);\n\n  \n\n  state              = (double *) malloc( (nx+2*hs)*(nz+2*hs)*NUM_VARS*sizeof(double) );\n  state_tmp          = (double *) malloc( (nx+2*hs)*(nz+2*hs)*NUM_VARS*sizeof(double) );\n  flux               = (double *) malloc( (nx+1)*(nz+1)*NUM_VARS*sizeof(double) );\n  tend               = (double *) malloc( nx*nz*NUM_VARS*sizeof(double) );\n  hy_dens_cell       = (double *) malloc( (nz+2*hs)*sizeof(double) );\n  hy_dens_theta_cell = (double *) malloc( (nz+2*hs)*sizeof(double) );\n  hy_dens_int        = (double *) malloc( (nz+1)*sizeof(double) );\n  hy_dens_theta_int  = (double *) malloc( (nz+1)*sizeof(double) );\n  hy_pressure_int    = (double *) malloc( (nz+1)*sizeof(double) );\n  sendbuf_l          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  sendbuf_r          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  recvbuf_l          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  recvbuf_r          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n\n  \n\n  dt = fmin(dx,dz) / max_speed * cfl;\n  \n\n  etime = 0.;\n  output_counter = 0.;\n\n  \n\n  if (masterproc) {\n    printf( \"nx_glob, nz_glob: %d %d\\n\", nx_glob, nz_glob);\n    printf( \"dx,dz: %lf %lf\\n\",dx,dz);\n    printf( \"dt: %lf\\n\",dt);\n  }\n  \n\n  ierr = MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  \n\n  \n\n  for (k=0; k<nz+2*hs; k++) {\n    for (i=0; i<nx+2*hs; i++) {\n      \n\n      for (ll=0; ll<NUM_VARS; ll++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n        state[inds] = 0.;\n      }\n      \n\n      for (kk=0; kk<nqpoints; kk++) {\n        for (ii=0; ii<nqpoints; ii++) {\n          \n\n          x = (i_beg + i-hs+0.5)*dx + (qpoints[ii]-0.5)*dx;\n          z = (k_beg + k-hs+0.5)*dz + (qpoints[kk]-0.5)*dz;\n\n          \n\n          if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (x,z,r,u,w,t,hr,ht); }\n\n          \n\n          inds = ID_DENS*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + r                         * qweights[ii]*qweights[kk];\n          inds = ID_UMOM*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + (r+hr)*u                  * qweights[ii]*qweights[kk];\n          inds = ID_WMOM*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + (r+hr)*w                  * qweights[ii]*qweights[kk];\n          inds = ID_RHOT*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + ( (r+hr)*(t+ht) - hr*ht ) * qweights[ii]*qweights[kk];\n        }\n      }\n      for (ll=0; ll<NUM_VARS; ll++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n        state_tmp[inds] = state[inds];\n      }\n    }\n  }\n  \n\n  for (k=0; k<nz+2*hs; k++) {\n    hy_dens_cell      [k] = 0.;\n    hy_dens_theta_cell[k] = 0.;\n    for (kk=0; kk<nqpoints; kk++) {\n      z = (k_beg + k-hs+0.5)*dz;\n      \n\n      if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (0.,z,r,u,w,t,hr,ht); }\n      hy_dens_cell      [k] = hy_dens_cell      [k] + hr    * qweights[kk];\n      hy_dens_theta_cell[k] = hy_dens_theta_cell[k] + hr*ht * qweights[kk];\n    }\n  }\n  \n\n  for (k=0; k<nz+1; k++) {\n    z = (k_beg + k)*dz;\n    if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (0.,z,r,u,w,t,hr,ht); }\n    hy_dens_int      [k] = hr;\n    hy_dens_theta_int[k] = hr*ht;\n    hy_pressure_int  [k] = C0*pow((hr*ht),gamm);\n  }\n}\n\nvoid finalize() {\n  int ierr;\n  free( state );\n  free( state_tmp );\n  free( flux );\n  free( tend );\n  free( hy_dens_cell );\n  free( hy_dens_theta_cell );\n  free( hy_dens_int );\n  free( hy_dens_theta_int );\n  free( hy_pressure_int );\n  free( sendbuf_l );\n  free( sendbuf_r );\n  free( recvbuf_l );\n  free( recvbuf_r );\n  ierr = MPI_Finalize();\n}\n\n\n\n\n\nvoid reductions( \n    double &mass, \n    double &te, \n    const int hs,\n    const int nx,\n    const int nz,\n    const double dx,\n    const double dz,\n    const double *d_state, \n    const double *d_hy_dens_cell, \n    const double *d_hy_dens_theta_cell)\n{\n  double* d_mass, *d_te;\n  hipMalloc((void**)&d_mass, sizeof(double));\n  hipMalloc((void**)&d_te, sizeof(double));\n\n  dim3 gws ((nx+15)/16, (nz+15)/16, 1);\n  dim3 lws (16, 16, 1);\n\n  hipMemset(d_mass, 0, sizeof(double));\n  hipMemset(d_te, 0, sizeof(double));\n\n  acc_mass_te <<< gws, lws >>> (\n    d_mass, d_te, d_state, d_hy_dens_cell, d_hy_dens_theta_cell, nx, nz, dx, dz);\n\n  double glob[2], loc[2];\n\n  hipMemcpy(loc, d_mass, sizeof(double), hipMemcpyDeviceToHost);\n  hipMemcpy(loc+1, d_te, sizeof(double), hipMemcpyDeviceToHost);\n\n  int ierr = MPI_Allreduce(loc,glob,2,MPI_DOUBLE,MPI_SUM,MPI_COMM_WORLD);\n\n  hipFree(d_mass);\n  hipFree(d_te);\n\n  mass = glob[0];\n  te   = glob[1];\n}\n\n\n\n\n\n\n\nvoid semi_discrete_step( \n    const int hs, \n    const int nx, \n    const int nz, \n    const int k_beg, \n    const int i_beg, \n    const double dx, \n    const double dz, \n    const double dt, \n    int dir, \n    const int data_spec_int,\n    double *d_state_init , \n    double *d_state_forcing , \n    double *d_state_out, \n    double *d_flux , \n    double *d_tend,\n    double *d_hy_dens_cell , \n    double *d_hy_dens_theta_cell , \n    double *d_hy_dens_int , \n    double *d_hy_dens_theta_int , \n    double *d_hy_pressure_int , \n    double *d_sendbuf_l , \n    double *d_sendbuf_r , \n    double *d_recvbuf_l , \n    double *d_recvbuf_r)\n{\n  if (dir == DIR_X) {\n    \n\n    set_halo_values_x(\n        hs, \n        nx, \n        nz, \n        k_beg, \n        dz, \n        d_state_forcing, \n        d_hy_dens_cell , \n        d_hy_dens_theta_cell , \n        d_sendbuf_l,\n        d_sendbuf_r,\n        d_recvbuf_l,\n        d_recvbuf_r);\n\n    \n\n    compute_tendencies_x(hs, nx, nz, dx, d_state_forcing, d_flux, d_tend,\n                         d_hy_dens_cell, d_hy_dens_theta_cell);\n\n  } else if (dir == DIR_Z) {\n    \n\n    set_halo_values_z(hs, nx, nz, i_beg, dx, data_spec_int, d_state_forcing);\n    \n\n    compute_tendencies_z(hs, nx, nz, dz, d_state_forcing, d_flux, d_tend,\n                         d_hy_dens_int,d_hy_dens_theta_int,d_hy_pressure_int);\n  }\n\n  \n\n  dim3 tend_gws ((nx+15)/16, (nz+15)/16, NUM_VARS);\n  dim3 tend_lws (16, 16, 1);\n\n  update_fluid_state <<< tend_gws, tend_lws >>> (d_state_init, d_state_out, d_tend,\n    nx, nz, hs, dt);\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvoid perform_timestep( \n    double *d_state , \n    double *d_state_tmp , \n    double *d_flux , \n    double *d_tend ,\n    double *d_hy_dens_cell , \n    double *d_hy_dens_theta_cell , \n    double *d_hy_dens_int , \n    double *d_hy_dens_theta_int , \n    double *d_hy_pressure_int , \n    double *d_sendbuf_l , \n    double *d_sendbuf_r , \n    double *d_recvbuf_l , \n    double *d_recvbuf_r , \n    const double dt)\n{\n\n\n#define SEMI_DSTEP(dt, dir, state, next_state) \\\n    semi_discrete_step(hs, nx, nz, k_beg, i_beg, dx, dz, dt, dir ,\\\n\t\t    data_spec_int, d_state, state, \\\n\t\t    next_state, d_flux, d_tend, \\\n\t\t    d_hy_dens_cell, d_hy_dens_theta_cell,\\\n\t\t    d_hy_dens_int, d_hy_dens_theta_int,\\\n\t\t    d_hy_pressure_int, d_sendbuf_l,\\\n\t\t    d_sendbuf_r, d_recvbuf_l, d_recvbuf_r);\n\n  if (direction_switch) {\n    SEMI_DSTEP(dt/3, DIR_X, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_X, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_X, d_state_tmp, d_state)\n    SEMI_DSTEP(dt/3, DIR_Z, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_Z, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_Z, d_state_tmp, d_state)\n  } else {\n    SEMI_DSTEP(dt/3, DIR_Z, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_Z, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_Z, d_state_tmp, d_state)\n    SEMI_DSTEP(dt/3, DIR_X, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_X, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_X, d_state_tmp, d_state)\n  }\n\n  if (direction_switch) { direction_switch = 0; } else { direction_switch = 1; }\n}\n\n\n\nint main(int argc, char **argv) {\n\n  \n\n  \n\n  \n\n  nx_glob = NX;               \n\n  nz_glob = NZ;               \n\n  sim_time = SIM_TIME;        \n\n  data_spec_int = DATA_SPEC;  \n\n\n  \n\n\n  init( &argc , &argv );\n\n  double *d_state_tmp;\n  const int state_size = (nz+2*hs)*(nx+2*hs)*NUM_VARS;\n  const int state_size_byte = (nz+2*hs)*(nx+2*hs)*NUM_VARS*sizeof(double);\n  hipMalloc((void**)&d_state_tmp, state_size_byte);\n  hipMemcpy(d_state_tmp, state, state_size_byte, hipMemcpyHostToDevice);\n\n  double *d_state;\n  hipMalloc((void**)&d_state, state_size_byte);\n  hipMemcpy(d_state, state, state_size_byte, hipMemcpyHostToDevice);\n\n  double *d_hy_dens_cell;\n  hipMalloc((void**)&d_hy_dens_cell, (nz+2*hs)*sizeof(double));\n  hipMemcpy(d_hy_dens_cell, hy_dens_cell, (nz+2*hs)*sizeof(double), hipMemcpyHostToDevice);\n\n  double *d_hy_dens_theta_cell;\n  hipMalloc((void**)&d_hy_dens_theta_cell, (nz+2*hs)*sizeof(double));\n  hipMemcpy(d_hy_dens_theta_cell, hy_dens_theta_cell, (nz+2*hs)*sizeof(double), hipMemcpyHostToDevice);\n\n  double *d_hy_dens_int;\n  hipMalloc((void**)&d_hy_dens_int, (nz+1)*sizeof(double));\n  hipMemcpy(d_hy_dens_int, hy_dens_int, (nz+1)*sizeof(double), hipMemcpyHostToDevice);\n  \n  double *d_hy_dens_theta_int;\n  hipMalloc((void**)&d_hy_dens_theta_int, (nz+1)*sizeof(double));\n  hipMemcpy(d_hy_dens_theta_int, hy_dens_theta_int, (nz+1)*sizeof(double), hipMemcpyHostToDevice);\n\n  double *d_hy_pressure_int;\n  hipMalloc((void**)&d_hy_pressure_int, (nz+1)*sizeof(double));\n  hipMemcpy(d_hy_pressure_int, hy_pressure_int, (nz+1)*sizeof(double), hipMemcpyHostToDevice);\n\n  double *d_flux;\n  hipMalloc((void**)&d_flux, (nz+1)*(nx+1)*NUM_VARS*sizeof(double));\n\n  double *d_tend;\n  hipMalloc((void**)&d_tend, nz*nx*NUM_VARS*sizeof(double));\n\n  double *d_sendbuf_l;\n  hipMalloc((void**)&d_sendbuf_l, hs*nz*NUM_VARS*sizeof(double));\n\n  double *d_sendbuf_r;\n  hipMalloc((void**)&d_sendbuf_r, hs*nz*NUM_VARS*sizeof(double));\n\n  double *d_recvbuf_l;\n  hipMalloc((void**)&d_recvbuf_l, hs*nz*NUM_VARS*sizeof(double));\n\n  double *d_recvbuf_r;\n  hipMalloc((void**)&d_recvbuf_r, hs*nz*NUM_VARS*sizeof(double));\n\n  \n\n  reductions(mass0, te0, hs, nx, nz, dx, dz, d_state, d_hy_dens_cell, d_hy_dens_theta_cell);\n\n  \n\n  auto c_start = clock();\n\n  while (etime < sim_time) {\n    \n\n    if (etime + dt > sim_time) { dt = sim_time - etime; }\n    \n\n    perform_timestep(\n        d_state,\n        d_state_tmp,\n        d_flux,\n        d_tend,\n        d_hy_dens_cell, \n        d_hy_dens_theta_cell, \n        d_hy_dens_int, \n        d_hy_dens_theta_int, \n        d_hy_pressure_int, \n        d_sendbuf_l, \n        d_sendbuf_r, \n        d_recvbuf_l, \n        d_recvbuf_r, \n        dt);\n\n    \n\n    etime = etime + dt;\n  }\n\n  auto c_end = clock();\n  if (masterproc)\n    printf(\"Total main time step loop: %lf sec\\n\", ( (double) (c_end-c_start) ) / CLOCKS_PER_SEC);\n\n  \n\n  reductions(mass, te, hs, nx, nz, dx, dz, d_state, d_hy_dens_cell, d_hy_dens_theta_cell);\n\n  printf( \"d_mass: %le\\n\" , (mass - mass0) / mass0 );\n  printf( \"d_te:   %le\\n\" , (te   - te0  ) / te0   );\n\n  finalize();\n\n  hipFree(d_state);\n  hipFree(d_state_tmp);\n  hipFree(d_flux);\n  hipFree(d_tend);\n  hipFree(d_hy_dens_cell); \n  hipFree(d_hy_dens_theta_cell); \n  hipFree(d_hy_dens_int); \n  hipFree(d_hy_dens_theta_int); \n  hipFree(d_hy_pressure_int); \n  hipFree(d_sendbuf_l); \n  hipFree(d_sendbuf_r); \n  hipFree(d_recvbuf_l); \n  hipFree(d_recvbuf_r); \n  \n  return 0;\n}\n"}}
{"kernel_name": "miniWeather", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <mpi.h>\n#include <time.h>\n\nconst double pi        = 3.14159265358979323846264338327;   \n\nconst double grav      = 9.8;                               \n\nconst double cp        = 1004.;                             \n\nconst double cv        = 717.;                              \n\nconst double rd        = 287.;                              \n\nconst double p0        = 1.e5;                              \n\nconst double C0        = 27.5629410929725921310572974482;   \n\nconst double gamm      = 1.40027894002789400278940027894;   \n\n\n\nconst double xlen      = 2.e4;    \n\nconst double zlen      = 1.e4;    \n\nconst double hv_beta   = 0.25;     \n\nconst double cfl       = 1.50;    \n\nconst double max_speed = 450;        \n\nconst int hs        = 2;          \n\nconst int sten_size = 4;          \n\n\n\n\nconst int NUM_VARS = 4;           \n\nconst int ID_DENS  = 0;           \n\nconst int ID_UMOM  = 1;           \n\nconst int ID_WMOM  = 2;           \n\nconst int ID_RHOT  = 3;           \n\nconst int DIR_X = 1;              \n\nconst int DIR_Z = 2;              \n\nconst int DATA_SPEC_COLLISION       = 1;\nconst int DATA_SPEC_THERMAL         = 2;\nconst int DATA_SPEC_MOUNTAIN        = 3;\nconst int DATA_SPEC_TURBULENCE      = 4;\nconst int DATA_SPEC_DENSITY_CURRENT = 5;\nconst int DATA_SPEC_INJECTION       = 6;\n\nconst int nqpoints = 3;\ndouble qpoints [] = { 0.112701665379258311482073460022E0 , 0.500000000000000000000000000000E0 , 0.887298334620741688517926539980E0 };\ndouble qweights[] = { 0.277777777777777777777777777779E0 , 0.444444444444444444444444444444E0 , 0.277777777777777777777777777779E0 };\n\n\n\n\n\n\n\ndouble sim_time;              \n\ndouble dt;                    \n\nint    nx, nz;                \n\ndouble dx, dz;                \n\nint    nx_glob, nz_glob;      \n\nint    i_beg, k_beg;          \n\nint    nranks, myrank;        \n\nint    left_rank, right_rank; \n\nint    masterproc;            \n\ndouble data_spec_int;         \n\ndouble *hy_dens_cell;         \n\ndouble *hy_dens_theta_cell;   \n\ndouble *hy_dens_int;          \n\ndouble *hy_dens_theta_int;    \n\ndouble *hy_pressure_int;      \n\n\n\n\n\n\n\n\ndouble etime;                 \n\ndouble output_counter;        \n\n\n\ndouble *state;                \n\ndouble *state_tmp;            \n\ndouble *flux;                 \n\ndouble *tend;                 \n\ndouble *sendbuf_l;            \n\ndouble *sendbuf_r;            \n\ndouble *recvbuf_l;            \n\ndouble *recvbuf_r;            \n\nint    num_out = 0;           \n\nint    direction_switch = 1;\ndouble mass0, te0;            \n\ndouble mass , te ;            \n\n\n\n\ndouble dmin( double a , double b ) { if (a<b) {return a;} else {return b;} };\n\n\n\n\nvoid   init                 ( int *argc , char ***argv );\nvoid   finalize             ( );\nvoid   injection            ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   density_current      ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   turbulence           ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   mountain_waves       ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   thermal              ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   collision            ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   hydro_const_theta    ( double z                   , double &r , double &t );\nvoid   hydro_const_bvfreq   ( double z , double bv_freq0 , double &r , double &t );\ndouble sample_ellipse_cosine( double x , double z , double amp , double x0 , double z0 , double xrad , double zrad );\nvoid   perform_timestep     ( double *state , double *state_tmp , double *flux , double *tend , double dt );\nvoid   semi_discrete_step   ( double *state_init , double *state_forcing , double *state_out , double dt , int dir , double *flux , double *tend );\nvoid   compute_tendencies_x ( double *state , double *flux , double *tend );\nvoid   compute_tendencies_z ( double *state , double *flux , double *tend );\nvoid   set_halo_values_x    ( double *state );\nvoid   set_halo_values_z    ( double *state );\nvoid   reductions           ( double &mass , double &te );\n\n\n\n\n\n\n\n\nint main(int argc, char **argv) {\n  \n\n  \n\n  \n\n  \n\n  \n\n  nx_glob = NX;      \n\n  nz_glob = NZ;       \n\n  sim_time = SIM_TIME;     \n\n  data_spec_int = DATA_SPEC;  \n\n  \n\n  \n\n  \n\n\n  init( &argc , &argv );\n#pragma omp target data map(to:state_tmp[:(nz+2*hs)*(nx+2*hs)*NUM_VARS], \\\n                               state[:(nz+2*hs)*(nx+2*hs)*NUM_VARS], \\\n                               hy_dens_cell[:nz+2*hs], \\\n                               hy_dens_theta_cell[:nz+2*hs],\\\n                               hy_dens_int[:nz+1],\\\n                               hy_dens_theta_int[:nz+1],\\\n                               hy_pressure_int[:nz+1]) \\\n                        map(alloc:flux[:(nz+1)*(nx+1)*NUM_VARS],\\\n                                  tend[:nz*nx*NUM_VARS],\\\n                                  sendbuf_l[:hs*nz*NUM_VARS],\\\n                                  sendbuf_r[:hs*nz*NUM_VARS],\\\n                                  recvbuf_l[:hs*nz*NUM_VARS],\\\n                                  recvbuf_r[:hs*nz*NUM_VARS]) \n{\n\n  \n\n  reductions(mass0,te0);\n\n  \n\n  \n\n  \n\n  auto c_start = clock();\n  while (etime < sim_time) {\n    \n\n    if (etime + dt > sim_time) { dt = sim_time - etime; }\n    \n\n    perform_timestep(state,state_tmp,flux,tend,dt);\n    \n\n    etime = etime + dt;\n  }\n  auto c_end = clock();\n  if (masterproc) {\n     printf(\"CPU Time: %lf sec\\n\",( (double) (c_end-c_start) ) / CLOCKS_PER_SEC);\n  }\n\n  \n\n  reductions(mass,te);\n}\n\n  printf( \"d_mass: %le\\n\" , (mass - mass0)/mass0 );\n  printf( \"d_te:   %le\\n\" , (te   - te0  )/te0   );\n\n  finalize();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvoid perform_timestep( double *state , double *state_tmp , double *flux , double *tend , double dt ) {\n  if (direction_switch) {\n    semi_discrete_step( state , state     , state_tmp , dt / 3 , DIR_X , flux , tend );\n    semi_discrete_step( state , state_tmp , state_tmp , dt / 2 , DIR_X , flux , tend );\n    semi_discrete_step( state , state_tmp , state     , dt / 1 , DIR_X , flux , tend );\n    semi_discrete_step( state , state     , state_tmp , dt / 3 , DIR_Z , flux , tend );\n    semi_discrete_step( state , state_tmp , state_tmp , dt / 2 , DIR_Z , flux , tend );\n    semi_discrete_step( state , state_tmp , state     , dt / 1 , DIR_Z , flux , tend );\n  } else {\n    semi_discrete_step( state , state     , state_tmp , dt / 3 , DIR_Z , flux , tend );\n    semi_discrete_step( state , state_tmp , state_tmp , dt / 2 , DIR_Z , flux , tend );\n    semi_discrete_step( state , state_tmp , state     , dt / 1 , DIR_Z , flux , tend );\n    semi_discrete_step( state , state     , state_tmp , dt / 3 , DIR_X , flux , tend );\n    semi_discrete_step( state , state_tmp , state_tmp , dt / 2 , DIR_X , flux , tend );\n    semi_discrete_step( state , state_tmp , state     , dt / 1 , DIR_X , flux , tend );\n  }\n  if (direction_switch) { direction_switch = 0; } else { direction_switch = 1; }\n}\n\n\n\n\n\n\n\n\nvoid semi_discrete_step( double *state_init , double *state_forcing , double *state_out , double dt , int dir , double *flux , double *tend ) {\n  int i, k, ll, inds, indt;\n  if        (dir == DIR_X) {\n    \n\n    set_halo_values_x(state_forcing);\n    \n\n    compute_tendencies_x(state_forcing,flux,tend);\n  } else if (dir == DIR_Z) {\n    \n\n    set_halo_values_z(state_forcing);\n    \n\n    compute_tendencies_z(state_forcing,flux,tend);\n  }\n\n  \n\n#pragma omp target teams distribute parallel for collapse(3) private(inds,indt)\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (k=0; k<nz; k++) {\n      for (i=0; i<nx; i++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n        indt = ll*nz*nx + k*nx + i;\n        state_out[inds] = state_init[inds] + dt * tend[indt];\n      }\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\nvoid compute_tendencies_x( double *state , double *flux , double *tend ) {\n  int    i,k,ll,s,inds,indf1,indf2,indt;\n  double r,u,w,t,p, stencil[4], d3_vals[NUM_VARS], vals[NUM_VARS], hv_coef;\n  \n\n  hv_coef = -hv_beta * dx / (16*dt);\n  \n\n#pragma omp target teams distribute parallel for collapse(2) private(ll,s,inds,stencil,vals,d3_vals,r,u,w,t,p)\n  for (k=0; k<nz; k++) {\n    for (i=0; i<nx+1; i++) {\n      \n\n      for (ll=0; ll<NUM_VARS; ll++) {\n        for (s=0; s < sten_size; s++) {\n          inds = ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+s;\n          stencil[s] = state[inds];\n        }\n        \n\n        vals[ll] = -stencil[0]/12 + 7*stencil[1]/12 + 7*stencil[2]/12 - stencil[3]/12;\n        \n\n        d3_vals[ll] = -stencil[0] + 3*stencil[1] - 3*stencil[2] + stencil[3];\n      }\n\n      \n\n      r = vals[ID_DENS] + hy_dens_cell[k+hs];\n      u = vals[ID_UMOM] / r;\n      w = vals[ID_WMOM] / r;\n      t = ( vals[ID_RHOT] + hy_dens_theta_cell[k+hs] ) / r;\n      p = C0*pow((r*t),gamm);\n\n      \n\n      flux[ID_DENS*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u     - hv_coef*d3_vals[ID_DENS];\n      flux[ID_UMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*u+p - hv_coef*d3_vals[ID_UMOM];\n      flux[ID_WMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*w   - hv_coef*d3_vals[ID_WMOM];\n      flux[ID_RHOT*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*t   - hv_coef*d3_vals[ID_RHOT];\n    }\n  }\n\n  \n\n#pragma omp target teams distribute parallel for collapse(3) private(indt,indf1,indf2)\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (k=0; k<nz; k++) {\n      for (i=0; i<nx; i++) {\n        indt  = ll* nz   * nx    + k* nx    + i  ;\n        indf1 = ll*(nz+1)*(nx+1) + k*(nx+1) + i  ;\n        indf2 = ll*(nz+1)*(nx+1) + k*(nx+1) + i+1;\n        tend[indt] = -( flux[indf2] - flux[indf1] ) / dx;\n      }\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\nvoid compute_tendencies_z( double *state , double *flux , double *tend ) {\n  int    i,k,ll,s, inds, indf1, indf2, indt;\n  double r,u,w,t,p, stencil[4], d3_vals[NUM_VARS], vals[NUM_VARS], hv_coef;\n  \n\n  hv_coef = -hv_beta * dz / (16*dt);\n  \n\n#pragma omp target teams distribute parallel for collapse(2) private(ll,s,inds,stencil,vals,d3_vals,r,u,w,t,p)\n  for (k=0; k<nz+1; k++) {\n    for (i=0; i<nx; i++) {\n      \n\n      for (ll=0; ll<NUM_VARS; ll++) {\n        for (s=0; s<sten_size; s++) {\n          inds = ll*(nz+2*hs)*(nx+2*hs) + (k+s)*(nx+2*hs) + i+hs;\n          stencil[s] = state[inds];\n        }\n        \n\n        vals[ll] = -stencil[0]/12 + 7*stencil[1]/12 + 7*stencil[2]/12 - stencil[3]/12;\n        \n\n        d3_vals[ll] = -stencil[0] + 3*stencil[1] - 3*stencil[2] + stencil[3];\n      }\n\n      \n\n      r = vals[ID_DENS] + hy_dens_int[k];\n      u = vals[ID_UMOM] / r;\n      w = vals[ID_WMOM] / r;\n      t = ( vals[ID_RHOT] + hy_dens_theta_int[k] ) / r;\n      p = C0*pow((r*t),gamm) - hy_pressure_int[k];\n      \n\n      if (k == 0 || k == nz) {\n        w                = 0;\n        d3_vals[ID_DENS] = 0;\n      }\n\n      \n\n      flux[ID_DENS*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w     - hv_coef*d3_vals[ID_DENS];\n      flux[ID_UMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*u   - hv_coef*d3_vals[ID_UMOM];\n      flux[ID_WMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*w+p - hv_coef*d3_vals[ID_WMOM];\n      flux[ID_RHOT*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*t   - hv_coef*d3_vals[ID_RHOT];\n    }\n  }\n\n  \n\n#pragma omp target teams distribute parallel for collapse(3) private(indt,indf1,indf2)\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (k=0; k<nz; k++) {\n      for (i=0; i<nx; i++) {\n        indt  = ll* nz   * nx    + k* nx    + i  ;\n        indf1 = ll*(nz+1)*(nx+1) + (k  )*(nx+1) + i;\n        indf2 = ll*(nz+1)*(nx+1) + (k+1)*(nx+1) + i;\n        tend[indt] = -( flux[indf2] - flux[indf1] ) / dz;\n        if (ll == ID_WMOM) {\n          inds = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n          tend[indt] = tend[indt] - state[inds]*grav;\n        }\n      }\n    }\n  }\n}\n\n\n\n\nvoid set_halo_values_x( double *state ) {\n  int k, ll, ind_r, ind_u, ind_t, i, s, ierr;\n  double z;\n  MPI_Request req_r[2], req_s[2];\n\n  \n\n  ierr = MPI_Irecv(recvbuf_l,hs*nz*NUM_VARS,MPI_DOUBLE, left_rank,0,MPI_COMM_WORLD,&req_r[0]);\n  ierr = MPI_Irecv(recvbuf_r,hs*nz*NUM_VARS,MPI_DOUBLE,right_rank,1,MPI_COMM_WORLD,&req_r[1]);\n\n  \n\n#pragma omp target teams distribute parallel for collapse(3)\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (k=0; k<nz; k++) {\n      for (s=0; s<hs; s++) {\n        sendbuf_l[ll*nz*hs + k*hs + s] = state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + hs+s];\n        sendbuf_r[ll*nz*hs + k*hs + s] = state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + nx+s];\n      }\n    }\n  }\n\n#pragma omp target update from(sendbuf_l[:nz*hs*NUM_VARS],sendbuf_r[:nz*hs*NUM_VARS])\n\n  \n\n  ierr = MPI_Isend(sendbuf_l,hs*nz*NUM_VARS,MPI_DOUBLE, left_rank,1,MPI_COMM_WORLD,&req_s[0]);\n  ierr = MPI_Isend(sendbuf_r,hs*nz*NUM_VARS,MPI_DOUBLE,right_rank,0,MPI_COMM_WORLD,&req_s[1]);\n\n  \n\n  ierr = MPI_Waitall(2,req_r,MPI_STATUSES_IGNORE);\n\n#pragma omp target update to(recvbuf_l[:nz*hs*NUM_VARS],recvbuf_r[:nz*hs*NUM_VARS])\n\n  \n\n#pragma omp target teams distribute parallel for collapse(3)\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (k=0; k<nz; k++) {\n      for (s=0; s<hs; s++) {\n        state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + s      ] = recvbuf_l[ll*nz*hs + k*hs + s];\n        state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + nx+hs+s] = recvbuf_r[ll*nz*hs + k*hs + s];\n      }\n    }\n  }\n\n  \n\n  ierr = MPI_Waitall(2,req_s,MPI_STATUSES_IGNORE);\n\n  if (data_spec_int == DATA_SPEC_INJECTION) {\n    if (myrank == 0) {\n#pragma omp target teams distribute parallel for private(z,ind_r,ind_u,ind_t) collapse(2)\n      for (k=0; k<nz; k++) {\n        for (i=0; i<hs; i++) {\n          z = (k_beg + k+0.5)*dz;\n          if (fabs(z-3*zlen/4) <= zlen/16) {\n            ind_r = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n            ind_u = ID_UMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n            ind_t = ID_RHOT*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n            state[ind_u] = (state[ind_r]+hy_dens_cell[k+hs]) * 50.;\n            state[ind_t] = (state[ind_r]+hy_dens_cell[k+hs]) * 298. - hy_dens_theta_cell[k+hs];\n          }\n        }\n      }\n    }\n  }\n\n}\n\n\n\n\n\n\nvoid set_halo_values_z( double *state ) {\n  int          i, ll;\n  const double mnt_width = xlen/8;\n  double       x, xloc, mnt_deriv;\n#pragma omp target teams distribute parallel for collapse(2) private(x,xloc,mnt_deriv)\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (i=0; i<nx+2*hs; i++) {\n      if (ll == ID_WMOM) {\n        state[ll*(nz+2*hs)*(nx+2*hs) + (0      )*(nx+2*hs) + i] = 0.;\n        state[ll*(nz+2*hs)*(nx+2*hs) + (1      )*(nx+2*hs) + i] = 0.;\n        state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs  )*(nx+2*hs) + i] = 0.;\n        state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs+1)*(nx+2*hs) + i] = 0.;\n        \n\n        if (data_spec_int == DATA_SPEC_MOUNTAIN) {\n          x = (i_beg+i-hs+0.5)*dx;\n          if ( fabs(x-xlen/4) < mnt_width ) {\n            xloc = (x-(xlen/4)) / mnt_width;\n            \n\n            mnt_deriv = -pi*cos(pi*xloc/2)*sin(pi*xloc/2)*10/dx;\n            \n\n            state[ID_WMOM*(nz+2*hs)*(nx+2*hs) + (0)*(nx+2*hs) + i] = mnt_deriv*state[ID_UMOM*(nz+2*hs)*(nx+2*hs) + hs*(nx+2*hs) + i];\n            state[ID_WMOM*(nz+2*hs)*(nx+2*hs) + (1)*(nx+2*hs) + i] = mnt_deriv*state[ID_UMOM*(nz+2*hs)*(nx+2*hs) + hs*(nx+2*hs) + i];\n          }\n        }\n      } else {\n        state[ll*(nz+2*hs)*(nx+2*hs) + (0      )*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (hs     )*(nx+2*hs) + i];\n        state[ll*(nz+2*hs)*(nx+2*hs) + (1      )*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (hs     )*(nx+2*hs) + i];\n        state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs  )*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs-1)*(nx+2*hs) + i];\n        state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs+1)*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs-1)*(nx+2*hs) + i];\n      }\n    }\n  }\n}\n\n\nvoid init( int *argc , char ***argv ) {\n  int    i, k, ii, kk, ll, ierr, inds, i_end;\n  double x, z, r, u, w, t, hr, ht, nper;\n\n  ierr = MPI_Init(argc,argv);\n\n  \n\n  dx = xlen / nx_glob;\n  dz = zlen / nz_glob;\n\n  ierr = MPI_Comm_size(MPI_COMM_WORLD,&nranks);\n  ierr = MPI_Comm_rank(MPI_COMM_WORLD,&myrank);\n  nper = ( (double) nx_glob ) / nranks;\n  i_beg = round( nper* (myrank)    );\n  i_end = round( nper*((myrank)+1) )-1;\n  nx = i_end - i_beg + 1;\n  left_rank  = myrank - 1;\n  if (left_rank == -1) left_rank = nranks-1;\n  right_rank = myrank + 1;\n  if (right_rank == nranks) right_rank = 0;\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  k_beg = 0;\n  nz = nz_glob;\n  masterproc = (myrank == 0);\n\n  \n\n  state              = (double *) malloc( (nx+2*hs)*(nz+2*hs)*NUM_VARS*sizeof(double) );\n  state_tmp          = (double *) malloc( (nx+2*hs)*(nz+2*hs)*NUM_VARS*sizeof(double) );\n  flux               = (double *) malloc( (nx+1)*(nz+1)*NUM_VARS*sizeof(double) );\n  tend               = (double *) malloc( nx*nz*NUM_VARS*sizeof(double) );\n  hy_dens_cell       = (double *) malloc( (nz+2*hs)*sizeof(double) );\n  hy_dens_theta_cell = (double *) malloc( (nz+2*hs)*sizeof(double) );\n  hy_dens_int        = (double *) malloc( (nz+1)*sizeof(double) );\n  hy_dens_theta_int  = (double *) malloc( (nz+1)*sizeof(double) );\n  hy_pressure_int    = (double *) malloc( (nz+1)*sizeof(double) );\n  sendbuf_l          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  sendbuf_r          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  recvbuf_l          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  recvbuf_r          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n\n  \n\n  dt = dmin(dx,dz) / max_speed * cfl;\n  \n\n  etime = 0.;\n  output_counter = 0.;\n\n  \n\n  if (masterproc) {\n    printf( \"nx_glob, nz_glob: %d %d\\n\", nx_glob, nz_glob);\n    printf( \"dx,dz: %lf %lf\\n\",dx,dz);\n    printf( \"dt: %lf\\n\",dt);\n  }\n  \n\n  ierr = MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  \n\n  \n\n  for (k=0; k<nz+2*hs; k++) {\n    for (i=0; i<nx+2*hs; i++) {\n      \n\n      for (ll=0; ll<NUM_VARS; ll++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n        state[inds] = 0.;\n      }\n      \n\n      for (kk=0; kk<nqpoints; kk++) {\n        for (ii=0; ii<nqpoints; ii++) {\n          \n\n          x = (i_beg + i-hs+0.5)*dx + (qpoints[ii]-0.5)*dx;\n          z = (k_beg + k-hs+0.5)*dz + (qpoints[kk]-0.5)*dz;\n\n          \n\n          if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (x,z,r,u,w,t,hr,ht); }\n\n          \n\n          inds = ID_DENS*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + r                         * qweights[ii]*qweights[kk];\n          inds = ID_UMOM*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + (r+hr)*u                  * qweights[ii]*qweights[kk];\n          inds = ID_WMOM*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + (r+hr)*w                  * qweights[ii]*qweights[kk];\n          inds = ID_RHOT*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + ( (r+hr)*(t+ht) - hr*ht ) * qweights[ii]*qweights[kk];\n        }\n      }\n      for (ll=0; ll<NUM_VARS; ll++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n        state_tmp[inds] = state[inds];\n      }\n    }\n  }\n  \n\n  for (k=0; k<nz+2*hs; k++) {\n    hy_dens_cell      [k] = 0.;\n    hy_dens_theta_cell[k] = 0.;\n    for (kk=0; kk<nqpoints; kk++) {\n      z = (k_beg + k-hs+0.5)*dz;\n      \n\n      if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (0.,z,r,u,w,t,hr,ht); }\n      hy_dens_cell      [k] = hy_dens_cell      [k] + hr    * qweights[kk];\n      hy_dens_theta_cell[k] = hy_dens_theta_cell[k] + hr*ht * qweights[kk];\n    }\n  }\n  \n\n  for (k=0; k<nz+1; k++) {\n    z = (k_beg + k)*dz;\n    if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (0.,z,r,u,w,t,hr,ht); }\n    hy_dens_int      [k] = hr;\n    hy_dens_theta_int[k] = hr*ht;\n    hy_pressure_int  [k] = C0*pow((hr*ht),gamm);\n  }\n}\n\n\n\n\n\n\n\n\n\n\nvoid injection( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n}\n\n\n\n\n\n\n\n\n\n\nvoid density_current( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z,-20. ,xlen/2,5000.,4000.,2000.);\n}\n\n\n\n\n\n\n\n\nvoid turbulence( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  \n\n  \n\n  \n\n  \n\n}\n\n\n\n\n\n\n\n\nvoid mountain_waves( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_bvfreq(z,0.02,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 15.;\n  w = 0.;\n}\n\n\n\n\n\n\n\n\n\n\nvoid thermal( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z, 3. ,xlen/2,2000.,2000.,2000.);\n}\n\n\n\n\n\n\n\n\n\n\nvoid collision( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z, 20.,xlen/2,2000.,2000.,2000.);\n  t = t + sample_ellipse_cosine(x,z,-20.,xlen/2,8000.,2000.,2000.);\n}\n\n\n\n\n\n\n\n\nvoid hydro_const_theta( double z , double &r , double &t ) {\n  const double theta0 = 300.;  \n\n  const double exner0 = 1.;    \n\n  double       p,exner,rt;\n  \n\n  t = theta0;                                  \n\n  exner = exner0 - grav * z / (cp * theta0);   \n\n  p = p0 * pow(exner,(cp/rd));                 \n\n  rt = pow((p / C0),(1. / gamm));             \n\n  r = rt / t;                                  \n\n}\n\n\n\n\n\n\n\n\n\n\nvoid hydro_const_bvfreq( double z , double bv_freq0 , double &r , double &t ) {\n  const double theta0 = 300.;  \n\n  const double exner0 = 1.;    \n\n  double       p, exner, rt;\n  t = theta0 * exp( bv_freq0*bv_freq0 / grav * z );                                    \n\n  exner = exner0 - grav*grav / (cp * bv_freq0*bv_freq0) * (t - theta0) / (t * theta0); \n\n  p = p0 * pow(exner,(cp/rd));                                                         \n\n  rt = pow((p / C0),(1. / gamm));                                                  \n\n  r = rt / t;                                                                          \n\n}\n\n\n\n\n\n\n\n\ndouble sample_ellipse_cosine( double x , double z , double amp , double x0 , double z0 , double xrad , double zrad ) {\n  double dist;\n  \n\n  dist = sqrt( ((x-x0)/xrad)*((x-x0)/xrad) + ((z-z0)/zrad)*((z-z0)/zrad) ) * pi / 2.;\n  \n\n  if (dist <= pi / 2.) {\n    return amp * pow(cos(dist),2.);\n  } else {\n    return 0.;\n  }\n}\n\n\n\nvoid finalize() {\n  int ierr;\n  free( state );\n  free( state_tmp );\n  free( flux );\n  free( tend );\n  free( hy_dens_cell );\n  free( hy_dens_theta_cell );\n  free( hy_dens_int );\n  free( hy_dens_theta_int );\n  free( hy_pressure_int );\n  free( sendbuf_l );\n  free( sendbuf_r );\n  free( recvbuf_l );\n  free( recvbuf_r );\n  ierr = MPI_Finalize();\n}\n\n\n\n\nvoid reductions( double &mass , double &te ) {\n  mass = 0;\n  te   = 0;\n\n#pragma omp target map(to: state[0:(nz+2*hs)*(nx+2*hs)*NUM_VARS]) map(tofrom: mass, te) \n  {\n  #pragma omp teams distribute parallel for collapse(2) reduction(+:mass,te)\n  for (int k=0; k<nz; k++) {\n    for (int i=0; i<nx; i++) {\n      int ind_r = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n      int ind_u = ID_UMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n      int ind_w = ID_WMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n      int ind_t = ID_RHOT*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n      double r  =   state[ind_r] + hy_dens_cell[hs+k];             \n\n      double u  =   state[ind_u] / r;                              \n\n      double w  =   state[ind_w] / r;                              \n\n      double th = ( state[ind_t] + hy_dens_theta_cell[hs+k] ) / r; \n\n      double p  = C0*pow(r*th,gamm);                               \n\n      double t  = th / pow(p0/p,rd/cp);                            \n\n      double ke = r*(u*u+w*w);                                     \n\n      double ie = r*cv*t;                                          \n\n      mass += r        *dx*dz; \n\n      te   += (ke + ie)*dx*dz; \n\n    }\n  }\n  }\n  double glob[2], loc[2];\n  loc[0] = mass;\n  loc[1] = te;\n  int ierr = MPI_Allreduce(loc,glob,2,MPI_DOUBLE,MPI_SUM,MPI_COMM_WORLD);\n  mass = glob[0];\n  te   = glob[1];\n}\n\n\n"}}
{"kernel_name": "miniWeather", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <mpi.h>\n#include <time.h>\n\nconst double pi        = 3.14159265358979323846264338327;   \n\nconst double grav      = 9.8;                               \n\nconst double cp        = 1004.;                             \n\nconst double cv        = 717.;                              \n\nconst double rd        = 287.;                              \n\nconst double p0        = 1.e5;                              \n\nconst double C0        = 27.5629410929725921310572974482;   \n\nconst double gamm      = 1.40027894002789400278940027894;   \n\n\n\nconst double xlen      = 2.e4;    \n\nconst double zlen      = 1.e4;    \n\nconst double hv_beta   = 0.25;     \n\nconst double cfl       = 1.50;    \n\nconst double max_speed = 450;        \n\nconst int hs        = 2;          \n\nconst int sten_size = 4;          \n\n\n\n\nconst int NUM_VARS = 4;           \n\nconst int ID_DENS  = 0;           \n\nconst int ID_UMOM  = 1;           \n\nconst int ID_WMOM  = 2;           \n\nconst int ID_RHOT  = 3;           \n\nconst int DIR_X = 1;              \n\nconst int DIR_Z = 2;              \n\nconst int DATA_SPEC_COLLISION       = 1;\nconst int DATA_SPEC_THERMAL         = 2;\nconst int DATA_SPEC_MOUNTAIN        = 3;\nconst int DATA_SPEC_TURBULENCE      = 4;\nconst int DATA_SPEC_DENSITY_CURRENT = 5;\nconst int DATA_SPEC_INJECTION       = 6;\n\nconst int nqpoints = 3;\ndouble qpoints [] = { 0.112701665379258311482073460022E0 , 0.500000000000000000000000000000E0 , 0.887298334620741688517926539980E0 };\ndouble qweights[] = { 0.277777777777777777777777777779E0 , 0.444444444444444444444444444444E0 , 0.277777777777777777777777777779E0 };\n\n\n\n\n\n\n\ndouble sim_time;              \n\ndouble dt;                    \n\nint    nx, nz;                \n\ndouble dx, dz;                \n\nint    nx_glob, nz_glob;      \n\nint    i_beg, k_beg;          \n\nint    nranks, myrank;        \n\nint    left_rank, right_rank; \n\nint    masterproc;            \n\ndouble data_spec_int;         \n\ndouble *hy_dens_cell;         \n\ndouble *hy_dens_theta_cell;   \n\ndouble *hy_dens_int;          \n\ndouble *hy_dens_theta_int;    \n\ndouble *hy_pressure_int;      \n\n\n\n\n\n\n\n\ndouble etime;                 \n\ndouble output_counter;        \n\n\n\ndouble *state;                \n\ndouble *state_tmp;            \n\ndouble *flux;                 \n\ndouble *tend;                 \n\ndouble *sendbuf_l;            \n\ndouble *sendbuf_r;            \n\ndouble *recvbuf_l;            \n\ndouble *recvbuf_r;            \n\nint    num_out = 0;           \n\nint    direction_switch = 1;\ndouble mass0, te0;            \n\ndouble mass , te ;            \n\n\n\n\ndouble dmin( double a , double b ) { if (a<b) {return a;} else {return b;} };\n\n\n\n\nvoid   init                 ( int *argc , char ***argv );\nvoid   finalize             ( );\nvoid   injection            ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   density_current      ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   turbulence           ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   mountain_waves       ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   thermal              ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   collision            ( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht );\nvoid   hydro_const_theta    ( double z                   , double &r , double &t );\nvoid   hydro_const_bvfreq   ( double z , double bv_freq0 , double &r , double &t );\ndouble sample_ellipse_cosine( double x , double z , double amp , double x0 , double z0 , double xrad , double zrad );\nvoid   perform_timestep     ( double *state , double *state_tmp , double *flux , double *tend , double dt );\nvoid   semi_discrete_step   ( double *state_init , double *state_forcing , double *state_out , double dt , int dir , double *flux , double *tend );\nvoid   compute_tendencies_x ( double *state , double *flux , double *tend );\nvoid   compute_tendencies_z ( double *state , double *flux , double *tend );\nvoid   set_halo_values_x    ( double *state );\nvoid   set_halo_values_z    ( double *state );\nvoid   reductions           ( double &mass , double &te );\n\n\n\n\n\n\n\n\nint main(int argc, char **argv) {\n  \n\n  \n\n  \n\n  \n\n  \n\n  nx_glob = NX;      \n\n  nz_glob = NZ;       \n\n  sim_time = SIM_TIME;     \n\n  data_spec_int = DATA_SPEC;  \n\n  \n\n  \n\n  \n\n\n  init( &argc , &argv );\n{\n\n  \n\n  reductions(mass0,te0);\n\n  \n\n  \n\n  \n\n  auto c_start = clock();\n  while (etime < sim_time) {\n    \n\n    if (etime + dt > sim_time) { dt = sim_time - etime; }\n    \n\n    perform_timestep(state,state_tmp,flux,tend,dt);\n    \n\n    etime = etime + dt;\n  }\n  auto c_end = clock();\n  if (masterproc) {\n     printf(\"CPU Time: %lf sec\\n\",( (double) (c_end-c_start) ) / CLOCKS_PER_SEC);\n  }\n\n  \n\n  reductions(mass,te);\n}\n\n  printf( \"d_mass: %le\\n\" , (mass - mass0)/mass0 );\n  printf( \"d_te:   %le\\n\" , (te   - te0  )/te0   );\n\n  finalize();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvoid perform_timestep( double *state , double *state_tmp , double *flux , double *tend , double dt ) {\n  if (direction_switch) {\n    semi_discrete_step( state , state     , state_tmp , dt / 3 , DIR_X , flux , tend );\n    semi_discrete_step( state , state_tmp , state_tmp , dt / 2 , DIR_X , flux , tend );\n    semi_discrete_step( state , state_tmp , state     , dt / 1 , DIR_X , flux , tend );\n    semi_discrete_step( state , state     , state_tmp , dt / 3 , DIR_Z , flux , tend );\n    semi_discrete_step( state , state_tmp , state_tmp , dt / 2 , DIR_Z , flux , tend );\n    semi_discrete_step( state , state_tmp , state     , dt / 1 , DIR_Z , flux , tend );\n  } else {\n    semi_discrete_step( state , state     , state_tmp , dt / 3 , DIR_Z , flux , tend );\n    semi_discrete_step( state , state_tmp , state_tmp , dt / 2 , DIR_Z , flux , tend );\n    semi_discrete_step( state , state_tmp , state     , dt / 1 , DIR_Z , flux , tend );\n    semi_discrete_step( state , state     , state_tmp , dt / 3 , DIR_X , flux , tend );\n    semi_discrete_step( state , state_tmp , state_tmp , dt / 2 , DIR_X , flux , tend );\n    semi_discrete_step( state , state_tmp , state     , dt / 1 , DIR_X , flux , tend );\n  }\n  if (direction_switch) { direction_switch = 0; } else { direction_switch = 1; }\n}\n\n\n\n\n\n\n\n\nvoid semi_discrete_step( double *state_init , double *state_forcing , double *state_out , double dt , int dir , double *flux , double *tend ) {\n  int i, k, ll, inds, indt;\n  if        (dir == DIR_X) {\n    \n\n    set_halo_values_x(state_forcing);\n    \n\n    compute_tendencies_x(state_forcing,flux,tend);\n  } else if (dir == DIR_Z) {\n    \n\n    set_halo_values_z(state_forcing);\n    \n\n    compute_tendencies_z(state_forcing,flux,tend);\n  }\n\n  \n\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (k=0; k<nz; k++) {\n      for (i=0; i<nx; i++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n        indt = ll*nz*nx + k*nx + i;\n        state_out[inds] = state_init[inds] + dt * tend[indt];\n      }\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\nvoid compute_tendencies_x( double *state , double *flux , double *tend ) {\n  int    i,k,ll,s,inds,indf1,indf2,indt;\n  double r,u,w,t,p, stencil[4], d3_vals[NUM_VARS], vals[NUM_VARS], hv_coef;\n  \n\n  hv_coef = -hv_beta * dx / (16*dt);\n  \n\n  for (k=0; k<nz; k++) {\n    for (i=0; i<nx+1; i++) {\n      \n\n      for (ll=0; ll<NUM_VARS; ll++) {\n        for (s=0; s < sten_size; s++) {\n          inds = ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+s;\n          stencil[s] = state[inds];\n        }\n        \n\n        vals[ll] = -stencil[0]/12 + 7*stencil[1]/12 + 7*stencil[2]/12 - stencil[3]/12;\n        \n\n        d3_vals[ll] = -stencil[0] + 3*stencil[1] - 3*stencil[2] + stencil[3];\n      }\n\n      \n\n      r = vals[ID_DENS] + hy_dens_cell[k+hs];\n      u = vals[ID_UMOM] / r;\n      w = vals[ID_WMOM] / r;\n      t = ( vals[ID_RHOT] + hy_dens_theta_cell[k+hs] ) / r;\n      p = C0*pow((r*t),gamm);\n\n      \n\n      flux[ID_DENS*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u     - hv_coef*d3_vals[ID_DENS];\n      flux[ID_UMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*u+p - hv_coef*d3_vals[ID_UMOM];\n      flux[ID_WMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*w   - hv_coef*d3_vals[ID_WMOM];\n      flux[ID_RHOT*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*t   - hv_coef*d3_vals[ID_RHOT];\n    }\n  }\n\n  \n\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (k=0; k<nz; k++) {\n      for (i=0; i<nx; i++) {\n        indt  = ll* nz   * nx    + k* nx    + i  ;\n        indf1 = ll*(nz+1)*(nx+1) + k*(nx+1) + i  ;\n        indf2 = ll*(nz+1)*(nx+1) + k*(nx+1) + i+1;\n        tend[indt] = -( flux[indf2] - flux[indf1] ) / dx;\n      }\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\nvoid compute_tendencies_z( double *state , double *flux , double *tend ) {\n  int    i,k,ll,s, inds, indf1, indf2, indt;\n  double r,u,w,t,p, stencil[4], d3_vals[NUM_VARS], vals[NUM_VARS], hv_coef;\n  \n\n  hv_coef = -hv_beta * dz / (16*dt);\n  \n\n  for (k=0; k<nz+1; k++) {\n    for (i=0; i<nx; i++) {\n      \n\n      for (ll=0; ll<NUM_VARS; ll++) {\n        for (s=0; s<sten_size; s++) {\n          inds = ll*(nz+2*hs)*(nx+2*hs) + (k+s)*(nx+2*hs) + i+hs;\n          stencil[s] = state[inds];\n        }\n        \n\n        vals[ll] = -stencil[0]/12 + 7*stencil[1]/12 + 7*stencil[2]/12 - stencil[3]/12;\n        \n\n        d3_vals[ll] = -stencil[0] + 3*stencil[1] - 3*stencil[2] + stencil[3];\n      }\n\n      \n\n      r = vals[ID_DENS] + hy_dens_int[k];\n      u = vals[ID_UMOM] / r;\n      w = vals[ID_WMOM] / r;\n      t = ( vals[ID_RHOT] + hy_dens_theta_int[k] ) / r;\n      p = C0*pow((r*t),gamm) - hy_pressure_int[k];\n      \n\n      if (k == 0 || k == nz) {\n        w                = 0;\n        d3_vals[ID_DENS] = 0;\n      }\n\n      \n\n      flux[ID_DENS*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w     - hv_coef*d3_vals[ID_DENS];\n      flux[ID_UMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*u   - hv_coef*d3_vals[ID_UMOM];\n      flux[ID_WMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*w+p - hv_coef*d3_vals[ID_WMOM];\n      flux[ID_RHOT*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*t   - hv_coef*d3_vals[ID_RHOT];\n    }\n  }\n\n  \n\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (k=0; k<nz; k++) {\n      for (i=0; i<nx; i++) {\n        indt  = ll* nz   * nx    + k* nx    + i  ;\n        indf1 = ll*(nz+1)*(nx+1) + (k  )*(nx+1) + i;\n        indf2 = ll*(nz+1)*(nx+1) + (k+1)*(nx+1) + i;\n        tend[indt] = -( flux[indf2] - flux[indf1] ) / dz;\n        if (ll == ID_WMOM) {\n          inds = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n          tend[indt] = tend[indt] - state[inds]*grav;\n        }\n      }\n    }\n  }\n}\n\n\n\n\nvoid set_halo_values_x( double *state ) {\n  int k, ll, ind_r, ind_u, ind_t, i, s, ierr;\n  double z;\n  MPI_Request req_r[2], req_s[2];\n\n  \n\n  ierr = MPI_Irecv(recvbuf_l,hs*nz*NUM_VARS,MPI_DOUBLE, left_rank,0,MPI_COMM_WORLD,&req_r[0]);\n  ierr = MPI_Irecv(recvbuf_r,hs*nz*NUM_VARS,MPI_DOUBLE,right_rank,1,MPI_COMM_WORLD,&req_r[1]);\n\n  \n\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (k=0; k<nz; k++) {\n      for (s=0; s<hs; s++) {\n        sendbuf_l[ll*nz*hs + k*hs + s] = state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + hs+s];\n        sendbuf_r[ll*nz*hs + k*hs + s] = state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + nx+s];\n      }\n    }\n  }\n\n\n  \n\n  ierr = MPI_Isend(sendbuf_l,hs*nz*NUM_VARS,MPI_DOUBLE, left_rank,1,MPI_COMM_WORLD,&req_s[0]);\n  ierr = MPI_Isend(sendbuf_r,hs*nz*NUM_VARS,MPI_DOUBLE,right_rank,0,MPI_COMM_WORLD,&req_s[1]);\n\n  \n\n  ierr = MPI_Waitall(2,req_r,MPI_STATUSES_IGNORE);\n\n\n  \n\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (k=0; k<nz; k++) {\n      for (s=0; s<hs; s++) {\n        state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + s      ] = recvbuf_l[ll*nz*hs + k*hs + s];\n        state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + nx+hs+s] = recvbuf_r[ll*nz*hs + k*hs + s];\n      }\n    }\n  }\n\n  \n\n  ierr = MPI_Waitall(2,req_s,MPI_STATUSES_IGNORE);\n\n  if (data_spec_int == DATA_SPEC_INJECTION) {\n    if (myrank == 0) {\n      for (k=0; k<nz; k++) {\n        for (i=0; i<hs; i++) {\n          z = (k_beg + k+0.5)*dz;\n          if (fabs(z-3*zlen/4) <= zlen/16) {\n            ind_r = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n            ind_u = ID_UMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n            ind_t = ID_RHOT*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n            state[ind_u] = (state[ind_r]+hy_dens_cell[k+hs]) * 50.;\n            state[ind_t] = (state[ind_r]+hy_dens_cell[k+hs]) * 298. - hy_dens_theta_cell[k+hs];\n          }\n        }\n      }\n    }\n  }\n\n}\n\n\n\n\n\n\nvoid set_halo_values_z( double *state ) {\n  int          i, ll;\n  const double mnt_width = xlen/8;\n  double       x, xloc, mnt_deriv;\n  for (ll=0; ll<NUM_VARS; ll++) {\n    for (i=0; i<nx+2*hs; i++) {\n      if (ll == ID_WMOM) {\n        state[ll*(nz+2*hs)*(nx+2*hs) + (0      )*(nx+2*hs) + i] = 0.;\n        state[ll*(nz+2*hs)*(nx+2*hs) + (1      )*(nx+2*hs) + i] = 0.;\n        state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs  )*(nx+2*hs) + i] = 0.;\n        state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs+1)*(nx+2*hs) + i] = 0.;\n        \n\n        if (data_spec_int == DATA_SPEC_MOUNTAIN) {\n          x = (i_beg+i-hs+0.5)*dx;\n          if ( fabs(x-xlen/4) < mnt_width ) {\n            xloc = (x-(xlen/4)) / mnt_width;\n            \n\n            mnt_deriv = -pi*cos(pi*xloc/2)*sin(pi*xloc/2)*10/dx;\n            \n\n            state[ID_WMOM*(nz+2*hs)*(nx+2*hs) + (0)*(nx+2*hs) + i] = mnt_deriv*state[ID_UMOM*(nz+2*hs)*(nx+2*hs) + hs*(nx+2*hs) + i];\n            state[ID_WMOM*(nz+2*hs)*(nx+2*hs) + (1)*(nx+2*hs) + i] = mnt_deriv*state[ID_UMOM*(nz+2*hs)*(nx+2*hs) + hs*(nx+2*hs) + i];\n          }\n        }\n      } else {\n        state[ll*(nz+2*hs)*(nx+2*hs) + (0      )*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (hs     )*(nx+2*hs) + i];\n        state[ll*(nz+2*hs)*(nx+2*hs) + (1      )*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (hs     )*(nx+2*hs) + i];\n        state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs  )*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs-1)*(nx+2*hs) + i];\n        state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs+1)*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs-1)*(nx+2*hs) + i];\n      }\n    }\n  }\n}\n\n\nvoid init( int *argc , char ***argv ) {\n  int    i, k, ii, kk, ll, ierr, inds, i_end;\n  double x, z, r, u, w, t, hr, ht, nper;\n\n  ierr = MPI_Init(argc,argv);\n\n  \n\n  dx = xlen / nx_glob;\n  dz = zlen / nz_glob;\n\n  ierr = MPI_Comm_size(MPI_COMM_WORLD,&nranks);\n  ierr = MPI_Comm_rank(MPI_COMM_WORLD,&myrank);\n  nper = ( (double) nx_glob ) / nranks;\n  i_beg = round( nper* (myrank)    );\n  i_end = round( nper*((myrank)+1) )-1;\n  nx = i_end - i_beg + 1;\n  left_rank  = myrank - 1;\n  if (left_rank == -1) left_rank = nranks-1;\n  right_rank = myrank + 1;\n  if (right_rank == nranks) right_rank = 0;\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  k_beg = 0;\n  nz = nz_glob;\n  masterproc = (myrank == 0);\n\n  \n\n  state              = (double *) malloc( (nx+2*hs)*(nz+2*hs)*NUM_VARS*sizeof(double) );\n  state_tmp          = (double *) malloc( (nx+2*hs)*(nz+2*hs)*NUM_VARS*sizeof(double) );\n  flux               = (double *) malloc( (nx+1)*(nz+1)*NUM_VARS*sizeof(double) );\n  tend               = (double *) malloc( nx*nz*NUM_VARS*sizeof(double) );\n  hy_dens_cell       = (double *) malloc( (nz+2*hs)*sizeof(double) );\n  hy_dens_theta_cell = (double *) malloc( (nz+2*hs)*sizeof(double) );\n  hy_dens_int        = (double *) malloc( (nz+1)*sizeof(double) );\n  hy_dens_theta_int  = (double *) malloc( (nz+1)*sizeof(double) );\n  hy_pressure_int    = (double *) malloc( (nz+1)*sizeof(double) );\n  sendbuf_l          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  sendbuf_r          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  recvbuf_l          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  recvbuf_r          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n\n  \n\n  dt = dmin(dx,dz) / max_speed * cfl;\n  \n\n  etime = 0.;\n  output_counter = 0.;\n\n  \n\n  if (masterproc) {\n    printf( \"nx_glob, nz_glob: %d %d\\n\", nx_glob, nz_glob);\n    printf( \"dx,dz: %lf %lf\\n\",dx,dz);\n    printf( \"dt: %lf\\n\",dt);\n  }\n  \n\n  ierr = MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  \n\n  \n\n  for (k=0; k<nz+2*hs; k++) {\n    for (i=0; i<nx+2*hs; i++) {\n      \n\n      for (ll=0; ll<NUM_VARS; ll++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n        state[inds] = 0.;\n      }\n      \n\n      for (kk=0; kk<nqpoints; kk++) {\n        for (ii=0; ii<nqpoints; ii++) {\n          \n\n          x = (i_beg + i-hs+0.5)*dx + (qpoints[ii]-0.5)*dx;\n          z = (k_beg + k-hs+0.5)*dz + (qpoints[kk]-0.5)*dz;\n\n          \n\n          if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (x,z,r,u,w,t,hr,ht); }\n\n          \n\n          inds = ID_DENS*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + r                         * qweights[ii]*qweights[kk];\n          inds = ID_UMOM*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + (r+hr)*u                  * qweights[ii]*qweights[kk];\n          inds = ID_WMOM*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + (r+hr)*w                  * qweights[ii]*qweights[kk];\n          inds = ID_RHOT*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + ( (r+hr)*(t+ht) - hr*ht ) * qweights[ii]*qweights[kk];\n        }\n      }\n      for (ll=0; ll<NUM_VARS; ll++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n        state_tmp[inds] = state[inds];\n      }\n    }\n  }\n  \n\n  for (k=0; k<nz+2*hs; k++) {\n    hy_dens_cell      [k] = 0.;\n    hy_dens_theta_cell[k] = 0.;\n    for (kk=0; kk<nqpoints; kk++) {\n      z = (k_beg + k-hs+0.5)*dz;\n      \n\n      if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (0.,z,r,u,w,t,hr,ht); }\n      hy_dens_cell      [k] = hy_dens_cell      [k] + hr    * qweights[kk];\n      hy_dens_theta_cell[k] = hy_dens_theta_cell[k] + hr*ht * qweights[kk];\n    }\n  }\n  \n\n  for (k=0; k<nz+1; k++) {\n    z = (k_beg + k)*dz;\n    if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (0.,z,r,u,w,t,hr,ht); }\n    hy_dens_int      [k] = hr;\n    hy_dens_theta_int[k] = hr*ht;\n    hy_pressure_int  [k] = C0*pow((hr*ht),gamm);\n  }\n}\n\n\n\n\n\n\n\n\n\n\nvoid injection( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n}\n\n\n\n\n\n\n\n\n\n\nvoid density_current( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z,-20. ,xlen/2,5000.,4000.,2000.);\n}\n\n\n\n\n\n\n\n\nvoid turbulence( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  \n\n  \n\n  \n\n  \n\n}\n\n\n\n\n\n\n\n\nvoid mountain_waves( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_bvfreq(z,0.02,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 15.;\n  w = 0.;\n}\n\n\n\n\n\n\n\n\n\n\nvoid thermal( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z, 3. ,xlen/2,2000.,2000.,2000.);\n}\n\n\n\n\n\n\n\n\n\n\nvoid collision( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z, 20.,xlen/2,2000.,2000.,2000.);\n  t = t + sample_ellipse_cosine(x,z,-20.,xlen/2,8000.,2000.,2000.);\n}\n\n\n\n\n\n\n\n\nvoid hydro_const_theta( double z , double &r , double &t ) {\n  const double theta0 = 300.;  \n\n  const double exner0 = 1.;    \n\n  double       p,exner,rt;\n  \n\n  t = theta0;                                  \n\n  exner = exner0 - grav * z / (cp * theta0);   \n\n  p = p0 * pow(exner,(cp/rd));                 \n\n  rt = pow((p / C0),(1. / gamm));             \n\n  r = rt / t;                                  \n\n}\n\n\n\n\n\n\n\n\n\n\nvoid hydro_const_bvfreq( double z , double bv_freq0 , double &r , double &t ) {\n  const double theta0 = 300.;  \n\n  const double exner0 = 1.;    \n\n  double       p, exner, rt;\n  t = theta0 * exp( bv_freq0*bv_freq0 / grav * z );                                    \n\n  exner = exner0 - grav*grav / (cp * bv_freq0*bv_freq0) * (t - theta0) / (t * theta0); \n\n  p = p0 * pow(exner,(cp/rd));                                                         \n\n  rt = pow((p / C0),(1. / gamm));                                                  \n\n  r = rt / t;                                                                          \n\n}\n\n\n\n\n\n\n\n\ndouble sample_ellipse_cosine( double x , double z , double amp , double x0 , double z0 , double xrad , double zrad ) {\n  double dist;\n  \n\n  dist = sqrt( ((x-x0)/xrad)*((x-x0)/xrad) + ((z-z0)/zrad)*((z-z0)/zrad) ) * pi / 2.;\n  \n\n  if (dist <= pi / 2.) {\n    return amp * pow(cos(dist),2.);\n  } else {\n    return 0.;\n  }\n}\n\n\n\nvoid finalize() {\n  int ierr;\n  free( state );\n  free( state_tmp );\n  free( flux );\n  free( tend );\n  free( hy_dens_cell );\n  free( hy_dens_theta_cell );\n  free( hy_dens_int );\n  free( hy_dens_theta_int );\n  free( hy_pressure_int );\n  free( sendbuf_l );\n  free( sendbuf_r );\n  free( recvbuf_l );\n  free( recvbuf_r );\n  ierr = MPI_Finalize();\n}\n\n\n\n\nvoid reductions( double &mass , double &te ) {\n  mass = 0;\n  te   = 0;\n\n  {\n    for (int k=0; k<nz; k++) {\n    for (int i=0; i<nx; i++) {\n      int ind_r = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n      int ind_u = ID_UMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n      int ind_w = ID_WMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n      int ind_t = ID_RHOT*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n      double r  =   state[ind_r] + hy_dens_cell[hs+k];             \n\n      double u  =   state[ind_u] / r;                              \n\n      double w  =   state[ind_w] / r;                              \n\n      double th = ( state[ind_t] + hy_dens_theta_cell[hs+k] ) / r; \n\n      double p  = C0*pow(r*th,gamm);                               \n\n      double t  = th / pow(p0/p,rd/cp);                            \n\n      double ke = r*(u*u+w*w);                                     \n\n      double ie = r*cv*t;                                          \n\n      mass += r        *dx*dz; \n\n      te   += (ke + ie)*dx*dz; \n\n    }\n  }\n  }\n  double glob[2], loc[2];\n  loc[0] = mass;\n  loc[1] = te;\n  int ierr = MPI_Allreduce(loc,glob,2,MPI_DOUBLE,MPI_SUM,MPI_COMM_WORLD);\n  mass = glob[0];\n  te   = glob[1];\n}\n\n"}}
{"kernel_name": "miniWeather", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <mpi.h>\n#include <time.h>\n#include <sycl/sycl.hpp>\n\nconst double pi        = 3.14159265358979323846264338327;   \n\nconst double grav      = 9.8;                               \n\nconst double cp        = 1004.;                             \n\nconst double cv        = 717.;                              \n\nconst double rd        = 287.;                              \n\nconst double p0        = 1.e5;                              \n\nconst double C0        = 27.5629410929725921310572974482;   \n\nconst double gamm      = 1.40027894002789400278940027894;   \n\n\n\nconst double xlen      = 2.e4;    \n\nconst double zlen      = 1.e4;    \n\nconst double hv_beta   = 0.25;     \n\nconst double cfl       = 1.50;    \n\nconst double max_speed = 450;        \n\nconst int hs        = 2;          \n\nconst int sten_size = 4;          \n\n\n\n\nconst int NUM_VARS = 4;           \n\nconst int ID_DENS  = 0;           \n\nconst int ID_UMOM  = 1;           \n\nconst int ID_WMOM  = 2;           \n\nconst int ID_RHOT  = 3;           \n\nconst int DIR_X = 1;              \n\nconst int DIR_Z = 2;              \n\nconst int DATA_SPEC_COLLISION       = 1;\nconst int DATA_SPEC_THERMAL         = 2;\nconst int DATA_SPEC_MOUNTAIN        = 3;\nconst int DATA_SPEC_TURBULENCE      = 4;\nconst int DATA_SPEC_DENSITY_CURRENT = 5;\nconst int DATA_SPEC_INJECTION       = 6;\n\nconst int nqpoints = 3;\ndouble qpoints [] = { 0.112701665379258311482073460022E0 , 0.500000000000000000000000000000E0 , 0.887298334620741688517926539980E0 };\ndouble qweights[] = { 0.277777777777777777777777777779E0 , 0.444444444444444444444444444444E0 , 0.277777777777777777777777777779E0 };\n\n\n\n\n\n\n\ndouble sim_time;              \n\ndouble dt;                    \n\nint    nx, nz;                \n\ndouble dx, dz;                \n\nint    nx_glob, nz_glob;      \n\nint    i_beg, k_beg;          \n\nint    nranks, myrank;        \n\nint    left_rank, right_rank; \n\nint    masterproc;            \n\ndouble data_spec_int;         \n\ndouble *hy_dens_cell;         \n\ndouble *hy_dens_theta_cell;   \n\ndouble *hy_dens_int;          \n\ndouble *hy_dens_theta_int;    \n\ndouble *hy_pressure_int;      \n\n\n\n\n\n\n\n\ndouble etime;                 \n\ndouble output_counter;        \n\n\n\ndouble *state;                \n\ndouble *state_tmp;            \n\ndouble *flux;                 \n\ndouble *tend;                 \n\ndouble *sendbuf_l;            \n\ndouble *sendbuf_r;            \n\ndouble *recvbuf_l;            \n\ndouble *recvbuf_r;            \n\nint    num_out = 0;           \n\nint    direction_switch = 1;\ndouble mass0, te0;            \n\ndouble mass , te ;            \n\n\n\n\n\n\n\n\nvoid hydro_const_theta( double z , double &r , double &t ) {\n  const double theta0 = 300.;  \n\n  const double exner0 = 1.;    \n\n  double       p,exner,rt;\n  \n\n  t = theta0;                                  \n\n  exner = exner0 - grav * z / (cp * theta0);   \n\n  p = p0 * pow(exner,(cp/rd));                 \n\n  rt = pow((p / C0),(1. / gamm));             \n\n  r = rt / t;                                  \n\n}\n\n\n\n\n\n\n\n\n\n\nvoid hydro_const_bvfreq( double z , double bv_freq0 , double &r , double &t ) {\n  const double theta0 = 300.;  \n\n  const double exner0 = 1.;    \n\n  double       p, exner, rt;\n  t = theta0 * exp( bv_freq0*bv_freq0 / grav * z );                                    \n\n  exner = exner0 - grav*grav / (cp * bv_freq0*bv_freq0) * (t - theta0) / (t * theta0); \n\n  p = p0 * pow(exner,(cp/rd));                                                         \n\n  rt = pow((p / C0),(1. / gamm));                                                  \n\n  r = rt / t;                                                                          \n\n}\n\n\n\n\n\n\n\n\ndouble sample_ellipse_cosine( double x , double z , double amp , double x0 , double z0 , double xrad , double zrad ) {\n  double dist;\n  \n\n  dist = sqrt( ((x-x0)/xrad)*((x-x0)/xrad) + ((z-z0)/zrad)*((z-z0)/zrad) ) * pi / 2.;\n  \n\n  if (dist <= pi / 2.) {\n    return amp * pow(cos(dist),2.);\n  } else {\n    return 0.;\n  }\n}\n\n\n\n\n\n\n\n\n\n\nvoid injection( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n}\n\n\n\n\n\n\n\n\n\n\nvoid density_current( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z,-20. ,xlen/2,5000.,4000.,2000.);\n}\n\n\n\n\n\n\n\n\nvoid turbulence( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  \n\n  \n\n  \n\n  \n\n}\n\n\n\n\n\n\n\n\nvoid mountain_waves( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_bvfreq(z,0.02,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 15.;\n  w = 0.;\n}\n\n\n\n\n\n\n\n\n\n\nvoid thermal( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z, 3. ,xlen/2,2000.,2000.,2000.);\n}\n\n\n\n\n\n\n\n\n\n\nvoid collision( double x , double z , double &r , double &u , double &w , double &t , double &hr , double &ht ) {\n  hydro_const_theta(z,hr,ht);\n  r = 0.;\n  t = 0.;\n  u = 0.;\n  w = 0.;\n  t = t + sample_ellipse_cosine(x,z, 20.,xlen/2,2000.,2000.,2000.);\n  t = t + sample_ellipse_cosine(x,z,-20.,xlen/2,8000.,2000.,2000.);\n}\n\n\n\n\n\n\n\n\n\n\nvoid compute_tendencies_x(\n    const int hs,\n    const int nx,\n    const int nz,\n    const double dx,\n    double *d_state,\n    double *d_flux,\n    double *d_tend,\n    double *d_hy_dens_cell,\n    double *d_hy_dens_theta_cell,\n    sycl::queue &q )\n{\n  sycl::range<3> flux_gws (1, (nz+15)/16*16, (nx+16)/16*16);\n  sycl::range<3> flux_lws (1, 16, 16);\n\n  \n\n  double hv_coef = -hv_beta * dx / (16*dt);\n  \n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class compute_flux_x>(\n      sycl::nd_range<3>(flux_gws, flux_lws), [=] (sycl::nd_item<3> item) {\n      int k = item.get_global_id(1);\n      int i = item.get_global_id(2);\n      double stencil[4], d3_vals[NUM_VARS], vals[NUM_VARS];\n\n      if (i < nx+1 && k < nz) {\n        \n\n        for (int ll=0; ll<NUM_VARS; ll++) {\n          for (int s=0; s < sten_size; s++) {\n            int inds = ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+s;\n            stencil[s] = d_state[inds];\n          }\n          \n\n          vals[ll] = -stencil[0]/12 + 7*stencil[1]/12 + 7*stencil[2]/12 - stencil[3]/12;\n          \n\n          d3_vals[ll] = -stencil[0] + 3*stencil[1] - 3*stencil[2] + stencil[3];\n        }\n\n        \n\n        double r = vals[ID_DENS] + d_hy_dens_cell[k+hs];\n        double u = vals[ID_UMOM] / r;\n        double w = vals[ID_WMOM] / r;\n        double t = ( vals[ID_RHOT] + d_hy_dens_theta_cell[k+hs] ) / r;\n        double p = C0*sycl::pow((r*t),gamm);\n\n        \n\n        d_flux[ID_DENS*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u     - hv_coef*d3_vals[ID_DENS];\n        d_flux[ID_UMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*u+p - hv_coef*d3_vals[ID_UMOM];\n        d_flux[ID_WMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*w   - hv_coef*d3_vals[ID_WMOM];\n        d_flux[ID_RHOT*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*t   - hv_coef*d3_vals[ID_RHOT];\n      }\n    });\n  });\n\n  \n\n  sycl::range<3> tend_gws (NUM_VARS, (nz+15)/16*16, (nx+15)/16*16);\n  sycl::range<3> tend_lws (1, 16, 16);\n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class compute_tend_x>(\n      sycl::nd_range<3>(tend_gws, tend_lws), [=] (sycl::nd_item<3> item) {\n      int ll = item.get_global_id(0);\n      int k = item.get_global_id(1);\n      int i = item.get_global_id(2);\n      if (i < nx && k < nz) {\n        int indt  = ll* nz   * nx    + k* nx    + i  ;\n        int indf1 = ll*(nz+1)*(nx+1) + k*(nx+1) + i  ;\n        int indf2 = ll*(nz+1)*(nx+1) + k*(nx+1) + i+1;\n        d_tend[indt] = -( d_flux[indf2] - d_flux[indf1] ) / dx;\n      }\n    });\n  });\n}\n\n\n\n\n\n\n\n\n\n\nvoid compute_tendencies_z(\n    const int hs,\n    const int nx,\n    const int nz,\n    const double dz,\n    double *d_state,\n    double *d_flux,\n    double *d_tend,\n    double *d_hy_dens_int,\n    double *d_hy_dens_theta_int,\n    double *d_hy_pressure_int,\n    sycl::queue &q )\n{\n  \n\n  double hv_coef = -hv_beta * dz / (16*dt);\n\n  \n\n\n  sycl::range<3> flux_gws (1, (nz+16)/16*16, (nx+15)/16*16);\n  sycl::range<3> flux_lws (1, 16, 16);\n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class compute_flux_z>(\n      sycl::nd_range<3>(flux_gws, flux_lws), [=] (sycl::nd_item<3> item) {\n      int k = item.get_global_id(1);\n      int i = item.get_global_id(2);\n      double stencil[4], d3_vals[NUM_VARS], vals[NUM_VARS];\n\n      if (i < nx && k < nz+1) {\n        \n\n        for (int ll=0; ll<NUM_VARS; ll++) {\n          for (int s=0; s<sten_size; s++) {\n            int inds = ll*(nz+2*hs)*(nx+2*hs) + (k+s)*(nx+2*hs) + i+hs;\n            stencil[s] = d_state[inds];\n          }\n          \n\n          vals[ll] = -stencil[0]/12 + 7*stencil[1]/12 + 7*stencil[2]/12 - stencil[3]/12;\n          \n\n          d3_vals[ll] = -stencil[0] + 3*stencil[1] - 3*stencil[2] + stencil[3];\n        }\n\n        \n\n        double r = vals[ID_DENS] + d_hy_dens_int[k];\n        double u = vals[ID_UMOM] / r;\n        double w = vals[ID_WMOM] / r;\n        double t = ( vals[ID_RHOT] + d_hy_dens_theta_int[k] ) / r;\n        double p = C0*sycl::pow((r*t),gamm) - d_hy_pressure_int[k];\n        \n\n        if (k == 0 || k == nz) {\n          w                = 0;\n          d3_vals[ID_DENS] = 0;\n        }\n\n        \n\n        d_flux[ID_DENS*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w     - hv_coef*d3_vals[ID_DENS];\n        d_flux[ID_UMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*u   - hv_coef*d3_vals[ID_UMOM];\n        d_flux[ID_WMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*w+p - hv_coef*d3_vals[ID_WMOM];\n        d_flux[ID_RHOT*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*t   - hv_coef*d3_vals[ID_RHOT];\n      }\n    });\n  });\n\n  \n\n  sycl::range<3> tend_gws (NUM_VARS, (nz+15)/16*16, (nx+15)/16*16);\n  sycl::range<3> tend_lws (1, 16, 16);\n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class compute_tend_z>(\n      sycl::nd_range<3>(tend_gws, tend_lws), [=] (sycl::nd_item<3> item) {\n      int ll = item.get_global_id(0);\n      int k = item.get_global_id(1);\n      int i = item.get_global_id(2);\n      if (i < nx && k < nz) {\n        int indt  = ll* nz   * nx    + k* nx    + i  ;\n        int indf1 = ll*(nz+1)*(nx+1) + (k  )*(nx+1) + i;\n        int indf2 = ll*(nz+1)*(nx+1) + (k+1)*(nx+1) + i;\n        d_tend[indt] = -( d_flux[indf2] - d_flux[indf1] ) / dz;\n        if (ll == ID_WMOM) {\n          int inds = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n          d_tend[indt] = d_tend[indt] - d_state[inds]*grav;\n        }\n      }\n    });\n  });\n}\n\n\n\n\nvoid set_halo_values_x(\n    const int hs,\n    const int nx,\n    const int nz,\n    const int k_beg,\n    const double dz,\n    double *d_state,\n    double *d_hy_dens_cell,\n    double *d_hy_dens_theta_cell,\n    double *d_sendbuf_l,\n    double *d_sendbuf_r,\n    double *d_recvbuf_l,\n    double *d_recvbuf_r,\n    sycl::queue &q )\n{\n  int ierr;\n  MPI_Request req_r[2], req_s[2];\n\n  \n\n  ierr = MPI_Irecv(recvbuf_l,hs*nz*NUM_VARS,MPI_DOUBLE, left_rank,0,MPI_COMM_WORLD,&req_r[0]);\n  ierr = MPI_Irecv(recvbuf_r,hs*nz*NUM_VARS,MPI_DOUBLE,right_rank,1,MPI_COMM_WORLD,&req_r[1]);\n\n  \n\n  sycl::range<3> buffer_gws (NUM_VARS, (nz+15)/16*16, (hs+15)/16*16);\n  sycl::range<3> buffer_lws (1, 16, 16);\n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class pack_send_buf>(\n      sycl::nd_range<3>(buffer_gws, buffer_lws), [=] (sycl::nd_item<3> item) {\n      int ll = item.get_global_id(0);\n      int k = item.get_global_id(1);\n      int s = item.get_global_id(2);\n      if (s < hs && k < nz) {\n        d_sendbuf_l[ll*nz*hs + k*hs + s] = d_state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + hs+s];\n        d_sendbuf_r[ll*nz*hs + k*hs + s] = d_state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + nx+s];\n      }\n    });\n  });\n\n  q.memcpy(sendbuf_l, d_sendbuf_l, sizeof(double)*hs*nz*NUM_VARS);\n  q.memcpy(sendbuf_r, d_sendbuf_r, sizeof(double)*hs*nz*NUM_VARS);\n  q.wait();\n\n  \n\n\n  \n\n  ierr = MPI_Isend(sendbuf_l,hs*nz*NUM_VARS,MPI_DOUBLE, left_rank,1,MPI_COMM_WORLD,&req_s[0]);\n  ierr = MPI_Isend(sendbuf_r,hs*nz*NUM_VARS,MPI_DOUBLE,right_rank,0,MPI_COMM_WORLD,&req_s[1]);\n\n  \n\n  ierr = MPI_Waitall(2,req_r,MPI_STATUSES_IGNORE);\n\n  q.memcpy(d_recvbuf_l, recvbuf_l, sizeof(double)*hs*nz*NUM_VARS);\n  q.memcpy(d_recvbuf_r, recvbuf_r, sizeof(double)*hs*nz*NUM_VARS);\n\n  \n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class unpack_recv_buf>(\n      sycl::nd_range<3>(buffer_gws, buffer_lws), [=] (sycl::nd_item<3> item) {\n      int ll = item.get_global_id(0);\n      int k = item.get_global_id(1);\n      int s = item.get_global_id(2);\n      if (s < hs && k < nz) {\n        d_state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + s      ] = d_recvbuf_l[ll*nz*hs + k*hs + s];\n        d_state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + nx+hs+s] = d_recvbuf_r[ll*nz*hs + k*hs + s];\n      }\n    });\n  });\n\n  \n\n  ierr = MPI_Waitall(2,req_s,MPI_STATUSES_IGNORE);\n\n  if (data_spec_int == DATA_SPEC_INJECTION) {\n    if (myrank == 0) {\n      sycl::range<3> inj_gws (1, (nz+15)/16*16, (hs+15)/16*16);\n      sycl::range<3> inj_lws (1, 16, 16);\n\n      q.submit([&] (sycl::handler &cgh) {\n        cgh.parallel_for<class update_state_x>(\n          sycl::nd_range<3>(inj_gws, inj_lws), [=] (sycl::nd_item<3> item) {\n          int k = item.get_global_id(1);\n          int i = item.get_global_id(2);\n          if (i < hs && k < nz) {\n            double z = (k_beg + k+0.5)*dz;\n            if (sycl::fabs(z-3*zlen/4) <= zlen/16) {\n              int ind_r = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n              int ind_u = ID_UMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n              int ind_t = ID_RHOT*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n              d_state[ind_u] = (d_state[ind_r]+d_hy_dens_cell[k+hs]) * 50.;\n              d_state[ind_t] = (d_state[ind_r]+d_hy_dens_cell[k+hs]) * 298. - d_hy_dens_theta_cell[k+hs];\n            }\n          }\n        });\n      });\n    }\n  }\n}\n\n\n\n\n\n\nvoid set_halo_values_z(\n    const int hs, const int nx, const int nz,\n    const int i_beg,\n    const double dx,\n    const int data_spec_int,\n    double *d_state,\n    sycl::queue &q )\n{\n  const double mnt_width = xlen/8;\n\n  sycl::range<3> gws (1, (NUM_VARS+15)/16*16, (nx+2*hs+15)/16*16);\n  sycl::range<3> lws (1, 16, 16);\n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class update_state_z>(\n      sycl::nd_range<3>(gws, lws), [=] (sycl::nd_item<3> item) {\n      int ll = item.get_global_id(1);\n      int i = item.get_global_id(2);\n      if (i < nx+2*hs && ll < NUM_VARS) {\n        if (ll == ID_WMOM) {\n          d_state[ll*(nz+2*hs)*(nx+2*hs) + (0      )*(nx+2*hs) + i] = 0.;\n          d_state[ll*(nz+2*hs)*(nx+2*hs) + (1      )*(nx+2*hs) + i] = 0.;\n          d_state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs  )*(nx+2*hs) + i] = 0.;\n          d_state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs+1)*(nx+2*hs) + i] = 0.;\n          \n\n          if (data_spec_int == DATA_SPEC_MOUNTAIN) {\n            double x = (i_beg+i-hs+0.5)*dx;\n            if ( sycl::fabs(x-xlen/4) < mnt_width ) {\n              double xloc = (x-(xlen/4)) / mnt_width;\n              \n\n              double mnt_deriv = -pi*sycl::cos(pi*xloc/2)*sycl::sin(pi*xloc/2)*10/dx;\n              \n\n              d_state[ID_WMOM*(nz+2*hs)*(nx+2*hs) + (0)*(nx+2*hs) + i] = mnt_deriv*d_state[ID_UMOM*(nz+2*hs)*(nx+2*hs) + hs*(nx+2*hs) + i];\n              d_state[ID_WMOM*(nz+2*hs)*(nx+2*hs) + (1)*(nx+2*hs) + i] = mnt_deriv*d_state[ID_UMOM*(nz+2*hs)*(nx+2*hs) + hs*(nx+2*hs) + i];\n            }\n          }\n        } else {\n          d_state[ll*(nz+2*hs)*(nx+2*hs) + (0      )*(nx+2*hs) + i] = d_state[ll*(nz+2*hs)*(nx+2*hs) + (hs     )*(nx+2*hs) + i];\n          d_state[ll*(nz+2*hs)*(nx+2*hs) + (1      )*(nx+2*hs) + i] = d_state[ll*(nz+2*hs)*(nx+2*hs) + (hs     )*(nx+2*hs) + i];\n          d_state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs  )*(nx+2*hs) + i] = d_state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs-1)*(nx+2*hs) + i];\n          d_state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs+1)*(nx+2*hs) + i] = d_state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs-1)*(nx+2*hs) + i];\n        }\n      }\n    });\n  });\n}\n\n\nvoid init( int *argc , char ***argv ) {\n  int    i, k, ii, kk, ll, ierr, inds, i_end;\n  double x, z, r, u, w, t, hr, ht, nper;\n\n  ierr = MPI_Init(argc,argv);\n\n  \n\n  dx = xlen / nx_glob;\n  dz = zlen / nz_glob;\n\n  ierr = MPI_Comm_size(MPI_COMM_WORLD,&nranks);\n  ierr = MPI_Comm_rank(MPI_COMM_WORLD,&myrank);\n  nper = ( (double) nx_glob ) / nranks;\n  i_beg = round( nper* (myrank)    );\n  i_end = round( nper*((myrank)+1) )-1;\n  nx = i_end - i_beg + 1;\n  left_rank  = myrank - 1;\n  if (left_rank == -1) left_rank = nranks-1;\n  right_rank = myrank + 1;\n  if (right_rank == nranks) right_rank = 0;\n\n\n  \n\n  \n\n  \n\n\n  \n\n  k_beg = 0;\n  nz = nz_glob;\n  masterproc = (myrank == 0);\n\n  \n\n  state              = (double *) malloc( (nx+2*hs)*(nz+2*hs)*NUM_VARS*sizeof(double) );\n  state_tmp          = (double *) malloc( (nx+2*hs)*(nz+2*hs)*NUM_VARS*sizeof(double) );\n  flux               = (double *) malloc( (nx+1)*(nz+1)*NUM_VARS*sizeof(double) );\n  tend               = (double *) malloc( nx*nz*NUM_VARS*sizeof(double) );\n  hy_dens_cell       = (double *) malloc( (nz+2*hs)*sizeof(double) );\n  hy_dens_theta_cell = (double *) malloc( (nz+2*hs)*sizeof(double) );\n  hy_dens_int        = (double *) malloc( (nz+1)*sizeof(double) );\n  hy_dens_theta_int  = (double *) malloc( (nz+1)*sizeof(double) );\n  hy_pressure_int    = (double *) malloc( (nz+1)*sizeof(double) );\n  sendbuf_l          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  sendbuf_r          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  recvbuf_l          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n  recvbuf_r          = (double *) malloc( hs*nz*NUM_VARS*sizeof(double) );\n\n  \n\n  dt = fmin(dx,dz) / max_speed * cfl;\n  \n\n  etime = 0.;\n  output_counter = 0.;\n\n  \n\n  if (masterproc) {\n    printf( \"nx_glob, nz_glob: %d %d\\n\", nx_glob, nz_glob);\n    printf( \"dx,dz: %lf %lf\\n\",dx,dz);\n    printf( \"dt: %lf\\n\",dt);\n  }\n  \n\n  ierr = MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  \n\n  \n\n  for (k=0; k<nz+2*hs; k++) {\n    for (i=0; i<nx+2*hs; i++) {\n      \n\n      for (ll=0; ll<NUM_VARS; ll++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n        state[inds] = 0.;\n      }\n      \n\n      for (kk=0; kk<nqpoints; kk++) {\n        for (ii=0; ii<nqpoints; ii++) {\n          \n\n          x = (i_beg + i-hs+0.5)*dx + (qpoints[ii]-0.5)*dx;\n          z = (k_beg + k-hs+0.5)*dz + (qpoints[kk]-0.5)*dz;\n\n          \n\n          if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(x,z,r,u,w,t,hr,ht); }\n          if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (x,z,r,u,w,t,hr,ht); }\n\n          \n\n          inds = ID_DENS*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + r                         * qweights[ii]*qweights[kk];\n          inds = ID_UMOM*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + (r+hr)*u                  * qweights[ii]*qweights[kk];\n          inds = ID_WMOM*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + (r+hr)*w                  * qweights[ii]*qweights[kk];\n          inds = ID_RHOT*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n          state[inds] = state[inds] + ( (r+hr)*(t+ht) - hr*ht ) * qweights[ii]*qweights[kk];\n        }\n      }\n      for (ll=0; ll<NUM_VARS; ll++) {\n        inds = ll*(nz+2*hs)*(nx+2*hs) + k*(nx+2*hs) + i;\n        state_tmp[inds] = state[inds];\n      }\n    }\n  }\n  \n\n  for (k=0; k<nz+2*hs; k++) {\n    hy_dens_cell      [k] = 0.;\n    hy_dens_theta_cell[k] = 0.;\n    for (kk=0; kk<nqpoints; kk++) {\n      z = (k_beg + k-hs+0.5)*dz;\n      \n\n      if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(0.,z,r,u,w,t,hr,ht); }\n      if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (0.,z,r,u,w,t,hr,ht); }\n      hy_dens_cell      [k] = hy_dens_cell      [k] + hr    * qweights[kk];\n      hy_dens_theta_cell[k] = hy_dens_theta_cell[k] + hr*ht * qweights[kk];\n    }\n  }\n  \n\n  for (k=0; k<nz+1; k++) {\n    z = (k_beg + k)*dz;\n    if (data_spec_int == DATA_SPEC_COLLISION      ) { collision      (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_THERMAL        ) { thermal        (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_MOUNTAIN       ) { mountain_waves (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_TURBULENCE     ) { turbulence     (0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_DENSITY_CURRENT) { density_current(0.,z,r,u,w,t,hr,ht); }\n    if (data_spec_int == DATA_SPEC_INJECTION      ) { injection      (0.,z,r,u,w,t,hr,ht); }\n    hy_dens_int      [k] = hr;\n    hy_dens_theta_int[k] = hr*ht;\n    hy_pressure_int  [k] = C0*pow((hr*ht),gamm);\n  }\n}\n\n\n\nvoid finalize() {\n  int ierr;\n  free( state );\n  free( state_tmp );\n  free( flux );\n  free( tend );\n  free( hy_dens_cell );\n  free( hy_dens_theta_cell );\n  free( hy_dens_int );\n  free( hy_dens_theta_int );\n  free( hy_pressure_int );\n  free( sendbuf_l );\n  free( sendbuf_r );\n  free( recvbuf_l );\n  free( recvbuf_r );\n  ierr = MPI_Finalize();\n}\n\n\n\n\n\n\n\nstatic inline void atomicAdd(double& val, const double delta)\n{\n  sycl::atomic_ref<double, sycl::memory_order::relaxed,\n                   sycl::memory_scope::device,\n                   sycl::access::address_space::global_space> ref(val);\n  ref.fetch_add(delta);\n}\n\nvoid reductions(\n    double &mass,\n    double &te,\n    const int hs,\n    const int nx,\n    const int nz,\n    const double dx,\n    const double dz,\n    const double *d_state,\n    const double *d_hy_dens_cell,\n    const double *d_hy_dens_theta_cell,\n    sycl::queue &q )\n{\n  double* d_mass, *d_te;\n  d_mass = sycl::malloc_device<double>(1, q);\n  d_te = sycl::malloc_device<double>(1, q);\n\n  sycl::range<3> gws (1, (nz+15)/16*16, (nx+15)/16*16);\n  sycl::range<3> lws (1, 16, 16);\n\n  q.memset(d_mass, 0, sizeof(double));\n  q.memset(d_te, 0, sizeof(double));\n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class reduce>(\n      sycl::nd_range<3>(gws, lws), [=] (sycl::nd_item<3> item) {\n      int k = item.get_global_id(1);\n      int i = item.get_global_id(2);\n      if (k < nz && i < nx) {\n        int ind_r = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n        int ind_u = ID_UMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n        int ind_w = ID_WMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n        int ind_t = ID_RHOT*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n        double r  =  d_state[ind_r] + d_hy_dens_cell[hs+k];           \n\n        double u  =  d_state[ind_u] / r;                              \n\n        double w  =  d_state[ind_w] / r;                              \n\n        double th = ( d_state[ind_t] + d_hy_dens_theta_cell[hs+k] ) / r; \n\n        double p  = C0*sycl::pow(r*th,gamm);                               \n\n        double t  = th / sycl::pow(p0/p,rd/cp);                            \n\n        double ke = r*(u*u+w*w);                                     \n\n        double ie = r*cv*t;                                          \n\n\n        \n\n        \n\n        atomicAdd(d_mass[0], r*dx*dz);\n        atomicAdd(d_te[0], (ke+ie)*dx*dz);\n      }\n    });\n  });\n\n  double glob[2], loc[2];\n\n  q.memcpy(loc, d_mass, sizeof(double));\n  q.memcpy(loc+1, d_te, sizeof(double));\n  q.wait();\n\n  int ierr = MPI_Allreduce(loc,glob,2,MPI_DOUBLE,MPI_SUM,MPI_COMM_WORLD);\n\n  free(d_mass, q);\n  free(d_te, q);\n\n  mass = glob[0];\n  te   = glob[1];\n}\n\n\n\n\n\n\n\nvoid semi_discrete_step(\n    const int hs,\n    const int nx,\n    const int nz,\n    const int k_beg,\n    const int i_beg,\n    const double dx ,\n    const double dz ,\n    const double dt ,\n    int dir ,\n    const int data_spec_int,\n    double *d_state_init ,\n    double *d_state_forcing ,\n    double *d_state_out,\n    double *d_flux ,\n    double *d_tend,\n    double *d_hy_dens_cell ,\n    double *d_hy_dens_theta_cell ,\n    double *d_hy_dens_int ,\n    double *d_hy_dens_theta_int ,\n    double *d_hy_pressure_int ,\n    double *d_sendbuf_l ,\n    double *d_sendbuf_r ,\n    double *d_recvbuf_l ,\n    double *d_recvbuf_r ,\n    sycl::queue &q  )\n{\n  if (dir == DIR_X) {\n    \n\n    set_halo_values_x(\n        hs,\n        nx,\n        nz,\n        k_beg,\n        dz,\n        d_state_forcing,\n        d_hy_dens_cell ,\n        d_hy_dens_theta_cell ,\n        d_sendbuf_l,\n        d_sendbuf_r,\n        d_recvbuf_l,\n        d_recvbuf_r,\n        q);\n    \n\n    compute_tendencies_x(hs, nx, nz, dx, d_state_forcing, d_flux, d_tend,\n                         d_hy_dens_cell, d_hy_dens_theta_cell, q);\n  } else if (dir == DIR_Z) {\n    \n\n    set_halo_values_z(hs, nx, nz, i_beg, dx, data_spec_int, d_state_forcing, q);\n    \n\n    compute_tendencies_z(hs, nx, nz, dz, d_state_forcing, d_flux, d_tend,\n                         d_hy_dens_int,d_hy_dens_theta_int,d_hy_pressure_int,q);\n  }\n\n  \n\n  sycl::range<3> tend_gws (NUM_VARS, (nz+15)/16*16, (nx+15)/16*16);\n  sycl::range<3> tend_lws (1, 16, 16);\n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class update_fluid_state>(\n      sycl::nd_range<3>(tend_gws, tend_lws), [=] (sycl::nd_item<3> item) {\n      int ll = item.get_global_id(0);\n      int k = item.get_global_id(1);\n      int i = item.get_global_id(2);\n      if (i < nx && k < nz) {\n        int inds = ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n        int indt = ll*nz*nx + k*nx + i;\n        d_state_out[inds] = d_state_init[inds] + dt * d_tend[indt];\n      }\n    });\n  });\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvoid perform_timestep(\n    double *d_state ,\n    double *d_state_tmp ,\n    double *d_flux ,\n    double *d_tend ,\n    double *d_hy_dens_cell ,\n    double *d_hy_dens_theta_cell ,\n    double *d_hy_dens_int ,\n    double *d_hy_dens_theta_int ,\n    double *d_hy_pressure_int ,\n    double *d_sendbuf_l ,\n    double *d_sendbuf_r ,\n    double *d_recvbuf_l ,\n    double *d_recvbuf_r ,\n    const double dt,\n    sycl::queue &q ) {\n\n\n\n#define SEMI_DSTEP(dt, dir, state, next_state) \\\n    semi_discrete_step(hs, nx, nz, k_beg, i_beg, dx, dz, dt, dir ,\\\n\t\t    data_spec_int, d_state, state, \\\n\t\t    next_state, d_flux, d_tend, \\\n\t\t    d_hy_dens_cell, d_hy_dens_theta_cell,\\\n\t\t    d_hy_dens_int, d_hy_dens_theta_int,\\\n\t\t    d_hy_pressure_int, d_sendbuf_l,\\\n\t\t    d_sendbuf_r, d_recvbuf_l, d_recvbuf_r, q);\n\n  if (direction_switch) {\n    SEMI_DSTEP(dt/3, DIR_X, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_X, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_X, d_state_tmp, d_state)\n    SEMI_DSTEP(dt/3, DIR_Z, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_Z, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_Z, d_state_tmp, d_state)\n  } else {\n    SEMI_DSTEP(dt/3, DIR_Z, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_Z, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_Z, d_state_tmp, d_state)\n    SEMI_DSTEP(dt/3, DIR_X, d_state, d_state_tmp)\n    SEMI_DSTEP(dt/2, DIR_X, d_state_tmp, d_state_tmp)\n    SEMI_DSTEP(dt/1, DIR_X, d_state_tmp, d_state)\n  }\n\n  if (direction_switch) { direction_switch = 0; } else { direction_switch = 1; }\n}\n\n\n\n\n\n\n\nint main(int argc, char **argv) {\n  \n\n  \n\n  \n\n  \n\n  \n\n  nx_glob = NX;               \n\n  nz_glob = NZ;               \n\n  sim_time = SIM_TIME;        \n\n  data_spec_int = DATA_SPEC;  \n\n  \n\n  \n\n  \n\n\n  init( &argc , &argv );\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  const int state_size = (nz+2*hs)*(nx+2*hs)*NUM_VARS;\n  const int state_size_byte = (nz+2*hs)*(nx+2*hs)*NUM_VARS*sizeof(double);\n  double *d_state_tmp = sycl::malloc_device<double>(state_size, q);\n  q.memcpy(d_state_tmp, state, state_size_byte);\n\n  double *d_state = sycl::malloc_device<double>(state_size, q);\n  q.memcpy(d_state, state, state_size_byte);\n\n  double *d_hy_dens_cell = sycl::malloc_device<double>(nz+2*hs, q);\n  q.memcpy(d_hy_dens_cell, hy_dens_cell, (nz+2*hs)*sizeof(double));\n\n  double *d_hy_dens_theta_cell = sycl::malloc_device<double>(nz+2*hs, q);\n  q.memcpy(d_hy_dens_theta_cell, hy_dens_theta_cell, (nz+2*hs)*sizeof(double));\n\n  double *d_hy_dens_int = sycl::malloc_device<double>(nz+1, q);\n  q.memcpy(d_hy_dens_int, hy_dens_int, (nz+1)*sizeof(double));\n\n  double *d_hy_dens_theta_int = sycl::malloc_device<double>(nz+1, q);\n  q.memcpy(d_hy_dens_theta_int, hy_dens_theta_int, (nz+1)*sizeof(double));\n\n  double *d_hy_pressure_int = sycl::malloc_device<double>(nz+1, q);\n  q.memcpy(d_hy_pressure_int, hy_pressure_int, (nz+1)*sizeof(double));\n\n  double *d_flux = sycl::malloc_device<double>((nz+1)*(nx+1)*NUM_VARS, q);\n\n  double *d_tend = sycl::malloc_device<double>(nz*nx*NUM_VARS, q);\n\n  double *d_sendbuf_l = sycl::malloc_device<double>(hs*nz*NUM_VARS, q);\n  double *d_sendbuf_r = sycl::malloc_device<double>(hs*nz*NUM_VARS, q);\n  double *d_recvbuf_l = sycl::malloc_device<double>(hs*nz*NUM_VARS, q);\n  double *d_recvbuf_r = sycl::malloc_device<double>(hs*nz*NUM_VARS, q);\n\n  \n\n  reductions(mass0, te0, hs, nx, nz, dx, dz, d_state, d_hy_dens_cell, d_hy_dens_theta_cell, q);\n\n  \n\n  \n\n  \n\n  auto c_start = clock();\n\n  while (etime < sim_time) {\n    \n\n    if (etime + dt > sim_time) { dt = sim_time - etime; }\n    \n\n    perform_timestep(\n        d_state,\n        d_state_tmp,\n        d_flux,\n        d_tend,\n        d_hy_dens_cell,\n        d_hy_dens_theta_cell,\n        d_hy_dens_int,\n        d_hy_dens_theta_int,\n        d_hy_pressure_int,\n        d_sendbuf_l,\n        d_sendbuf_r,\n        d_recvbuf_l,\n        d_recvbuf_r,\n        dt,\n        q);\n\n    \n\n    etime = etime + dt;\n  }\n\n  auto c_end = clock();\n  if (masterproc)\n    printf(\"Total main time step loop: %lf sec\\n\", ( (double) (c_end-c_start) ) / CLOCKS_PER_SEC);\n\n  \n\n  reductions(mass, te, hs, nx, nz, dx, dz, d_state, d_hy_dens_cell, d_hy_dens_theta_cell, q);\n\n  printf( \"d_mass: %le\\n\" , (mass - mass0) / mass0 );\n  printf( \"d_te:   %le\\n\" , (te   - te0  ) / te0   );\n\n  finalize();\n\n  sycl::free(d_state, q);\n  sycl::free(d_state_tmp, q);\n  sycl::free(d_flux, q);\n  sycl::free(d_tend, q);\n  sycl::free(d_hy_dens_cell, q);\n  sycl::free(d_hy_dens_theta_cell, q);\n  sycl::free(d_hy_dens_int, q);\n  sycl::free(d_hy_dens_theta_int, q);\n  sycl::free(d_hy_pressure_int, q);\n  sycl::free(d_sendbuf_l, q);\n  sycl::free(d_sendbuf_r, q);\n  sycl::free(d_recvbuf_l, q);\n  sycl::free(d_recvbuf_r, q);\n\n  return 0;\n}\n"}}
{"kernel_name": "particle-diffusion", "parallel_api": "cuda", "code": {"motionsim.cu": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <cmath>\n#include <ctime>\n#include <iomanip>\n#include <iostream>\n#include <chrono>\n#include <cuda.h>\n\n\n\n\n\n\nvoid usage(std::string programName) {\n  std::cout << \" Incorrect number of parameters \" << std::endl;\n  std::cout << \" Usage: \";\n  std::cout << programName << \" <Number of iterations within the kernel> \";\n  std::cout << \"<Kernel execution count>\\n\\n\";\n}\n\n\n\ntemplate <typename T>\nvoid print_matrix(T** matrix, size_t size_X, size_t size_Y) {\n  std::cout << std::endl;\n  for (size_t i = 0; i < size_X; ++i) {\n    for (size_t j = 0; j < size_Y; ++j) {\n      std::cout << std::setw(3) << matrix[i][j] << \" \";\n    }\n    std::cout << std::endl;\n  }\n}\n\n\n\ntemplate <typename T>\nvoid print_vector(T* vector, size_t n) {\n  std::cout << std::endl;\n  for (size_t i = 0; i < n; ++i) {\n    std::cout << vector[i] << \" \";\n  }\n  std::cout << std::endl;\n}\n\n__global__\nvoid Simulation(float*__restrict__ a_particleX,\n                float*__restrict__ a_particleY,\n\t\tconst float*__restrict__ a_randomX,\n                const float*__restrict__ a_randomY, \n\t\tsize_t *__restrict__ a_map,\n                const size_t n_particles,\n                unsigned int nIterations,\n                int grid_size,\n                float radius)\n{\n  size_t ii = blockDim.x * blockIdx.x + threadIdx.x;\n  if (ii >= n_particles) return;\n  \n\n  \n\n  \n\n  \n\n  \n\n  size_t iter = 0;\n  float pX = a_particleX[ii];\n  float pY = a_particleY[ii];\n  size_t map_base = ii * grid_size * grid_size;\n  while (iter < nIterations) {\n    \n\n    \n\n    \n\n    \n\n\n    float randnumX = a_randomX[iter * n_particles + ii];\n    float randnumY = a_randomY[iter * n_particles + ii];\n\n    \n\n    float displacementX = randnumX / 1000.0f - 0.0495f;\n    float displacementY = randnumY / 1000.0f - 0.0495f;\n\n    \n\n    pX += displacementX;\n    pY += displacementY;\n\n    \n\n    float dX = pX - trunc(pX);\n    float dY = pY - trunc(pY);\n\n    \n\n    int iX = floor(pX);\n    int iY = floor(pY);\n\n    \n\n    if ((pX < grid_size) && (pY < grid_size) && (pX >= 0) && (pY >= 0)) {\n      \n\n      \n\n      if ((dX * dX + dY * dY <= radius * radius))\n        \n\n        a_map[map_base + iY * grid_size + iX]++;\n    }\n\n    iter++;\n\n  }  \n\n\n  a_particleX[ii] = pX;\n  a_particleY[ii] = pY;\n}\n\n\n\nvoid motion_device(float* particleX, float* particleY,\n                   float* randomX, float* randomY, int** grid, size_t grid_size,\n                   size_t n_particles, int nIterations, float radius,\n                   size_t* map, int nRepeat) {\n\n  cudaDeviceProp devProp;\n  cudaGetDeviceProperties(&devProp, 0);\n\n  std::cout << \" Running on \" << devProp.name << std::endl;\n  std::cout << \" The device max work-group size is \" << devProp.maxThreadsPerBlock << std::endl;\n  std::cout << \" The number of iterations is \" << nIterations << std::endl;\n  std::cout << \" The number of kernel execution is \" << nRepeat << std::endl;\n  std::cout << \" The number of particles is \" << n_particles << std::endl;\n\n  \n\n  \n\n  srand(17);\n  \n  \n\n  const size_t scale = 100;\n\n  \n\n  for (size_t i = 0; i < n_particles * nIterations; i++) {\n    randomX[i] = rand() % scale;\n    randomY[i] = rand() % scale;\n  }\n\n  float *d_randomX;\n  float *d_randomY;\n  float *d_particleX;\n  float *d_particleY;\n  size_t *d_map;\n  cudaMalloc((void**)&d_randomX, sizeof(float) * n_particles * nIterations);\n  cudaMalloc((void**)&d_randomY, sizeof(float) * n_particles * nIterations);\n  cudaMalloc((void**)&d_particleX, sizeof(float) * n_particles);\n  cudaMalloc((void**)&d_particleY, sizeof(float) * n_particles);\n  size_t MAP_SIZE = n_particles * grid_size * grid_size;\n  cudaMalloc((void**)&d_map, sizeof(size_t) * MAP_SIZE);\n\n  cudaMemcpy(d_randomX, randomX, sizeof(float) * n_particles * nIterations, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_randomY, randomY, sizeof(float) * n_particles * nIterations, cudaMemcpyHostToDevice);\n\n  double time_total = 0.0;\n\n  for (int i = 0; i < nRepeat; i++) {\n    \n\n    cudaMemcpy(d_particleX, particleX, sizeof(float) * n_particles, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_particleY, particleY, sizeof(float) * n_particles, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_map, map, sizeof(size_t) * MAP_SIZE, cudaMemcpyHostToDevice);\n\n    cudaDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    Simulation<<< dim3((n_particles + 255) / 256), dim3(256) >>> (\n      d_particleX, \n      d_particleY, \n      d_randomX, \n      d_randomY, \n      d_map, \n      n_particles,\n      nIterations,\n      grid_size,\n      radius);\n\n    cudaDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    time_total += time;\n  }\n\n  std::cout << std::endl;\n  std::cout << \"Average kernel execution time: \" << (time_total * 1e-9) / nRepeat << \" (s)\";\n  std::cout << std::endl;\n\n  cudaMemcpy(map, d_map, sizeof(size_t) * MAP_SIZE, cudaMemcpyDeviceToHost);\n\n  \n\n  \n\n  \n\n  for (size_t i = 0; i < n_particles; ++i) {\n    for (size_t y = 0; y < grid_size; y++) {\n      for (size_t x = 0; x < grid_size; x++) {\n        if (map[i * grid_size * grid_size + y * grid_size + x] > 0) {\n          grid[y][x] += map[i * grid_size * grid_size + y * grid_size + x];\n        }\n      }\n    }\n  }  \n\n\n  cudaFree(d_randomX);\n  cudaFree(d_randomY);\n  cudaFree(d_particleX);\n  cudaFree(d_particleY);\n  cudaFree(d_map);\n}  \n\n\n\nint main(int argc, char* argv[]) {\n  if (argc != 3) {\n    usage(argv[0]);\n    return 1;\n  }\n\n  \n\n  int nIterations = std::stoi(argv[1]);\n  int nRepeat = std::stoi(argv[2]);\n\n  \n\n  const size_t grid_size = 21;    \n\n  const size_t n_particles = 147456;  \n\n  const float radius = 0.5;       \n\n\n  \n\n  int** grid = new int*[grid_size];\n  for (size_t i = 0; i < grid_size; i++) grid[i] = new int[grid_size];\n\n  \n\n  float* randomX = new float[n_particles * nIterations];\n  float* randomY = new float[n_particles * nIterations];\n\n  \n\n  float* particleX = new float[n_particles];\n  float* particleY = new float[n_particles];\n\n  \n\n  size_t MAP_SIZE = n_particles * grid_size * grid_size;\n  size_t* map = new size_t[MAP_SIZE];\n\n  \n\n  for (size_t i = 0; i < n_particles; i++) {\n    \n\n    particleX[i] = 10.0;\n    particleY[i] = 10.0;\n\n    for (size_t y = 0; y < grid_size; y++) {\n      for (size_t x = 0; x < grid_size; x++) {\n        map[i * grid_size * grid_size + y * grid_size + x] = 0;\n      }\n    }\n  }\n\n  for (size_t y = 0; y < grid_size; y++) {\n    for (size_t x = 0; x < grid_size; x++) {\n      grid[y][x] = 0;\n    }\n  }\n\n  \n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  motion_device(particleX, particleY, randomX, randomY, grid, grid_size,\n                n_particles, nIterations, radius, map, nRepeat);\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  std::cout << std::endl;\n  std::cout << \"Simulation time: \" << time * 1e-9 << \" (s) \";\n  std::cout << std::endl;\n\n  \n\n  if (grid_size <= 64) {\n    std::cout << \"\\n ********************** OUTPUT GRID: \" << std::endl;\n    print_matrix<int>(grid, grid_size, grid_size);\n  }\n\n  \n\n  for (size_t i = 0; i < grid_size; i++) delete grid[i];\n\n  delete[] grid;\n  delete[] particleX;\n  delete[] particleY;\n  delete[] randomX;\n  delete[] randomY;\n  delete[] map;\n\n  return 0;\n}\n"}}
{"kernel_name": "particle-diffusion", "parallel_api": "hip", "code": {"motionsim.cu": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <cmath>\n#include <ctime>\n#include <iomanip>\n#include <iostream>\n#include <chrono>\n#include <hip/hip_runtime.h>\n\n\n\n\n\n\nvoid usage(std::string programName) {\n  std::cout << \" Incorrect number of parameters \" << std::endl;\n  std::cout << \" Usage: \";\n  std::cout << programName << \" <Number of iterations within the kernel> \";\n  std::cout << \"<Kernel execution count>\\n\\n\";\n}\n\n\n\ntemplate <typename T>\nvoid print_matrix(T** matrix, size_t size_X, size_t size_Y) {\n  std::cout << std::endl;\n  for (size_t i = 0; i < size_X; ++i) {\n    for (size_t j = 0; j < size_Y; ++j) {\n      std::cout << std::setw(3) << matrix[i][j] << \" \";\n    }\n    std::cout << std::endl;\n  }\n}\n\n\n\ntemplate <typename T>\nvoid print_vector(T* vector, size_t n) {\n  std::cout << std::endl;\n  for (size_t i = 0; i < n; ++i) {\n    std::cout << vector[i] << \" \";\n  }\n  std::cout << std::endl;\n}\n\n__global__\nvoid Simulation(float*__restrict__ a_particleX,\n                float*__restrict__ a_particleY,\n\t\tconst float*__restrict__ a_randomX,\n                const float*__restrict__ a_randomY, \n\t\tsize_t *__restrict__ a_map,\n                const size_t n_particles,\n                unsigned int nIterations,\n                int grid_size,\n                float radius)\n{\n  size_t ii = blockDim.x * blockIdx.x + threadIdx.x;\n  if (ii >= n_particles) return;\n  \n\n  \n\n  \n\n  \n\n  \n\n  size_t iter = 0;\n  float pX = a_particleX[ii];\n  float pY = a_particleY[ii];\n  size_t map_base = ii * grid_size * grid_size;\n  while (iter < nIterations) {\n    \n\n    \n\n    \n\n    \n\n\n    float randnumX = a_randomX[iter * n_particles + ii];\n    float randnumY = a_randomY[iter * n_particles + ii];\n\n    \n\n    float displacementX = randnumX / 1000.0f - 0.0495f;\n    float displacementY = randnumY / 1000.0f - 0.0495f;\n\n    \n\n    pX += displacementX;\n    pY += displacementY;\n\n    \n\n    float dX = pX - trunc(pX);\n    float dY = pY - trunc(pY);\n\n    \n\n    int iX = floor(pX);\n    int iY = floor(pY);\n\n    \n\n    if ((pX < grid_size) && (pY < grid_size) && (pX >= 0) && (pY >= 0)) {\n      \n\n      \n\n      if ((dX * dX + dY * dY <= radius * radius))\n        \n\n        a_map[map_base + iY * grid_size + iX]++;\n    }\n\n    iter++;\n\n  }  \n\n\n  a_particleX[ii] = pX;\n  a_particleY[ii] = pY;\n}\n\n\n\nvoid motion_device(float* particleX, float* particleY,\n                   float* randomX, float* randomY, int** grid, size_t grid_size,\n                   size_t n_particles, int nIterations, float radius,\n                   size_t* map, int nRepeat) {\n\n  hipDeviceProp_t devProp;\n  hipGetDeviceProperties(&devProp, 0);\n\n  std::cout << \" Running on \" << devProp.name << std::endl;\n  std::cout << \" The device max work-group size is \" << devProp.maxThreadsPerBlock << std::endl;\n  std::cout << \" The number of iterations is \" << nIterations << std::endl;\n  std::cout << \" The number of kernel execution is \" << nRepeat << std::endl;\n  std::cout << \" The number of particles is \" << n_particles << std::endl;\n\n  \n\n  \n\n  srand(17);\n  \n  \n\n  const size_t scale = 100;\n\n  \n\n  for (size_t i = 0; i < n_particles * nIterations; i++) {\n    randomX[i] = rand() % scale;\n    randomY[i] = rand() % scale;\n  }\n\n  float *d_randomX;\n  float *d_randomY;\n  float *d_particleX;\n  float *d_particleY;\n  size_t *d_map;\n  hipMalloc((void**)&d_randomX, sizeof(float) * n_particles * nIterations);\n  hipMalloc((void**)&d_randomY, sizeof(float) * n_particles * nIterations);\n  hipMalloc((void**)&d_particleX, sizeof(float) * n_particles);\n  hipMalloc((void**)&d_particleY, sizeof(float) * n_particles);\n  size_t MAP_SIZE = n_particles * grid_size * grid_size;\n  hipMalloc((void**)&d_map, sizeof(size_t) * MAP_SIZE);\n\n  hipMemcpy(d_randomX, randomX, sizeof(float) * n_particles * nIterations, hipMemcpyHostToDevice);\n  hipMemcpy(d_randomY, randomY, sizeof(float) * n_particles * nIterations, hipMemcpyHostToDevice);\n\n  double time_total = 0.0;\n\n  for (int i = 0; i < nRepeat; i++) {\n    \n\n    hipMemcpy(d_particleX, particleX, sizeof(float) * n_particles, hipMemcpyHostToDevice);\n    hipMemcpy(d_particleY, particleY, sizeof(float) * n_particles, hipMemcpyHostToDevice);\n    hipMemcpy(d_map, map, sizeof(size_t) * MAP_SIZE, hipMemcpyHostToDevice);\n\n    hipDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    Simulation<<< dim3((n_particles + 255) / 256), dim3(256) >>> (\n      d_particleX, \n      d_particleY, \n      d_randomX, \n      d_randomY, \n      d_map, \n      n_particles,\n      nIterations,\n      grid_size,\n      radius);\n\n    hipDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    time_total += time;\n  }\n\n  std::cout << std::endl;\n  std::cout << \"Average kernel execution time: \" << (time_total * 1e-9) / nRepeat << \" (s)\";\n  std::cout << std::endl;\n\n  hipMemcpy(map, d_map, sizeof(size_t) * MAP_SIZE, hipMemcpyDeviceToHost);\n\n  \n\n  \n\n  \n\n  for (size_t i = 0; i < n_particles; ++i) {\n    for (size_t y = 0; y < grid_size; y++) {\n      for (size_t x = 0; x < grid_size; x++) {\n        if (map[i * grid_size * grid_size + y * grid_size + x] > 0) {\n          grid[y][x] += map[i * grid_size * grid_size + y * grid_size + x];\n        }\n      }\n    }\n  }  \n\n\n  hipFree(d_randomX);\n  hipFree(d_randomY);\n  hipFree(d_particleX);\n  hipFree(d_particleY);\n  hipFree(d_map);\n}  \n\n\n\nint main(int argc, char* argv[]) {\n  if (argc != 3) {\n    usage(argv[0]);\n    return 1;\n  }\n\n  \n\n  int nIterations = std::stoi(argv[1]);\n  int nRepeat = std::stoi(argv[2]);\n\n  \n\n  const size_t grid_size = 21;    \n\n  const size_t n_particles = 147456;  \n\n  const float radius = 0.5;       \n\n\n  \n\n  int** grid = new int*[grid_size];\n  for (size_t i = 0; i < grid_size; i++) grid[i] = new int[grid_size];\n\n  \n\n  float* randomX = new float[n_particles * nIterations];\n  float* randomY = new float[n_particles * nIterations];\n\n  \n\n  float* particleX = new float[n_particles];\n  float* particleY = new float[n_particles];\n\n  \n\n  size_t MAP_SIZE = n_particles * grid_size * grid_size;\n  size_t* map = new size_t[MAP_SIZE];\n\n  \n\n  for (size_t i = 0; i < n_particles; i++) {\n    \n\n    particleX[i] = 10.0;\n    particleY[i] = 10.0;\n\n    for (size_t y = 0; y < grid_size; y++) {\n      for (size_t x = 0; x < grid_size; x++) {\n        map[i * grid_size * grid_size + y * grid_size + x] = 0;\n      }\n    }\n  }\n\n  for (size_t y = 0; y < grid_size; y++) {\n    for (size_t x = 0; x < grid_size; x++) {\n      grid[y][x] = 0;\n    }\n  }\n\n  \n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  motion_device(particleX, particleY, randomX, randomY, grid, grid_size,\n                n_particles, nIterations, radius, map, nRepeat);\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  std::cout << std::endl;\n  std::cout << \"Simulation time: \" << time * 1e-9 << \" (s) \";\n  std::cout << std::endl;\n\n  \n\n  if (grid_size <= 64) {\n    std::cout << \"\\n ********************** OUTPUT GRID: \" << std::endl;\n    print_matrix<int>(grid, grid_size, grid_size);\n  }\n\n  \n\n  for (size_t i = 0; i < grid_size; i++) delete grid[i];\n\n  delete[] grid;\n  delete[] particleX;\n  delete[] particleY;\n  delete[] randomX;\n  delete[] randomY;\n  delete[] map;\n\n  return 0;\n}\n"}}
{"kernel_name": "particle-diffusion", "parallel_api": "omp", "code": {"motionsim.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <cmath>\n#include <ctime>\n#include <iomanip>\n#include <iostream>\n\n\n\n\n\n\nvoid usage(std::string programName) {\n  std::cout << \" Incorrect number of parameters \" << std::endl;\n  std::cout << \" Usage: \";\n  std::cout << programName << \" <Number of iterations within the kernel> \";\n  std::cout << \"<Kernel execution count>\\n\\n\";\n}\n\n\n\ntemplate <typename T>\nvoid print_matrix(T** matrix, size_t size_X, size_t size_Y) {\n  std::cout << std::endl;\n  for (size_t i = 0; i < size_X; ++i) {\n    for (size_t j = 0; j < size_Y; ++j) {\n      std::cout << std::setw(3) << matrix[i][j] << \" \";\n    }\n    std::cout << std::endl;\n  }\n}\n\n\n\ntemplate <typename T>\nvoid print_vector(T* vector, size_t n) {\n  std::cout << std::endl;\n  for (size_t i = 0; i < n; ++i) {\n    std::cout << vector[i] << \" \";\n  }\n  std::cout << std::endl;\n}\n\n\n\nvoid motion_device(float* particleX, float* particleY,\n                   float* randomX, float* randomY, int** grid, size_t grid_size,\n                   size_t n_particles, int nIterations, float radius,\n                   size_t* map, int nRepeat) {\n  srand(17);\n\n  \n\n  const size_t scale = 100;\n\n  \n\n  for (size_t i = 0; i < n_particles * nIterations; i++) {\n    randomX[i] = rand() % scale;\n    randomY[i] = rand() % scale;\n  }\n\n  const size_t MAP_SIZE = n_particles * grid_size * grid_size;\n\n  #pragma omp target data map(to: randomX[0:n_particles * nIterations], \\\n                                  randomY[0:n_particles * nIterations]) \\\n                          map(tofrom: particleX[0:n_particles], \\\n                                      particleY[0:n_particles], \\\n                                      map[0:MAP_SIZE])\n  {\n    std::cout << \" The number of kernel execution is \" << nRepeat << std::endl;\n    std::cout << \" The number of particles is \" << n_particles << std::endl;\n\n    double time_total = 0.0;\n\n    for (int i = 0; i < nRepeat; i++) {\n\n      #pragma omp target update to (particleX[0:n_particles])\n      #pragma omp target update to (particleY[0:n_particles])\n      #pragma omp target update to (map[0:MAP_SIZE])\n\n      auto start = std::chrono::steady_clock::now();\n\n      #pragma omp target teams distribute parallel for simd thread_limit(256) \n      for (int ii = 0; ii < n_particles; ii++) {\n\n        \n\n        \n\n        \n\n        \n\n        \n\n        size_t iter = 0;\n        float pX = particleX[ii];\n        float pY = particleY[ii];\n        size_t map_base = ii * grid_size * grid_size;\n\n        while (iter < nIterations) {\n          \n\n          \n\n          \n\n          \n\n\n          float randnumX = randomX[iter * n_particles + ii];\n          float randnumY = randomY[iter * n_particles + ii];\n\n          \n\n          float displacementX = randnumX / 1000.0f - 0.0495f;\n          float displacementY = randnumY / 1000.0f - 0.0495f;\n\n          \n\n          pX += displacementX;\n          pY += displacementY;\n\n          \n\n          float dX = pX - truncf(pX);\n          float dY = pY - truncf(pY);\n\n          \n\n          int iX = floorf(pX);\n          int iY = floorf(pY);\n\n          \n\n          if ((pX < grid_size) && (pY < grid_size) && (pX >= 0) && (pY >= 0)) {\n            \n\n            \n\n            if ((dX * dX + dY * dY <= radius * radius))\n              \n\n              map[map_base + iY * grid_size + iX]++;\n          }\n\n          iter++;\n\n        }  \n\n\n        particleX[ii] = pX;\n        particleY[ii] = pY;\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      time_total += time;\n    }\n\n    std::cout << std::endl;\n    std::cout << \"Average kernel execution time: \" << (time_total * 1e-9) / nRepeat << \" (s)\";\n    std::cout << std::endl;\n  }\n\n  \n\n  \n\n  \n\n  for (size_t i = 0; i < n_particles; ++i) {\n    for (size_t y = 0; y < grid_size; y++) {\n      for (size_t x = 0; x < grid_size; x++) {\n        if (map[i * grid_size * grid_size + y * grid_size + x] > 0) {\n          grid[y][x] += map[i * grid_size * grid_size + y * grid_size + x];\n        }\n      }\n    }\n  }  \n\n}  \n\n\nint main(int argc, char* argv[]) {\n  if (argc != 3) {\n    usage(argv[0]);\n    return 1;\n  }\n\n  \n\n  int nIterations = std::stoi(argv[1]);\n  int nRepeat = std::stoi(argv[2]);\n\n  \n\n  const size_t grid_size = 21;    \n\n  const size_t n_particles = 147456;  \n\n  const float radius = 0.5;       \n\n\n  \n\n  int** grid = new int*[grid_size];\n  for (size_t i = 0; i < grid_size; i++) grid[i] = new int[grid_size];\n\n  \n\n  float* randomX = new float[n_particles * nIterations];\n  float* randomY = new float[n_particles * nIterations];\n\n  \n\n  float* particleX = new float[n_particles];\n  float* particleY = new float[n_particles];\n\n  \n\n  const size_t MAP_SIZE = n_particles * grid_size * grid_size;\n  size_t* map = new size_t[MAP_SIZE];\n\n  \n\n  for (size_t i = 0; i < n_particles; i++) {\n    \n\n    particleX[i] = 10.0;\n    particleY[i] = 10.0;\n\n    for (size_t y = 0; y < grid_size; y++) {\n      for (size_t x = 0; x < grid_size; x++) {\n        map[i * grid_size * grid_size + y * grid_size + x] = 0;\n      }\n    }\n  }\n\n  for (size_t y = 0; y < grid_size; y++) {\n    for (size_t x = 0; x < grid_size; x++) {\n      grid[y][x] = 0;\n    }\n  }\n\n  \n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  motion_device(particleX, particleY, randomX, randomY, grid, grid_size,\n                n_particles, nIterations, radius, map, nRepeat);\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  std::cout << std::endl;\n  std::cout << \"Simulation time: \" << time * 1e-9 << \" (s) \";\n  std::cout << std::endl;\n\n  \n\n  if (grid_size <= 64) {\n    std::cout << \"\\n ********************** OUTPUT GRID: \" << std::endl;\n    print_matrix<int>(grid, grid_size, grid_size);\n  }\n\n  \n\n  for (size_t i = 0; i < grid_size; i++) delete grid[i];\n\n  delete[] grid;\n  delete[] particleX;\n  delete[] particleY;\n  delete[] randomX;\n  delete[] randomY;\n  delete[] map;\n\n  return 0;\n}\n"}}
{"kernel_name": "particle-diffusion", "parallel_api": "serial", "code": {"motionsim.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <cmath>\n#include <ctime>\n#include <iomanip>\n#include <iostream>\n\n\n\n\n\n\nvoid usage(std::string programName) {\n  std::cout << \" Incorrect number of parameters \" << std::endl;\n  std::cout << \" Usage: \";\n  std::cout << programName << \" <Number of iterations within the kernel> \";\n  std::cout << \"<Kernel execution count>\\n\\n\";\n}\n\n\n\ntemplate <typename T>\nvoid print_matrix(T** matrix, size_t size_X, size_t size_Y) {\n  std::cout << std::endl;\n  for (size_t i = 0; i < size_X; ++i) {\n    for (size_t j = 0; j < size_Y; ++j) {\n      std::cout << std::setw(3) << matrix[i][j] << \" \";\n    }\n    std::cout << std::endl;\n  }\n}\n\n\n\ntemplate <typename T>\nvoid print_vector(T* vector, size_t n) {\n  std::cout << std::endl;\n  for (size_t i = 0; i < n; ++i) {\n    std::cout << vector[i] << \" \";\n  }\n  std::cout << std::endl;\n}\n\n\n\nvoid motion_device(float* particleX, float* particleY,\n                   float* randomX, float* randomY, int** grid, size_t grid_size,\n                   size_t n_particles, int nIterations, float radius,\n                   size_t* map, int nRepeat) {\n  srand(17);\n\n  \n\n  const size_t scale = 100;\n\n  \n\n  for (size_t i = 0; i < n_particles * nIterations; i++) {\n    randomX[i] = rand() % scale;\n    randomY[i] = rand() % scale;\n  }\n\n  const size_t MAP_SIZE = n_particles * grid_size * grid_size;\n\n    {\n    std::cout << \" The number of kernel execution is \" << nRepeat << std::endl;\n    std::cout << \" The number of particles is \" << n_particles << std::endl;\n\n    double time_total = 0.0;\n\n    for (int i = 0; i < nRepeat; i++) {\n\n                  \n      auto start = std::chrono::steady_clock::now();\n\n            for (int ii = 0; ii < n_particles; ii++) {\n\n        \n\n        \n\n        \n\n        \n\n        \n\n        size_t iter = 0;\n        float pX = particleX[ii];\n        float pY = particleY[ii];\n        size_t map_base = ii * grid_size * grid_size;\n\n        while (iter < nIterations) {\n          \n\n          \n\n          \n\n          \n\n\n          float randnumX = randomX[iter * n_particles + ii];\n          float randnumY = randomY[iter * n_particles + ii];\n\n          \n\n          float displacementX = randnumX / 1000.0f - 0.0495f;\n          float displacementY = randnumY / 1000.0f - 0.0495f;\n\n          \n\n          pX += displacementX;\n          pY += displacementY;\n\n          \n\n          float dX = pX - truncf(pX);\n          float dY = pY - truncf(pY);\n\n          \n\n          int iX = floorf(pX);\n          int iY = floorf(pY);\n\n          \n\n          if ((pX < grid_size) && (pY < grid_size) && (pX >= 0) && (pY >= 0)) {\n            \n\n            \n\n            if ((dX * dX + dY * dY <= radius * radius))\n              \n\n              map[map_base + iY * grid_size + iX]++;\n          }\n\n          iter++;\n\n        }  \n\n\n        particleX[ii] = pX;\n        particleY[ii] = pY;\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      time_total += time;\n    }\n\n    std::cout << std::endl;\n    std::cout << \"Average kernel execution time: \" << (time_total * 1e-9) / nRepeat << \" (s)\";\n    std::cout << std::endl;\n  }\n\n  \n\n  \n\n  \n\n  for (size_t i = 0; i < n_particles; ++i) {\n    for (size_t y = 0; y < grid_size; y++) {\n      for (size_t x = 0; x < grid_size; x++) {\n        if (map[i * grid_size * grid_size + y * grid_size + x] > 0) {\n          grid[y][x] += map[i * grid_size * grid_size + y * grid_size + x];\n        }\n      }\n    }\n  }  \n\n}  \n\n\nint main(int argc, char* argv[]) {\n  if (argc != 3) {\n    usage(argv[0]);\n    return 1;\n  }\n\n  \n\n  int nIterations = std::stoi(argv[1]);\n  int nRepeat = std::stoi(argv[2]);\n\n  \n\n  const size_t grid_size = 21;    \n\n  const size_t n_particles = 147456;  \n\n  const float radius = 0.5;       \n\n\n  \n\n  int** grid = new int*[grid_size];\n  for (size_t i = 0; i < grid_size; i++) grid[i] = new int[grid_size];\n\n  \n\n  float* randomX = new float[n_particles * nIterations];\n  float* randomY = new float[n_particles * nIterations];\n\n  \n\n  float* particleX = new float[n_particles];\n  float* particleY = new float[n_particles];\n\n  \n\n  const size_t MAP_SIZE = n_particles * grid_size * grid_size;\n  size_t* map = new size_t[MAP_SIZE];\n\n  \n\n  for (size_t i = 0; i < n_particles; i++) {\n    \n\n    particleX[i] = 10.0;\n    particleY[i] = 10.0;\n\n    for (size_t y = 0; y < grid_size; y++) {\n      for (size_t x = 0; x < grid_size; x++) {\n        map[i * grid_size * grid_size + y * grid_size + x] = 0;\n      }\n    }\n  }\n\n  for (size_t y = 0; y < grid_size; y++) {\n    for (size_t x = 0; x < grid_size; x++) {\n      grid[y][x] = 0;\n    }\n  }\n\n  \n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  motion_device(particleX, particleY, randomX, randomY, grid, grid_size,\n                n_particles, nIterations, radius, map, nRepeat);\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  std::cout << std::endl;\n  std::cout << \"Simulation time: \" << time * 1e-9 << \" (s) \";\n  std::cout << std::endl;\n\n  \n\n  if (grid_size <= 64) {\n    std::cout << \"\\n ********************** OUTPUT GRID: \" << std::endl;\n    print_matrix<int>(grid, grid_size, grid_size);\n  }\n\n  \n\n  for (size_t i = 0; i < grid_size; i++) delete grid[i];\n\n  delete[] grid;\n  delete[] particleX;\n  delete[] particleY;\n  delete[] randomX;\n  delete[] randomY;\n  delete[] map;\n\n  return 0;\n}"}}
{"kernel_name": "particle-diffusion", "parallel_api": "sycl", "code": {"motionsim.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <cmath>\n#include <ctime>\n#include <iomanip>\n#include <iostream>\n#include <sycl/sycl.hpp>\n\n\n\n\n\n\nvoid usage(std::string programName) {\n  std::cout << \" Incorrect number of parameters \" << std::endl;\n  std::cout << \" Usage: \";\n  std::cout << programName << \" <Number of iterations within the kernel> \";\n  std::cout << \"<Kernel execution count>\\n\\n\";\n}\n\n\n\ntemplate <typename T>\nvoid print_matrix(T** matrix, size_t size_X, size_t size_Y) {\n  std::cout << std::endl;\n  for (size_t i = 0; i < size_X; ++i) {\n    for (size_t j = 0; j < size_Y; ++j) {\n      std::cout << std::setw(3) << matrix[i][j] << \" \";\n    }\n    std::cout << std::endl;\n  }\n}\n\n\n\ntemplate <typename T>\nvoid print_vector(T* vector, size_t n) {\n  std::cout << std::endl;\n  for (size_t i = 0; i < n; ++i) {\n    std::cout << vector[i] << \" \";\n  }\n  std::cout << std::endl;\n}\n\n\n\nvoid motion_device(float* particleX, float* particleY,\n                   float* randomX, float* randomY, int** grid, size_t grid_size,\n                   size_t n_particles, int nIterations, float radius,\n                   size_t* map, int nRepeat) {\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  auto device = q.get_device();\n  auto deviceName = device.get_info<sycl::info::device::name>();\n  auto maxBlockSize = device.get_info<sycl::info::device::max_work_group_size>();\n\n  std::cout << \" Running on \" << deviceName << std::endl;\n  std::cout << \" The device max work-group size is \" << maxBlockSize << std::endl;\n  std::cout << \" The number of iterations is \" << nIterations << std::endl;\n  std::cout << \" The number of kernel execution is \" << nRepeat << std::endl;\n  std::cout << \" The number of particles is \" << n_particles << std::endl;\n\n  \n\n  \n\n  srand(17);\n\n  \n\n  const size_t scale = 100;\n\n  \n\n  for (size_t i = 0; i < n_particles * nIterations; i++) {\n    randomX[i] = rand() % scale;\n    randomY[i] = rand() % scale;\n  }\n\n  const size_t map_size = n_particles * grid_size * grid_size;\n\n  float *d_randomX = sycl::malloc_device<float>(n_particles * nIterations, q);\n  q.memcpy(d_randomX, randomX, n_particles * nIterations * sizeof(float));\n\n  float *d_randomY = sycl::malloc_device<float>(n_particles * nIterations, q);\n  q.memcpy(d_randomY, randomY, n_particles * nIterations * sizeof(float));\n\n  float *d_particleX = sycl::malloc_device<float>(n_particles, q);\n  float *d_particleY = sycl::malloc_device<float>(n_particles, q);\n  size_t *d_map = sycl::malloc_device<size_t>(map_size, q);\n\n  sycl::range<1> gws ((n_particles + 255) / 256 * 256);\n  sycl::range<1> lws (256);\n\n  double time_total = 0.0;\n\n  for (int i = 0; i < nRepeat; i++) {\n\n    q.memcpy(d_particleX, particleX, n_particles * sizeof(float));\n    q.memcpy(d_particleY, particleY, n_particles * sizeof(float));\n    q.memcpy(d_map, map, map_size * sizeof(size_t));\n\n    q.wait();\n    auto start = std::chrono::steady_clock::now();\n\n    q.submit([&](sycl::handler& cgh) {\n      cgh.parallel_for<class motionsim>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        size_t ii = item.get_global_id(0);\n        if (ii >= n_particles) return;\n\n        \n\n        \n\n        \n\n        \n\n        \n\n        float pX = d_particleX[ii];\n        float pY = d_particleY[ii];\n        size_t map_base = ii * grid_size * grid_size;\n        size_t iter = 0;\n        while (iter < nIterations) {\n          \n\n          \n\n          \n\n          \n\n\n          float randnumX = d_randomX[iter * n_particles + ii];\n          float randnumY = d_randomY[iter * n_particles + ii];\n\n          \n\n          float displacementX = (float)randnumX / 1000.0f - 0.0495f;\n          float displacementY = (float)randnumY / 1000.0f - 0.0495f;\n\n          \n\n          pX += displacementX;\n          pY += displacementY;\n\n          \n\n          float dX = pX - sycl::trunc(pX);\n          float dY = pY - sycl::trunc(pY);\n\n          \n\n          int iX = sycl::floor(pX);\n          int iY = sycl::floor(pY);\n\n          \n\n          if ((pX < grid_size) & (pY < grid_size) & (pX >= 0) & (pY >= 0)) {\n            \n\n            \n\n            if (dX * dX + dY * dY <= radius * radius)\n              \n\n              d_map[map_base + iY * grid_size + iX]++;\n          }\n\n          iter++;\n        }  \n\n\n        d_particleX[ii] = pX;\n        d_particleY[ii] = pY;\n\n      });\n    }).wait();\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    time_total += time;\n  }\n\n  std::cout << std::endl;\n  std::cout << \"Average kernel execution time: \" << (time_total * 1e-9) / nRepeat << \" (s)\";\n  std::cout << std::endl;\n\n  q.memcpy(map, d_map, map_size * sizeof(size_t)).wait();\n\n  \n\n  \n\n  \n\n  for (size_t i = 0; i < n_particles; ++i) {\n    for (size_t y = 0; y < grid_size; y++) {\n      for (size_t x = 0; x < grid_size; x++) {\n        if (map[i * grid_size * grid_size + y * grid_size + x] > 0) {\n          grid[y][x] += map[i * grid_size * grid_size + y * grid_size + x];\n        }\n      }\n    }\n  }  \n\n\n  sycl::free(d_randomX, q);\n  sycl::free(d_randomY, q);\n  sycl::free(d_particleX, q);\n  sycl::free(d_particleY, q);\n  sycl::free(d_map, q);\n}  \n\n\n\nint main(int argc, char* argv[]) {\n  if (argc != 3) {\n    usage(argv[0]);\n    return 1;\n  }\n\n  \n\n  int nIterations = std::stoi(argv[1]);\n  int nRepeat = std::stoi(argv[2]);\n\n  \n\n  const size_t grid_size = 21;    \n\n  const size_t n_particles = 147456;  \n\n  const float radius = 0.5;       \n\n\n  \n\n  int** grid = new int*[grid_size];\n  for (size_t i = 0; i < grid_size; i++) grid[i] = new int[grid_size];\n\n  \n\n  float* randomX = new float[n_particles * nIterations];\n  float* randomY = new float[n_particles * nIterations];\n\n  \n\n  float* particleX = new float[n_particles];\n  float* particleY = new float[n_particles];\n\n  \n\n  const size_t MAP_SIZE = n_particles * grid_size * grid_size;\n  size_t* map = new size_t[MAP_SIZE];\n\n  \n\n  for (size_t i = 0; i < n_particles; i++) {\n    \n\n    particleX[i] = 10.0;\n    particleY[i] = 10.0;\n\n    for (size_t y = 0; y < grid_size; y++) {\n      for (size_t x = 0; x < grid_size; x++) {\n        map[i * grid_size * grid_size + y * grid_size + x] = 0;\n      }\n    }\n  }\n\n  for (size_t y = 0; y < grid_size; y++) {\n    for (size_t x = 0; x < grid_size; x++) {\n      grid[y][x] = 0;\n    }\n  }\n\n  \n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  motion_device(particleX, particleY, randomX, randomY, grid, grid_size,\n                n_particles, nIterations, radius, map, nRepeat);\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  std::cout << std::endl;\n  std::cout << \"Simulation time: \" << time * 1e-9 << \" (s) \";\n  std::cout << std::endl;\n\n  \n\n  if (grid_size <= 64) {\n    std::cout << \"\\n ********************** OUTPUT GRID: \" << std::endl;\n    print_matrix<int>(grid, grid_size, grid_size);\n  }\n\n  \n\n  for (size_t i = 0; i < grid_size; i++) delete grid[i];\n\n  delete[] grid;\n  delete[] particleX;\n  delete[] particleY;\n  delete[] randomX;\n  delete[] randomY;\n  delete[] map;\n\n  return 0;\n}\n"}}
{"kernel_name": "particlefilter", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <limits.h>\n#include <math.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <float.h>\n#include <time.h>\n#include <sys/time.h>\n#include <iostream>\n#include <cuda.h>\n\n#define BLOCK_X 16\n#define BLOCK_Y 16\n#define PI 3.1415926535897932f\n#define A 1103515245\n#define C 12345\n#define M INT_MAX\n#define SCALE_FACTOR 300.0f\n\n#ifndef BLOCK_SIZE\n#define BLOCK_SIZE 256\n#endif\n\n#include \"kernel_find_index.h\"\n#include \"kernel_likelihood.h\"\n#include \"kernel_normalize_weights.h\"\n#include \"kernel_sum.h\"\n\n#ifndef FLT_MAX\n#define FLT_MAX 3.40282347e+38\n#endif\n\n\n\n\nlong long get_time() {\n  struct timeval tv;\n  gettimeofday(&tv, NULL);\n  return (tv.tv_sec * 1000000) +tv.tv_usec;\n}\n\n\n\nfloat elapsed_time(long long start_time, long long end_time) {\n  return (float) (end_time - start_time) / (1000 * 1000);\n}\n\n\n\nfloat randu(int * seed, int index) {\n  int num = A * seed[index] + C;\n  seed[index] = num % M;\n  return fabs(seed[index] / ((float) M));\n}\n\n\n\nfloat randn(int * seed, int index) {\n  \n\n  float u = randu(seed, index);\n  float v = randu(seed, index);\n  float cosine = cos(2 * PI * v);\n  float rt = -2 * log(u);\n  return sqrt(rt) * cosine;\n}\n\n\n\nfloat roundFloat(float value) {\n  int newValue = (int) (value);\n  if (value - newValue < .5)\n    return newValue;\n  else\n    return newValue++;\n}\n\n\n\nvoid setIf(int testValue, int newValue, unsigned char * array3D, int * dimX, int * dimY, int * dimZ) {\n  int x, y, z;\n  for (x = 0; x < *dimX; x++) {\n    for (y = 0; y < *dimY; y++) {\n      for (z = 0; z < *dimZ; z++) {\n        if (array3D[x * *dimY * *dimZ + y * *dimZ + z] == testValue)\n          array3D[x * *dimY * *dimZ + y * *dimZ + z] = newValue;\n      }\n    }\n  }\n}\n\n\n\nvoid addNoise(unsigned char * array3D, int * dimX, int * dimY, int * dimZ, int * seed) {\n  int x, y, z;\n  for (x = 0; x < *dimX; x++) {\n    for (y = 0; y < *dimY; y++) {\n      for (z = 0; z < *dimZ; z++) {\n        array3D[x * *dimY * *dimZ + y * *dimZ + z] = array3D[x * *dimY * *dimZ + y * *dimZ + z] + (unsigned char) (5 * randn(seed, 0));\n      }\n    }\n  }\n}\n\n\n\nvoid strelDisk(int * disk, int radius) {\n  int diameter = radius * 2 - 1;\n  int x, y;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      float distance = sqrt(pow((float) (x - radius + 1), 2) + pow((float) (y - radius + 1), 2));\n      if (distance < radius)\n        disk[x * diameter + y] = 1;\n      else\n        disk[x * diameter + y] = 0;\n    }\n  }\n}\n\n\n\nvoid dilate_matrix(unsigned char * matrix, int posX, int posY, int posZ, int dimX, int dimY, int dimZ, int error) {\n  int startX = posX - error;\n  while (startX < 0)\n    startX++;\n  int startY = posY - error;\n  while (startY < 0)\n    startY++;\n  int endX = posX + error;\n  while (endX > dimX)\n    endX--;\n  int endY = posY + error;\n  while (endY > dimY)\n    endY--;\n  int x, y;\n  for (x = startX; x < endX; x++) {\n    for (y = startY; y < endY; y++) {\n      float distance = sqrt(pow((float) (x - posX), 2) + pow((float) (y - posY), 2));\n      if (distance < error)\n        matrix[x * dimY * dimZ + y * dimZ + posZ] = 1;\n    }\n  }\n}\n\n\n\nvoid imdilate_disk(unsigned char * matrix, int dimX, int dimY, int dimZ, int error, unsigned char * newMatrix) {\n  int x, y, z;\n  for (z = 0; z < dimZ; z++) {\n    for (x = 0; x < dimX; x++) {\n      for (y = 0; y < dimY; y++) {\n        if (matrix[x * dimY * dimZ + y * dimZ + z] == 1) {\n          dilate_matrix(newMatrix, x, y, z, dimX, dimY, dimZ, error);\n        }\n      }\n    }\n  }\n}\n\n\n\nvoid getneighbors(int * se, int numOnes, int * neighbors, int radius) {\n  int x, y;\n  int neighY = 0;\n  int center = radius - 1;\n  int diameter = radius * 2 - 1;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      if (se[x * diameter + y]) {\n        neighbors[neighY * 2] = (int) (y - center);\n        neighbors[neighY * 2 + 1] = (int) (x - center);\n        neighY++;\n      }\n    }\n  }\n}\n\n\n\nvoid videoSequence(unsigned char * I, int IszX, int IszY, int Nfr, int * seed) {\n  int k;\n  int max_size = IszX * IszY * Nfr;\n  \n\n  int x0 = (int) roundFloat(IszY / 2.0);\n  int y0 = (int) roundFloat(IszX / 2.0);\n  I[x0 * IszY * Nfr + y0 * Nfr + 0] = 1;\n\n  \n\n  int xk, yk, pos;\n  for (k = 1; k < Nfr; k++) {\n    xk = abs(x0 + (k - 1));\n    yk = abs(y0 - 2 * (k - 1));\n    pos = yk * IszY * Nfr + xk * Nfr + k;\n    if (pos >= max_size)\n      pos = 0;\n    I[pos] = 1;\n  }\n\n  \n\n  unsigned char * newMatrix = (unsigned char *) calloc(IszX * IszY * Nfr, sizeof(unsigned char));\n  imdilate_disk(I, IszX, IszY, Nfr, 5, newMatrix);\n  int x, y;\n  for (x = 0; x < IszX; x++) {\n    for (y = 0; y < IszY; y++) {\n      for (k = 0; k < Nfr; k++) {\n        I[x * IszY * Nfr + y * Nfr + k] = newMatrix[x * IszY * Nfr + y * Nfr + k];\n      }\n    }\n  }\n  free(newMatrix);\n\n  \n\n  setIf(0, 100, I, &IszX, &IszY, &Nfr);\n  setIf(1, 228, I, &IszX, &IszY, &Nfr);\n  \n\n  addNoise(I, &IszX, &IszY, &Nfr, seed);\n\n}\n\n\n\nint findIndex(float * CDF, int lengthCDF, float value) {\n  int index = -1;\n  int x;\n  for (x = 0; x < lengthCDF; x++) {\n    if (CDF[x] >= value) {\n      index = x;\n      break;\n    }\n  }\n  if (index == -1) {\n    return lengthCDF - 1;\n  }\n  return index;\n}\n\n\n\nint particleFilter(unsigned char * I, int IszX, int IszY, int Nfr, int * seed, int Nparticles) {\n  int max_size = IszX * IszY*Nfr;\n  \n\n  float xe = roundFloat(IszY / 2.0);\n  float ye = roundFloat(IszX / 2.0);\n\n  \n\n  int radius = 5;\n  int diameter = radius * 2 - 1;\n  int * disk = (int*) calloc(diameter * diameter, sizeof (int));\n  strelDisk(disk, radius);\n  int countOnes = 0;\n  int x, y;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      if (disk[x * diameter + y] == 1)\n        countOnes++;\n    }\n  }\n  int * objxy = (int *) calloc(countOnes * 2, sizeof(int));\n  getneighbors(disk, countOnes, objxy, radius);\n\n  \n\n  float * weights = (float *) calloc(Nparticles, sizeof(float));\n  for (x = 0; x < Nparticles; x++) {\n    weights[x] = 1 / ((float) (Nparticles));\n  }\n  \n\n  float * likelihood = (float *) calloc(Nparticles + 1, sizeof (float));\n  float * arrayX = (float *) calloc(Nparticles, sizeof (float));\n  float * arrayY = (float *) calloc(Nparticles, sizeof (float));\n  float * xj = (float *) calloc(Nparticles, sizeof (float));\n  float * yj = (float *) calloc(Nparticles, sizeof (float));\n  float * CDF = (float *) calloc(Nparticles, sizeof(float));\n\n  \n\n  int * ind = (int*) calloc(countOnes * Nparticles, sizeof(int));\n  float * u = (float *) calloc(Nparticles, sizeof(float));\n\n  \n\n  \n\n  \n\n  for (x = 0; x < Nparticles; x++) {\n    xj[x] = xe;\n    yj[x] = ye;\n  }\n\n  long long offload_start = get_time();\n\n  int num_blocks = (Nparticles + BLOCK_SIZE - 1) / BLOCK_SIZE;\n#ifdef DEBUG\n  printf(\"BLOCK_SIZE=%d \\n\",BLOCK_SIZE);\n#endif\n\n  float* likelihood_GPU;\n  float* arrayX_GPU;\n  float* arrayY_GPU;\n  float* xj_GPU;\n  float* yj_GPU;\n  float* CDF_GPU;\n  float* partial_sums_GPU;\n  float* u_GPU;\n  int* objxy_GPU;\n  int* ind_GPU;\n  int* seed_GPU;\n  float* weights_GPU;\n  unsigned char* I_GPU;\n\n  cudaMalloc((void**)&likelihood_GPU, (Nparticles + 1)*sizeof(float));\n\n  cudaMalloc((void**)&arrayX_GPU, Nparticles*sizeof(float));\n  cudaMalloc((void**)&arrayY_GPU, Nparticles*sizeof(float));\n  cudaMalloc((void**)&xj_GPU, Nparticles*sizeof(float));\n  cudaMalloc((void**)&yj_GPU, Nparticles*sizeof(float));\n  cudaMemcpy(xj_GPU, xj, Nparticles*sizeof(float), cudaMemcpyHostToDevice);\n  cudaMemcpy(yj_GPU, yj, Nparticles*sizeof(float), cudaMemcpyHostToDevice);\n  cudaMalloc((void**)&CDF_GPU, Nparticles*sizeof(float));\n  cudaMalloc((void**)&u_GPU, Nparticles*sizeof(float));\n  \n\n\n  cudaMalloc((void**)&ind_GPU, countOnes*Nparticles*sizeof(int));\n  \n\n\n  cudaMalloc((void**)&weights_GPU, Nparticles*sizeof(float));\n  \n\n  \n\n  \n\n  cudaMemcpy(weights_GPU, weights, Nparticles*sizeof(float), cudaMemcpyHostToDevice);\n\n  cudaMalloc((void**)&I_GPU, IszX * IszY * Nfr * sizeof(unsigned char));\n  cudaMemcpy(I_GPU, I, IszX * IszY * Nfr * sizeof(unsigned char), cudaMemcpyHostToDevice);\n\n  cudaMalloc((void**)&seed_GPU, Nparticles*sizeof(int));\n  cudaMemcpy(seed_GPU, seed, Nparticles*sizeof(int), cudaMemcpyHostToDevice);\n\n  cudaMalloc((void**)&partial_sums_GPU, (Nparticles+1)*sizeof(float));\n  \n\n\n  cudaMalloc((void**)&objxy_GPU, 2*countOnes*sizeof(int));\n  cudaMemcpy(objxy_GPU, objxy, 2*countOnes*sizeof(int), cudaMemcpyHostToDevice);\n\n  cudaDeviceSynchronize();\n  long long start = get_time();\n  \n  for (int k = 1; k < Nfr; k++) {\n    \n\n    kernel_likelihood<<<num_blocks, BLOCK_SIZE>>>(\n        arrayX_GPU, arrayY_GPU, xj_GPU, yj_GPU, ind_GPU,\n        objxy_GPU, likelihood_GPU, I_GPU, weights_GPU, seed_GPU, partial_sums_GPU,\n        Nparticles, countOnes, IszY, Nfr, k, max_size);\n\n#ifdef DEBUG\n    float * sum = (float *) calloc(Nparticles + 1, sizeof (float));\n    cudaMemcpy(sum, partial_sums_GPU, (Nparticles+1)*sizeof(float), cudaMemcpyDeviceToHost);\n    for (int i = 0; i < Nparticles+1; i++)\n      printf(\"%f \", sum[i]);\n    printf(\"\\n\");\n#endif\n\n    kernel_sum<<<1, 1>>>(partial_sums_GPU, Nparticles);\n\n#ifdef DEBUG\n    \n\n    cudaMemcpy(sum, partial_sums_GPU, sizeof(float), cudaMemcpyDeviceToHost);\n    printf(\"kernel sum: frame=%d partial_sums[0]=%f\\n\", k, sum[0]);\n    free(sum);\n#endif\n\n    kernel_normalize_weights<<<num_blocks, BLOCK_SIZE>>>(\n        weights_GPU,\n        partial_sums_GPU,\n        CDF_GPU,\n        u_GPU,\n        seed_GPU,\n        Nparticles );\n\n    kernel_find_index<<<num_blocks, BLOCK_SIZE>>>(\n        arrayX_GPU,\n        arrayY_GPU,\n        CDF_GPU,\n        u_GPU,\n        xj_GPU,\n        yj_GPU,\n        Nparticles );\n  } \n\n\n  cudaDeviceSynchronize();\n  long long end = get_time();\n  printf(\"Average execution time of kernels: %f (s)\\n\",\n         elapsed_time(start, end) / (Nfr-1));\n\n  cudaMemcpy(arrayX, arrayX_GPU, Nparticles*sizeof(float), cudaMemcpyDeviceToHost);\n  cudaMemcpy(arrayY, arrayY_GPU, Nparticles*sizeof(float), cudaMemcpyDeviceToHost);\n  cudaMemcpy(weights, weights_GPU, Nparticles*sizeof(float), cudaMemcpyDeviceToHost);\n\n  cudaFree(likelihood_GPU);\n  cudaFree(arrayX_GPU);\n  cudaFree(arrayY_GPU);\n  cudaFree(xj_GPU);\n  cudaFree(yj_GPU);\n  cudaFree(CDF_GPU);\n  cudaFree(partial_sums_GPU);\n  cudaFree(objxy_GPU);\n  cudaFree(u_GPU);\n  cudaFree(ind_GPU);\n  cudaFree(seed_GPU);\n  cudaFree(weights_GPU);\n  cudaFree(I_GPU);\n\n  long long offload_end = get_time();\n  printf(\"Device offloading time: %f (s)\\n\", elapsed_time(offload_start, offload_end));\n\n  xe = 0;\n  ye = 0;\n  \n\n  for (x = 0; x < Nparticles; x++) {\n    xe += arrayX[x] * weights[x];\n    ye += arrayY[x] * weights[x];\n  }\n  float distance = sqrt(pow((float) (xe - (int) roundFloat(IszY / 2.0)), 2) + pow((float) (ye - (int) roundFloat(IszX / 2.0)), 2));\n\n  \n\n  FILE *fid;\n  fid=fopen(\"output.txt\", \"w+\");\n  if( fid == NULL ){\n    printf( \"The file was not opened for writing\\n\" );\n    return -1;\n  }\n  fprintf(fid, \"XE: %f\\n\", xe);\n  fprintf(fid, \"YE: %f\\n\", ye);\n  fprintf(fid, \"distance: %f\\n\", distance);\n  fclose(fid);\n\n  \n\n  free(likelihood);\n  free(arrayX);\n  free(arrayY);\n  free(xj);\n  free(yj);\n  free(CDF);\n  free(ind);\n  free(u);\n  return 0;\n}\n\nint main(int argc, char * argv[]) {\n\n  const char* usage = \"./main -x <dimX> -y <dimY> -z <Nfr> -np <Nparticles>\";\n  \n\n  if (argc != 9) {\n    printf(\"%s\\n\", usage);\n    return 0;\n  }\n  \n\n  if (strcmp(argv[1], \"-x\") || strcmp(argv[3], \"-y\") || strcmp(argv[5], \"-z\") || strcmp(argv[7], \"-np\")) {\n    printf(\"%s\\n\", usage);\n    return 0;\n  }\n\n  int IszX, IszY, Nfr, Nparticles;\n\n  \n\n  if (sscanf(argv[2], \"%d\", &IszX) == EOF) {\n    printf(\"ERROR: dimX input is incorrect\");\n    return 0;\n  }\n\n  if (IszX <= 0) {\n    printf(\"dimX must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[4], \"%d\", &IszY) == EOF) {\n    printf(\"ERROR: dimY input is incorrect\");\n    return 0;\n  }\n\n  if (IszY <= 0) {\n    printf(\"dimY must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[6], \"%d\", &Nfr) == EOF) {\n    printf(\"ERROR: Number of frames input is incorrect\");\n    return 0;\n  }\n\n  if (Nfr <= 0) {\n    printf(\"number of frames must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[8], \"%d\", &Nparticles) == EOF) {\n    printf(\"ERROR: Number of particles input is incorrect\");\n    return 0;\n  }\n\n  if (Nparticles <= 0) {\n    printf(\"Number of particles must be > 0\\n\");\n    return 0;\n  }\n\n#ifdef DEBUG\n  printf(\"dimX=%d dimY=%d Nfr=%d Nparticles=%d\\n\", \n      IszX, IszY, Nfr, Nparticles);\n#endif\n\n  \n\n  int * seed = (int *) calloc(Nparticles, sizeof(int));\n  int i;\n  for (i = 0; i < Nparticles; i++)\n    seed[i] = i+1;\n\n  \n\n  unsigned char * I = (unsigned char *) calloc(IszX * IszY * Nfr, sizeof(unsigned char));\n  long long start = get_time();\n\n  \n\n  videoSequence(I, IszX, IszY, Nfr, seed);\n  long long endVideoSequence = get_time();\n  printf(\"VIDEO SEQUENCE TOOK %f (s)\\n\", elapsed_time(start, endVideoSequence));\n\n  \n\n  particleFilter(I, IszX, IszY, Nfr, seed, Nparticles);\n  long long endParticleFilter = get_time();\n  printf(\"PARTICLE FILTER TOOK %f (s)\\n\", elapsed_time(endVideoSequence, endParticleFilter));\n\n  printf(\"ENTIRE PROGRAM TOOK %f (s)\\n\", elapsed_time(start, endParticleFilter));\n\n  free(seed);\n  free(I);\n  return 0;\n}\n"}}
{"kernel_name": "particlefilter", "parallel_api": "hip", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <limits.h>\n#include <math.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <float.h>\n#include <time.h>\n#include <sys/time.h>\n#include <iostream>\n#include <hip/hip_runtime.h>\n\n#define BLOCK_X 16\n#define BLOCK_Y 16\n#define PI 3.1415926535897932f\n#define A 1103515245\n#define C 12345\n#define M INT_MAX\n#define SCALE_FACTOR 300.0f\n\n#ifndef BLOCK_SIZE\n#define BLOCK_SIZE 256\n#endif\n\n#include \"kernel_find_index.h\"\n#include \"kernel_likelihood.h\"\n#include \"kernel_normalize_weights.h\"\n#include \"kernel_sum.h\"\n\n#ifndef FLT_MAX\n#define FLT_MAX 3.40282347e+38\n#endif\n\n\n\n\nlong long get_time() {\n  struct timeval tv;\n  gettimeofday(&tv, NULL);\n  return (tv.tv_sec * 1000000) +tv.tv_usec;\n}\n\n\n\nfloat elapsed_time(long long start_time, long long end_time) {\n  return (float) (end_time - start_time) / (1000 * 1000);\n}\n\n\n\nfloat randu(int * seed, int index) {\n  int num = A * seed[index] + C;\n  seed[index] = num % M;\n  return fabs(seed[index] / ((float) M));\n}\n\n\n\nfloat randn(int * seed, int index) {\n  \n\n  float u = randu(seed, index);\n  float v = randu(seed, index);\n  float cosine = cos(2 * PI * v);\n  float rt = -2 * log(u);\n  return sqrt(rt) * cosine;\n}\n\n\n\nfloat roundFloat(float value) {\n  int newValue = (int) (value);\n  if (value - newValue < .5)\n    return newValue;\n  else\n    return newValue++;\n}\n\n\n\nvoid setIf(int testValue, int newValue, unsigned char * array3D, int * dimX, int * dimY, int * dimZ) {\n  int x, y, z;\n  for (x = 0; x < *dimX; x++) {\n    for (y = 0; y < *dimY; y++) {\n      for (z = 0; z < *dimZ; z++) {\n        if (array3D[x * *dimY * *dimZ + y * *dimZ + z] == testValue)\n          array3D[x * *dimY * *dimZ + y * *dimZ + z] = newValue;\n      }\n    }\n  }\n}\n\n\n\nvoid addNoise(unsigned char * array3D, int * dimX, int * dimY, int * dimZ, int * seed) {\n  int x, y, z;\n  for (x = 0; x < *dimX; x++) {\n    for (y = 0; y < *dimY; y++) {\n      for (z = 0; z < *dimZ; z++) {\n        array3D[x * *dimY * *dimZ + y * *dimZ + z] = array3D[x * *dimY * *dimZ + y * *dimZ + z] + (unsigned char) (5 * randn(seed, 0));\n      }\n    }\n  }\n}\n\n\n\nvoid strelDisk(int * disk, int radius) {\n  int diameter = radius * 2 - 1;\n  int x, y;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      float distance = sqrt(pow((float) (x - radius + 1), 2) + pow((float) (y - radius + 1), 2));\n      if (distance < radius)\n        disk[x * diameter + y] = 1;\n      else\n        disk[x * diameter + y] = 0;\n    }\n  }\n}\n\n\n\nvoid dilate_matrix(unsigned char * matrix, int posX, int posY, int posZ, int dimX, int dimY, int dimZ, int error) {\n  int startX = posX - error;\n  while (startX < 0)\n    startX++;\n  int startY = posY - error;\n  while (startY < 0)\n    startY++;\n  int endX = posX + error;\n  while (endX > dimX)\n    endX--;\n  int endY = posY + error;\n  while (endY > dimY)\n    endY--;\n  int x, y;\n  for (x = startX; x < endX; x++) {\n    for (y = startY; y < endY; y++) {\n      float distance = sqrt(pow((float) (x - posX), 2) + pow((float) (y - posY), 2));\n      if (distance < error)\n        matrix[x * dimY * dimZ + y * dimZ + posZ] = 1;\n    }\n  }\n}\n\n\n\nvoid imdilate_disk(unsigned char * matrix, int dimX, int dimY, int dimZ, int error, unsigned char * newMatrix) {\n  int x, y, z;\n  for (z = 0; z < dimZ; z++) {\n    for (x = 0; x < dimX; x++) {\n      for (y = 0; y < dimY; y++) {\n        if (matrix[x * dimY * dimZ + y * dimZ + z] == 1) {\n          dilate_matrix(newMatrix, x, y, z, dimX, dimY, dimZ, error);\n        }\n      }\n    }\n  }\n}\n\n\n\nvoid getneighbors(int * se, int numOnes, int * neighbors, int radius) {\n  int x, y;\n  int neighY = 0;\n  int center = radius - 1;\n  int diameter = radius * 2 - 1;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      if (se[x * diameter + y]) {\n        neighbors[neighY * 2] = (int) (y - center);\n        neighbors[neighY * 2 + 1] = (int) (x - center);\n        neighY++;\n      }\n    }\n  }\n}\n\n\n\nvoid videoSequence(unsigned char * I, int IszX, int IszY, int Nfr, int * seed) {\n  int k;\n  int max_size = IszX * IszY * Nfr;\n  \n\n  int x0 = (int) roundFloat(IszY / 2.0);\n  int y0 = (int) roundFloat(IszX / 2.0);\n  I[x0 * IszY * Nfr + y0 * Nfr + 0] = 1;\n\n  \n\n  int xk, yk, pos;\n  for (k = 1; k < Nfr; k++) {\n    xk = abs(x0 + (k - 1));\n    yk = abs(y0 - 2 * (k - 1));\n    pos = yk * IszY * Nfr + xk * Nfr + k;\n    if (pos >= max_size)\n      pos = 0;\n    I[pos] = 1;\n  }\n\n  \n\n  unsigned char * newMatrix = (unsigned char *) calloc(IszX * IszY * Nfr, sizeof(unsigned char));\n  imdilate_disk(I, IszX, IszY, Nfr, 5, newMatrix);\n  int x, y;\n  for (x = 0; x < IszX; x++) {\n    for (y = 0; y < IszY; y++) {\n      for (k = 0; k < Nfr; k++) {\n        I[x * IszY * Nfr + y * Nfr + k] = newMatrix[x * IszY * Nfr + y * Nfr + k];\n      }\n    }\n  }\n  free(newMatrix);\n\n  \n\n  setIf(0, 100, I, &IszX, &IszY, &Nfr);\n  setIf(1, 228, I, &IszX, &IszY, &Nfr);\n  \n\n  addNoise(I, &IszX, &IszY, &Nfr, seed);\n\n}\n\n\n\nint findIndex(float * CDF, int lengthCDF, float value) {\n  int index = -1;\n  int x;\n  for (x = 0; x < lengthCDF; x++) {\n    if (CDF[x] >= value) {\n      index = x;\n      break;\n    }\n  }\n  if (index == -1) {\n    return lengthCDF - 1;\n  }\n  return index;\n}\n\n\n\nint particleFilter(unsigned char * I, int IszX, int IszY, int Nfr, int * seed, int Nparticles) {\n  int max_size = IszX * IszY*Nfr;\n  \n\n  float xe = roundFloat(IszY / 2.0);\n  float ye = roundFloat(IszX / 2.0);\n\n  \n\n  int radius = 5;\n  int diameter = radius * 2 - 1;\n  int * disk = (int*) calloc(diameter * diameter, sizeof (int));\n  strelDisk(disk, radius);\n  int countOnes = 0;\n  int x, y;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      if (disk[x * diameter + y] == 1)\n        countOnes++;\n    }\n  }\n  int * objxy = (int *) calloc(countOnes * 2, sizeof(int));\n  getneighbors(disk, countOnes, objxy, radius);\n\n  \n\n  float * weights = (float *) calloc(Nparticles, sizeof(float));\n  for (x = 0; x < Nparticles; x++) {\n    weights[x] = 1 / ((float) (Nparticles));\n  }\n  \n\n  float * likelihood = (float *) calloc(Nparticles + 1, sizeof (float));\n  float * arrayX = (float *) calloc(Nparticles, sizeof (float));\n  float * arrayY = (float *) calloc(Nparticles, sizeof (float));\n  float * xj = (float *) calloc(Nparticles, sizeof (float));\n  float * yj = (float *) calloc(Nparticles, sizeof (float));\n  float * CDF = (float *) calloc(Nparticles, sizeof(float));\n\n  \n\n  int * ind = (int*) calloc(countOnes * Nparticles, sizeof(int));\n  float * u = (float *) calloc(Nparticles, sizeof(float));\n\n  \n\n  \n\n  \n\n  for (x = 0; x < Nparticles; x++) {\n    xj[x] = xe;\n    yj[x] = ye;\n  }\n\n  long long offload_start = get_time();\n\n  int num_blocks = (Nparticles + BLOCK_SIZE - 1) / BLOCK_SIZE;\n#ifdef DEBUG\n  printf(\"BLOCK_SIZE=%d \\n\",BLOCK_SIZE);\n#endif\n\n  float* likelihood_GPU;\n  float* arrayX_GPU;\n  float* arrayY_GPU;\n  float* xj_GPU;\n  float* yj_GPU;\n  float* CDF_GPU;\n  float* partial_sums_GPU;\n  float* u_GPU;\n  int* objxy_GPU;\n  int* ind_GPU;\n  int* seed_GPU;\n  float* weights_GPU;\n  unsigned char* I_GPU;\n\n  hipMalloc((void**)&likelihood_GPU, (Nparticles + 1)*sizeof(float));\n\n  hipMalloc((void**)&arrayX_GPU, Nparticles*sizeof(float));\n  hipMalloc((void**)&arrayY_GPU, Nparticles*sizeof(float));\n  hipMalloc((void**)&xj_GPU, Nparticles*sizeof(float));\n  hipMalloc((void**)&yj_GPU, Nparticles*sizeof(float));\n  hipMemcpy(xj_GPU, xj, Nparticles*sizeof(float), hipMemcpyHostToDevice);\n  hipMemcpy(yj_GPU, yj, Nparticles*sizeof(float), hipMemcpyHostToDevice);\n  hipMalloc((void**)&CDF_GPU, Nparticles*sizeof(float));\n  hipMalloc((void**)&u_GPU, Nparticles*sizeof(float));\n  \n\n\n  hipMalloc((void**)&ind_GPU, countOnes*Nparticles*sizeof(int));\n  \n\n\n  hipMalloc((void**)&weights_GPU, Nparticles*sizeof(float));\n  \n\n  \n\n  \n\n  hipMemcpy(weights_GPU, weights, Nparticles*sizeof(float), hipMemcpyHostToDevice);\n\n  hipMalloc((void**)&I_GPU, IszX * IszY * Nfr * sizeof(unsigned char));\n  hipMemcpy(I_GPU, I, IszX * IszY * Nfr * sizeof(unsigned char), hipMemcpyHostToDevice);\n\n  hipMalloc((void**)&seed_GPU, Nparticles*sizeof(int));\n  hipMemcpy(seed_GPU, seed, Nparticles*sizeof(int), hipMemcpyHostToDevice);\n\n  hipMalloc((void**)&partial_sums_GPU, (Nparticles+1)*sizeof(float));\n  \n\n\n  hipMalloc((void**)&objxy_GPU, 2*countOnes*sizeof(int));\n  hipMemcpy(objxy_GPU, objxy, 2*countOnes*sizeof(int), hipMemcpyHostToDevice);\n\n  hipDeviceSynchronize();\n  long long start = get_time();\n  \n  for (int k = 1; k < Nfr; k++) {\n    \n\n    hipLaunchKernelGGL(kernel_likelihood, num_blocks, BLOCK_SIZE, 0, 0, \n        arrayX_GPU, arrayY_GPU, xj_GPU, yj_GPU, ind_GPU,\n        objxy_GPU, likelihood_GPU, I_GPU, weights_GPU, seed_GPU, partial_sums_GPU,\n        Nparticles, countOnes, IszY, Nfr, k, max_size);\n\n#ifdef DEBUG\n    float * sum = (float *) calloc(Nparticles + 1, sizeof (float));\n    hipMemcpy(sum, partial_sums_GPU, (Nparticles+1)*sizeof(float), hipMemcpyDeviceToHost);\n    for (int i = 0; i < Nparticles+1; i++)\n      printf(\"%f \", sum[i]);\n    printf(\"\\n\");\n#endif\n\n    hipLaunchKernelGGL(kernel_sum, 1, 1, 0, 0, partial_sums_GPU, Nparticles);\n\n#ifdef DEBUG\n    \n\n    hipMemcpy(sum, partial_sums_GPU, sizeof(float), hipMemcpyDeviceToHost);\n    printf(\"kernel sum: frame=%d partial_sums[0]=%f\\n\", k, sum[0]);\n    free(sum);\n#endif\n\n    hipLaunchKernelGGL(kernel_normalize_weights, num_blocks, BLOCK_SIZE, 0, 0, \n        weights_GPU,\n        partial_sums_GPU,\n        CDF_GPU,\n        u_GPU,\n        seed_GPU,\n        Nparticles );\n\n    hipLaunchKernelGGL(kernel_find_index, num_blocks, BLOCK_SIZE, 0, 0, \n        arrayX_GPU,\n        arrayY_GPU,\n        CDF_GPU,\n        u_GPU,\n        xj_GPU,\n        yj_GPU,\n        Nparticles );\n  } \n\n\n  hipDeviceSynchronize();\n  long long end = get_time();\n  printf(\"Average execution time of kernels: %f (s)\\n\",\n         elapsed_time(start, end) / (Nfr-1));\n\n  hipMemcpy(arrayX, arrayX_GPU, Nparticles*sizeof(float), hipMemcpyDeviceToHost);\n  hipMemcpy(arrayY, arrayY_GPU, Nparticles*sizeof(float), hipMemcpyDeviceToHost);\n  hipMemcpy(weights, weights_GPU, Nparticles*sizeof(float), hipMemcpyDeviceToHost);\n\n  hipFree(likelihood_GPU);\n  hipFree(arrayX_GPU);\n  hipFree(arrayY_GPU);\n  hipFree(xj_GPU);\n  hipFree(yj_GPU);\n  hipFree(CDF_GPU);\n  hipFree(partial_sums_GPU);\n  hipFree(objxy_GPU);\n  hipFree(u_GPU);\n  hipFree(ind_GPU);\n  hipFree(seed_GPU);\n  hipFree(weights_GPU);\n  hipFree(I_GPU);\n\n  long long offload_end = get_time();\n  printf(\"Device offloading time: %f (s)\\n\", elapsed_time(offload_start, offload_end));\n\n  xe = 0;\n  ye = 0;\n  \n\n  for (x = 0; x < Nparticles; x++) {\n    xe += arrayX[x] * weights[x];\n    ye += arrayY[x] * weights[x];\n  }\n  float distance = sqrt(pow((float) (xe - (int) roundFloat(IszY / 2.0)), 2) + pow((float) (ye - (int) roundFloat(IszX / 2.0)), 2));\n\n  \n\n  FILE *fid;\n  fid=fopen(\"output.txt\", \"w+\");\n  if( fid == NULL ){\n    printf( \"The file was not opened for writing\\n\" );\n    return -1;\n  }\n  fprintf(fid, \"XE: %f\\n\", xe);\n  fprintf(fid, \"YE: %f\\n\", ye);\n  fprintf(fid, \"distance: %f\\n\", distance);\n  fclose(fid);\n\n  \n\n  free(likelihood);\n  free(arrayX);\n  free(arrayY);\n  free(xj);\n  free(yj);\n  free(CDF);\n  free(ind);\n  free(u);\n  return 0;\n}\n\nint main(int argc, char * argv[]) {\n\n  const char* usage = \"./main -x <dimX> -y <dimY> -z <Nfr> -np <Nparticles>\";\n  \n\n  if (argc != 9) {\n    printf(\"%s\\n\", usage);\n    return 0;\n  }\n  \n\n  if (strcmp(argv[1], \"-x\") || strcmp(argv[3], \"-y\") || strcmp(argv[5], \"-z\") || strcmp(argv[7], \"-np\")) {\n    printf(\"%s\\n\", usage);\n    return 0;\n  }\n\n  int IszX, IszY, Nfr, Nparticles;\n\n  \n\n  if (sscanf(argv[2], \"%d\", &IszX) == EOF) {\n    printf(\"ERROR: dimX input is incorrect\");\n    return 0;\n  }\n\n  if (IszX <= 0) {\n    printf(\"dimX must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[4], \"%d\", &IszY) == EOF) {\n    printf(\"ERROR: dimY input is incorrect\");\n    return 0;\n  }\n\n  if (IszY <= 0) {\n    printf(\"dimY must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[6], \"%d\", &Nfr) == EOF) {\n    printf(\"ERROR: Number of frames input is incorrect\");\n    return 0;\n  }\n\n  if (Nfr <= 0) {\n    printf(\"number of frames must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[8], \"%d\", &Nparticles) == EOF) {\n    printf(\"ERROR: Number of particles input is incorrect\");\n    return 0;\n  }\n\n  if (Nparticles <= 0) {\n    printf(\"Number of particles must be > 0\\n\");\n    return 0;\n  }\n\n#ifdef DEBUG\n  printf(\"dimX=%d dimY=%d Nfr=%d Nparticles=%d\\n\", \n      IszX, IszY, Nfr, Nparticles);\n#endif\n\n  \n\n  int * seed = (int *) calloc(Nparticles, sizeof(int));\n  int i;\n  for (i = 0; i < Nparticles; i++)\n    seed[i] = i+1;\n\n  \n\n  unsigned char * I = (unsigned char *) calloc(IszX * IszY * Nfr, sizeof(unsigned char));\n  long long start = get_time();\n\n  \n\n  videoSequence(I, IszX, IszY, Nfr, seed);\n  long long endVideoSequence = get_time();\n  printf(\"VIDEO SEQUENCE TOOK %f (s)\\n\", elapsed_time(start, endVideoSequence));\n\n  \n\n  particleFilter(I, IszX, IszY, Nfr, seed, Nparticles);\n  long long endParticleFilter = get_time();\n  printf(\"PARTICLE FILTER TOOK %f (s)\\n\", elapsed_time(endVideoSequence, endParticleFilter));\n\n  printf(\"ENTIRE PROGRAM TOOK %f (s)\\n\", elapsed_time(start, endParticleFilter));\n\n  free(seed);\n  free(I);\n  return 0;\n}\n"}}
{"kernel_name": "particlefilter", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <limits.h>\n#include <math.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <float.h>\n#include <time.h>\n#include <sys/time.h>\n#include <omp.h>\n\n#define BLOCK_X 16\n#define BLOCK_Y 16\n#define PI 3.1415926535897932f\n#define A 1103515245\n#define C 12345\n#define M INT_MAX\n#define SCALE_FACTOR 300.0f\n\n#ifndef BLOCK_SIZE \n#define BLOCK_SIZE 256\n#endif\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#ifndef FLT_MAX\n#define FLT_MAX 3.40282347e+38\n#endif\n\n\n\nlong long get_time() {\n  struct timeval tv;\n  gettimeofday(&tv, NULL);\n  return (tv.tv_sec * 1000000) + tv.tv_usec;\n}\n\n\n\nfloat elapsed_time(long long start_time, long long end_time) {\n  return (float) (end_time - start_time) / (1000 * 1000);\n}\n\n\n\nfloat randu(int * seed, int index) {\n  int num = A * seed[index] + C;\n  seed[index] = num % M;\n  return fabs(seed[index] / ((float) M));\n}\n\n\n\nfloat randn(int * seed, int index) {\n  \n\n  float u = randu(seed, index);\n  float v = randu(seed, index);\n  float cosine = cos(2 * PI * v);\n  float rt = -2 * log(u);\n  return sqrt(rt) * cosine;\n}\n\n\n\nfloat roundFloat(float value) {\n  int newValue = (int) (value);\n  if (value - newValue < .5)\n    return newValue;\n  else\n    return newValue++;\n}\n\n\n\nvoid setIf(int testValue, int newValue, unsigned char * array3D, int * dimX, int * dimY, int * dimZ) {\n  int x, y, z;\n  for (x = 0; x < *dimX; x++) {\n    for (y = 0; y < *dimY; y++) {\n      for (z = 0; z < *dimZ; z++) {\n        if (array3D[x * *dimY * *dimZ + y * *dimZ + z] == testValue)\n          array3D[x * *dimY * *dimZ + y * *dimZ + z] = newValue;\n      }\n    }\n  }\n}\n\n\n\nvoid addNoise(unsigned char * array3D, int * dimX, int * dimY, int * dimZ, int * seed) {\n  int x, y, z;\n  for (x = 0; x < *dimX; x++) {\n    for (y = 0; y < *dimY; y++) {\n      for (z = 0; z < *dimZ; z++) {\n        array3D[x * *dimY * *dimZ + y * *dimZ + z] = array3D[x * *dimY * *dimZ + y * *dimZ + z] + (unsigned char) (5 * randn(seed, 0));\n      }\n    }\n  }\n}\n\n\n\nvoid strelDisk(int * disk, int radius) {\n  int diameter = radius * 2 - 1;\n  int x, y;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      float distance = sqrt(pow((float) (x - radius + 1), 2) + pow((float) (y - radius + 1), 2));\n      if (distance < radius)\n        disk[x * diameter + y] = 1;\n      else\n        disk[x * diameter + y] = 0;\n    }\n  }\n}\n\n\n\nvoid dilate_matrix(unsigned char * matrix, int posX, int posY, int posZ, int dimX, int dimY, int dimZ, int error) {\n  int startX = posX - error;\n  while (startX < 0)\n    startX++;\n  int startY = posY - error;\n  while (startY < 0)\n    startY++;\n  int endX = posX + error;\n  while (endX > dimX)\n    endX--;\n  int endY = posY + error;\n  while (endY > dimY)\n    endY--;\n  int x, y;\n  for (x = startX; x < endX; x++) {\n    for (y = startY; y < endY; y++) {\n      float distance = sqrt(pow((float) (x - posX), 2) + pow((float) (y - posY), 2));\n      if (distance < error)\n        matrix[x * dimY * dimZ + y * dimZ + posZ] = 1;\n    }\n  }\n}\n\n\n\nvoid imdilate_disk(unsigned char * matrix, int dimX, int dimY, int dimZ, int error, unsigned char * newMatrix) {\n  int x, y, z;\n  for (z = 0; z < dimZ; z++) {\n    for (x = 0; x < dimX; x++) {\n      for (y = 0; y < dimY; y++) {\n        if (matrix[x * dimY * dimZ + y * dimZ + z] == 1) {\n          dilate_matrix(newMatrix, x, y, z, dimX, dimY, dimZ, error);\n        }\n      }\n    }\n  }\n}\n\n\n\nvoid getneighbors(int * se, int numOnes, int * neighbors, int radius) {\n  int x, y;\n  int neighY = 0;\n  int center = radius - 1;\n  int diameter = radius * 2 - 1;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      if (se[x * diameter + y]) {\n        neighbors[neighY * 2] = (int) (y - center);\n        neighbors[neighY * 2 + 1] = (int) (x - center);\n        neighY++;\n      }\n    }\n  }\n}\n\n\n\nvoid videoSequence(unsigned char * I, int IszX, int IszY, int Nfr, int * seed) {\n  int k;\n  int max_size = IszX * IszY * Nfr;\n  \n\n  int x0 = (int) roundFloat(IszY / 2.0);\n  int y0 = (int) roundFloat(IszX / 2.0);\n  I[x0 * IszY * Nfr + y0 * Nfr + 0] = 1;\n\n  \n\n  int xk, yk, pos;\n  for (k = 1; k < Nfr; k++) {\n    xk = abs(x0 + (k - 1));\n    yk = abs(y0 - 2 * (k - 1));\n    pos = yk * IszY * Nfr + xk * Nfr + k;\n    if (pos >= max_size)\n      pos = 0;\n    I[pos] = 1;\n  }\n\n  \n\n  unsigned char * newMatrix = (unsigned char *) calloc(IszX * IszY * Nfr, sizeof(unsigned char));\n  imdilate_disk(I, IszX, IszY, Nfr, 5, newMatrix);\n  int x, y;\n  for (x = 0; x < IszX; x++) {\n    for (y = 0; y < IszY; y++) {\n      for (k = 0; k < Nfr; k++) {\n        I[x * IszY * Nfr + y * Nfr + k] = newMatrix[x * IszY * Nfr + y * Nfr + k];\n      }\n    }\n  }\n  free(newMatrix);\n\n  \n\n  setIf(0, 100, I, &IszX, &IszY, &Nfr);\n  setIf(1, 228, I, &IszX, &IszY, &Nfr);\n  \n\n  addNoise(I, &IszX, &IszY, &Nfr, seed);\n\n}\n\n\n\nint findIndex(float * CDF, int lengthCDF, float value) {\n  int index = -1;\n  int x;\n  for (x = 0; x < lengthCDF; x++) {\n    if (CDF[x] >= value) {\n      index = x;\n      break;\n    }\n  }\n  if (index == -1) {\n    return lengthCDF - 1;\n  }\n  return index;\n}\n\n\n\nint particleFilter(unsigned char * I, int IszX, int IszY, int Nfr, int * seed, int Nparticles) {\n  int max_size = IszX * IszY*Nfr;\n  \n\n  float xe = roundFloat(IszY / 2.0);\n  float ye = roundFloat(IszX / 2.0);\n\n  \n\n  int radius = 5;\n  int diameter = radius * 2 - 1;\n  int * disk = (int*) calloc(diameter * diameter, sizeof (int));\n  strelDisk(disk, radius);\n  int countOnes = 0;\n  int x, y;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      if (disk[x * diameter + y] == 1)\n        countOnes++;\n    }\n  }\n  int * objxy = (int *) calloc(countOnes * 2, sizeof(int));\n  getneighbors(disk, countOnes, objxy, radius);\n\n  \n\n  float * weights = (float *) calloc(Nparticles, sizeof(float));\n  for (x = 0; x < Nparticles; x++) {\n    weights[x] = 1 / ((float) (Nparticles));\n  }\n  \n\n  float * likelihood = (float *) calloc(Nparticles + 1, sizeof (float));\n  float * partial_sums = (float *) calloc(Nparticles + 1, sizeof (float));\n  float * arrayX = (float *) calloc(Nparticles, sizeof (float));\n  float * arrayY = (float *) calloc(Nparticles, sizeof (float));\n  float * xj = (float *) calloc(Nparticles, sizeof (float));\n  float * yj = (float *) calloc(Nparticles, sizeof (float));\n  float * CDF = (float *) calloc(Nparticles, sizeof(float));\n\n\n  \n\n  int * ind = (int*) calloc(countOnes * Nparticles, sizeof(int));\n  float * u = (float *) calloc(Nparticles, sizeof(float));\n\n  \n\n  \n\n  \n\n  for (x = 0; x < Nparticles; x++) {\n\n    xj[x] = xe;\n    yj[x] = ye;\n  }\n\n  long long offload_start = get_time();\n\n\n  int k;\n\n  int num_blocks = (Nparticles + BLOCK_SIZE - 1) / BLOCK_SIZE;\n#ifdef DEBUG\n  printf(\"BLOCK_SIZE=%d \\n\",BLOCK_SIZE);\n#endif\n\n#pragma omp target data \\\n  map(alloc: likelihood[0:Nparticles+1], \\\n             ind[0:countOnes*Nparticles], \\\n             u[0:Nparticles], \\\n             partial_sums[0:Nparticles+1], \\\n             CDF[0:Nparticles]) \\\n  map(from: arrayX[0:Nparticles], \\\n            arrayY[0:Nparticles]) \\\n  map(tofrom: weights[0:Nparticles]) \\\n  map(to: xj[0:Nparticles], \\\n          yj[0:Nparticles], \\\n          seed[0:Nparticles], \\\n          I[0:IszX * IszY * Nfr], \\\n          objxy[0:2*countOnes])\n  {\n    long long start = get_time();\n\n    for (k = 1; k < Nfr; k++) {\n      \n\n      #pragma omp target teams num_teams(num_blocks) thread_limit(BLOCK_SIZE)\n      {\n        float weights_local[BLOCK_SIZE];\n        #pragma omp parallel\n        {\n          int block_id = omp_get_team_num();\n          int thread_id = omp_get_thread_num();\n          int block_dim = omp_get_num_threads();\n          int i = block_id * block_dim + thread_id;\n          int y;\n          int indX, indY;\n          float u, v;\n\n          if(i < Nparticles){\n            arrayX[i] = xj[i];\n            arrayY[i] = yj[i];\n            weights[i] = 1.0f / ((float) (Nparticles)); \n            seed[i] = (A*seed[i] + C) % M;\n            u = fabsf(seed[i]/((float)M));\n            seed[i] = (A*seed[i] + C) % M;\n            v = fabsf(seed[i]/((float)M));\n            arrayX[i] += 1.0f + 5.0f*(sqrtf(-2.0f*logf(u))*cosf(2.0f*PI*v));\n\n            seed[i] = (A*seed[i] + C) % M;\n            u = fabsf(seed[i]/((float)M));\n            seed[i] = (A*seed[i] + C) % M;\n            v = fabsf(seed[i]/((float)M));\n            arrayY[i] += -2.0f + 2.0f*(sqrtf(-2.0f*logf(u))*cosf(2.0f*PI*v));\n          }\n\n          #pragma omp barrier\n\n          if(i < Nparticles)\n          {\n            for(y = 0; y < countOnes; y++){\n\n              int iX = arrayX[i];\n              int iY = arrayY[i];\n              int rnd_iX = (arrayX[i] - iX) < .5f ? iX : iX++;\n              int rnd_iY = (arrayY[i] - iY) < .5f ? iY : iY++;\n              indX = rnd_iX + objxy[y*2 + 1];\n              indY = rnd_iY + objxy[y*2];\n\n              ind[i*countOnes + y] = abs(indX*IszY*Nfr + indY*Nfr + k);\n              if(ind[i*countOnes + y] >= max_size)\n                ind[i*countOnes + y] = 0;\n            }\n            float likelihoodSum = 0.0f;\n            for(int x = 0; x < countOnes; x++)\n              likelihoodSum += ((I[ind[i*countOnes + x]] - 100) * (I[ind[i*countOnes + x]] - 100) -\n                  (I[ind[i*countOnes + x]] - 228) * (I[ind[i*countOnes + x]] - 228)) / 50.0f;\n            likelihood[i] = likelihoodSum/countOnes-SCALE_FACTOR;\n\n            weights[i] = weights[i] * expf(likelihood[i]);\n\n          }\n\n          weights_local[thread_id] = (i < Nparticles) ?  weights[i] : 0.f;\n\n          #pragma omp barrier\n\n          for(unsigned int s=block_dim/2; s>0; s>>=1)\n          {\n            if(thread_id < s)\n            {\n              weights_local[thread_id] += weights_local[thread_id + s];\n            }\n          #pragma omp barrier\n          }\n          if(thread_id == 0)\n          {\n            partial_sums[block_id] = weights_local[0];\n          }\n        }\n      }\n\n      #pragma omp target\n      {\n        float sum = 0;\n        int num_blocks = (Nparticles + BLOCK_SIZE - 1) / BLOCK_SIZE;\n        for (int x = 0; x < num_blocks; x++) {\n          sum += partial_sums[x];\n        }\n        partial_sums[0] = sum;\n      }\n\n#ifdef DEBUG\n      \n\n#pragma omp target update from (partial_sums[0:1])\n      printf(\"kernel sum: frame=%d partial_sums[0]=%f\\n\",\n          k, partial_sums[0]);\n#endif\n\n      #pragma omp target teams num_teams(num_blocks) thread_limit(BLOCK_SIZE)\n      {\n        float u1;\n        float sumWeights; \n        #pragma omp parallel\n        {\n          int local_id = omp_get_thread_num();\n          int i = omp_get_team_num() * omp_get_num_threads() + local_id;\n          if(0 == local_id)\n            sumWeights = partial_sums[0];\n\n          #pragma omp barrier\n          if(i < Nparticles) {\n            weights[i] = weights[i]/sumWeights;\n          }\n\n          #pragma omp barrier\n          if(i == 0) {\n            CDF[0] = weights[0];\n            for(int x = 1; x < Nparticles; x++){\n              CDF[x] = weights[x] + CDF[x-1];\n            }\n\n            seed[i] = (A*seed[i] + C) % M;\n            float p = fabsf(seed[i]/((float)M));\n            seed[i] = (A*seed[i] + C) % M;\n            float q = fabsf(seed[i]/((float)M));\n            u[0] = (1.0f/((float)(Nparticles))) * \n              (sqrtf(-2.0f*logf(p))*cosf(2.0f*PI*q));\n            \n\n          }\n\n          #pragma omp barrier\n          if(0 == local_id)\n            u1 = u[0];\n\n          #pragma omp barrier\n          if(i < Nparticles)\n          {\n            u[i] = u1 + i/((float)(Nparticles));\n          }\n        }\n      }\n\n#ifdef DEBUG\n\n#pragma omp target update from (arrayX[0:Nparticles])\n#pragma omp target update from (arrayY[0:Nparticles])\n#pragma omp target update from (weights[0:Nparticles])\n\n      xe = 0;\n      ye = 0;\n      float total=0.0;\n      \n\n      for (x = 0; x < Nparticles; x++) {\n        xe += arrayX[x] * weights[x];\n        ye += arrayY[x] * weights[x];\n        total+= weights[x];\n      }\n      printf(\"total weight: %lf\\n\", total);\n      printf(\"XE: %lf\\n\", xe);\n      printf(\"YE: %lf\\n\", ye);\n      float distance = sqrt(pow((float) (xe - (int) roundFloat(IszY / 2.0)), 2) + pow((float) (ye - (int) roundFloat(IszX / 2.0)), 2));\n      printf(\"distance: %lf\\n\", distance);\n#endif\n\n      #pragma omp target teams distribute parallel for thread_limit(BLOCK_SIZE)\n      for (int i = 0; i < Nparticles; i++)\n      {\n        int index = -1;\n        int x;\n\n        for(x = 0; x < Nparticles; x++){\n          if(CDF[x] >= u[i]){\n            index = x;\n            break;\n          }\n        }\n        if(index == -1){\n          index = Nparticles-1;\n        }\n\n        xj[i] = arrayX[index];\n        yj[i] = arrayY[index];\n      }\n    }\n\n\n    long long end = get_time();\n    printf(\"Average execution time of kernels: %f (s)\\n\",\n           elapsed_time(start, end) / (Nfr-1));\n\n  } \n\n\n  long long offload_end = get_time();\n  printf(\"Device offloading time: %lf (s)\\n\", elapsed_time(offload_start, offload_end));\n\n  xe = 0;\n  ye = 0;\n  \n\n  for (x = 0; x < Nparticles; x++) {\n    xe += arrayX[x] * weights[x];\n    ye += arrayY[x] * weights[x];\n  }\n  float distance = sqrt(pow((float) (xe - (int) roundFloat(IszY / 2.0)), 2) + pow((float) (ye - (int) roundFloat(IszX / 2.0)), 2));\n\n  \n\n  FILE *fid;\n  fid=fopen(\"output.txt\", \"w+\");\n  if( fid == NULL ){\n    printf( \"The file was not opened for writing\\n\" );\n    return -1;\n  }\n  fprintf(fid, \"XE: %lf\\n\", xe);\n  fprintf(fid, \"YE: %lf\\n\", ye);\n  fprintf(fid, \"distance: %lf\\n\", distance);\n  fclose(fid);\n\n  \n\n  free(likelihood);\n  free(partial_sums);\n  free(arrayX);\n  free(arrayY);\n  free(xj);\n  free(yj);\n  free(CDF);\n  free(ind);\n  free(u);\n  return 0;\n}\n\nint main(int argc, char * argv[]) {\n\n  const char* usage = \"./main -x <dimX> -y <dimY> -z <Nfr> -np <Nparticles>\";\n  \n\n  if (argc != 9) {\n    printf(\"%s\\n\", usage);\n    return 0;\n  }\n  \n\n  if (strcmp(argv[1], \"-x\") || strcmp(argv[3], \"-y\") || strcmp(argv[5], \"-z\") || strcmp(argv[7], \"-np\")) {\n    printf(\"%s\\n\", usage);\n    return 0;\n  }\n\n  int IszX, IszY, Nfr, Nparticles;\n\n  \n\n  if (sscanf(argv[2], \"%d\", &IszX) == EOF) {\n    printf(\"ERROR: dimX input is incorrect\");\n    return 0;\n  }\n\n  if (IszX <= 0) {\n    printf(\"dimX must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[4], \"%d\", &IszY) == EOF) {\n    printf(\"ERROR: dimY input is incorrect\");\n    return 0;\n  }\n\n  if (IszY <= 0) {\n    printf(\"dimY must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[6], \"%d\", &Nfr) == EOF) {\n    printf(\"ERROR: Number of frames input is incorrect\");\n    return 0;\n  }\n\n  if (Nfr <= 0) {\n    printf(\"number of frames must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[8], \"%d\", &Nparticles) == EOF) {\n    printf(\"ERROR: Number of particles input is incorrect\");\n    return 0;\n  }\n\n  if (Nparticles <= 0) {\n    printf(\"Number of particles must be > 0\\n\");\n    return 0;\n  }\n\n#ifdef DEBUG\n  printf(\"dimX=%d dimY=%d Nfr=%d Nparticles=%d\\n\", \n      IszX, IszY, Nfr, Nparticles);\n#endif\n\n  \n\n  int * seed = (int *) calloc(Nparticles, sizeof(int));\n  int i;\n  for (i = 0; i < Nparticles; i++)\n    seed[i] = i+1;\n\n  \n\n  unsigned char * I = (unsigned char *) calloc(IszX * IszY * Nfr, sizeof(unsigned char));\n  long long start = get_time();\n\n  \n\n  videoSequence(I, IszX, IszY, Nfr, seed);\n  long long endVideoSequence = get_time();\n  printf(\"VIDEO SEQUENCE TOOK %f (s)\\n\", elapsed_time(start, endVideoSequence));\n\n  \n\n  particleFilter(I, IszX, IszY, Nfr, seed, Nparticles);\n  long long endParticleFilter = get_time();\n  printf(\"PARTICLE FILTER TOOK %f (s)\\n\", elapsed_time(endVideoSequence, endParticleFilter));\n\n  printf(\"ENTIRE PROGRAM TOOK %f (s)\\n\", elapsed_time(start, endParticleFilter));\n\n  free(seed);\n  free(I);\n  return 0;\n}\n"}}
{"kernel_name": "particlefilter", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <limits.h>\n#include <math.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <float.h>\n#include <time.h>\n#include <sys/time.h>\n\n#define BLOCK_X 16\n#define BLOCK_Y 16\n#define PI 3.1415926535897932f\n#define A 1103515245\n#define C 12345\n#define M INT_MAX\n#define SCALE_FACTOR 300.0f\n\n#ifndef BLOCK_SIZE \n#define BLOCK_SIZE 256\n#endif\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#ifndef FLT_MAX\n#define FLT_MAX 3.40282347e+38\n#endif\n\n\n\nlong long get_time() {\n  struct timeval tv;\n  gettimeofday(&tv, NULL);\n  return (tv.tv_sec * 1000000) + tv.tv_usec;\n}\n\n\n\nfloat elapsed_time(long long start_time, long long end_time) {\n  return (float) (end_time - start_time) / (1000 * 1000);\n}\n\n\n\nfloat randu(int * seed, int index) {\n  int num = A * seed[index] + C;\n  seed[index] = num % M;\n  return fabs(seed[index] / ((float) M));\n}\n\n\n\nfloat randn(int * seed, int index) {\n  \n\n  float u = randu(seed, index);\n  float v = randu(seed, index);\n  float cosine = cos(2 * PI * v);\n  float rt = -2 * log(u);\n  return sqrt(rt) * cosine;\n}\n\n\n\nfloat roundFloat(float value) {\n  int newValue = (int) (value);\n  if (value - newValue < .5)\n    return newValue;\n  else\n    return newValue++;\n}\n\n\n\nvoid setIf(int testValue, int newValue, unsigned char * array3D, int * dimX, int * dimY, int * dimZ) {\n  int x, y, z;\n  for (x = 0; x < *dimX; x++) {\n    for (y = 0; y < *dimY; y++) {\n      for (z = 0; z < *dimZ; z++) {\n        if (array3D[x * *dimY * *dimZ + y * *dimZ + z] == testValue)\n          array3D[x * *dimY * *dimZ + y * *dimZ + z] = newValue;\n      }\n    }\n  }\n}\n\n\n\nvoid addNoise(unsigned char * array3D, int * dimX, int * dimY, int * dimZ, int * seed) {\n  int x, y, z;\n  for (x = 0; x < *dimX; x++) {\n    for (y = 0; y < *dimY; y++) {\n      for (z = 0; z < *dimZ; z++) {\n        array3D[x * *dimY * *dimZ + y * *dimZ + z] = array3D[x * *dimY * *dimZ + y * *dimZ + z] + (unsigned char) (5 * randn(seed, 0));\n      }\n    }\n  }\n}\n\n\n\nvoid strelDisk(int * disk, int radius) {\n  int diameter = radius * 2 - 1;\n  int x, y;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      float distance = sqrt(pow((float) (x - radius + 1), 2) + pow((float) (y - radius + 1), 2));\n      if (distance < radius)\n        disk[x * diameter + y] = 1;\n      else\n        disk[x * diameter + y] = 0;\n    }\n  }\n}\n\n\n\nvoid dilate_matrix(unsigned char * matrix, int posX, int posY, int posZ, int dimX, int dimY, int dimZ, int error) {\n  int startX = posX - error;\n  while (startX < 0)\n    startX++;\n  int startY = posY - error;\n  while (startY < 0)\n    startY++;\n  int endX = posX + error;\n  while (endX > dimX)\n    endX--;\n  int endY = posY + error;\n  while (endY > dimY)\n    endY--;\n  int x, y;\n  for (x = startX; x < endX; x++) {\n    for (y = startY; y < endY; y++) {\n      float distance = sqrt(pow((float) (x - posX), 2) + pow((float) (y - posY), 2));\n      if (distance < error)\n        matrix[x * dimY * dimZ + y * dimZ + posZ] = 1;\n    }\n  }\n}\n\n\n\nvoid imdilate_disk(unsigned char * matrix, int dimX, int dimY, int dimZ, int error, unsigned char * newMatrix) {\n  int x, y, z;\n  for (z = 0; z < dimZ; z++) {\n    for (x = 0; x < dimX; x++) {\n      for (y = 0; y < dimY; y++) {\n        if (matrix[x * dimY * dimZ + y * dimZ + z] == 1) {\n          dilate_matrix(newMatrix, x, y, z, dimX, dimY, dimZ, error);\n        }\n      }\n    }\n  }\n}\n\n\n\nvoid getneighbors(int * se, int numOnes, int * neighbors, int radius) {\n  int x, y;\n  int neighY = 0;\n  int center = radius - 1;\n  int diameter = radius * 2 - 1;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      if (se[x * diameter + y]) {\n        neighbors[neighY * 2] = (int) (y - center);\n        neighbors[neighY * 2 + 1] = (int) (x - center);\n        neighY++;\n      }\n    }\n  }\n}\n\n\n\nvoid videoSequence(unsigned char * I, int IszX, int IszY, int Nfr, int * seed) {\n  int k;\n  int max_size = IszX * IszY * Nfr;\n  \n\n  int x0 = (int) roundFloat(IszY / 2.0);\n  int y0 = (int) roundFloat(IszX / 2.0);\n  I[x0 * IszY * Nfr + y0 * Nfr + 0] = 1;\n\n  \n\n  int xk, yk, pos;\n  for (k = 1; k < Nfr; k++) {\n    xk = abs(x0 + (k - 1));\n    yk = abs(y0 - 2 * (k - 1));\n    pos = yk * IszY * Nfr + xk * Nfr + k;\n    if (pos >= max_size)\n      pos = 0;\n    I[pos] = 1;\n  }\n\n  \n\n  unsigned char * newMatrix = (unsigned char *) calloc(IszX * IszY * Nfr, sizeof(unsigned char));\n  imdilate_disk(I, IszX, IszY, Nfr, 5, newMatrix);\n  int x, y;\n  for (x = 0; x < IszX; x++) {\n    for (y = 0; y < IszY; y++) {\n      for (k = 0; k < Nfr; k++) {\n        I[x * IszY * Nfr + y * Nfr + k] = newMatrix[x * IszY * Nfr + y * Nfr + k];\n      }\n    }\n  }\n  free(newMatrix);\n\n  \n\n  setIf(0, 100, I, &IszX, &IszY, &Nfr);\n  setIf(1, 228, I, &IszX, &IszY, &Nfr);\n  \n\n  addNoise(I, &IszX, &IszY, &Nfr, seed);\n\n}\n\n\n\nint findIndex(float * CDF, int lengthCDF, float value) {\n  int index = -1;\n  int x;\n  for (x = 0; x < lengthCDF; x++) {\n    if (CDF[x] >= value) {\n      index = x;\n      break;\n    }\n  }\n  if (index == -1) {\n    return lengthCDF - 1;\n  }\n  return index;\n}\n\n\n\nint particleFilter(unsigned char * I, int IszX, int IszY, int Nfr, int * seed, int Nparticles) {\n  int max_size = IszX * IszY*Nfr;\n  \n\n  float xe = roundFloat(IszY / 2.0);\n  float ye = roundFloat(IszX / 2.0);\n\n  \n\n  int radius = 5;\n  int diameter = radius * 2 - 1;\n  int * disk = (int*) calloc(diameter * diameter, sizeof (int));\n  strelDisk(disk, radius);\n  int countOnes = 0;\n  int x, y;\n  for (x = 0; x < diameter; x++) {\n    for (y = 0; y < diameter; y++) {\n      if (disk[x * diameter + y] == 1)\n        countOnes++;\n    }\n  }\n  int * objxy = (int *) calloc(countOnes * 2, sizeof(int));\n  getneighbors(disk, countOnes, objxy, radius);\n\n  \n\n  float * weights = (float *) calloc(Nparticles, sizeof(float));\n  for (x = 0; x < Nparticles; x++) {\n    weights[x] = 1 / ((float) (Nparticles));\n  }\n  \n\n  float * likelihood = (float *) calloc(Nparticles + 1, sizeof (float));\n  float * partial_sums = (float *) calloc(Nparticles + 1, sizeof (float));\n  float * arrayX = (float *) calloc(Nparticles, sizeof (float));\n  float * arrayY = (float *) calloc(Nparticles, sizeof (float));\n  float * xj = (float *) calloc(Nparticles, sizeof (float));\n  float * yj = (float *) calloc(Nparticles, sizeof (float));\n  float * CDF = (float *) calloc(Nparticles, sizeof(float));\n\n\n  \n\n  int * ind = (int*) calloc(countOnes * Nparticles, sizeof(int));\n  float * u = (float *) calloc(Nparticles, sizeof(float));\n\n  \n\n  \n\n  \n\n  for (x = 0; x < Nparticles; x++) {\n\n    xj[x] = xe;\n    yj[x] = ye;\n  }\n\n  long long offload_start = get_time();\n\n\n  int k;\n\n  int num_blocks = (Nparticles + BLOCK_SIZE - 1) / BLOCK_SIZE;\n#ifdef DEBUG\n  printf(\"BLOCK_SIZE=%d \\n\",BLOCK_SIZE);\n#endif\n\n  {\n    long long start = get_time();\n\n    for (k = 1; k < Nfr; k++) {\n      \n\n            {\n        float weights_local[BLOCK_SIZE];\n                {\n          int block_id = omp_get_team_num();\n          int thread_id = omp_get_thread_num();\n          int block_dim = omp_get_num_threads();\n          int i = block_id * block_dim + thread_id;\n          int y;\n          int indX, indY;\n          float u, v;\n\n          if(i < Nparticles){\n            arrayX[i] = xj[i];\n            arrayY[i] = yj[i];\n            weights[i] = 1.0f / ((float) (Nparticles)); \n            seed[i] = (A*seed[i] + C) % M;\n            u = fabsf(seed[i]/((float)M));\n            seed[i] = (A*seed[i] + C) % M;\n            v = fabsf(seed[i]/((float)M));\n            arrayX[i] += 1.0f + 5.0f*(sqrtf(-2.0f*logf(u))*cosf(2.0f*PI*v));\n\n            seed[i] = (A*seed[i] + C) % M;\n            u = fabsf(seed[i]/((float)M));\n            seed[i] = (A*seed[i] + C) % M;\n            v = fabsf(seed[i]/((float)M));\n            arrayY[i] += -2.0f + 2.0f*(sqrtf(-2.0f*logf(u))*cosf(2.0f*PI*v));\n          }\n\n          \n          if(i < Nparticles)\n          {\n            for(y = 0; y < countOnes; y++){\n\n              int iX = arrayX[i];\n              int iY = arrayY[i];\n              int rnd_iX = (arrayX[i] - iX) < .5f ? iX : iX++;\n              int rnd_iY = (arrayY[i] - iY) < .5f ? iY : iY++;\n              indX = rnd_iX + objxy[y*2 + 1];\n              indY = rnd_iY + objxy[y*2];\n\n              ind[i*countOnes + y] = abs(indX*IszY*Nfr + indY*Nfr + k);\n              if(ind[i*countOnes + y] >= max_size)\n                ind[i*countOnes + y] = 0;\n            }\n            float likelihoodSum = 0.0f;\n            for(int x = 0; x < countOnes; x++)\n              likelihoodSum += ((I[ind[i*countOnes + x]] - 100) * (I[ind[i*countOnes + x]] - 100) -\n                  (I[ind[i*countOnes + x]] - 228) * (I[ind[i*countOnes + x]] - 228)) / 50.0f;\n            likelihood[i] = likelihoodSum/countOnes-SCALE_FACTOR;\n\n            weights[i] = weights[i] * expf(likelihood[i]);\n\n          }\n\n          weights_local[thread_id] = (i < Nparticles) ?  weights[i] : 0.f;\n\n          \n          for(unsigned int s=block_dim/2; s>0; s>>=1)\n          {\n            if(thread_id < s)\n            {\n              weights_local[thread_id] += weights_local[thread_id + s];\n            }\n                    }\n          if(thread_id == 0)\n          {\n            partial_sums[block_id] = weights_local[0];\n          }\n        }\n      }\n\n            {\n        float sum = 0;\n        int num_blocks = (Nparticles + BLOCK_SIZE - 1) / BLOCK_SIZE;\n        for (int x = 0; x < num_blocks; x++) {\n          sum += partial_sums[x];\n        }\n        partial_sums[0] = sum;\n      }\n\n#ifdef DEBUG\n      \n\n      printf(\"kernel sum: frame=%d partial_sums[0]=%f\\n\",\n          k, partial_sums[0]);\n#endif\n\n            {\n        float u1;\n        float sumWeights; \n                {\n          int local_id = omp_get_thread_num();\n          int i = omp_get_team_num() * omp_get_num_threads() + local_id;\n          if(0 == local_id)\n            sumWeights = partial_sums[0];\n\n                    if(i < Nparticles) {\n            weights[i] = weights[i]/sumWeights;\n          }\n\n                    if(i == 0) {\n            CDF[0] = weights[0];\n            for(int x = 1; x < Nparticles; x++){\n              CDF[x] = weights[x] + CDF[x-1];\n            }\n\n            seed[i] = (A*seed[i] + C) % M;\n            float p = fabsf(seed[i]/((float)M));\n            seed[i] = (A*seed[i] + C) % M;\n            float q = fabsf(seed[i]/((float)M));\n            u[0] = (1.0f/((float)(Nparticles))) * \n              (sqrtf(-2.0f*logf(p))*cosf(2.0f*PI*q));\n            \n\n          }\n\n                    if(0 == local_id)\n            u1 = u[0];\n\n                    if(i < Nparticles)\n          {\n            u[i] = u1 + i/((float)(Nparticles));\n          }\n        }\n      }\n\n#ifdef DEBUG\n\n\n      xe = 0;\n      ye = 0;\n      float total=0.0;\n      \n\n      for (x = 0; x < Nparticles; x++) {\n        xe += arrayX[x] * weights[x];\n        ye += arrayY[x] * weights[x];\n        total+= weights[x];\n      }\n      printf(\"total weight: %lf\\n\", total);\n      printf(\"XE: %lf\\n\", xe);\n      printf(\"YE: %lf\\n\", ye);\n      float distance = sqrt(pow((float) (xe - (int) roundFloat(IszY / 2.0)), 2) + pow((float) (ye - (int) roundFloat(IszX / 2.0)), 2));\n      printf(\"distance: %lf\\n\", distance);\n#endif\n\n            for (int i = 0; i < Nparticles; i++)\n      {\n        int index = -1;\n        int x;\n\n        for(x = 0; x < Nparticles; x++){\n          if(CDF[x] >= u[i]){\n            index = x;\n            break;\n          }\n        }\n        if(index == -1){\n          index = Nparticles-1;\n        }\n\n        xj[i] = arrayX[index];\n        yj[i] = arrayY[index];\n      }\n    }\n\n\n    long long end = get_time();\n    printf(\"Average execution time of kernels: %f (s)\\n\",\n           elapsed_time(start, end) / (Nfr-1));\n\n  } \n\n\n  long long offload_end = get_time();\n  printf(\"Device offloading time: %lf (s)\\n\", elapsed_time(offload_start, offload_end));\n\n  xe = 0;\n  ye = 0;\n  \n\n  for (x = 0; x < Nparticles; x++) {\n    xe += arrayX[x] * weights[x];\n    ye += arrayY[x] * weights[x];\n  }\n  float distance = sqrt(pow((float) (xe - (int) roundFloat(IszY / 2.0)), 2) + pow((float) (ye - (int) roundFloat(IszX / 2.0)), 2));\n\n  \n\n  FILE *fid;\n  fid=fopen(\"output.txt\", \"w+\");\n  if( fid == NULL ){\n    printf( \"The file was not opened for writing\\n\" );\n    return -1;\n  }\n  fprintf(fid, \"XE: %lf\\n\", xe);\n  fprintf(fid, \"YE: %lf\\n\", ye);\n  fprintf(fid, \"distance: %lf\\n\", distance);\n  fclose(fid);\n\n  \n\n  free(likelihood);\n  free(partial_sums);\n  free(arrayX);\n  free(arrayY);\n  free(xj);\n  free(yj);\n  free(CDF);\n  free(ind);\n  free(u);\n  return 0;\n}\n\nint main(int argc, char * argv[]) {\n\n  const char* usage = \"./main -x <dimX> -y <dimY> -z <Nfr> -np <Nparticles>\";\n  \n\n  if (argc != 9) {\n    printf(\"%s\\n\", usage);\n    return 0;\n  }\n  \n\n  if (strcmp(argv[1], \"-x\") || strcmp(argv[3], \"-y\") || strcmp(argv[5], \"-z\") || strcmp(argv[7], \"-np\")) {\n    printf(\"%s\\n\", usage);\n    return 0;\n  }\n\n  int IszX, IszY, Nfr, Nparticles;\n\n  \n\n  if (sscanf(argv[2], \"%d\", &IszX) == EOF) {\n    printf(\"ERROR: dimX input is incorrect\");\n    return 0;\n  }\n\n  if (IszX <= 0) {\n    printf(\"dimX must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[4], \"%d\", &IszY) == EOF) {\n    printf(\"ERROR: dimY input is incorrect\");\n    return 0;\n  }\n\n  if (IszY <= 0) {\n    printf(\"dimY must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[6], \"%d\", &Nfr) == EOF) {\n    printf(\"ERROR: Number of frames input is incorrect\");\n    return 0;\n  }\n\n  if (Nfr <= 0) {\n    printf(\"number of frames must be > 0\\n\");\n    return 0;\n  }\n\n  \n\n  if (sscanf(argv[8], \"%d\", &Nparticles) == EOF) {\n    printf(\"ERROR: Number of particles input is incorrect\");\n    return 0;\n  }\n\n  if (Nparticles <= 0) {\n    printf(\"Number of particles must be > 0\\n\");\n    return 0;\n  }\n\n#ifdef DEBUG\n  printf(\"dimX=%d dimY=%d Nfr=%d Nparticles=%d\\n\", \n      IszX, IszY, Nfr, Nparticles);\n#endif\n\n  \n\n  int * seed = (int *) calloc(Nparticles, sizeof(int));\n  int i;\n  for (i = 0; i < Nparticles; i++)\n    seed[i] = i+1;\n\n  \n\n  unsigned char * I = (unsigned char *) calloc(IszX * IszY * Nfr, sizeof(unsigned char));\n  long long start = get_time();\n\n  \n\n  videoSequence(I, IszX, IszY, Nfr, seed);\n  long long endVideoSequence = get_time();\n  printf(\"VIDEO SEQUENCE TOOK %f (s)\\n\", elapsed_time(start, endVideoSequence));\n\n  \n\n  particleFilter(I, IszX, IszY, Nfr, seed, Nparticles);\n  long long endParticleFilter = get_time();\n  printf(\"PARTICLE FILTER TOOK %f (s)\\n\", elapsed_time(endVideoSequence, endParticleFilter));\n\n  printf(\"ENTIRE PROGRAM TOOK %f (s)\\n\", elapsed_time(start, endParticleFilter));\n\n  free(seed);\n  free(I);\n  return 0;\n}"}}
{"kernel_name": "pathfinder", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include <assert.h>\n#include <iostream>\n#include <sys/time.h>\n#include <cuda.h>\n\n\n\n\n#define HALO     1\n#define STR_SIZE 256\n#define DEVICE   0\n#define M_SEED   9\n#define IN_RANGE(x, min, max)  ((x)>=(min) && (x)<=(max))\n#define CLAMP_RANGE(x, min, max) x = (x<(min)) ? min : ((x>(max)) ? max : x )\n#define MIN(a, b) ((a)<=(b) ? (a) : (b))\n\n\nvoid fatal(char *s)\n{\n  fprintf(stderr, \"error: %s\\n\", s);\n}\n\ndouble get_time() {\n  struct timeval t;\n  gettimeofday(&t,NULL);\n  return t.tv_sec+t.tv_usec*1e-6;\n}\n\n__global__ void pathfinder (\n    const int*__restrict__ gpuWall,\n    const int*__restrict__ gpuSrc,\n          int*__restrict__ gpuResult,\n          int*__restrict__ outputBuffer,\n    const int iteration,\n    const int theHalo,\n    const int borderCols,\n    const int cols,\n    const int t)\n{\n  int BLOCK_SIZE = blockDim.x;\n  int bx = blockIdx.x;\n  int tx = threadIdx.x;\n  __shared__ int prev[250];\n  __shared__ int result[250];\n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  int small_block_cols = BLOCK_SIZE - (iteration*theHalo*2);\n\n  \n\n  \n\n  int blkX = (small_block_cols*bx) - borderCols;\n  int blkXmax = blkX+BLOCK_SIZE-1;\n\n  \n\n  int xidx = blkX+tx;\n\n  \n\n  \n\n  \n\n  int validXmin = (blkX < 0) ? -blkX : 0;\n  int validXmax = (blkXmax > cols-1) ? BLOCK_SIZE-1-(blkXmax-cols+1) : BLOCK_SIZE-1;\n\n  int W = tx-1;\n  int E = tx+1;\n\n  W = (W < validXmin) ? validXmin : W;\n  E = (E > validXmax) ? validXmax : E;\n\n  bool isValid = IN_RANGE(tx, validXmin, validXmax);\n\n  if(IN_RANGE(xidx, 0, cols-1))\n  {\n    prev[tx] = gpuSrc[xidx];\n  }\n\n  __syncthreads();\n\n  bool computed;\n  for (int i = 0; i < iteration; i++)\n  {\n    computed = false;\n\n    if( IN_RANGE(tx, i+1, BLOCK_SIZE-i-2) && isValid )\n    {\n      computed = true;\n      int left = prev[W];\n      int up = prev[tx];\n      int right = prev[E];\n      int shortest = MIN(left, up);\n      shortest = MIN(shortest, right);\n\n      int index = cols*(t+i)+xidx;\n      result[tx] = shortest + gpuWall[index];\n\n      \n\n      \n\n      if (tx==11 && i==0)\n      {\n        \n\n        int bufIndex = gpuSrc[xidx];\n        \n\n        outputBuffer[bufIndex] = 1;\n      }\n      \n\n    }\n\n    __syncthreads();\n\n    if(i==iteration-1)\n    {\n      \n\n      \n\n      break;\n    }\n\n    if(computed)\n    {\n      \n\n      prev[tx] = result[tx];\n    }\n    __syncthreads();\n  }\n\n  \n\n  \n\n  \n\n  if (computed)\n  {\n    gpuResult[xidx] = result[tx];\n  }\n}\n\nint main(int argc, char** argv)\n{\n  \n\n  int   rows, cols;\n  int*  data;\n  int** wall;\n  int*  result;\n  int   pyramid_height;\n\n  if (argc == 4)\n  {\n    cols = atoi(argv[1]);\n    rows = atoi(argv[2]);\n    pyramid_height = atoi(argv[3]);\n  }\n  else\n  {\n    printf(\"Usage: %s <column length> <row length> <pyramid_height>\\n\", argv[0]);\n\n    exit(0);\n  }\n\n  data = new int[rows * cols];\n  wall = new int*[rows];\n  for (int n = 0; n < rows; n++)\n  {\n    \n\n    wall[n] = data + cols * n;\n  }\n  result = new int[cols];\n\n  int seed = M_SEED;\n  srand(seed);\n\n  for (int i = 0; i < rows; i++)\n  {\n    for (int j = 0; j < cols; j++)\n    {\n      wall[i][j] = rand() % 10;\n    }\n  }\n#ifdef BENCH_PRINT\n  for (int i = 0; i < rows; i++)\n  {\n    for (int j = 0; j < cols; j++)\n    {\n      printf(\"%d \", wall[i][j]);\n    }\n    printf(\"\\n\");\n  }\n#endif\n\n  \n\n  const int borderCols = (pyramid_height) * HALO;\n\n  \n\n\n  int size = rows * cols; \n\n\n  \n\n  int lws = 250;\n  int* outputBuffer = (int*)calloc(16384, sizeof(int));\n  int theHalo = HALO;\n\n  double offload_start = get_time();\n\n  int* d_gpuWall;\n  cudaMalloc((void**)&d_gpuWall, sizeof(int)*(size-cols));\n  cudaMemcpy(d_gpuWall, data+cols, sizeof(int)*(size-cols), cudaMemcpyHostToDevice);\n\n  int* d_gpuSrc;\n  cudaMalloc((void**)&d_gpuSrc, sizeof(int)*cols);\n  cudaMemcpy(d_gpuSrc, data, sizeof(int)*cols, cudaMemcpyHostToDevice);\n\n  int* d_gpuResult;\n  cudaMalloc((void**)&d_gpuResult, sizeof(int)*cols);\n\n  int* d_outputBuffer;\n  cudaMalloc((void**)&d_outputBuffer, sizeof(int)*16384);\n\n  dim3 gridDim (size/lws);\n  dim3 blockDim (lws);\n\n  cudaDeviceSynchronize();\n  double kstart = get_time();\n\n  for (int t = 0; t < rows - 1; t += pyramid_height)\n  {\n    \n\n    int iteration = MIN(pyramid_height, rows-t-1);\n\n    pathfinder<<<gridDim, blockDim>>>(\n        d_gpuWall, d_gpuSrc, d_gpuResult, d_outputBuffer, \n        iteration, theHalo, borderCols, cols, t);\n\n    int* temp = d_gpuResult;\n    d_gpuResult = d_gpuSrc;\n    d_gpuSrc = temp;\n  }\n\n  cudaDeviceSynchronize();\n  double kend = get_time();\n  printf(\"Total kernel execution time: %lf (s)\\n\", kend - kstart);\n\n  cudaMemcpy(result, d_gpuSrc, sizeof(int)*cols, cudaMemcpyDeviceToHost);\n  cudaMemcpy(outputBuffer, d_outputBuffer, sizeof(int)*16348, cudaMemcpyDeviceToHost);\n\n  cudaFree(d_gpuResult);\n  cudaFree(d_gpuSrc);\n  cudaFree(d_gpuWall);\n  cudaFree(d_outputBuffer);\n\n  double offload_end = get_time();\n  printf(\"Device offloading time = %lf (s)\\n\", offload_end - offload_start);\n\n  \n\n  outputBuffer[16383] = '\\0';\n\n#ifdef BENCH_PRINT\n  for (int i = 0; i < cols; i++)\n    printf(\"%d \", data[i]);\n  printf(\"\\n\");\n  for (int i = 0; i < cols; i++)\n    printf(\"%d \", result[i]);\n  printf(\"\\n\");\n#endif\n\n  \n\n  delete[] data;\n  delete[] wall;\n  delete[] result;\n  free(outputBuffer);\n\n  return EXIT_SUCCESS;\n}\n"}}
{"kernel_name": "pathfinder", "parallel_api": "hip", "code": {"main.cu": "\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include <assert.h>\n#include <iostream>\n#include <sys/time.h>\n#include <hip/hip_runtime.h>\n\n\n\n\n#define HALO     1\n#define STR_SIZE 256\n#define DEVICE   0\n#define M_SEED   9\n#define IN_RANGE(x, min, max)  ((x)>=(min) && (x)<=(max))\n#define CLAMP_RANGE(x, min, max) x = (x<(min)) ? min : ((x>(max)) ? max : x )\n#define MIN(a, b) ((a)<=(b) ? (a) : (b))\n\n\nvoid fatal(char *s)\n{\n  fprintf(stderr, \"error: %s\\n\", s);\n}\n\ndouble get_time() {\n  struct timeval t;\n  gettimeofday(&t,NULL);\n  return t.tv_sec+t.tv_usec*1e-6;\n}\n\n__global__ void pathfinder (\n    const int*__restrict__ gpuWall,\n    const int*__restrict__ gpuSrc,\n          int*__restrict__ gpuResult,\n          int*__restrict__ outputBuffer,\n    const int iteration,\n    const int theHalo,\n    const int borderCols,\n    const int cols,\n    const int t)\n{\n  int BLOCK_SIZE = blockDim.x;\n  int bx = blockIdx.x;\n  int tx = threadIdx.x;\n  __shared__ int prev[250];\n  __shared__ int result[250];\n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  int small_block_cols = BLOCK_SIZE - (iteration*theHalo*2);\n\n  \n\n  \n\n  int blkX = (small_block_cols*bx) - borderCols;\n  int blkXmax = blkX+BLOCK_SIZE-1;\n\n  \n\n  int xidx = blkX+tx;\n\n  \n\n  \n\n  \n\n  int validXmin = (blkX < 0) ? -blkX : 0;\n  int validXmax = (blkXmax > cols-1) ? BLOCK_SIZE-1-(blkXmax-cols+1) : BLOCK_SIZE-1;\n\n  int W = tx-1;\n  int E = tx+1;\n\n  W = (W < validXmin) ? validXmin : W;\n  E = (E > validXmax) ? validXmax : E;\n\n  bool isValid = IN_RANGE(tx, validXmin, validXmax);\n\n  if(IN_RANGE(xidx, 0, cols-1))\n  {\n    prev[tx] = gpuSrc[xidx];\n  }\n\n  __syncthreads();\n\n  bool computed;\n  for (int i = 0; i < iteration; i++)\n  {\n    computed = false;\n\n    if( IN_RANGE(tx, i+1, BLOCK_SIZE-i-2) && isValid )\n    {\n      computed = true;\n      int left = prev[W];\n      int up = prev[tx];\n      int right = prev[E];\n      int shortest = MIN(left, up);\n      shortest = MIN(shortest, right);\n\n      int index = cols*(t+i)+xidx;\n      result[tx] = shortest + gpuWall[index];\n\n      \n\n      \n\n      if (tx==11 && i==0)\n      {\n        \n\n        int bufIndex = gpuSrc[xidx];\n        \n\n        outputBuffer[bufIndex] = 1;\n      }\n      \n\n    }\n\n    __syncthreads();\n\n    if(i==iteration-1)\n    {\n      \n\n      \n\n      break;\n    }\n\n    if(computed)\n    {\n      \n\n      prev[tx] = result[tx];\n    }\n    __syncthreads();\n  }\n\n  \n\n  \n\n  \n\n  if (computed)\n  {\n    gpuResult[xidx] = result[tx];\n  }\n}\n\nint main(int argc, char** argv)\n{\n  \n\n  int   rows, cols;\n  int*  data;\n  int** wall;\n  int*  result;\n  int   pyramid_height;\n\n  if (argc == 4)\n  {\n    cols = atoi(argv[1]);\n    rows = atoi(argv[2]);\n    pyramid_height = atoi(argv[3]);\n  }\n  else\n  {\n    printf(\"Usage: %s <column length> <row length> <pyramid_height>\\n\", argv[0]);\n\n    exit(0);\n  }\n\n  data = new int[rows * cols];\n  wall = new int*[rows];\n  for (int n = 0; n < rows; n++)\n  {\n    \n\n    wall[n] = data + cols * n;\n  }\n  result = new int[cols];\n\n  int seed = M_SEED;\n  srand(seed);\n\n  for (int i = 0; i < rows; i++)\n  {\n    for (int j = 0; j < cols; j++)\n    {\n      wall[i][j] = rand() % 10;\n    }\n  }\n#ifdef BENCH_PRINT\n  for (int i = 0; i < rows; i++)\n  {\n    for (int j = 0; j < cols; j++)\n    {\n      printf(\"%d \", wall[i][j]);\n    }\n    printf(\"\\n\");\n  }\n#endif\n\n  \n\n  const int borderCols = (pyramid_height) * HALO;\n\n  \n\n\n  int size = rows * cols; \n\n\n  \n\n  int lws = 250;\n  int* outputBuffer = (int*)calloc(16384, sizeof(int));\n  int theHalo = HALO;\n\n  double offload_start = get_time();\n\n  int* d_gpuWall;\n  hipMalloc((void**)&d_gpuWall, sizeof(int)*(size-cols));\n  hipMemcpy(d_gpuWall, data+cols, sizeof(int)*(size-cols), hipMemcpyHostToDevice);\n\n  int* d_gpuSrc;\n  hipMalloc((void**)&d_gpuSrc, sizeof(int)*cols);\n  hipMemcpy(d_gpuSrc, data, sizeof(int)*cols, hipMemcpyHostToDevice);\n\n  int* d_gpuResult;\n  hipMalloc((void**)&d_gpuResult, sizeof(int)*cols);\n\n  int* d_outputBuffer;\n  hipMalloc((void**)&d_outputBuffer, sizeof(int)*16384);\n\n  dim3 gridDim (size/lws);\n  dim3 blockDim (lws);\n\n  double kstart = 0.0;\n\n  for (int t = 0; t < rows - 1; t += pyramid_height)\n  {\n    if (t == pyramid_height) {\n      hipDeviceSynchronize();\n      kstart = get_time();\n    }\n\n    \n\n    int iteration = MIN(pyramid_height, rows-t-1);\n\n    hipLaunchKernelGGL(pathfinder, gridDim, blockDim, 0, 0, \n        d_gpuWall, d_gpuSrc, d_gpuResult, d_outputBuffer, \n        iteration, theHalo, borderCols, cols, t);\n\n    int* temp = d_gpuResult;\n    d_gpuResult = d_gpuSrc;\n    d_gpuSrc = temp;\n  }\n\n  hipDeviceSynchronize();\n  double kend = get_time();\n  printf(\"Total kernel execution time: %lf (s)\\n\", kend - kstart);\n\n  hipMemcpy(result, d_gpuSrc, sizeof(int)*cols, hipMemcpyDeviceToHost);\n  hipMemcpy(outputBuffer, d_outputBuffer, sizeof(int)*16348, hipMemcpyDeviceToHost);\n\n  hipFree(d_gpuResult);\n  hipFree(d_gpuSrc);\n  hipFree(d_gpuWall);\n  hipFree(d_outputBuffer);\n\n  double offload_end = get_time();\n  printf(\"Device offloading time = %lf (s)\\n\", offload_end - offload_start);\n\n  \n\n  outputBuffer[16383] = '\\0';\n\n#ifdef BENCH_PRINT\n  for (int i = 0; i < cols; i++)\n    printf(\"%d \", data[i]);\n  printf(\"\\n\");\n  for (int i = 0; i < cols; i++)\n    printf(\"%d \", result[i]);\n  printf(\"\\n\");\n#endif\n\n  \n\n  delete[] data;\n  delete[] wall;\n  delete[] result;\n  free(outputBuffer);\n\n  return EXIT_SUCCESS;\n}\n"}}
{"kernel_name": "pathfinder", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include <assert.h>\n#include <iostream>\n#include <sys/time.h>\n#include <string.h>\n#include <omp.h>\n\nusing namespace std;\n\n\n\n#define HALO     1\n#define STR_SIZE 256\n#define DEVICE   0\n#define M_SEED   9\n#define IN_RANGE(x, min, max)  ((x)>=(min) && (x)<=(max))\n#define CLAMP_RANGE(x, min, max) x = (x<(min)) ? min : ((x>(max)) ? max : x )\n#define MIN(a, b) ((a)<=(b) ? (a) : (b))\n\n\nvoid fatal(char *s)\n{\n  fprintf(stderr, \"error: %s\\n\", s);\n}\n\ndouble get_time() {\n  struct timeval t;\n  gettimeofday(&t,NULL);\n  return t.tv_sec+t.tv_usec*1e-6;\n}\n\nint main(int argc, char** argv)\n{\n  \n\n  int   rows, cols;\n  int*  data;\n  int** wall;\n  int*  result;\n  int   pyramid_height;\n\n  if (argc == 4)\n  {\n    cols = atoi(argv[1]);\n    rows = atoi(argv[2]);\n    pyramid_height = atoi(argv[3]);\n  }\n  else\n  {\n    printf(\"Usage: %s <column length> <row length> <pyramid_height>\\n\", argv[0]);\n\n    exit(0);\n  }\n\n  data = new int[rows * cols];\n  wall = new int*[rows];\n  for (int n = 0; n < rows; n++)\n  {\n    \n\n    wall[n] = data + cols * n;\n  }\n  result = new int[cols];\n\n  int seed = M_SEED;\n  srand(seed);\n\n  for (int i = 0; i < rows; i++)\n  {\n    for (int j = 0; j < cols; j++)\n    {\n      wall[i][j] = rand() % 10;\n    }\n  }\n#ifdef BENCH_PRINT\n  for (int i = 0; i < rows; i++)\n  {\n    for (int j = 0; j < cols; j++)\n    {\n      printf(\"%d \", wall[i][j]);\n    }\n    printf(\"\\n\");\n  }\n#endif\n\n  \n\n  const int borderCols = (pyramid_height) * HALO;\n\n  \n\n\n  const int size = rows * cols;  \n\n  const int lws = 250;\n  const int gws = size/lws;  \n\n  int theHalo = HALO;\n  int* outputBuffer = (int*)calloc(16384, sizeof(int));\n\n  double offload_start = get_time();\n\n  \n\n  int* gpuWall = data+cols;\n  \n\n  \n\n  int* gpuSrc = (int*) malloc (sizeof(int)*cols);\n  int* gpuResult = (int*) malloc (sizeof(int)*cols);\n  memcpy(gpuSrc, data, cols*sizeof(int));\n\n#pragma omp target data map(to: gpuSrc[0:cols]) \\\n                        map(alloc: gpuResult[0:cols]) \\\n                        map(to: gpuWall[0:size-cols]) \\\n                        map(from: outputBuffer[0:16384])\n  {\n    double kstart = 0.0;\n\n    for (int t = 0; t < rows - 1; t += pyramid_height)\n    {\n      if (t == pyramid_height) {\n        kstart = get_time();\n      }\n\n      \n\n      int iteration = MIN(pyramid_height, rows-t-1);\n\n      #pragma omp target teams num_teams(gws) thread_limit(lws)\n      {\n        int prev[lws];\n        int result[lws];\n        #pragma omp parallel \n        {\n          \n\n          int BLOCK_SIZE = omp_get_num_threads();\n          int bx = omp_get_team_num();\n          int tx = omp_get_thread_num();\n\n          \n\n          \n\n          \n\n          \n\n\n          \n\n          int small_block_cols = BLOCK_SIZE - (iteration*theHalo*2);\n\n          \n\n          \n\n          int blkX = (small_block_cols*bx) - borderCols;\n          int blkXmax = blkX+BLOCK_SIZE-1;\n\n          \n\n          int xidx = blkX+tx;\n\n          \n\n          \n\n          \n\n          int validXmin = (blkX < 0) ? -blkX : 0;\n          int validXmax = (blkXmax > cols-1) ? BLOCK_SIZE-1-(blkXmax-cols+1) : BLOCK_SIZE-1;\n\n          int W = tx-1;\n          int E = tx+1;\n\n          W = (W < validXmin) ? validXmin : W;\n          E = (E > validXmax) ? validXmax : E;\n\n          bool isValid = IN_RANGE(tx, validXmin, validXmax);\n\n          if(IN_RANGE(xidx, 0, cols-1))\n          {\n            prev[tx] = gpuSrc[xidx];\n          }\n\n          #pragma omp barrier\n\n          bool computed;\n          for (int i = 0; i < iteration; i++)\n          {\n            computed = false;\n\n            if( IN_RANGE(tx, i+1, BLOCK_SIZE-i-2) && isValid )\n            {\n              computed = true;\n              int left = prev[W];\n              int up = prev[tx];\n              int right = prev[E];\n              int shortest = MIN(left, up);\n              shortest = MIN(shortest, right);\n\n              int index = cols*(t+i)+xidx;\n              result[tx] = shortest + gpuWall[index];\n\n              \n\n              \n\n              if (tx==11 && i==0)\n              {\n                \n\n                int bufIndex = gpuSrc[xidx];\n                \n\n                outputBuffer[bufIndex] = 1;\n              }\n              \n\n            }\n\n            #pragma omp barrier\n\n            if(i==iteration-1)\n            {\n              \n\n              \n\n              break;\n            }\n\n            if(computed)\n            {\n              \n\n              prev[tx] = result[tx];\n            }\n            #pragma omp barrier\n          }\n\n          \n\n          \n\n          \n\n          if (computed)\n          {\n            gpuResult[xidx] = result[tx];\n          }\n        }\n      } \n      int *temp = gpuResult;\n      gpuResult = gpuSrc;\n      gpuSrc = temp;\n    }\n\n    double kend = get_time();\n    printf(\"Total kernel execution time: %lf (s)\\n\", kend - kstart);\n\n    #pragma omp target update from(gpuSrc[0:cols])\n  }\n\n  double offload_end = get_time();\n  printf(\"Device offloading time = %lf(s)\\n\", offload_end - offload_start);\n\n  \n\n  outputBuffer[16383] = '\\0';\n\n#ifdef BENCH_PRINT\n  for (int i = 0; i < cols; i++)\n    printf(\"%d \", data[i]);\n  printf(\"\\n\");\n  for (int i = 0; i < cols; i++)\n    printf(\"%d \", gpuSrc[i]);\n  printf(\"\\n\");\n#endif\n\n  \n\n  delete[] data;\n  delete[] wall;\n  delete[] result;\n  free(outputBuffer);\n  free(gpuSrc);\n  free(gpuResult);\n\n  return EXIT_SUCCESS;\n}\n"}}
{"kernel_name": "pathfinder", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include <assert.h>\n#include <iostream>\n#include <sys/time.h>\n#include <string.h>\n\nusing namespace std;\n\n\n\n#define HALO     1\n#define STR_SIZE 256\n#define DEVICE   0\n#define M_SEED   9\n#define IN_RANGE(x, min, max)  ((x)>=(min) && (x)<=(max))\n#define CLAMP_RANGE(x, min, max) x = (x<(min)) ? min : ((x>(max)) ? max : x )\n#define MIN(a, b) ((a)<=(b) ? (a) : (b))\n\n\nvoid fatal(char *s)\n{\n  fprintf(stderr, \"error: %s\\n\", s);\n}\n\ndouble get_time() {\n  struct timeval t;\n  gettimeofday(&t,NULL);\n  return t.tv_sec+t.tv_usec*1e-6;\n}\n\nint main(int argc, char** argv)\n{\n  \n\n  int   rows, cols;\n  int*  data;\n  int** wall;\n  int*  result;\n  int   pyramid_height;\n\n  if (argc == 4)\n  {\n    cols = atoi(argv[1]);\n    rows = atoi(argv[2]);\n    pyramid_height = atoi(argv[3]);\n  }\n  else\n  {\n    printf(\"Usage: %s <column length> <row length> <pyramid_height>\\n\", argv[0]);\n\n    exit(0);\n  }\n\n  data = new int[rows * cols];\n  wall = new int*[rows];\n  for (int n = 0; n < rows; n++)\n  {\n    \n\n    wall[n] = data + cols * n;\n  }\n  result = new int[cols];\n\n  int seed = M_SEED;\n  srand(seed);\n\n  for (int i = 0; i < rows; i++)\n  {\n    for (int j = 0; j < cols; j++)\n    {\n      wall[i][j] = rand() % 10;\n    }\n  }\n#ifdef BENCH_PRINT\n  for (int i = 0; i < rows; i++)\n  {\n    for (int j = 0; j < cols; j++)\n    {\n      printf(\"%d \", wall[i][j]);\n    }\n    printf(\"\\n\");\n  }\n#endif\n\n  \n\n  const int borderCols = (pyramid_height) * HALO;\n\n  \n\n\n  const int size = rows * cols;  \n\n  const int lws = 250;\n  const int gws = size/lws;  \n\n  int theHalo = HALO;\n  int* outputBuffer = (int*)calloc(16384, sizeof(int));\n\n  double offload_start = get_time();\n\n  \n\n  int* gpuWall = data+cols;\n  \n\n  \n\n  int* gpuSrc = (int*) malloc (sizeof(int)*cols);\n  int* gpuResult = (int*) malloc (sizeof(int)*cols);\n  memcpy(gpuSrc, data, cols*sizeof(int));\n\n  {\n    double kstart = 0.0;\n\n    for (int t = 0; t < rows - 1; t += pyramid_height)\n    {\n      if (t == pyramid_height) {\n        kstart = get_time();\n      }\n\n      \n\n      int iteration = MIN(pyramid_height, rows-t-1);\n\n            {\n        int prev[lws];\n        int result[lws];\n                {\n          \n\n          int BLOCK_SIZE = omp_get_num_threads();\n          int bx = omp_get_team_num();\n          int tx = omp_get_thread_num();\n\n          \n\n          \n\n          \n\n          \n\n\n          \n\n          int small_block_cols = BLOCK_SIZE - (iteration*theHalo*2);\n\n          \n\n          \n\n          int blkX = (small_block_cols*bx) - borderCols;\n          int blkXmax = blkX+BLOCK_SIZE-1;\n\n          \n\n          int xidx = blkX+tx;\n\n          \n\n          \n\n          \n\n          int validXmin = (blkX < 0) ? -blkX : 0;\n          int validXmax = (blkXmax > cols-1) ? BLOCK_SIZE-1-(blkXmax-cols+1) : BLOCK_SIZE-1;\n\n          int W = tx-1;\n          int E = tx+1;\n\n          W = (W < validXmin) ? validXmin : W;\n          E = (E > validXmax) ? validXmax : E;\n\n          bool isValid = IN_RANGE(tx, validXmin, validXmax);\n\n          if(IN_RANGE(xidx, 0, cols-1))\n          {\n            prev[tx] = gpuSrc[xidx];\n          }\n\n          \n          bool computed;\n          for (int i = 0; i < iteration; i++)\n          {\n            computed = false;\n\n            if( IN_RANGE(tx, i+1, BLOCK_SIZE-i-2) && isValid )\n            {\n              computed = true;\n              int left = prev[W];\n              int up = prev[tx];\n              int right = prev[E];\n              int shortest = MIN(left, up);\n              shortest = MIN(shortest, right);\n\n              int index = cols*(t+i)+xidx;\n              result[tx] = shortest + gpuWall[index];\n\n              \n\n              \n\n              if (tx==11 && i==0)\n              {\n                \n\n                int bufIndex = gpuSrc[xidx];\n                \n\n                outputBuffer[bufIndex] = 1;\n              }\n              \n\n            }\n\n            \n            if(i==iteration-1)\n            {\n              \n\n              \n\n              break;\n            }\n\n            if(computed)\n            {\n              \n\n              prev[tx] = result[tx];\n            }\n                      }\n\n          \n\n          \n\n          \n\n          if (computed)\n          {\n            gpuResult[xidx] = result[tx];\n          }\n        }\n      } \n      int *temp = gpuResult;\n      gpuResult = gpuSrc;\n      gpuSrc = temp;\n    }\n\n    double kend = get_time();\n    printf(\"Total kernel execution time: %lf (s)\\n\", kend - kstart);\n\n      }\n\n  double offload_end = get_time();\n  printf(\"Device offloading time = %lf(s)\\n\", offload_end - offload_start);\n\n  \n\n  outputBuffer[16383] = '\\0';\n\n#ifdef BENCH_PRINT\n  for (int i = 0; i < cols; i++)\n    printf(\"%d \", data[i]);\n  printf(\"\\n\");\n  for (int i = 0; i < cols; i++)\n    printf(\"%d \", gpuSrc[i]);\n  printf(\"\\n\");\n#endif\n\n  \n\n  delete[] data;\n  delete[] wall;\n  delete[] result;\n  free(outputBuffer);\n  free(gpuSrc);\n  free(gpuResult);\n\n  return EXIT_SUCCESS;\n}"}}
{"kernel_name": "pathfinder", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include <assert.h>\n#include <iostream>\n#include <sys/time.h>\n#include <sycl/sycl.hpp>\n\n\n\n\n#define HALO     1\n#define STR_SIZE 256\n#define DEVICE   0\n#define M_SEED   9\n#define IN_RANGE(x, min, max)\t((x)>=(min) && (x)<=(max))\n#define CLAMP_RANGE(x, min, max) x = (x<(min)) ? min : ((x>(max)) ? max : x )\n#define MIN(a, b) ((a)<=(b) ? (a) : (b))\n\n\nvoid fatal(char *s)\n{\n  fprintf(stderr, \"error: %s\\n\", s);\n}\n\ndouble get_time() {\n  struct timeval t;\n  gettimeofday(&t,NULL);\n  return t.tv_sec+t.tv_usec*1e-6;\n}\n\nint main(int argc, char** argv)\n{\n  \n\n  int   rows, cols;\n  int*  data;\n  int** wall;\n  int*  result;\n  int   pyramid_height;\n\n  if (argc == 4)\n  {\n    cols = atoi(argv[1]);\n    rows = atoi(argv[2]);\n    pyramid_height = atoi(argv[3]);\n  }\n  else\n  {\n    printf(\"Usage: %s <column length> <row length> <pyramid_height>\\n\", argv[0]);\n\n    exit(0);\n  }\n\n  data = new int[rows * cols];\n  wall = new int*[rows];\n  for (int n = 0; n < rows; n++)\n  {\n    \n\n    wall[n] = data + cols * n;\n  }\n  result = new int[cols];\n\n  int seed = M_SEED;\n  srand(seed);\n\n  for (int i = 0; i < rows; i++)\n  {\n    for (int j = 0; j < cols; j++)\n    {\n      wall[i][j] = rand() % 10;\n    }\n  }\n#ifdef BENCH_PRINT\n  for (int i = 0; i < rows; i++)\n  {\n    for (int j = 0; j < cols; j++)\n    {\n      printf(\"%d \", wall[i][j]);\n    }\n    printf(\"\\n\");\n  }\n#endif\n\n  \n\n  const int borderCols = (pyramid_height) * HALO;\n\n  \n\n\n  int size = rows * cols; \n\n\n  \n\n#ifdef USE_GPU\n  int block_size = 250;\n#else\n  int block_size = 4000;\n#endif\n  int* outputBuffer = (int*)calloc(16384, sizeof(int));\n  int theHalo = HALO;\n\n  double offload_start = get_time();\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  int *d_gpuWall = sycl::malloc_device<int>(size-cols, q);\n  q.memcpy(d_gpuWall, data+cols, sizeof(int)*(size-cols));\n\n  int *d_gpuSrc = sycl::malloc_device<int>(cols, q);\n  q.memcpy(d_gpuSrc, data, sizeof(int)*cols);\n\n  int *d_gpuResult = sycl::malloc_device<int>(cols, q);\n  int *d_outputBuffer = sycl::malloc_device<int>(16384, q);\n\n  sycl::range<1> gws(size);\n  sycl::range<1> lws(block_size);\n\n  q.wait();\n  double kstart = get_time();\n\n  for (int t = 0; t < rows - 1; t += pyramid_height)\n  {\n    \n\n    int iteration = MIN(pyramid_height, rows-t-1);\n\n    q.submit([&](sycl::handler& cgh) {\n      sycl::local_accessor <int, 1> prev (lws, cgh);\n      sycl::local_accessor <int, 1> result (lws, cgh);\n      \n\n      cgh.parallel_for<class dynproc_kernel>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n          #include \"kernel.sycl\"\n      });\n    });\n\n    int* temp = d_gpuResult;\n    d_gpuResult = d_gpuSrc;\n    d_gpuSrc = temp;\n  } \n\n\n  q.wait();\n  double kend = get_time();\n  printf(\"Total kernel execution time: %lf (s)\\n\", kend - kstart);\n\n  q.memcpy(result, d_gpuSrc, sizeof(int)*cols);\n  q.memcpy(outputBuffer, d_outputBuffer, sizeof(int)*16348);\n  q.wait();\n\n  sycl::free(d_gpuResult, q);\n  sycl::free(d_gpuSrc, q);\n  sycl::free(d_gpuWall, q);\n  sycl::free(d_outputBuffer, q);\n\n  double offload_end = get_time();\n  printf(\"Device offloading time = %lf(s)\\n\", offload_end - offload_start);\n\n  \n\n  outputBuffer[16383] = '\\0';\n\n#ifdef BENCH_PRINT\n  for (int i = 0; i < cols; i++)\n    printf(\"%d \", data[i]);\n  printf(\"\\n\");\n  for (int i = 0; i < cols; i++)\n    printf(\"%d \", result[i]);\n  printf(\"\\n\");\n#endif\n\n  \n\n  delete[] data;\n  delete[] wall;\n  delete[] result;\n  free(outputBuffer);\n\n  return EXIT_SUCCESS;\n}\n", "kernel.sycl": "int BLOCK_SIZE = item.get_local_range(0);\nint bx = item.get_group(0);\nint tx = item.get_local_id(0);\n\n\n\n\n\n\n\n\n\n\n\n\nint small_block_cols = BLOCK_SIZE - (iteration*theHalo*2);\n\n\n\n\n\nint blkX = (small_block_cols*bx) - borderCols;\nint blkXmax = blkX+BLOCK_SIZE-1;\n\n\n\nint xidx = blkX+tx;\n\n\n\n\n\n\n\nint validXmin = (blkX < 0) ? -blkX : 0;\nint validXmax = (blkXmax > cols-1) ? BLOCK_SIZE-1-(blkXmax-cols+1) : BLOCK_SIZE-1;\n\nint W = tx-1;\nint E = tx+1;\n\nW = (W < validXmin) ? validXmin : W;\nE = (E > validXmax) ? validXmax : E;\n\nbool isValid = IN_RANGE(tx, validXmin, validXmax);\n\nif(IN_RANGE(xidx, 0, cols-1))\n{\n  prev[tx] = d_gpuSrc[xidx];\n}\n\nitem.barrier(sycl::access::fence_space::local_space);\n\nbool computed;\nfor (int i = 0; i < iteration; i++)\n{\n  computed = false;\n\n  if( IN_RANGE(tx, i+1, BLOCK_SIZE-i-2) && isValid )\n  {\n    computed = true;\n    int left = prev[W];\n    int up = prev[tx];\n    int right = prev[E];\n    int shortest = MIN(left, up);\n    shortest = MIN(shortest, right);\n\n    int index = cols*(t+i)+xidx;\n    result[tx] = shortest + d_gpuWall[index];\n\n    \n\n    \n\n    if (tx==11 && i==0)\n    {\n      \n\n      int bufIndex = d_gpuSrc[xidx];\n      \n\n      d_outputBuffer[bufIndex] = 1;\n    }\n    \n\n  }\n\n  item.barrier(sycl::access::fence_space::local_space);\n\n  if(i==iteration-1)\n  {\n    \n\n    \n\n    break;\n  }\n\n  if(computed)\n  {\n    \n\n    prev[tx] = result[tx];\n  }\n  item.barrier(sycl::access::fence_space::local_space);\n}\n\n\n\n\n\n\n\nif (computed)\n{\n  d_gpuResult[xidx] = result[tx];\n}\n\n\n\n\n"}}
{"kernel_name": "projectile", "parallel_api": "cuda", "code": {"Projectile.cu": "\n\n\n\n\n\n\n\n\n\n\n#include <chrono>\n#include <vector>\n#include <cuda.h>\n#include \"Projectile.hpp\"\n\n#ifdef DEBUG\nstatic const int num_elements = 100;\n#else\nstatic const int num_elements = 10000000;\n#endif\nconst float kPIValue = 3.1415;\nconst float kGValue = 9.81;\nconst int BLOCK_SIZE = 256; \n\n\n\n\n\n\n__global__ void CalculateRange(const Projectile *obj, Projectile *pObj) {  \n  \n  int i = blockDim.x*blockIdx.x + threadIdx.x;\n  if (i >= num_elements) return;\n  float proj_angle = obj[i].getangle();\n  float proj_vel = obj[i].getvelocity();\n  float sin_value = sinf(proj_angle * kPIValue / 180.0f);\n  float cos_value = cosf(proj_angle * kPIValue / 180.0f);\n  float total_time = fabsf((2 * proj_vel * sin_value)) / kGValue;\n  float max_range =  fabsf(proj_vel * total_time * cos_value);\n  float max_height = (proj_vel * proj_vel * sin_value * sin_value) / 2.0f *\n                     kGValue;  \n\n\n  pObj[i].setRangeandTime(max_range, total_time, proj_angle, proj_vel, max_height);\n}\n\n\n\n\n\nvoid GpuParallel(std::vector<Projectile>& in_vect,\n                 std::vector<Projectile>& out_vect,\n                 const int repeat)\n{\n  Projectile *bufin_vect, *bufout_vect;\n\n  cudaMalloc((void**)&bufin_vect, sizeof(Projectile) * num_elements);\n  cudaMalloc((void**)&bufout_vect, sizeof(Projectile) * num_elements);\n  cudaMemcpy(bufin_vect, in_vect.data(), sizeof(Projectile) * num_elements, cudaMemcpyHostToDevice);\n\n  dim3 grids ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);\n  dim3 blocks (BLOCK_SIZE);\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++)\n    CalculateRange <<< grids, blocks >>> (bufin_vect, bufout_vect);\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  cudaMemcpy(out_vect.data(), bufout_vect, sizeof(Projectile) * num_elements, cudaMemcpyDeviceToHost);\n  cudaFree(bufin_vect);\n  cudaFree(bufout_vect);\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  float init_angle = 0.0f;\n  float init_vel = 0.0f;\n  vector<Projectile> input_vect1, out_parallel_vect2, out_scalar_vect3;\n\n  \n\n  srand(2);\n  for (int i = 0; i < num_elements; i++) {\n    init_angle = rand() % 90 + 10;\n    init_vel = rand() % 400 + 10;\n    input_vect1.push_back(Projectile(init_angle, init_vel, 1.0f, 1.0f, 1.0f));\n    out_parallel_vect2.push_back(Projectile());\n    out_scalar_vect3.push_back(Projectile());\n  }\n\n  GpuParallel(input_vect1, out_parallel_vect2, repeat);\n      \n#ifdef DEBUG\n  for (int i = 0; i < num_elements; i++)\n  {\n    \n\n    cout << \"Parallel \" << out_parallel_vect2[i];\n  } \n#endif\n  return 0;\n}\n"}}
{"kernel_name": "projectile", "parallel_api": "hip", "code": {"Projectile.cu": "\n\n\n\n\n\n\n\n\n\n\n#include <chrono>\n#include <vector>\n#include <hip/hip_runtime.h>\n#include \"Projectile.hpp\"\n\n#ifdef DEBUG\nstatic const int num_elements = 100;\n#else\nstatic const int num_elements = 10000000;\n#endif\nconst float kPIValue = 3.1415;\nconst float kGValue = 9.81;\nconst int BLOCK_SIZE = 256; \n\n\n\n\n\n\n__global__ void CalculateRange(const Projectile *obj, Projectile *pObj) {  \n  \n  int i = blockDim.x*blockIdx.x + threadIdx.x;\n  if (i >= num_elements) return;\n  float proj_angle = obj[i].getangle();\n  float proj_vel = obj[i].getvelocity();\n  float sin_value = sinf(proj_angle * kPIValue / 180.0f);\n  float cos_value = cosf(proj_angle * kPIValue / 180.0f);\n  float total_time = fabsf((2 * proj_vel * sin_value)) / kGValue;\n  float max_range =  fabsf(proj_vel * total_time * cos_value);\n  float max_height = (proj_vel * proj_vel * sin_value * sin_value) / 2.0f *\n                     kGValue;  \n\n\n  pObj[i].setRangeandTime(max_range, total_time, proj_angle, proj_vel, max_height);\n}\n\n\n\n\n\nvoid GpuParallel(std::vector<Projectile>& in_vect,\n                 std::vector<Projectile>& out_vect,\n                 const int repeat)\n{\n  Projectile *bufin_vect, *bufout_vect;\n\n  hipMalloc((void**)&bufin_vect, sizeof(Projectile) * num_elements);\n  hipMalloc((void**)&bufout_vect, sizeof(Projectile) * num_elements);\n  hipMemcpy(bufin_vect, in_vect.data(), sizeof(Projectile) * num_elements, hipMemcpyHostToDevice);\n\n  dim3 grids ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);\n  dim3 blocks (BLOCK_SIZE);\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++)\n    hipLaunchKernelGGL(CalculateRange, grids, blocks , 0, 0, bufin_vect, bufout_vect);\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  hipMemcpy(out_vect.data(), bufout_vect, sizeof(Projectile) * num_elements, hipMemcpyDeviceToHost);\n  hipFree(bufin_vect);\n  hipFree(bufout_vect);\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  float init_angle = 0.0f;\n  float init_vel = 0.0f;\n  vector<Projectile> input_vect1, out_parallel_vect2, out_scalar_vect3;\n\n  \n\n  srand(2);\n  for (int i = 0; i < num_elements; i++) {\n    init_angle = rand() % 90 + 10;\n    init_vel = rand() % 400 + 10;\n    input_vect1.push_back(Projectile(init_angle, init_vel, 1.0f, 1.0f, 1.0f));\n    out_parallel_vect2.push_back(Projectile());\n    out_scalar_vect3.push_back(Projectile());\n  }\n\n  GpuParallel(input_vect1, out_parallel_vect2, repeat);\n      \n#ifdef DEBUG\n  for (int i = 0; i < num_elements; i++)\n  {\n    \n\n    cout << \"Parallel \" << out_parallel_vect2[i];\n  } \n#endif\n  return 0;\n}\n"}}
{"kernel_name": "projectile", "parallel_api": "omp", "code": {"Projectile.cpp": "\n\n\n\n\n\n\n\n\n\n\n#include <chrono>\n#include <vector>\n#include <cstdlib>\n#include \"Projectile.hpp\"\n\nusing namespace std;\n\n#ifdef DEBUG\nstatic const int num_elements = 100;\n#else\nstatic const int num_elements = 10000000;\n#endif\nconst float kPIValue = 3.1415;\nconst float kGValue = 9.81;\nconst int BLOCK_SIZE = 256;\n\n\n\n\n\nvoid GpuParallel(std::vector<Projectile>& in_vect,\n                 std::vector<Projectile>& out_vect,\n                 const int repeat)\n{\n  Projectile *obj = in_vect.data();\n  Projectile *pObj = out_vect.data();\n\n  #pragma omp target data map(to: obj[0:num_elements]) map(from: pObj[0:num_elements])\n  {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      #pragma omp target teams distribute parallel for thread_limit(BLOCK_SIZE)\n      for (int i = 0; i < num_elements; i++) {\n        float proj_angle = obj[i].getangle();\n        float proj_vel = obj[i].getvelocity();\n        float sin_value = sinf(proj_angle * kPIValue / 180.0f);\n        float cos_value = cosf(proj_angle * kPIValue / 180.0f);\n        float total_time = fabsf((2 * proj_vel * sin_value)) / kGValue;\n        float max_range = fabsf(proj_vel * total_time * cos_value);\n        float max_height = (proj_vel * proj_vel * sin_value * sin_value) / 2.0f *\n                           kGValue;  \n\n        pObj[i].setRangeandTime(max_range, total_time, proj_angle, proj_vel, max_height);\n      }\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n  }\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  float init_angle = 0.0f;\n  float init_vel = 0.0f;\n  vector<Projectile> input_vect1, out_parallel_vect2, out_scalar_vect3;\n\n  \n\n  srand(2);\n  for (int i = 0; i < num_elements; i++) {\n    init_angle = rand() % 90 + 10;\n    init_vel = rand() % 400 + 10;\n    input_vect1.push_back(Projectile(init_angle, init_vel, 1.0f, 1.0f, 1.0f));\n    out_parallel_vect2.push_back(Projectile());\n    out_scalar_vect3.push_back(Projectile());\n  }\n\n  GpuParallel(input_vect1, out_parallel_vect2, repeat);\n      \n#ifdef DEBUG\n  for (int i = 0; i < num_elements; i++)\n  {\n    \n\n    cout << \"Parallel \" << out_parallel_vect2[i];\n  }\n#endif\n  return 0;\n}\n"}}
{"kernel_name": "projectile", "parallel_api": "serial", "code": {"Projectile.cpp": "\n\n\n\n\n\n\n\n\n\n\n#include <chrono>\n#include <vector>\n#include <cstdlib>\n#include \"Projectile.hpp\"\n\nusing namespace std;\n\n#ifdef DEBUG\nstatic const int num_elements = 100;\n#else\nstatic const int num_elements = 10000000;\n#endif\nconst float kPIValue = 3.1415;\nconst float kGValue = 9.81;\nconst int BLOCK_SIZE = 256;\n\n\n\n\n\nvoid GpuParallel(std::vector<Projectile>& in_vect,\n                 std::vector<Projectile>& out_vect,\n                 const int repeat)\n{\n  Projectile *obj = in_vect.data();\n  Projectile *pObj = out_vect.data();\n\n    {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n            for (int i = 0; i < num_elements; i++) {\n        float proj_angle = obj[i].getangle();\n        float proj_vel = obj[i].getvelocity();\n        float sin_value = sinf(proj_angle * kPIValue / 180.0f);\n        float cos_value = cosf(proj_angle * kPIValue / 180.0f);\n        float total_time = fabsf((2 * proj_vel * sin_value)) / kGValue;\n        float max_range = fabsf(proj_vel * total_time * cos_value);\n        float max_height = (proj_vel * proj_vel * sin_value * sin_value) / 2.0f *\n                           kGValue;  \n\n        pObj[i].setRangeandTime(max_range, total_time, proj_angle, proj_vel, max_height);\n      }\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n  }\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  float init_angle = 0.0f;\n  float init_vel = 0.0f;\n  vector<Projectile> input_vect1, out_parallel_vect2, out_scalar_vect3;\n\n  \n\n  srand(2);\n  for (int i = 0; i < num_elements; i++) {\n    init_angle = rand() % 90 + 10;\n    init_vel = rand() % 400 + 10;\n    input_vect1.push_back(Projectile(init_angle, init_vel, 1.0f, 1.0f, 1.0f));\n    out_parallel_vect2.push_back(Projectile());\n    out_scalar_vect3.push_back(Projectile());\n  }\n\n  GpuParallel(input_vect1, out_parallel_vect2, repeat);\n      \n#ifdef DEBUG\n  for (int i = 0; i < num_elements; i++)\n  {\n    \n\n    cout << \"Parallel \" << out_parallel_vect2[i];\n  }\n#endif\n  return 0;\n}"}}
{"kernel_name": "projectile", "parallel_api": "sycl", "code": {"Projectile.cpp": "\n\n\n\n\n\n\n\n\n\n\n#include <vector>\n#include <sycl/sycl.hpp>\n#include \"Projectile.hpp\"\n\n#ifdef DEBUG\nstatic const int num_elements = 100;\n#else\nstatic const int num_elements = 10000000;\n#endif\nconst float kPIValue = 3.1415;\nconst float kGValue = 9.81;\nconst int BLOCK_SIZE = 256;\n\n\n\n\n\n\n\nvoid GpuParallel(sycl::queue& q,\n                 std::vector<Projectile>& in_vect,\n                 std::vector<Projectile>& out_vect,\n                 const int repeat)\n{\n  Projectile *bufin_vect = sycl::malloc_device<Projectile>(num_elements, q);\n  q.memcpy(bufin_vect, in_vect.data(), sizeof(Projectile) * num_elements);\n\n  Projectile *bufout_vect = sycl::malloc_device<Projectile>(num_elements, q);\n\n  sycl::range<1> gws ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE);\n  sycl::range<1> lws (BLOCK_SIZE);\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    q.submit([&](sycl::handler& h) {\n      h.parallel_for<class projectile>(\n        sycl::nd_range<1>(gws, lws), [=](sycl::nd_item<1> item) {\n        int i = item.get_global_id(0); \n        if (i >= num_elements) return;\n        float proj_angle = bufin_vect[i].getangle();\n        float proj_vel = bufin_vect[i].getvelocity();\n        float sin_value = sycl::sin(proj_angle * kPIValue / 180.0f);\n        float cos_value = sycl::cos(proj_angle * kPIValue / 180.0f);\n        float total_time = sycl::fabs((2 * proj_vel * sin_value)) / kGValue;\n        float max_range = sycl::fabs(proj_vel * total_time * cos_value);\n        float max_height = (proj_vel * proj_vel * sin_value * sin_value) / 2.0f *\n                           kGValue;  \n\n\n        bufout_vect[i].setRangeandTime(max_range, total_time, proj_angle, proj_vel, max_height);\n      });\n    });\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  q.memcpy(out_vect.data(), bufout_vect, sizeof(Projectile) * num_elements).wait();\n  sycl::free(bufin_vect, q);\n  sycl::free(bufout_vect, q);\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  float init_angle = 0.0f;\n  float init_vel = 0.0f;\n  std::vector<Projectile> input_vect1, out_parallel_vect2, out_scalar_vect3;\n\n  \n\n  srand(2);\n  for (int i = 0; i < num_elements; i++) {\n    init_angle = rand() % 90 + 10;\n    init_vel = rand() % 400 + 10;\n    input_vect1.push_back(Projectile(init_angle, init_vel, 1.0f, 1.0f, 1.0f));\n    out_parallel_vect2.push_back(Projectile());\n    out_scalar_vect3.push_back(Projectile());\n  }\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  GpuParallel(q, input_vect1, out_parallel_vect2, repeat);\n      \n#ifdef DEBUG\n  for (int i = 0; i < num_elements; i++)\n  {\n    \n\n    std::cout << \"Parallel \" << out_parallel_vect2[i];\n  }\n#endif\n  return 0;\n}\n"}}
{"kernel_name": "rainflow", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <cuda.h>\n#include <chrono>\n#include \"reference.h\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n__device__\nvoid Extrema(const double* history, const int history_length, double *result, int& result_length)\n{\n  result[0] = history[0];\n\n  int eidx = 0;\n  for (int i = 1; i < history_length - 1; i++)\n    if ((history[i] > result[eidx] && history[i] > history[i + 1]) ||\n        (history[i] < result[eidx] && history[i] < history[i + 1]))\n      result[++eidx] = history[i];\n\n  result[++eidx] = history[history_length - 1];\n  result_length = eidx + 1;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n__device__\nvoid Execute(const double* history, const int history_length,\n             double *extrema, int* points, double3 *results,\n             int *results_length )\n{\n  int extrema_length = 0;\n  Extrema(history, history_length, extrema, extrema_length);\n\n  int pidx = -1, eidx = -1, ridx = -1;\n\n  for (int i = 0; i < extrema_length; i++)\n  {\n    points[++pidx] = ++eidx;\n    double xRange, yRange;\n    while (pidx >= 2 && (xRange = fabs(extrema[points[pidx - 1]] - extrema[points[pidx]]))\n           >= (yRange = fabs(extrema[points[pidx - 2]] - extrema[points[pidx - 1]])))\n    {\n      double yMean = 0.5 * (extrema[points[pidx - 2]] + extrema[points[pidx - 1]]);\n\n      if (pidx == 2)\n      {\n        results[++ridx] = make_double3( 0.5, yRange, yMean );\n        points[0] = points[1];\n        points[1] = points[2];\n        pidx = 1;\n      }\n      else\n      {\n        results[++ridx] = make_double3( 1.0, yRange, yMean );\n        points[pidx - 2] = points[pidx];\n        pidx -= 2;\n      }\n    }\n  }\n\n  for (int i = 0; i <= pidx - 1; i++)\n  {\n    double range = fabs(extrema[points[i]] - extrema[points[i + 1]]);\n    double mean = 0.5 * (extrema[points[i]] + extrema[points[i + 1]]);\n    results[++ridx] = make_double3 ( 0.5, range, mean );\n  }\n\n  *results_length = ridx + 1;\n}\n\n__global__\nvoid rainflow_count(const double *__restrict__ history,\n                    const int *__restrict__ history_lengths,\n                    double *__restrict__ extrema,\n                       int *__restrict__  points,\n                    double3 *__restrict__ results,\n                    int *__restrict__ result_length,\n                    const int num_history )\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= num_history) return;\n\n  const int offset = history_lengths[i];\n  const int history_length = history_lengths[i+1] - offset;\n  Execute(history + offset, \n          history_length,\n          extrema + offset,\n          points + offset,\n          results + offset,\n          result_length + i);\n}\n\nint main(int argc, char* argv[]) {\n  const int num_history = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n\n  int *history_lengths = (int*) malloc ((num_history + 1) * sizeof(int));\n  int *result_lengths = (int*) malloc (num_history * sizeof(int));\n  int *ref_result_lengths = (int*) malloc (num_history * sizeof(int));\n  \n  srand(123);\n\n  \n\n  const int scale = 100;\n  size_t total_length  = 0;\n\n  int n;\n  for (n = 0; n < num_history; n++) {\n     history_lengths[n] = total_length;\n     total_length += (rand() % 10 + 1) * scale;\n  }\n  history_lengths[n] = total_length;\n  \n  printf(\"Total history length = %zu\\n\", total_length);\n\n  double *history = (double*) malloc (total_length * sizeof(double));\n  for (size_t i = 0; i < total_length; i++) {\n    history[i] = rand() / (double)RAND_MAX;\n  }\n\n  double *extrema = (double*) malloc (total_length * sizeof(double));\n  double3 *results = (double3*) malloc (total_length * sizeof(double3));\n  int *points = (int*) malloc (total_length * sizeof(int));\n  \n  int *d_history_lengths;\n  cudaMalloc ((void**)&d_history_lengths, (num_history + 1) * sizeof(int));\n  cudaMemcpy(d_history_lengths, history_lengths, (num_history  + 1) * sizeof(int), cudaMemcpyHostToDevice);\n\n  int *d_result_lengths;\n  cudaMalloc((void**)&d_result_lengths, num_history * sizeof(int));\n\n  double *d_history;\n  cudaMalloc((void**)&d_history, total_length * sizeof(double));\n  cudaMemcpy(d_history, history, total_length * sizeof(double), cudaMemcpyHostToDevice);\n\n  double *d_extrema;\n  cudaMalloc((void**)&d_extrema, total_length * sizeof(double));\n\n  double3 *d_results;\n  cudaMalloc((void**)&d_results, total_length * sizeof(double3));\n\n  int *d_points;\n  cudaMalloc((void**)&d_points, total_length * sizeof(int));\n  \n  dim3 blocks (256);\n  dim3 grids (num_history / 256 + 1);\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (n = 0; n < repeat; n++) {\n    rainflow_count <<<grids, blocks>>> (\n      d_history, \n      d_history_lengths,\n      d_extrema,\n      d_points,\n      d_results,\n      d_result_lengths,\n      num_history\n    );\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  cudaMemcpy(result_lengths, d_result_lengths, num_history * sizeof(int), cudaMemcpyDeviceToHost);\n\n  reference (\n    history, \n    history_lengths,\n    extrema,\n    points,\n    results,\n    ref_result_lengths,\n    num_history\n  );\n\n  int error = memcmp(ref_result_lengths, result_lengths, num_history * sizeof(int));\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  cudaFree(d_history);\n  cudaFree(d_history_lengths);\n  cudaFree(d_extrema);\n  cudaFree(d_points);\n  cudaFree(d_results);\n  cudaFree(d_result_lengths);\n  free(history);\n  free(history_lengths);\n  free(extrema);\n  free(points);\n  free(results);\n  free(result_lengths);\n  free(ref_result_lengths);\n\n  return 0;\n}\n"}}
{"kernel_name": "rainflow", "parallel_api": "hip", "code": {"main.cu": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <hip/hip_runtime.h>\n#include <chrono>\n#include \"reference.h\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n__device__\nvoid Extrema(const double* history, const int history_length, double *result, int& result_length)\n{\n  result[0] = history[0];\n\n  int eidx = 0;\n  for (int i = 1; i < history_length - 1; i++)\n    if ((history[i] > result[eidx] && history[i] > history[i + 1]) ||\n        (history[i] < result[eidx] && history[i] < history[i + 1]))\n      result[++eidx] = history[i];\n\n  result[++eidx] = history[history_length - 1];\n  result_length = eidx + 1;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n__device__\nvoid Execute(const double* history, const int history_length,\n             double *extrema, int* points, double3 *results,\n             int *results_length )\n{\n  int extrema_length = 0;\n  Extrema(history, history_length, extrema, extrema_length);\n\n  int pidx = -1, eidx = -1, ridx = -1;\n\n  for (int i = 0; i < extrema_length; i++)\n  {\n    points[++pidx] = ++eidx;\n    double xRange, yRange;\n    while (pidx >= 2 && (xRange = fabs(extrema[points[pidx - 1]] - extrema[points[pidx]]))\n           >= (yRange = fabs(extrema[points[pidx - 2]] - extrema[points[pidx - 1]])))\n    {\n      double yMean = 0.5 * (extrema[points[pidx - 2]] + extrema[points[pidx - 1]]);\n\n      if (pidx == 2)\n      {\n        results[++ridx] = make_double3( 0.5, yRange, yMean );\n        points[0] = points[1];\n        points[1] = points[2];\n        pidx = 1;\n      }\n      else\n      {\n        results[++ridx] = make_double3( 1.0, yRange, yMean );\n        points[pidx - 2] = points[pidx];\n        pidx -= 2;\n      }\n    }\n  }\n\n  for (int i = 0; i <= pidx - 1; i++)\n  {\n    double range = fabs(extrema[points[i]] - extrema[points[i + 1]]);\n    double mean = 0.5 * (extrema[points[i]] + extrema[points[i + 1]]);\n    results[++ridx] = make_double3 ( 0.5, range, mean );\n  }\n\n  *results_length = ridx + 1;\n}\n\n__global__\nvoid rainflow_count(const double *__restrict__ history,\n                    const int *__restrict__ history_lengths,\n                    double *__restrict__ extrema,\n                       int *__restrict__  points,\n                    double3 *__restrict__ results,\n                    int *__restrict__ result_length,\n                    const int num_history )\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= num_history) return;\n\n  const int offset = history_lengths[i];\n  const int history_length = history_lengths[i+1] - offset;\n  Execute(history + offset, \n          history_length,\n          extrema + offset,\n          points + offset,\n          results + offset,\n          result_length + i);\n}\n\nint main(int argc, char* argv[]) {\n  const int num_history = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n\n  int *history_lengths = (int*) malloc ((num_history + 1) * sizeof(int));\n  int *result_lengths = (int*) malloc (num_history * sizeof(int));\n  int *ref_result_lengths = (int*) malloc (num_history * sizeof(int));\n  \n  srand(123);\n\n  \n\n  const int scale = 100;\n  size_t total_length  = 0;\n\n  int n;\n  for (n = 0; n < num_history; n++) {\n     history_lengths[n] = total_length;\n     total_length += (rand() % 10 + 1) * scale;\n  }\n  history_lengths[n] = total_length;\n\n  printf(\"Total history length = %zu\\n\", total_length);\n\n  double *history = (double*) malloc (total_length * sizeof(double));\n  for (size_t i = 0; i < total_length; i++) {\n    history[i] = rand() / (double)RAND_MAX;\n  }\n\n  double *extrema = (double*) malloc (total_length * sizeof(double));\n  double3 *results = (double3*) malloc (total_length * sizeof(double3));\n  int *points = (int*) malloc (total_length * sizeof(int));\n  \n  int *d_history_lengths;\n  hipMalloc ((void**)&d_history_lengths, (num_history + 1) * sizeof(int));\n  hipMemcpy(d_history_lengths, history_lengths, (num_history  + 1) * sizeof(int), hipMemcpyHostToDevice);\n\n  int *d_result_lengths;\n  hipMalloc((void**)&d_result_lengths, num_history * sizeof(int));\n\n  double *d_history;\n  hipMalloc((void**)&d_history, total_length * sizeof(double));\n  hipMemcpy(d_history, history, total_length * sizeof(double), hipMemcpyHostToDevice);\n\n  double *d_extrema;\n  hipMalloc((void**)&d_extrema, total_length * sizeof(double));\n\n  double3 *d_results;\n  hipMalloc((void**)&d_results, total_length * sizeof(double3));\n\n  int *d_points;\n  hipMalloc((void**)&d_points, total_length * sizeof(int));\n  \n  dim3 blocks (256);\n  dim3 grids (num_history / 256 + 1);\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (n = 0; n < repeat; n++) {\n    hipLaunchKernelGGL(rainflow_count, grids, blocks, 0, 0, \n      d_history, \n      d_history_lengths,\n      d_extrema,\n      d_points,\n      d_results,\n      d_result_lengths,\n      num_history\n    );\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  hipMemcpy(result_lengths, d_result_lengths, num_history * sizeof(int), hipMemcpyDeviceToHost);\n\n  reference (\n    history, \n    history_lengths,\n    extrema,\n    points,\n    results,\n    ref_result_lengths,\n    num_history\n  );\n\n  int error = memcmp(ref_result_lengths, result_lengths, num_history * sizeof(int));\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  hipFree(d_history);\n  hipFree(d_history_lengths);\n  hipFree(d_extrema);\n  hipFree(d_points);\n  hipFree(d_results);\n  hipFree(d_result_lengths);\n  free(history);\n  free(history_lengths);\n  free(extrema);\n  free(points);\n  free(results);\n  free(result_lengths);\n  free(ref_result_lengths);\n\n  return 0;\n}\n"}}
{"kernel_name": "rainflow", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <chrono>\n#include <omp.h>\n#include \"reference.h\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvoid Extrema(const double* history, const int history_length, double *result, int& result_length)\n{\n  result[0] = history[0];\n\n  int eidx = 0;\n  for (int i = 1; i < history_length - 1; i++)\n    if ((history[i] > result[eidx] && history[i] > history[i + 1]) ||\n        (history[i] < result[eidx] && history[i] < history[i + 1]))\n      result[++eidx] = history[i];\n\n  result[++eidx] = history[history_length - 1];\n  result_length = eidx + 1;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nvoid Execute(const double* history, const int history_length,\n             double *extrema, int* points, double3 *results,\n             int *results_length )\n{\n  int extrema_length = 0;\n  Extrema(history, history_length, extrema, extrema_length);\n\n  int pidx = -1, eidx = -1, ridx = -1;\n\n  for (int i = 0; i < extrema_length; i++)\n  {\n    points[++pidx] = ++eidx;\n    double xRange, yRange;\n    while (pidx >= 2 && (xRange = fabs(extrema[points[pidx - 1]] - extrema[points[pidx]]))\n           >= (yRange = fabs(extrema[points[pidx - 2]] - extrema[points[pidx - 1]])))\n    {\n      double yMean = 0.5 * (extrema[points[pidx - 2]] + extrema[points[pidx - 1]]);\n\n      if (pidx == 2)\n      {\n        results[++ridx] = { 0.5, yRange, yMean };\n        points[0] = points[1];\n        points[1] = points[2];\n        pidx = 1;\n      }\n      else\n      {\n        results[++ridx] = { 1.0, yRange, yMean };\n        points[pidx - 2] = points[pidx];\n        pidx -= 2;\n      }\n    }\n  }\n\n  for (int i = 0; i <= pidx - 1; i++)\n  {\n    double range = fabs(extrema[points[i]] - extrema[points[i + 1]]);\n    double mean = 0.5 * (extrema[points[i]] + extrema[points[i + 1]]);\n    results[++ridx] = { 0.5, range, mean };\n  }\n\n  *results_length = ridx + 1;\n}\n\nint main(int argc, char* argv[]) {\n  const int num_history = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n\n  int *history_lengths = (int*) malloc ((num_history + 1) * sizeof(int));\n  int *result_lengths = (int*) malloc (num_history * sizeof(int));\n  int *ref_result_lengths = (int*) malloc (num_history * sizeof(int));\n  \n  srand(123);\n\n  \n\n  const int scale = 100;\n  size_t total_length  = 0;\n\n  int n;\n  for (n = 0; n < num_history; n++) {\n     history_lengths[n] = total_length;\n     total_length += (rand() % 10 + 1) * scale;\n  }\n  history_lengths[n] = total_length;\n  \n  printf(\"Total history length = %zu\\n\", total_length);\n\n  double *history = (double*) malloc (total_length * sizeof(double));\n  for (size_t i = 0; i < total_length; i++) {\n    history[i] = rand() / (double)RAND_MAX;\n  }\n\n  double *extrema = (double*) malloc (total_length * sizeof(double));\n  double3 *results = (double3*) malloc (total_length * sizeof(double3));\n  int *points = (int*) malloc (total_length * sizeof(int));\n  \n  #pragma omp target data map(to: history_lengths[0:num_history+1], \\\n                                  history[0:total_length]) \\\n                          map(alloc: extrema[0:total_length], \\\n                                     points[0:total_length], \\\n                                     results[0:total_length]) \\\n                          map(from: result_lengths[0:num_history])\n  {\n    auto start = std::chrono::steady_clock::now();\n\n    for (n = 0; n < repeat; n++) {\n      #pragma omp target teams distribute parallel for thread_limit(256)\n      for (int i = 0; i < num_history; i++) {\n        const int offset = history_lengths[i];\n        const int history_length = history_lengths[i+1] - offset;\n        Execute(history + offset, \n                history_length,\n                extrema + offset,\n                points + offset,\n                results + offset,\n                result_lengths + i);\n      }\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n  }\n\n  reference (\n    history, \n    history_lengths,\n    extrema,\n    points,\n    results,\n    ref_result_lengths,\n    num_history\n  );\n\n  int error = memcmp(ref_result_lengths, result_lengths, num_history * sizeof(int));\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  free(history);\n  free(history_lengths);\n  free(extrema);\n  free(points);\n  free(results);\n  free(result_lengths);\n  free(ref_result_lengths);\n\n  return 0;\n}\n"}}
{"kernel_name": "rainflow", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <chrono>\n#include \"reference.h\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvoid Extrema(const double* history, const int history_length, double *result, int& result_length)\n{\n  result[0] = history[0];\n\n  int eidx = 0;\n  for (int i = 1; i < history_length - 1; i++)\n    if ((history[i] > result[eidx] && history[i] > history[i + 1]) ||\n        (history[i] < result[eidx] && history[i] < history[i + 1]))\n      result[++eidx] = history[i];\n\n  result[++eidx] = history[history_length - 1];\n  result_length = eidx + 1;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nvoid Execute(const double* history, const int history_length,\n             double *extrema, int* points, double3 *results,\n             int *results_length )\n{\n  int extrema_length = 0;\n  Extrema(history, history_length, extrema, extrema_length);\n\n  int pidx = -1, eidx = -1, ridx = -1;\n\n  for (int i = 0; i < extrema_length; i++)\n  {\n    points[++pidx] = ++eidx;\n    double xRange, yRange;\n    while (pidx >= 2 && (xRange = fabs(extrema[points[pidx - 1]] - extrema[points[pidx]]))\n           >= (yRange = fabs(extrema[points[pidx - 2]] - extrema[points[pidx - 1]])))\n    {\n      double yMean = 0.5 * (extrema[points[pidx - 2]] + extrema[points[pidx - 1]]);\n\n      if (pidx == 2)\n      {\n        results[++ridx] = { 0.5, yRange, yMean };\n        points[0] = points[1];\n        points[1] = points[2];\n        pidx = 1;\n      }\n      else\n      {\n        results[++ridx] = { 1.0, yRange, yMean };\n        points[pidx - 2] = points[pidx];\n        pidx -= 2;\n      }\n    }\n  }\n\n  for (int i = 0; i <= pidx - 1; i++)\n  {\n    double range = fabs(extrema[points[i]] - extrema[points[i + 1]]);\n    double mean = 0.5 * (extrema[points[i]] + extrema[points[i + 1]]);\n    results[++ridx] = { 0.5, range, mean };\n  }\n\n  *results_length = ridx + 1;\n}\n\nint main(int argc, char* argv[]) {\n  const int num_history = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n\n  int *history_lengths = (int*) malloc ((num_history + 1) * sizeof(int));\n  int *result_lengths = (int*) malloc (num_history * sizeof(int));\n  int *ref_result_lengths = (int*) malloc (num_history * sizeof(int));\n  \n  srand(123);\n\n  \n\n  const int scale = 100;\n  size_t total_length  = 0;\n\n  int n;\n  for (n = 0; n < num_history; n++) {\n     history_lengths[n] = total_length;\n     total_length += (rand() % 10 + 1) * scale;\n  }\n  history_lengths[n] = total_length;\n  \n  printf(\"Total history length = %zu\\n\", total_length);\n\n  double *history = (double*) malloc (total_length * sizeof(double));\n  for (size_t i = 0; i < total_length; i++) {\n    history[i] = rand() / (double)RAND_MAX;\n  }\n\n  double *extrema = (double*) malloc (total_length * sizeof(double));\n  double3 *results = (double3*) malloc (total_length * sizeof(double3));\n  int *points = (int*) malloc (total_length * sizeof(int));\n  \n    {\n    auto start = std::chrono::steady_clock::now();\n\n    for (n = 0; n < repeat; n++) {\n            for (int i = 0; i < num_history; i++) {\n        const int offset = history_lengths[i];\n        const int history_length = history_lengths[i+1] - offset;\n        Execute(history + offset, \n                history_length,\n                extrema + offset,\n                points + offset,\n                results + offset,\n                result_lengths + i);\n      }\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n  }\n\n  reference (\n    history, \n    history_lengths,\n    extrema,\n    points,\n    results,\n    ref_result_lengths,\n    num_history\n  );\n\n  int error = memcmp(ref_result_lengths, result_lengths, num_history * sizeof(int));\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  free(history);\n  free(history_lengths);\n  free(extrema);\n  free(points);\n  free(results);\n  free(result_lengths);\n  free(ref_result_lengths);\n\n  return 0;\n}"}}
{"kernel_name": "rainflow", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n\nusing double3 = sycl::double3;\n\n#include \"reference.h\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvoid Extrema(const double* history, const int history_length, double *result, int& result_length)\n{\n  result[0] = history[0];\n\n  int eidx = 0;\n  for (int i = 1; i < history_length - 1; i++)\n    if ((history[i] > result[eidx] && history[i] > history[i + 1]) ||\n        (history[i] < result[eidx] && history[i] < history[i + 1]))\n      result[++eidx] = history[i];\n\n  result[++eidx] = history[history_length - 1];\n  result_length = eidx + 1;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nvoid Execute(const double* history, const int history_length,\n             double *extrema, int* points, double3 *results,\n             int *results_length )\n{\n  int extrema_length = 0;\n  Extrema(history, history_length, extrema, extrema_length);\n\n  int pidx = -1, eidx = -1, ridx = -1;\n\n  for (int i = 0; i < extrema_length; i++)\n  {\n    points[++pidx] = ++eidx;\n    double xRange, yRange;\n    while (pidx >= 2 &&\n           (xRange = sycl::fabs(extrema[points[pidx - 1]] - extrema[points[pidx]])) >=\n           (yRange = sycl::fabs(extrema[points[pidx - 2]] - extrema[points[pidx - 1]])))\n    {\n      double yMean = 0.5 * (extrema[points[pidx - 2]] + extrema[points[pidx - 1]]);\n\n      if (pidx == 2)\n      {\n        results[++ridx] = { 0.5, yRange, yMean };\n        points[0] = points[1];\n        points[1] = points[2];\n        pidx = 1;\n      }\n      else\n      {\n        results[++ridx] = { 1.0, yRange, yMean };\n        points[pidx - 2] = points[pidx];\n        pidx -= 2;\n      }\n    }\n  }\n\n  for (int i = 0; i <= pidx - 1; i++)\n  {\n    double range = sycl::fabs(extrema[points[i]] - extrema[points[i + 1]]);\n    double mean = 0.5 * (extrema[points[i]] + extrema[points[i + 1]]);\n    results[++ridx] = { 0.5, range, mean };\n  }\n\n  *results_length = ridx + 1;\n}\n\nvoid rainflow_count(sycl::nd_item<1> &item,\n                    const double *__restrict history,\n                    const int *__restrict history_lengths,\n                    double *__restrict extrema,\n                       int *__restrict  points,\n                    double3 *__restrict results,\n                    int *__restrict result_length,\n                    const int num_history )\n{\n  int i = item.get_global_id(0);\n  if (i >= num_history) return;\n\n  const int offset = history_lengths[i];\n  const int history_length = history_lengths[i+1] - offset;\n  Execute(history + offset,\n          history_length,\n          extrema + offset,\n          points + offset,\n          results + offset,\n          result_length + i);\n}\n\nint main(int argc, char* argv[]) {\n  const int num_history = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n\n  int *history_lengths = (int*) malloc ((num_history + 1) * sizeof(int));\n  int *result_lengths = (int*) malloc (num_history * sizeof(int));\n  int *ref_result_lengths = (int*) malloc (num_history * sizeof(int));\n\n  srand(123);\n\n  \n\n  const int scale = 100;\n  size_t total_length  = 0;\n\n  int n;\n  for (n = 0; n < num_history; n++) {\n     history_lengths[n] = total_length;\n     total_length += (rand() % 10 + 1) * scale;\n  }\n  history_lengths[n] = total_length;\n\n  printf(\"Total history length = %zu\\n\", total_length);\n\n  double *history = (double*) malloc (total_length * sizeof(double));\n  for (size_t i = 0; i < total_length; i++) {\n    history[i] = rand() / (double)RAND_MAX;\n  }\n\n  double *extrema = (double*) malloc (total_length * sizeof(double));\n  double3 *results = (double3*) malloc (total_length * sizeof(double3));\n  int *points = (int*) malloc (total_length * sizeof(int));\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  int *d_history_lengths = sycl::malloc_device<int>(num_history + 1, q);\n  q.memcpy(d_history_lengths, history_lengths, sizeof(int) * (num_history + 1)); \n\n  int *d_result_lengths = sycl::malloc_device<int>(num_history, q);\n\n  double *d_history = sycl::malloc_device<double>(total_length, q);\n  q.memcpy(d_history, history, sizeof(double) * total_length);\n\n  double *d_extrema = sycl::malloc_device<double>(total_length, q);\n\n  double3 *d_results = sycl::malloc_device<double3>(total_length, q);\n\n  int *d_points = sycl::malloc_device<int>(total_length, q);\n\n  sycl::range<1> lws (256);\n  sycl::range<1> gws ((num_history / 256 + 1) * 256);\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  for (n = 0; n < repeat; n++) {\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class rainflow>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        rainflow_count (\n          item,\n          d_history,\n          d_history_lengths,\n          d_extrema,\n          d_points,\n          d_results,\n          d_result_lengths,\n          num_history);\n      });\n    });\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  q.memcpy(result_lengths, d_result_lengths, num_history * sizeof(int)).wait();\n\n  reference (\n    history,\n    history_lengths,\n    extrema,\n    points,\n    results,\n    ref_result_lengths,\n    num_history\n  );\n\n  int error = memcmp(ref_result_lengths, result_lengths, num_history * sizeof(int));\n  printf(\"%s\\n\", error ? \"FAIL\" : \"PASS\");\n\n  sycl::free(d_history, q);\n  sycl::free(d_history_lengths, q);\n  sycl::free(d_extrema, q);\n  sycl::free(d_points, q);\n  sycl::free(d_results, q);\n  sycl::free(d_result_lengths, q);\n  free(history);\n  free(history_lengths);\n  free(extrema);\n  free(points);\n  free(results);\n  free(result_lengths);\n  free(ref_result_lengths);\n\n  return 0;\n}\n"}}
{"kernel_name": "sheath", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n\n\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n#include <cuda.h>\n\n\n\nstatic void CUDA_ERROR(cudaError_t err)\n{\n  if (err != cudaSuccess)\n  {\n    printf(\"CUDA ERROR: %s, exiting\\n\", cudaGetErrorString(err));\n    exit(-1);\n  }\n}\n\n\n\n#define EPS_0 8.85418782e-12 \n\n#define K 1.38065e-23        \n\n#define ME 9.10938215e-31    \n\n#define QE 1.602176565e-19   \n\n#define AMU 1.660538921e-27  \n\n#define EV_TO_K 11604.52     \n\n\n\n\n#define PLASMA_DEN 1e16      \n\n#define NUM_IONS 500000      \n\n#define NUM_ELECTRONS 500000 \n\n#define DX 1e-4              \n\n#define NC 100               \n\n#define NUM_TS 1000          \n\n#define DT 1e-11             \n\n#define ELECTRON_TEMP 3.0    \n\n#define ION_TEMP 1.0         \n\n\n\n\n#define X0 0           \n\n#define XL NC* DX      \n\n#define XMAX (X0 + XL) \n\n\nconst int THREADS_PER_BLOCK = 256;\n\n\nstruct Domain\n{\n  const int ni      = NC + 1; \n\n  const double x0   = X0;\n  const double dx   = DX;\n  const double xl   = XL;\n  const double xmax = XMAX;\n\n  \n\n  double* phi; \n\n  double* ef;  \n\n  double* rho; \n\n\n  float* ndi; \n\n  float* nde; \n\n};\n\n\n\nstruct Particle\n{\n  double x;   \n\n  double v;   \n\n  bool alive; \n\n};\n\n\n\nstruct Species\n{\n  double mass;   \n\n  double charge; \n\n  double spwt;   \n\n\n  int np;             \n\n  int np_alloc;       \n\n  Particle* part;     \n\n};\n\n\n\ndouble rnd();\ndouble SampleVel(double v_th);\nvoid ScatterSpecies(Species* species, Particle* species_part_gpu, float* den, float* den_gpu, double &time);\nvoid ComputeRho(Species* ions, Species* electrons);\nbool SolvePotential(double* phi, double* rho);\nbool SolvePotentialDirect(double* phi, double* rho);\nvoid ComputeEF(double* phi, double* ef, double* ef_gpu);\nvoid PushSpecies(Species* species, Particle* species_part_gpu, double* ef);\nvoid RewindSpecies(Species* species, Particle* species_part_gpu, double* ef);\nvoid AddParticle(Species* species, double x, double v);\n__device__ double XtoL(double pos);\n__device__ double gather(double lc, const double* field);\n__device__ void scatter(double lc, float value, float* field);\n\nvoid WriteResults(int ts);\n\n\n\nDomain domain;\n\nFILE* file_res;\n\n\n\nint main(int argc, char* argv[])\n{\n  int p;\n  int ts; \n\n  double sp_time = 0.0; \n\n\n  domain.phi = new double[domain.ni]; \n\n  domain.rho = new double[domain.ni]; \n\n  domain.ef  = new double[domain.ni]; \n\n  domain.nde = new float[domain.ni];  \n\n  domain.ndi = new float[domain.ni];  \n\n\n  \n\n  double* phi = domain.phi;\n  double* rho = domain.rho;\n  double* ef  = domain.ef;\n  float* nde  = domain.nde;\n  float* ndi  = domain.ndi;\n\n  \n\n  float *nde_gpu, *ndi_gpu;\n  CUDA_ERROR(cudaMalloc((void**)&nde_gpu, domain.ni * sizeof(float)));\n  CUDA_ERROR(cudaMalloc((void**)&ndi_gpu, domain.ni * sizeof(float)));\n\n  double* ef_gpu;\n  CUDA_ERROR(cudaMalloc((void**)&ef_gpu, domain.ni * sizeof(double)));\n\n  Particle *ions_part_gpu;\n  CUDA_ERROR(cudaMalloc((void**)&ions_part_gpu, NUM_IONS * sizeof(Particle)));\n\n  Particle *electrons_part_gpu;\n  CUDA_ERROR(cudaMalloc((void**)&electrons_part_gpu, NUM_ELECTRONS * sizeof(Particle)));\n\n  \n\n  memset(phi, 0, sizeof(double) * domain.ni);\n\n  \n\n  Species ions;\n  Species electrons;\n\n  \n\n  ions.mass     = 16 * AMU;\n  ions.charge   = QE;\n  ions.spwt     = PLASMA_DEN * domain.xl / NUM_IONS;\n  ions.np       = 0;\n  ions.np_alloc = NUM_IONS;\n  ions.part     = new Particle[NUM_IONS];\n\n  electrons.mass     = ME; \n\n  electrons.charge   = -QE;\n  electrons.spwt     = PLASMA_DEN * domain.xl / NUM_ELECTRONS;\n  electrons.np       = 0;\n  electrons.np_alloc = NUM_ELECTRONS;\n  electrons.part     = new Particle[NUM_ELECTRONS];\n\n  \n\n  srand(123);\n\n  \n\n  double delta_ions = domain.xl / NUM_IONS;\n  double v_thi      = sqrt(2 * K * ION_TEMP * EV_TO_K / ions.mass);\n  for (p = 0; p < NUM_IONS; p++)\n  {\n    double x = domain.x0 + p * delta_ions;\n    double v = SampleVel(v_thi);\n    AddParticle(&ions, x, v);\n  }\n\n  \n\n  double delta_electrons = domain.xl / NUM_ELECTRONS;\n  double v_the           = sqrt(2 * K * ELECTRON_TEMP * EV_TO_K / electrons.mass);\n  for (p = 0; p < NUM_ELECTRONS; p++)\n  {\n    double x = domain.x0 + p * delta_electrons;\n    double v = SampleVel(v_the);\n    AddParticle(&electrons, x, v);\n  }\n\n  \n\n  CUDA_ERROR(cudaMemcpy(ions_part_gpu, ions.part,\n    NUM_IONS * sizeof(Particle), cudaMemcpyHostToDevice));\n\n  CUDA_ERROR(cudaMemcpy(electrons_part_gpu, electrons.part, \n    NUM_ELECTRONS * sizeof(Particle), cudaMemcpyHostToDevice));\n\n  \n\n  ScatterSpecies(&ions, ions_part_gpu, ndi, ndi_gpu, sp_time);\n  ScatterSpecies(&electrons, electrons_part_gpu, nde, nde_gpu, sp_time);\n\n  \n\n  ComputeRho(&ions, &electrons);\n\n  SolvePotential(phi, rho);\n\n  ComputeEF(phi, ef, ef_gpu);\n\n  RewindSpecies(&ions, ions_part_gpu, ef_gpu);\n  RewindSpecies(&electrons, electrons_part_gpu, ef_gpu);\n\n  \n\n  file_res = fopen(\"result.dat\", \"w\");\n  fprintf(file_res, \"VARIABLES = x nde ndi rho phi ef\\n\");\n  WriteResults(0);\n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  for (ts = 1; ts <= NUM_TS; ts++)\n  {\n    \n\n    ScatterSpecies(&ions, ions_part_gpu, ndi, ndi_gpu, sp_time);\n    ScatterSpecies(&electrons, electrons_part_gpu, nde, nde_gpu, sp_time);\n\n    ComputeRho(&ions, &electrons);\n    SolvePotential(phi, rho);\n    ComputeEF(phi, ef, ef_gpu);\n\n    \n\n    PushSpecies(&electrons, electrons_part_gpu, ef_gpu);\n    PushSpecies(&ions, ions_part_gpu, ef_gpu);\n\n    \n\n    if (ts % 25 == 0)\n    {\n      \n\n      double max_phi = abs(phi[0]);\n      for (int i = 0; i < domain.ni; i++)\n        if (abs(phi[i]) > max_phi)\n          max_phi = abs(phi[i]);\n\n      printf(\"TS:%i\\tnp_i:%d\\tnp_e:%d\\tdphi:%.3g\\n\", ts, ions.np, electrons.np,\n          max_phi - phi[0]);\n    }\n\n    \n\n    if (ts % 1000 == 0) WriteResults(ts);\n  }\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n  fclose(file_res);\n\n  \n\n  delete phi;\n  delete rho;\n  delete ef;\n  delete nde;\n  delete ndi;\n  cudaFree(nde_gpu);\n  cudaFree(ndi_gpu);\n  cudaFree(ef_gpu);\n\n  \n\n  delete ions.part;\n  delete electrons.part;\n  cudaFree(ions_part_gpu);\n  cudaFree(electrons_part_gpu);\n\n  printf(\"Total kernel execution time (scatter particles) : %.3g (s)\\n\", sp_time * 1e-9f),\n  printf(\"Total time for %d time steps: %.3g (s)\\n\", NUM_TS, time * 1e-9f);\n  printf(\"Time per time step: %.3g (ms)\\n\", (time * 1e-6f) / NUM_TS);\n\n  \n\n  CUDA_ERROR(cudaDeviceReset());\n\n  return 0;\n}\n\n\n\n\n\ndouble rnd()\n{\n  return rand() / (double)RAND_MAX;\n}\n\n\n\ndouble SampleVel(double v_th)\n{\n  const int M = 12;\n  double sum  = 0;\n  for (int i = 0; i < M; i++)\n    sum += rnd();\n\n  return sqrt(0.5) * v_th * (sum - M / 2.0) / sqrt(M / 12.0);\n}\n\n\n\n\n__global__ void scatterParticle(const Particle* __restrict particles, float*__restrict den, long N)\n{\n  \n\n  long p = blockIdx.x * blockDim.x + threadIdx.x;\n  if (p < N && particles[p].alive)\n  {\n    double lc = XtoL(particles[p].x);\n    scatter(lc, 1.f, den);\n  }\n}\n\n\n\nvoid ScatterSpecies(Species* species, Particle* species_part_gpu,\n                    float* den, float* den_gpu, double &time)\n{\n  \n\n  CUDA_ERROR(cudaMemset(den_gpu, 0, sizeof(float) * domain.ni));\n\n  int size = species->np_alloc;\n\n  \n\n  int nblocks = 1 + size / THREADS_PER_BLOCK;\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  scatterParticle<<<nblocks, THREADS_PER_BLOCK>>>(species_part_gpu, den_gpu, size);\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n  \n\n  CUDA_ERROR(cudaMemcpy(den, den_gpu, sizeof(float) * domain.ni, cudaMemcpyDeviceToHost));\n\n  \n\n  for (int i = 0; i < domain.ni; i++)\n    den[i] *= species->spwt / domain.dx;\n\n  \n\n  den[0] *= 2.0;\n  den[domain.ni - 1] *= 2.0;\n}\n\n\n\nvoid AddParticle(Species* species, double x, double v)\n{\n  \n\n  if (species->np > species->np_alloc - 1)\n  {\n    printf(\"Too many particles!\\n\");\n    exit(-1);\n  }\n\n  \n\n  species->part[species->np].x     = x;\n  species->part[species->np].v     = v;\n  species->part[species->np].alive = true;\n\n  \n\n  species->np++;\n}\n\n\n\nvoid ComputeRho(Species* ions, Species* electrons)\n{\n  double* rho = domain.rho;\n\n  for (int i = 0; i < domain.ni; i++)\n    rho[i] = ions->charge * domain.ndi[i] + electrons->charge * domain.nde[i];\n}\n\n\n\nbool SolvePotentialDirect(double* x, double* rho)\n{\n  \n\n  int ni     = domain.ni;\n  double dx2 = domain.dx * domain.dx;\n  int i;\n  double* a = new double[ni];\n  double* b = new double[ni];\n  double* c = new double[ni];\n\n  \n\n  for (i = 1; i < ni - 1; i++)\n  {\n    a[i] = 1;\n    b[i] = -2;\n    c[i] = 1;\n  }\n\n  \n\n  a[0]      = 0;\n  b[0]      = 1;\n  c[0]      = 0;\n  a[ni - 1] = 0;\n  b[ni - 1] = 1;\n  c[ni - 1] = 0;\n\n  \n\n  for (i = 1; i < domain.ni - 1; i++)\n    x[i] = -rho[i] * dx2 / EPS_0;\n\n  x[0]      = 0;\n  x[ni - 1] = 0;\n\n  \n\n  c[0] /= b[0]; \n\n  x[0] /= b[0]; \n\n  for (i = 1; i < ni; i++)\n  {\n    double id = (b[i] - c[i - 1] * a[i]); \n\n    c[i] /= id;                           \n\n    x[i] = (x[i] - x[i - 1] * a[i]) / id;\n  }\n\n  \n\n  for (i = ni - 2; i >= 0; i--)\n    x[i] = x[i] - c[i] * x[i + 1];\n\n  return true;\n}\n\n\n\nbool SolvePotential(double* phi, double* rho)\n{\n  double L2;\n  double dx2 = domain.dx * domain.dx; \n\n\n  \n\n  phi[0] = phi[domain.ni - 1] = 0;\n\n  \n\n  for (int solver_it = 0; solver_it < 40000; solver_it++)\n  {\n    \n\n    for (int i = 1; i < domain.ni - 1; i++)\n    {\n      \n\n      double g = 0.5 * (phi[i - 1] + phi[i + 1] + dx2 * rho[i] / EPS_0);\n      phi[i]   = phi[i] + 1.4 * (g - phi[i]);\n    }\n\n    \n\n    if (solver_it % 25 == 0)\n    {\n      double sum = 0;\n      for (int i = 1; i < domain.ni - 1; i++)\n      {\n        double R = -rho[i] / EPS_0 - (phi[i - 1] - 2 * phi[i] + phi[i + 1]) / dx2;\n        sum += R * R;\n      }\n      L2 = sqrt(sum) / domain.ni;\n      if (L2 < 1e-4)\n      {\n        return true;\n      }\n    }\n  }\n  printf(\"Gauss-Seidel solver failed to converge, L2=%.3g!\\n\", L2);\n  return false;\n}\n\n\n\nvoid ComputeEF(double* phi, double* ef, double* ef_gpu)\n{\n  for (int i = 1; i < domain.ni - 1; i++)\n    ef[i] = -(phi[i + 1] - phi[i - 1]) / (2 * domain.dx); \n\n\n  \n\n  ef[0]             = -(phi[1] - phi[0]) / domain.dx;\n  ef[domain.ni - 1] = -(phi[domain.ni - 1] - phi[domain.ni - 2]) / domain.dx;\n\n  \n\n  CUDA_ERROR(cudaMemcpy(ef_gpu, ef, domain.ni * sizeof(double), cudaMemcpyHostToDevice));\n}\n\n\n\n__global__ void pushParticle(Particle*__restrict particles, const double*__restrict ef, double qm, long N)\n{\n  \n\n  long p = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (p < N && particles[p].alive)\n  {\n    \n\n    Particle* part = &particles[p];\n\n    \n\n    double lc = XtoL(part->x);\n\n    \n\n    double part_ef = gather(lc, ef);\n\n    \n\n    part->v += DT * qm * part_ef;\n\n    \n\n    part->x += DT * part->v;\n\n    \n\n    if (part->x < X0 || part->x >= XMAX)\n      part->alive = false;\n  }\n}\n\n\n\nvoid PushSpecies(Species* species, Particle* species_part_gpu, double* ef)\n{\n  \n\n  double qm = species->charge / species->mass;\n\n  int size = species->np_alloc;\n\n  \n\n  int nblocks = 1 + size / THREADS_PER_BLOCK;\n  pushParticle<<<nblocks, THREADS_PER_BLOCK>>>(species_part_gpu, ef, qm, size);\n}\n\n\n\n\n\n__global__ void rewindParticle(Particle*__restrict particles, const double*__restrict ef, double qm, long N)\n{\n  \n\n  long p = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (p < N && particles[p].alive)\n  {\n    \n\n    Particle* part = &particles[p];\n\n    \n\n    double lc = XtoL(part->x);\n\n    \n\n    double part_ef = gather(lc, ef);\n\n    \n\n    part->v -= 0.5 * DT * qm * part_ef;\n  }\n}\n\n\n\n\nvoid RewindSpecies(Species* species, Particle* species_part_gpu, double* ef)\n{\n  \n\n  double qm = species->charge / species->mass;\n\n  int size = species->np_alloc;\n\n  \n\n  int nblocks = 1 + size / THREADS_PER_BLOCK;\n  rewindParticle<<<nblocks, THREADS_PER_BLOCK>>>(species_part_gpu, ef, qm, size);\n}\n\n\n\n\n__device__ double XtoL(double pos)\n{\n  double li = (pos - 0) / DX;\n  return li;\n}\n\n\n\n\n__device__ void scatter(double lc, float value, float* field)\n{\n  int i    = (int)lc;\n  float di = lc - i;\n  atomicAdd(&(field[i]), value * (1 - di));\n  atomicAdd(&(field[i + 1]), value * (di));\n}\n\n\n\n__device__ double gather(double lc, const double* field)\n{\n  int i     = (int)lc;\n  double di = lc - i;\n\n  \n\n  double val = field[i] * (1 - di) + field[i + 1] * (di);\n  return val;\n}\n\n\n\n\nvoid WriteResults(int ts)\n{\n  fprintf(file_res, \"ZONE I=%d T=ZONE_%06d\\n\", domain.ni, ts);\n  for (int i = 0; i < domain.ni; i++)\n  {\n    fprintf(file_res, \"%g %g %g %g %g %g\\n\", i * domain.dx, domain.nde[i], domain.ndi[i],\n        domain.rho[i], domain.phi[i], domain.ef[i]);\n  }\n\n  fflush(file_res);\n}\n"}}
{"kernel_name": "sheath", "parallel_api": "hip", "code": {"main.cu": "\n\n\n\n\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n\n\n\nstatic void HIP_ERROR(hipError_t err)\n{\n  if (err != hipSuccess)\n  {\n    printf(\"HIP ERROR: %s, exiting\\n\", hipGetErrorString(err));\n    exit(-1);\n  }\n}\n\n\n\n#define EPS_0 8.85418782e-12 \n\n#define K 1.38065e-23        \n\n#define ME 9.10938215e-31    \n\n#define QE 1.602176565e-19   \n\n#define AMU 1.660538921e-27  \n\n#define EV_TO_K 11604.52     \n\n\n\n\n#define PLASMA_DEN 1e16      \n\n#define NUM_IONS 500000      \n\n#define NUM_ELECTRONS 500000 \n\n#define DX 1e-4              \n\n#define NC 100               \n\n#define NUM_TS 1000          \n\n#define DT 1e-11             \n\n#define ELECTRON_TEMP 3.0    \n\n#define ION_TEMP 1.0         \n\n\n\n\n#define X0 0           \n\n#define XL NC* DX      \n\n#define XMAX (X0 + XL) \n\n\nconst int THREADS_PER_BLOCK = 256;\n\n\nstruct Domain\n{\n  const int ni      = NC + 1; \n\n  const double x0   = X0;\n  const double dx   = DX;\n  const double xl   = XL;\n  const double xmax = XMAX;\n\n  \n\n  double* phi; \n\n  double* ef;  \n\n  double* rho; \n\n\n  float* ndi; \n\n  float* nde; \n\n};\n\n\n\nstruct Particle\n{\n  double x;   \n\n  double v;   \n\n  bool alive; \n\n};\n\n\n\nstruct Species\n{\n  double mass;   \n\n  double charge; \n\n  double spwt;   \n\n\n  int np;             \n\n  int np_alloc;       \n\n  Particle* part;     \n\n};\n\n\n\ndouble rnd();\ndouble SampleVel(double v_th);\nvoid ScatterSpecies(Species* species, Particle* species_part_gpu, float* den, float* den_gpu, double &time);\nvoid ComputeRho(Species* ions, Species* electrons);\nbool SolvePotential(double* phi, double* rho);\nbool SolvePotentialDirect(double* phi, double* rho);\nvoid ComputeEF(double* phi, double* ef, double* ef_gpu);\nvoid PushSpecies(Species* species, Particle* species_part_gpu, double* ef);\nvoid RewindSpecies(Species* species, Particle* species_part_gpu, double* ef);\nvoid AddParticle(Species* species, double x, double v);\n__device__ double XtoL(double pos);\n__device__ double gather(double lc, const double* field);\n__device__ void scatter(double lc, float value, float* field);\n\nvoid WriteResults(int ts);\n\n\n\nDomain domain;\n\nFILE* file_res;\n\n\n\nint main(int argc, char* argv[])\n{\n  int p;\n  int ts; \n\n  double sp_time = 0.0; \n\n\n  domain.phi = new double[domain.ni]; \n\n  domain.rho = new double[domain.ni]; \n\n  domain.ef  = new double[domain.ni]; \n\n  domain.nde = new float[domain.ni];  \n\n  domain.ndi = new float[domain.ni];  \n\n\n  \n\n  double* phi = domain.phi;\n  double* rho = domain.rho;\n  double* ef  = domain.ef;\n  float* nde  = domain.nde;\n  float* ndi  = domain.ndi;\n\n  \n\n  float *nde_gpu, *ndi_gpu;\n  HIP_ERROR(hipMalloc((void**)&nde_gpu, domain.ni * sizeof(float)));\n  HIP_ERROR(hipMalloc((void**)&ndi_gpu, domain.ni * sizeof(float)));\n\n  double* ef_gpu;\n  HIP_ERROR(hipMalloc((void**)&ef_gpu, domain.ni * sizeof(double)));\n\n  Particle *ions_part_gpu;\n  HIP_ERROR(hipMalloc((void**)&ions_part_gpu, NUM_IONS * sizeof(Particle)));\n\n  Particle *electrons_part_gpu;\n  HIP_ERROR(hipMalloc((void**)&electrons_part_gpu, NUM_ELECTRONS * sizeof(Particle)));\n\n  \n\n  memset(phi, 0, sizeof(double) * domain.ni);\n\n  \n\n  Species ions;\n  Species electrons;\n\n  \n\n  ions.mass     = 16 * AMU;\n  ions.charge   = QE;\n  ions.spwt     = PLASMA_DEN * domain.xl / NUM_IONS;\n  ions.np       = 0;\n  ions.np_alloc = NUM_IONS;\n  ions.part     = new Particle[NUM_IONS];\n\n  electrons.mass     = ME; \n\n  electrons.charge   = -QE;\n  electrons.spwt     = PLASMA_DEN * domain.xl / NUM_ELECTRONS;\n  electrons.np       = 0;\n  electrons.np_alloc = NUM_ELECTRONS;\n  electrons.part     = new Particle[NUM_ELECTRONS];\n\n  \n\n  srand(123);\n\n  \n\n  double delta_ions = domain.xl / NUM_IONS;\n  double v_thi      = sqrt(2 * K * ION_TEMP * EV_TO_K / ions.mass);\n  for (p = 0; p < NUM_IONS; p++)\n  {\n    double x = domain.x0 + p * delta_ions;\n    double v = SampleVel(v_thi);\n    AddParticle(&ions, x, v);\n  }\n\n  \n\n  double delta_electrons = domain.xl / NUM_ELECTRONS;\n  double v_the           = sqrt(2 * K * ELECTRON_TEMP * EV_TO_K / electrons.mass);\n  for (p = 0; p < NUM_ELECTRONS; p++)\n  {\n    double x = domain.x0 + p * delta_electrons;\n    double v = SampleVel(v_the);\n    AddParticle(&electrons, x, v);\n  }\n\n  \n\n  HIP_ERROR(hipMemcpy(ions_part_gpu, ions.part,\n    NUM_IONS * sizeof(Particle), hipMemcpyHostToDevice));\n\n  HIP_ERROR(hipMemcpy(electrons_part_gpu, electrons.part, \n    NUM_ELECTRONS * sizeof(Particle), hipMemcpyHostToDevice));\n\n  \n\n  ScatterSpecies(&ions, ions_part_gpu, ndi, ndi_gpu, sp_time);\n  ScatterSpecies(&electrons, electrons_part_gpu, nde, nde_gpu, sp_time);\n\n  \n\n  ComputeRho(&ions, &electrons);\n\n  SolvePotential(phi, rho);\n\n  ComputeEF(phi, ef, ef_gpu);\n\n  RewindSpecies(&ions, ions_part_gpu, ef_gpu);\n  RewindSpecies(&electrons, electrons_part_gpu, ef_gpu);\n\n  \n\n  file_res = fopen(\"result.dat\", \"w\");\n  fprintf(file_res, \"VARIABLES = x nde ndi rho phi ef\\n\");\n  WriteResults(0);\n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  for (ts = 1; ts <= NUM_TS; ts++)\n  {\n    \n\n    ScatterSpecies(&ions, ions_part_gpu, ndi, ndi_gpu, sp_time);\n    ScatterSpecies(&electrons, electrons_part_gpu, nde, nde_gpu, sp_time);\n\n    ComputeRho(&ions, &electrons);\n    SolvePotential(phi, rho);\n    ComputeEF(phi, ef, ef_gpu);\n\n    \n\n    PushSpecies(&electrons, electrons_part_gpu, ef_gpu);\n    PushSpecies(&ions, ions_part_gpu, ef_gpu);\n\n    \n\n    if (ts % 25 == 0)\n    {\n      \n\n      double max_phi = abs(phi[0]);\n      for (int i = 0; i < domain.ni; i++)\n        if (abs(phi[i]) > max_phi)\n          max_phi = abs(phi[i]);\n\n      printf(\"TS:%i\\tnp_i:%d\\tnp_e:%d\\tdphi:%.3g\\n\", ts, ions.np, electrons.np,\n          max_phi - phi[0]);\n    }\n\n    \n\n    if (ts % 1000 == 0) WriteResults(ts);\n  }\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n  fclose(file_res);\n\n  \n\n  delete phi;\n  delete rho;\n  delete ef;\n  delete nde;\n  delete ndi;\n  hipFree(nde_gpu);\n  hipFree(ndi_gpu);\n  hipFree(ef_gpu);\n\n  \n\n  delete ions.part;\n  delete electrons.part;\n  hipFree(ions_part_gpu);\n  hipFree(electrons_part_gpu);\n\n  printf(\"Total kernel execution time (scatter particles) : %.3g (s)\\n\", sp_time * 1e-9f),\n  printf(\"Total time for %d time steps: %.3g (s)\\n\", NUM_TS, time * 1e-9f);\n  printf(\"Time per time step: %.3g (ms)\\n\", (time * 1e-6f) / NUM_TS);\n\n  \n\n  HIP_ERROR(hipDeviceReset());\n\n  return 0;\n}\n\n\n\n\n\ndouble rnd()\n{\n  return rand() / (double)RAND_MAX;\n}\n\n\n\ndouble SampleVel(double v_th)\n{\n  const int M = 12;\n  double sum  = 0;\n  for (int i = 0; i < M; i++)\n    sum += rnd();\n\n  return sqrt(0.5) * v_th * (sum - M / 2.0) / sqrt(M / 12.0);\n}\n\n\n\n\n__global__ void scatterParticle(const Particle* __restrict particles, float*__restrict den, long N)\n{\n  \n\n  long p = blockIdx.x * blockDim.x + threadIdx.x;\n  if (p < N && particles[p].alive)\n  {\n    double lc = XtoL(particles[p].x);\n    scatter(lc, 1.f, den);\n  }\n}\n\n\n\nvoid ScatterSpecies(Species* species, Particle* species_part_gpu,\n                    float* den, float* den_gpu, double &time)\n{\n  \n\n  HIP_ERROR(hipMemset(den_gpu, 0, sizeof(float) * domain.ni));\n\n  int size = species->np_alloc;\n\n  \n\n  int nblocks = 1 + size / THREADS_PER_BLOCK;\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  hipLaunchKernelGGL(scatterParticle, nblocks, THREADS_PER_BLOCK, 0, 0, species_part_gpu, den_gpu, size);\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n  \n\n  HIP_ERROR(hipMemcpy(den, den_gpu, sizeof(float) * domain.ni, hipMemcpyDeviceToHost));\n\n  \n\n  for (int i = 0; i < domain.ni; i++)\n    den[i] *= species->spwt / domain.dx;\n\n  \n\n  den[0] *= 2.0;\n  den[domain.ni - 1] *= 2.0;\n}\n\n\n\nvoid AddParticle(Species* species, double x, double v)\n{\n  \n\n  if (species->np > species->np_alloc - 1)\n  {\n    printf(\"Too many particles!\\n\");\n    exit(-1);\n  }\n\n  \n\n  species->part[species->np].x     = x;\n  species->part[species->np].v     = v;\n  species->part[species->np].alive = true;\n\n  \n\n  species->np++;\n}\n\n\n\nvoid ComputeRho(Species* ions, Species* electrons)\n{\n  double* rho = domain.rho;\n\n  for (int i = 0; i < domain.ni; i++)\n    rho[i] = ions->charge * domain.ndi[i] + electrons->charge * domain.nde[i];\n}\n\n\n\nbool SolvePotentialDirect(double* x, double* rho)\n{\n  \n\n  int ni     = domain.ni;\n  double dx2 = domain.dx * domain.dx;\n  int i;\n  double* a = new double[ni];\n  double* b = new double[ni];\n  double* c = new double[ni];\n\n  \n\n  for (i = 1; i < ni - 1; i++)\n  {\n    a[i] = 1;\n    b[i] = -2;\n    c[i] = 1;\n  }\n\n  \n\n  a[0]      = 0;\n  b[0]      = 1;\n  c[0]      = 0;\n  a[ni - 1] = 0;\n  b[ni - 1] = 1;\n  c[ni - 1] = 0;\n\n  \n\n  for (i = 1; i < domain.ni - 1; i++)\n    x[i] = -rho[i] * dx2 / EPS_0;\n\n  x[0]      = 0;\n  x[ni - 1] = 0;\n\n  \n\n  c[0] /= b[0]; \n\n  x[0] /= b[0]; \n\n  for (i = 1; i < ni; i++)\n  {\n    double id = (b[i] - c[i - 1] * a[i]); \n\n    c[i] /= id;                           \n\n    x[i] = (x[i] - x[i - 1] * a[i]) / id;\n  }\n\n  \n\n  for (i = ni - 2; i >= 0; i--)\n    x[i] = x[i] - c[i] * x[i + 1];\n\n  return true;\n}\n\n\n\nbool SolvePotential(double* phi, double* rho)\n{\n  double L2;\n  double dx2 = domain.dx * domain.dx; \n\n\n  \n\n  phi[0] = phi[domain.ni - 1] = 0;\n\n  \n\n  for (int solver_it = 0; solver_it < 40000; solver_it++)\n  {\n    \n\n    for (int i = 1; i < domain.ni - 1; i++)\n    {\n      \n\n      double g = 0.5 * (phi[i - 1] + phi[i + 1] + dx2 * rho[i] / EPS_0);\n      phi[i]   = phi[i] + 1.4 * (g - phi[i]);\n    }\n\n    \n\n    if (solver_it % 25 == 0)\n    {\n      double sum = 0;\n      for (int i = 1; i < domain.ni - 1; i++)\n      {\n        double R = -rho[i] / EPS_0 - (phi[i - 1] - 2 * phi[i] + phi[i + 1]) / dx2;\n        sum += R * R;\n      }\n      L2 = sqrt(sum) / domain.ni;\n      if (L2 < 1e-4)\n      {\n        return true;\n      }\n    }\n  }\n  printf(\"Gauss-Seidel solver failed to converge, L2=%.3g!\\n\", L2);\n  return false;\n}\n\n\n\nvoid ComputeEF(double* phi, double* ef, double* ef_gpu)\n{\n  for (int i = 1; i < domain.ni - 1; i++)\n    ef[i] = -(phi[i + 1] - phi[i - 1]) / (2 * domain.dx); \n\n\n  \n\n  ef[0]             = -(phi[1] - phi[0]) / domain.dx;\n  ef[domain.ni - 1] = -(phi[domain.ni - 1] - phi[domain.ni - 2]) / domain.dx;\n\n  \n\n  HIP_ERROR(hipMemcpy(ef_gpu, ef, domain.ni * sizeof(double), hipMemcpyHostToDevice));\n}\n\n\n\n__global__ void pushParticle(Particle*__restrict particles, const double*__restrict ef, double qm, long N)\n{\n  \n\n  long p = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (p < N && particles[p].alive)\n  {\n    \n\n    Particle* part = &particles[p];\n\n    \n\n    double lc = XtoL(part->x);\n\n    \n\n    double part_ef = gather(lc, ef);\n\n    \n\n    part->v += DT * qm * part_ef;\n\n    \n\n    part->x += DT * part->v;\n\n    \n\n    if (part->x < X0 || part->x >= XMAX)\n      part->alive = false;\n  }\n}\n\n\n\nvoid PushSpecies(Species* species, Particle* species_part_gpu, double* ef)\n{\n  \n\n  double qm = species->charge / species->mass;\n\n  int size = species->np_alloc;\n\n  \n\n  int nblocks = 1 + size / THREADS_PER_BLOCK;\n  hipLaunchKernelGGL(pushParticle, nblocks, THREADS_PER_BLOCK, 0, 0, species_part_gpu, ef, qm, size);\n}\n\n\n\n\n\n__global__ void rewindParticle(Particle*__restrict particles, const double*__restrict ef, double qm, long N)\n{\n  \n\n  long p = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (p < N && particles[p].alive)\n  {\n    \n\n    Particle* part = &particles[p];\n\n    \n\n    double lc = XtoL(part->x);\n\n    \n\n    double part_ef = gather(lc, ef);\n\n    \n\n    part->v -= 0.5 * DT * qm * part_ef;\n  }\n}\n\n\n\n\nvoid RewindSpecies(Species* species, Particle* species_part_gpu, double* ef)\n{\n  \n\n  double qm = species->charge / species->mass;\n\n  int size = species->np_alloc;\n\n  \n\n  int nblocks = 1 + size / THREADS_PER_BLOCK;\n  hipLaunchKernelGGL(rewindParticle, nblocks, THREADS_PER_BLOCK, 0, 0, species_part_gpu, ef, qm, size);\n}\n\n\n\n\n__device__ double XtoL(double pos)\n{\n  double li = (pos - 0) / DX;\n  return li;\n}\n\n\n\n\n__device__ void scatter(double lc, float value, float* field)\n{\n  int i    = (int)lc;\n  float di = lc - i;\n  atomicAdd(&(field[i]), value * (1 - di));\n  atomicAdd(&(field[i + 1]), value * (di));\n}\n\n\n\n__device__ double gather(double lc, const double* field)\n{\n  int i     = (int)lc;\n  double di = lc - i;\n\n  \n\n  double val = field[i] * (1 - di) + field[i + 1] * (di);\n  return val;\n}\n\n\n\n\nvoid WriteResults(int ts)\n{\n  fprintf(file_res, \"ZONE I=%d T=ZONE_%06d\\n\", domain.ni, ts);\n  for (int i = 0; i < domain.ni; i++)\n  {\n    fprintf(file_res, \"%g %g %g %g %g %g\\n\", i * domain.dx, domain.nde[i], domain.ndi[i],\n        domain.rho[i], domain.phi[i], domain.ef[i]);\n  }\n\n  fflush(file_res);\n}\n"}}
{"kernel_name": "sheath", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n#include <omp.h>\n\n\n\n\n#define EPS_0 8.85418782e-12 \n\n#define K 1.38065e-23        \n\n#define ME 9.10938215e-31    \n\n#define QE 1.602176565e-19   \n\n#define AMU 1.660538921e-27  \n\n#define EV_TO_K 11604.52     \n\n\n\n\n#define PLASMA_DEN 1e16      \n\n#define NUM_IONS 500000      \n\n#define NUM_ELECTRONS 500000 \n\n#define DX 1e-4              \n\n#define NC 100               \n\n#define NUM_TS 1000          \n\n#define DT 1e-11             \n\n#define ELECTRON_TEMP 3.0    \n\n#define ION_TEMP 1.0         \n\n\n\n\n#define X0 0           \n\n#define XL NC* DX      \n\n#define XMAX (X0 + XL) \n\n\nconst int THREADS_PER_BLOCK = 256;\n\n\nstruct Domain\n{\n  const int ni      = NC + 1; \n\n  const double x0   = X0;\n  const double dx   = DX;\n  const double xl   = XL;\n  const double xmax = XMAX;\n\n  \n\n  double* phi; \n\n  double* ef;  \n\n  double* rho; \n\n\n  float* ndi; \n\n  float* nde; \n\n};\n\n\n\nstruct Particle\n{\n  double x;   \n\n  double v;   \n\n  bool alive; \n\n};\n\n\n\nstruct Species\n{\n  double mass;   \n\n  double charge; \n\n  double spwt;   \n\n\n  int np;             \n\n  int np_alloc;       \n\n  Particle* part;     \n\n};\n\n\n\ndouble rnd();\ndouble SampleVel(double v_th);\nvoid ScatterSpecies(Species* species, Particle* particles, float* den, double &time);\nvoid ComputeRho(Species* ions, Species* electrons);\nbool SolvePotential(double* phi, double* rho);\nbool SolvePotentialDirect(double* phi, double* rho);\nvoid ComputeEF(double* phi, double* ef);\nvoid PushSpecies(Species* species, Particle* particles, double* ef);\nvoid RewindSpecies(Species* species, Particle* particles, double* ef);\nvoid AddParticle(Species* species, double x, double v);\ndouble XtoL(double pos);\ndouble gather(double lc, const double* field);\nvoid scatter(double lc, float value, float* field);\n\nvoid WriteResults(int ts);\n\n\n\nDomain domain;\n\nFILE* file_res;\n\n\n\nint main(int argc, char* argv[])\n{\n  int p;\n  int ts; \n\n  double sp_time = 0.0; \n\n\n  domain.phi = new double[domain.ni]; \n\n  domain.rho = new double[domain.ni]; \n\n  domain.ef  = new double[domain.ni]; \n\n  domain.nde = new float[domain.ni];  \n\n  domain.ndi = new float[domain.ni];  \n\n\n  \n\n  double* phi = domain.phi;\n  double* rho = domain.rho;\n  double* ef  = domain.ef;\n  float* nde  = domain.nde;\n  float* ndi  = domain.ndi;\n\n  \n\n  memset(phi, 0, sizeof(double) * domain.ni);\n\n  \n\n  Species ions;\n  Species electrons;\n\n  \n\n  ions.mass     = 16 * AMU;\n  ions.charge   = QE;\n  ions.spwt     = PLASMA_DEN * domain.xl / NUM_IONS;\n  ions.np       = 0;\n  ions.np_alloc = NUM_IONS;\n  ions.part     = new Particle[NUM_IONS];\n\n  electrons.mass     = ME; \n\n  electrons.charge   = -QE;\n  electrons.spwt     = PLASMA_DEN * domain.xl / NUM_ELECTRONS;\n  electrons.np       = 0;\n  electrons.np_alloc = NUM_ELECTRONS;\n  electrons.part     = new Particle[NUM_ELECTRONS];\n\n  Particle *ions_part = ions.part;\n  Particle *electrons_part = electrons.part;\n\n  \n\n  srand(123);\n\n  \n\n  double delta_ions = domain.xl / NUM_IONS;\n  double v_thi      = sqrt(2 * K * ION_TEMP * EV_TO_K / ions.mass);\n  for (p = 0; p < NUM_IONS; p++)\n  {\n    double x = domain.x0 + p * delta_ions;\n    double v = SampleVel(v_thi);\n    AddParticle(&ions, x, v);\n  }\n\n  \n\n  double delta_electrons = domain.xl / NUM_ELECTRONS;\n  double v_the           = sqrt(2 * K * ELECTRON_TEMP * EV_TO_K / electrons.mass);\n  for (p = 0; p < NUM_ELECTRONS; p++)\n  {\n    double x = domain.x0 + p * delta_electrons;\n    double v = SampleVel(v_the);\n    AddParticle(&electrons, x, v);\n  }\n\n  #pragma omp target data map(to: ions_part[0:NUM_IONS], electrons_part[0:NUM_ELECTRONS]) \\\n                          map(alloc: nde[0:domain.ni], ndi[0:domain.ni], ef[0:domain.ni])\n  {\n\n  \n\n  ScatterSpecies(&ions, ions_part, ndi, sp_time);\n  ScatterSpecies(&electrons, electrons_part, nde, sp_time);\n\n  \n\n  ComputeRho(&ions, &electrons);\n\n  SolvePotential(phi, rho);\n\n  ComputeEF(phi, ef);\n\n  RewindSpecies(&ions, ions_part, ef);\n  RewindSpecies(&electrons, electrons_part, ef);\n\n  \n\n  file_res = fopen(\"result.dat\", \"w\");\n  fprintf(file_res, \"VARIABLES = x nde ndi rho phi ef\\n\");\n  WriteResults(0);\n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  for (ts = 1; ts <= NUM_TS; ts++)\n  {\n    \n\n    ScatterSpecies(&ions, ions_part, ndi, sp_time);\n    ScatterSpecies(&electrons, electrons_part, nde, sp_time);\n\n    ComputeRho(&ions, &electrons);\n    SolvePotential(phi, rho);\n    ComputeEF(phi, ef);\n\n    \n\n    PushSpecies(&electrons, electrons_part, ef);\n    PushSpecies(&ions, ions_part, ef);\n\n    \n\n    if (ts % 25 == 0)\n    {\n      \n\n      double max_phi = abs(phi[0]);\n      for (int i = 0; i < domain.ni; i++)\n        if (abs(phi[i]) > max_phi)\n          max_phi = abs(phi[i]);\n\n      printf(\"TS:%i\\tnp_i:%d\\tnp_e:%d\\tdphi:%.3g\\n\", ts, ions.np, electrons.np,\n          max_phi - phi[0]);\n    }\n\n    \n\n    if (ts % 1000 == 0)\n      WriteResults(ts);\n  }\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n  fclose(file_res);\n\n  \n\n  delete phi;\n  delete rho;\n  delete ef;\n  delete nde;\n  delete ndi;\n\n  \n\n  delete ions.part;\n  delete electrons.part;\n\n  printf(\"Total kernel execution time (scatter particles) : %.3g (s)\\n\", sp_time * 1e-9f),\n  printf(\"Total time for %d time steps: %.3g (s)\\n\", NUM_TS, time * 1e-9f);\n  printf(\"Time per time step: %.3g (ms)\\n\", (time * 1e-6f) / NUM_TS);\n\n  } \n\n\n  return 0;\n}\n\n\n\n\n\ndouble rnd()\n{\n  return rand() / (double)RAND_MAX;\n}\n\n\n\ndouble SampleVel(double v_th)\n{\n  const int M = 12;\n  double sum  = 0;\n  for (int i = 0; i < M; i++)\n    sum += rnd();\n\n  return sqrt(0.5) * v_th * (sum - M / 2.0) / sqrt(M / 12.0);\n}\n\n\n\n\nvoid ScatterSpecies(Species* species, Particle* particles, float* den, double &time)\n{\n  \n\n  int nodes = domain.ni;\n\n  #pragma omp target teams distribute parallel for thread_limit(THREADS_PER_BLOCK)\n  for (int p = 0; p < nodes; p++) {\n    den[p] = 0;\n  }\n\n  int size = species->np_alloc;\n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  #pragma omp target teams distribute parallel for thread_limit(THREADS_PER_BLOCK)\n  for (long p = 0; p < size; p++)\n  if (particles[p].alive)\n  {\n    double lc = XtoL(particles[p].x);\n    scatter(lc, 1.f, den);\n  }\n\n  auto end = std::chrono::steady_clock::now();\n  time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n  \n\n  #pragma omp target update from (den[0:nodes])\n\n  \n\n  for (int i = 0; i < domain.ni; i++)\n    den[i] *= species->spwt / domain.dx;\n\n  \n\n  den[0] *= 2.0;\n  den[domain.ni - 1] *= 2.0;\n}\n\n\n\nvoid AddParticle(Species* species, double x, double v)\n{\n  \n\n  if (species->np > species->np_alloc - 1)\n  {\n    printf(\"Too many particles!\\n\");\n    exit(-1);\n  }\n\n  \n\n  species->part[species->np].x     = x;\n  species->part[species->np].v     = v;\n  species->part[species->np].alive = true;\n\n  \n\n  species->np++;\n}\n\n\n\nvoid ComputeRho(Species* ions, Species* electrons)\n{\n  double* rho = domain.rho;\n\n  for (int i = 0; i < domain.ni; i++)\n    rho[i] = ions->charge * domain.ndi[i] + electrons->charge * domain.nde[i];\n}\n\n\n\nbool SolvePotentialDirect(double* x, double* rho)\n{\n  \n\n  int ni     = domain.ni;\n  double dx2 = domain.dx * domain.dx;\n  int i;\n  double* a = new double[ni];\n  double* b = new double[ni];\n  double* c = new double[ni];\n\n  \n\n  for (i = 1; i < ni - 1; i++)\n  {\n    a[i] = 1;\n    b[i] = -2;\n    c[i] = 1;\n  }\n\n  \n\n  a[0]      = 0;\n  b[0]      = 1;\n  c[0]      = 0;\n  a[ni - 1] = 0;\n  b[ni - 1] = 1;\n  c[ni - 1] = 0;\n\n  \n\n  for (i = 1; i < domain.ni - 1; i++)\n    x[i] = -rho[i] * dx2 / EPS_0;\n\n  x[0]      = 0;\n  x[ni - 1] = 0;\n\n  \n\n  c[0] /= b[0]; \n\n  x[0] /= b[0]; \n\n  for (i = 1; i < ni; i++)\n  {\n    double id = (b[i] - c[i - 1] * a[i]); \n\n    c[i] /= id;                           \n\n    x[i] = (x[i] - x[i - 1] * a[i]) / id;\n  }\n\n  \n\n  for (i = ni - 2; i >= 0; i--)\n    x[i] = x[i] - c[i] * x[i + 1];\n\n  return true;\n}\n\n\n\nbool SolvePotential(double* phi, double* rho)\n{\n  double L2;\n  double dx2 = domain.dx * domain.dx; \n\n\n  \n\n  phi[0] = phi[domain.ni - 1] = 0;\n\n  \n\n  for (int solver_it = 0; solver_it < 40000; solver_it++)\n  {\n    \n\n    for (int i = 1; i < domain.ni - 1; i++)\n    {\n      \n\n      double g = 0.5 * (phi[i - 1] + phi[i + 1] + dx2 * rho[i] / EPS_0);\n      phi[i]   = phi[i] + 1.4 * (g - phi[i]);\n    }\n\n    \n\n    if (solver_it % 25 == 0)\n    {\n      double sum = 0;\n      for (int i = 1; i < domain.ni - 1; i++)\n      {\n        double R = -rho[i] / EPS_0 - (phi[i - 1] - 2 * phi[i] + phi[i + 1]) / dx2;\n        sum += R * R;\n      }\n      L2 = sqrt(sum) / domain.ni;\n      if (L2 < 1e-4)\n      {\n        return true;\n      }\n    }\n  }\n  printf(\"Gauss-Seidel solver failed to converge, L2=%.3g!\\n\", L2);\n  return false;\n}\n\n\n\nvoid ComputeEF(double* phi, double* ef)\n{\n  for (int i = 1; i < domain.ni - 1; i++)\n    ef[i] = -(phi[i + 1] - phi[i - 1]) / (2 * domain.dx); \n\n\n  \n\n  ef[0]             = -(phi[1] - phi[0]) / domain.dx;\n  ef[domain.ni - 1] = -(phi[domain.ni - 1] - phi[domain.ni - 2]) / domain.dx;\n\n  \n\n  #pragma omp target update to (ef[0:domain.ni])\n}\n\n\n\nvoid PushSpecies(Species* species, Particle* particles, double* ef)\n{\n  \n\n  double qm = species->charge / species->mass;\n\n  int size = species->np_alloc;\n\n  \n\n  #pragma omp target teams distribute parallel for thread_limit(THREADS_PER_BLOCK)\n  for (long p = 0; p < size; p++)\n    if (particles[p].alive)\n    {\n      \n\n      Particle* part = &particles[p];\n\n      \n\n      double lc = XtoL(part->x);\n\n      \n\n      double part_ef = gather(lc, ef);\n\n      \n\n      part->v += DT * qm * part_ef;\n\n      \n\n      part->x += DT * part->v;\n\n      \n\n      if (part->x < X0 || part->x >= XMAX)\n        part->alive = false;\n    }\n}\n\n\n\nvoid RewindSpecies(Species* species, Particle* particles, double* ef)\n{\n  \n\n  double qm = species->charge / species->mass;\n\n  int size = species->np_alloc;\n\n  \n\n  #pragma omp target teams distribute parallel for thread_limit(THREADS_PER_BLOCK)\n  for (long p = 0; p < size; p++)\n    if (particles[p].alive)\n    {\n      \n\n      Particle* part = &particles[p];\n\n      \n\n      double lc = XtoL(part->x);\n\n      \n\n      double part_ef = gather(lc, ef);\n\n      \n\n      part->v -= 0.5 * DT * qm * part_ef;\n    }\n}\n\n\n#pragma omp declare target\n\n\ndouble XtoL(double pos)\n{\n  double li = (pos - 0) / DX;\n  return li;\n}\n\n\n\nvoid scatter(double lc, float value, float* field)\n{\n  int i    = (int)lc;\n  float di = lc - i;\n#pragma omp atomic update\n  field[i] += value * (1 - di);\n#pragma omp atomic update\n  field[i + 1] += value * (di);\n}\n\n\n\ndouble gather(double lc, const double* field)\n{\n  int i     = (int)lc;\n  double di = lc - i;\n\n  \n\n  double val = field[i] * (1 - di) + field[i + 1] * (di);\n  return val;\n}\n#pragma omp end declare target\n\n\n\n\nvoid WriteResults(int ts)\n{\n  fprintf(file_res, \"ZONE I=%d T=ZONE_%06d\\n\", domain.ni, ts);\n  for (int i = 0; i < domain.ni; i++)\n  {\n    fprintf(file_res, \"%g %g %g %g %g %g\\n\", i * domain.dx, domain.nde[i], domain.ndi[i],\n        domain.rho[i], domain.phi[i], domain.ef[i]);\n  }\n\n  fflush(file_res);\n}\n"}}
{"kernel_name": "sheath", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n\n\n\n\n#define EPS_0 8.85418782e-12 \n\n#define K 1.38065e-23        \n\n#define ME 9.10938215e-31    \n\n#define QE 1.602176565e-19   \n\n#define AMU 1.660538921e-27  \n\n#define EV_TO_K 11604.52     \n\n\n\n\n#define PLASMA_DEN 1e16      \n\n#define NUM_IONS 500000      \n\n#define NUM_ELECTRONS 500000 \n\n#define DX 1e-4              \n\n#define NC 100               \n\n#define NUM_TS 1000          \n\n#define DT 1e-11             \n\n#define ELECTRON_TEMP 3.0    \n\n#define ION_TEMP 1.0         \n\n\n\n\n#define X0 0           \n\n#define XL NC* DX      \n\n#define XMAX (X0 + XL) \n\n\nconst int THREADS_PER_BLOCK = 256;\n\n\nstruct Domain\n{\n  const int ni      = NC + 1; \n\n  const double x0   = X0;\n  const double dx   = DX;\n  const double xl   = XL;\n  const double xmax = XMAX;\n\n  \n\n  double* phi; \n\n  double* ef;  \n\n  double* rho; \n\n\n  float* ndi; \n\n  float* nde; \n\n};\n\n\n\nstruct Particle\n{\n  double x;   \n\n  double v;   \n\n  bool alive; \n\n};\n\n\n\nstruct Species\n{\n  double mass;   \n\n  double charge; \n\n  double spwt;   \n\n\n  int np;             \n\n  int np_alloc;       \n\n  Particle* part;     \n\n};\n\n\n\ndouble rnd();\ndouble SampleVel(double v_th);\nvoid ScatterSpecies(Species* species, Particle* particles, float* den, double &time);\nvoid ComputeRho(Species* ions, Species* electrons);\nbool SolvePotential(double* phi, double* rho);\nbool SolvePotentialDirect(double* phi, double* rho);\nvoid ComputeEF(double* phi, double* ef);\nvoid PushSpecies(Species* species, Particle* particles, double* ef);\nvoid RewindSpecies(Species* species, Particle* particles, double* ef);\nvoid AddParticle(Species* species, double x, double v);\ndouble XtoL(double pos);\ndouble gather(double lc, const double* field);\nvoid scatter(double lc, float value, float* field);\n\nvoid WriteResults(int ts);\n\n\n\nDomain domain;\n\nFILE* file_res;\n\n\n\nint main(int argc, char* argv[])\n{\n  int p;\n  int ts; \n\n  double sp_time = 0.0; \n\n\n  domain.phi = new double[domain.ni]; \n\n  domain.rho = new double[domain.ni]; \n\n  domain.ef  = new double[domain.ni]; \n\n  domain.nde = new float[domain.ni];  \n\n  domain.ndi = new float[domain.ni];  \n\n\n  \n\n  double* phi = domain.phi;\n  double* rho = domain.rho;\n  double* ef  = domain.ef;\n  float* nde  = domain.nde;\n  float* ndi  = domain.ndi;\n\n  \n\n  memset(phi, 0, sizeof(double) * domain.ni);\n\n  \n\n  Species ions;\n  Species electrons;\n\n  \n\n  ions.mass     = 16 * AMU;\n  ions.charge   = QE;\n  ions.spwt     = PLASMA_DEN * domain.xl / NUM_IONS;\n  ions.np       = 0;\n  ions.np_alloc = NUM_IONS;\n  ions.part     = new Particle[NUM_IONS];\n\n  electrons.mass     = ME; \n\n  electrons.charge   = -QE;\n  electrons.spwt     = PLASMA_DEN * domain.xl / NUM_ELECTRONS;\n  electrons.np       = 0;\n  electrons.np_alloc = NUM_ELECTRONS;\n  electrons.part     = new Particle[NUM_ELECTRONS];\n\n  Particle *ions_part = ions.part;\n  Particle *electrons_part = electrons.part;\n\n  \n\n  srand(123);\n\n  \n\n  double delta_ions = domain.xl / NUM_IONS;\n  double v_thi      = sqrt(2 * K * ION_TEMP * EV_TO_K / ions.mass);\n  for (p = 0; p < NUM_IONS; p++)\n  {\n    double x = domain.x0 + p * delta_ions;\n    double v = SampleVel(v_thi);\n    AddParticle(&ions, x, v);\n  }\n\n  \n\n  double delta_electrons = domain.xl / NUM_ELECTRONS;\n  double v_the           = sqrt(2 * K * ELECTRON_TEMP * EV_TO_K / electrons.mass);\n  for (p = 0; p < NUM_ELECTRONS; p++)\n  {\n    double x = domain.x0 + p * delta_electrons;\n    double v = SampleVel(v_the);\n    AddParticle(&electrons, x, v);\n  }\n\n    {\n\n  \n\n  ScatterSpecies(&ions, ions_part, ndi, sp_time);\n  ScatterSpecies(&electrons, electrons_part, nde, sp_time);\n\n  \n\n  ComputeRho(&ions, &electrons);\n\n  SolvePotential(phi, rho);\n\n  ComputeEF(phi, ef);\n\n  RewindSpecies(&ions, ions_part, ef);\n  RewindSpecies(&electrons, electrons_part, ef);\n\n  \n\n  file_res = fopen(\"result.dat\", \"w\");\n  fprintf(file_res, \"VARIABLES = x nde ndi rho phi ef\\n\");\n  WriteResults(0);\n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  for (ts = 1; ts <= NUM_TS; ts++)\n  {\n    \n\n    ScatterSpecies(&ions, ions_part, ndi, sp_time);\n    ScatterSpecies(&electrons, electrons_part, nde, sp_time);\n\n    ComputeRho(&ions, &electrons);\n    SolvePotential(phi, rho);\n    ComputeEF(phi, ef);\n\n    \n\n    PushSpecies(&electrons, electrons_part, ef);\n    PushSpecies(&ions, ions_part, ef);\n\n    \n\n    if (ts % 25 == 0)\n    {\n      \n\n      double max_phi = abs(phi[0]);\n      for (int i = 0; i < domain.ni; i++)\n        if (abs(phi[i]) > max_phi)\n          max_phi = abs(phi[i]);\n\n      printf(\"TS:%i\\tnp_i:%d\\tnp_e:%d\\tdphi:%.3g\\n\", ts, ions.np, electrons.np,\n          max_phi - phi[0]);\n    }\n\n    \n\n    if (ts % 1000 == 0)\n      WriteResults(ts);\n  }\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n  fclose(file_res);\n\n  \n\n  delete phi;\n  delete rho;\n  delete ef;\n  delete nde;\n  delete ndi;\n\n  \n\n  delete ions.part;\n  delete electrons.part;\n\n  printf(\"Total kernel execution time (scatter particles) : %.3g (s)\\n\", sp_time * 1e-9f),\n  printf(\"Total time for %d time steps: %.3g (s)\\n\", NUM_TS, time * 1e-9f);\n  printf(\"Time per time step: %.3g (ms)\\n\", (time * 1e-6f) / NUM_TS);\n\n  } \n\n\n  return 0;\n}\n\n\n\n\n\ndouble rnd()\n{\n  return rand() / (double)RAND_MAX;\n}\n\n\n\ndouble SampleVel(double v_th)\n{\n  const int M = 12;\n  double sum  = 0;\n  for (int i = 0; i < M; i++)\n    sum += rnd();\n\n  return sqrt(0.5) * v_th * (sum - M / 2.0) / sqrt(M / 12.0);\n}\n\n\n\n\nvoid ScatterSpecies(Species* species, Particle* particles, float* den, double &time)\n{\n  \n\n  int nodes = domain.ni;\n\n    for (int p = 0; p < nodes; p++) {\n    den[p] = 0;\n  }\n\n  int size = species->np_alloc;\n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n    for (long p = 0; p < size; p++)\n  if (particles[p].alive)\n  {\n    double lc = XtoL(particles[p].x);\n    scatter(lc, 1.f, den);\n  }\n\n  auto end = std::chrono::steady_clock::now();\n  time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n  \n\n  \n  \n\n  for (int i = 0; i < domain.ni; i++)\n    den[i] *= species->spwt / domain.dx;\n\n  \n\n  den[0] *= 2.0;\n  den[domain.ni - 1] *= 2.0;\n}\n\n\n\nvoid AddParticle(Species* species, double x, double v)\n{\n  \n\n  if (species->np > species->np_alloc - 1)\n  {\n    printf(\"Too many particles!\\n\");\n    exit(-1);\n  }\n\n  \n\n  species->part[species->np].x     = x;\n  species->part[species->np].v     = v;\n  species->part[species->np].alive = true;\n\n  \n\n  species->np++;\n}\n\n\n\nvoid ComputeRho(Species* ions, Species* electrons)\n{\n  double* rho = domain.rho;\n\n  for (int i = 0; i < domain.ni; i++)\n    rho[i] = ions->charge * domain.ndi[i] + electrons->charge * domain.nde[i];\n}\n\n\n\nbool SolvePotentialDirect(double* x, double* rho)\n{\n  \n\n  int ni     = domain.ni;\n  double dx2 = domain.dx * domain.dx;\n  int i;\n  double* a = new double[ni];\n  double* b = new double[ni];\n  double* c = new double[ni];\n\n  \n\n  for (i = 1; i < ni - 1; i++)\n  {\n    a[i] = 1;\n    b[i] = -2;\n    c[i] = 1;\n  }\n\n  \n\n  a[0]      = 0;\n  b[0]      = 1;\n  c[0]      = 0;\n  a[ni - 1] = 0;\n  b[ni - 1] = 1;\n  c[ni - 1] = 0;\n\n  \n\n  for (i = 1; i < domain.ni - 1; i++)\n    x[i] = -rho[i] * dx2 / EPS_0;\n\n  x[0]      = 0;\n  x[ni - 1] = 0;\n\n  \n\n  c[0] /= b[0]; \n\n  x[0] /= b[0]; \n\n  for (i = 1; i < ni; i++)\n  {\n    double id = (b[i] - c[i - 1] * a[i]); \n\n    c[i] /= id;                           \n\n    x[i] = (x[i] - x[i - 1] * a[i]) / id;\n  }\n\n  \n\n  for (i = ni - 2; i >= 0; i--)\n    x[i] = x[i] - c[i] * x[i + 1];\n\n  return true;\n}\n\n\n\nbool SolvePotential(double* phi, double* rho)\n{\n  double L2;\n  double dx2 = domain.dx * domain.dx; \n\n\n  \n\n  phi[0] = phi[domain.ni - 1] = 0;\n\n  \n\n  for (int solver_it = 0; solver_it < 40000; solver_it++)\n  {\n    \n\n    for (int i = 1; i < domain.ni - 1; i++)\n    {\n      \n\n      double g = 0.5 * (phi[i - 1] + phi[i + 1] + dx2 * rho[i] / EPS_0);\n      phi[i]   = phi[i] + 1.4 * (g - phi[i]);\n    }\n\n    \n\n    if (solver_it % 25 == 0)\n    {\n      double sum = 0;\n      for (int i = 1; i < domain.ni - 1; i++)\n      {\n        double R = -rho[i] / EPS_0 - (phi[i - 1] - 2 * phi[i] + phi[i + 1]) / dx2;\n        sum += R * R;\n      }\n      L2 = sqrt(sum) / domain.ni;\n      if (L2 < 1e-4)\n      {\n        return true;\n      }\n    }\n  }\n  printf(\"Gauss-Seidel solver failed to converge, L2=%.3g!\\n\", L2);\n  return false;\n}\n\n\n\nvoid ComputeEF(double* phi, double* ef)\n{\n  for (int i = 1; i < domain.ni - 1; i++)\n    ef[i] = -(phi[i + 1] - phi[i - 1]) / (2 * domain.dx); \n\n\n  \n\n  ef[0]             = -(phi[1] - phi[0]) / domain.dx;\n  ef[domain.ni - 1] = -(phi[domain.ni - 1] - phi[domain.ni - 2]) / domain.dx;\n\n  \n\n  }\n\n\n\nvoid PushSpecies(Species* species, Particle* particles, double* ef)\n{\n  \n\n  double qm = species->charge / species->mass;\n\n  int size = species->np_alloc;\n\n  \n\n    for (long p = 0; p < size; p++)\n    if (particles[p].alive)\n    {\n      \n\n      Particle* part = &particles[p];\n\n      \n\n      double lc = XtoL(part->x);\n\n      \n\n      double part_ef = gather(lc, ef);\n\n      \n\n      part->v += DT * qm * part_ef;\n\n      \n\n      part->x += DT * part->v;\n\n      \n\n      if (part->x < X0 || part->x >= XMAX)\n        part->alive = false;\n    }\n}\n\n\n\nvoid RewindSpecies(Species* species, Particle* particles, double* ef)\n{\n  \n\n  double qm = species->charge / species->mass;\n\n  int size = species->np_alloc;\n\n  \n\n    for (long p = 0; p < size; p++)\n    if (particles[p].alive)\n    {\n      \n\n      Particle* part = &particles[p];\n\n      \n\n      double lc = XtoL(part->x);\n\n      \n\n      double part_ef = gather(lc, ef);\n\n      \n\n      part->v -= 0.5 * DT * qm * part_ef;\n    }\n}\n\n\n\n\ndouble XtoL(double pos)\n{\n  double li = (pos - 0) / DX;\n  return li;\n}\n\n\n\nvoid scatter(double lc, float value, float* field)\n{\n  int i    = (int)lc;\n  float di = lc - i;\n  field[i] += value * (1 - di);\n  field[i + 1] += value * (di);\n}\n\n\n\ndouble gather(double lc, const double* field)\n{\n  int i     = (int)lc;\n  double di = lc - i;\n\n  \n\n  double val = field[i] * (1 - di) + field[i + 1] * (di);\n  return val;\n}\n\n\n\n\nvoid WriteResults(int ts)\n{\n  fprintf(file_res, \"ZONE I=%d T=ZONE_%06d\\n\", domain.ni, ts);\n  for (int i = 0; i < domain.ni; i++)\n  {\n    fprintf(file_res, \"%g %g %g %g %g %g\\n\", i * domain.dx, domain.nde[i], domain.ndi[i],\n        domain.rho[i], domain.phi[i], domain.ef[i]);\n  }\n\n  fflush(file_res);\n}"}}
{"kernel_name": "sheath", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n\n\n\n\n#define EPS_0 8.85418782e-12 \n\n#define K 1.38065e-23        \n\n#define ME 9.10938215e-31    \n\n#define QE 1.602176565e-19   \n\n#define AMU 1.660538921e-27  \n\n#define EV_TO_K 11604.52     \n\n\n\n\n#define PLASMA_DEN 1e16      \n\n#define NUM_IONS 500000      \n\n#define NUM_ELECTRONS 500000 \n\n#define DX 1e-4              \n\n#define NC 100               \n\n#define NUM_TS 1000          \n\n#define DT 1e-11             \n\n#define ELECTRON_TEMP 3.0    \n\n#define ION_TEMP 1.0         \n\n\n\n\n#define X0 0           \n\n#define XL NC* DX      \n\n#define XMAX (X0 + XL) \n\n\nconst int THREADS_PER_BLOCK = 256;\n\n\nstruct Domain\n{\n  const int ni      = NC + 1; \n\n  const double x0   = X0;\n  const double dx   = DX;\n  const double xl   = XL;\n  const double xmax = XMAX;\n\n  \n\n  double* phi; \n\n  double* ef;  \n\n  double* rho; \n\n\n  float* ndi; \n\n  float* nde; \n\n};\n\n\n\nstruct Particle\n{\n  double x;   \n\n  double v;   \n\n  bool alive; \n\n};\n\n\n\nstruct Species\n{\n  double mass;   \n\n  double charge; \n\n  double spwt;   \n\n\n  int np;             \n\n  int np_alloc;       \n\n  Particle* part;     \n\n};\n\n\n\ndouble rnd();\ndouble SampleVel(double v_th);\nvoid ScatterSpecies(sycl::queue &q, Species* species, Particle *species_part_gpu,\n                    float* den, float *den_gpu, double &time);\nvoid ComputeRho(Species* ions, Species* electrons);\nbool SolvePotential(double* phi, double* rho);\nbool SolvePotentialDirect(double* phi, double* rho);\nvoid ComputeEF(sycl::queue &q, double* phi, double* ef, double *ef_gpu);\nvoid PushSpecies(sycl::queue &q, Species* species, Particle *species_part_gpu, double *ef_gpu);\nvoid RewindSpecies(sycl::queue &q, Species* species, Particle *species_part_gpu, double *ef_gpu);\nvoid AddParticle(Species* species, double x, double v);\ndouble XtoL(double pos);\ndouble gather(double lc, const double* field);\nvoid scatter(double lc, float value, float* field);\n\nvoid WriteResults(int ts);\n\n\n\nDomain domain;\n\nFILE* file_res;\n\n\n\nint main(int argc, char* argv[])\n{\n  int p;\n  int ts; \n\n  double sp_time = 0.0; \n\n\n  domain.phi = new double[domain.ni]; \n\n  domain.rho = new double[domain.ni]; \n\n  domain.ef  = new double[domain.ni]; \n\n  domain.nde = new float[domain.ni];  \n\n  domain.ndi = new float[domain.ni];  \n\n\n  \n\n  double* phi = domain.phi;\n  double* rho = domain.rho;\n  double* ef  = domain.ef;\n  float* nde  = domain.nde;\n  float* ndi  = domain.ndi;\n\n  \n\n  memset(phi, 0, sizeof(double) * domain.ni);\n\n  \n\n  Species ions;\n  Species electrons;\n\n  \n\n  ions.mass     = 16 * AMU;\n  ions.charge   = QE;\n  ions.spwt     = PLASMA_DEN * domain.xl / NUM_IONS;\n  ions.np       = 0;\n  ions.np_alloc = NUM_IONS;\n  ions.part     = new Particle[NUM_IONS];\n\n  electrons.mass     = ME; \n\n  electrons.charge   = -QE;\n  electrons.spwt     = PLASMA_DEN * domain.xl / NUM_ELECTRONS;\n  electrons.np       = 0;\n  electrons.np_alloc = NUM_ELECTRONS;\n  electrons.part     = new Particle[NUM_ELECTRONS];\n\n  \n\n  srand(123);\n\n  \n\n  double delta_ions = domain.xl / NUM_IONS;\n  double v_thi      = sqrt(2 * K * ION_TEMP * EV_TO_K / ions.mass);\n  for (p = 0; p < NUM_IONS; p++)\n  {\n    double x = domain.x0 + p * delta_ions;\n    double v = SampleVel(v_thi);\n    AddParticle(&ions, x, v);\n  }\n\n  \n\n  double delta_electrons = domain.xl / NUM_ELECTRONS;\n  double v_the           = sqrt(2 * K * ELECTRON_TEMP * EV_TO_K / electrons.mass);\n  for (p = 0; p < NUM_ELECTRONS; p++)\n  {\n    double x = domain.x0 + p * delta_electrons;\n    double v = SampleVel(v_the);\n    AddParticle(&electrons, x, v);\n  }\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  \n\n  float *nde_gpu = sycl::malloc_device<float>(domain.ni, q);\n  float *ndi_gpu = sycl::malloc_device<float>(domain.ni, q);\n  double *ef_gpu = sycl::malloc_device<double>(domain.ni, q);\n  Particle *ions_part_gpu = sycl::malloc_device<Particle>(NUM_IONS, q);\n  q.memcpy(ions_part_gpu, ions.part, NUM_IONS * sizeof(Particle)).wait();\n\n  Particle *electrons_part_gpu = sycl::malloc_device<Particle>(NUM_ELECTRONS, q);\n  q.memcpy(electrons_part_gpu, electrons.part, NUM_ELECTRONS * sizeof(Particle)).wait();\n\n  \n\n  ScatterSpecies(q, &ions, ions_part_gpu, ndi, ndi_gpu, sp_time);\n  ScatterSpecies(q, &electrons, electrons_part_gpu, nde, nde_gpu, sp_time);\n\n  \n\n  ComputeRho(&ions, &electrons);\n\n  SolvePotential(phi, rho);\n\n  ComputeEF(q, phi, ef, ef_gpu);\n\n  RewindSpecies(q, &ions, ions_part_gpu, ef_gpu);\n  RewindSpecies(q, &electrons, electrons_part_gpu, ef_gpu);\n\n  \n\n  file_res = fopen(\"result.dat\", \"w\");\n  fprintf(file_res, \"VARIABLES = x nde ndi rho phi ef\\n\");\n  WriteResults(0);\n\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  for (ts = 1; ts <= NUM_TS; ts++)\n  {\n    \n\n    ScatterSpecies(q, &ions, ions_part_gpu, ndi, ndi_gpu, sp_time);\n    ScatterSpecies(q, &electrons, electrons_part_gpu, nde, nde_gpu, sp_time);\n\n    ComputeRho(&ions, &electrons);\n    SolvePotential(phi, rho);\n    ComputeEF(q, phi, ef, ef_gpu);\n\n    \n\n    PushSpecies(q, &electrons, electrons_part_gpu, ef_gpu);\n    PushSpecies(q, &ions, ions_part_gpu, ef_gpu);\n\n    \n\n    if (ts % 25 == 0)\n    {\n      \n\n      double max_phi = abs(phi[0]);\n      for (int i = 0; i < domain.ni; i++)\n        if (abs(phi[i]) > max_phi)\n          max_phi = abs(phi[i]);\n\n      printf(\"TS:%i\\tnp_i:%d\\tnp_e:%d\\tdphi:%.3g\\n\", ts, ions.np, electrons.np,\n          max_phi - phi[0]);\n    }\n\n    \n\n    if (ts % 1000 == 0)\n      WriteResults(ts);\n  }\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n  fclose(file_res);\n\n  \n\n  delete phi;\n  delete rho;\n  delete ef;\n  delete nde;\n  delete ndi;\n  sycl::free(nde_gpu, q);\n  sycl::free(ndi_gpu, q);\n  sycl::free(ef_gpu, q);\n\n  \n\n  delete ions.part;\n  delete electrons.part;\n  sycl::free(ions_part_gpu, q);\n  sycl::free(electrons_part_gpu, q);\n\n  printf(\"Total kernel execution time (scatter particles) : %.3g (s)\\n\", sp_time * 1e-9f),\n  printf(\"Total time for %d time steps: %.3g (s)\\n\", NUM_TS, time * 1e-9f);\n  printf(\"Time per time step: %.3g (ms)\\n\", (time * 1e-6f) / NUM_TS);\n\n  return 0;\n}\n\n\n\n\n\ndouble rnd()\n{\n  return rand() / (double)RAND_MAX;\n}\n\n\n\ndouble SampleVel(double v_th)\n{\n  const int M = 12;\n  double sum  = 0;\n  for (int i = 0; i < M; i++)\n    sum += rnd();\n\n  return sqrt(0.5) * v_th * (sum - M / 2.0) / sqrt(M / 12.0);\n}\n\n\n\n\nvoid ScatterSpecies(sycl::queue &q,\n                    Species* species,\n                    Particle *species_part_gpu,\n                    float* den,\n                    float *den_gpu,\n                    double &time)\n{\n  \n\n  q.memset(den_gpu, 0, sizeof(float) * domain.ni);\n\n  int size = species->np_alloc;\n\n  \n\n  int nblocks = 1 + size / THREADS_PER_BLOCK;\n\n  sycl::range<1> gws (nblocks * THREADS_PER_BLOCK);\n  sycl::range<1> lws (THREADS_PER_BLOCK);\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class scatterParticle>(\n      sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n      long p = item.get_global_id(0);\n      if (p < size && species_part_gpu[p].alive)\n      {\n        double lc = XtoL(species_part_gpu[p].x);\n        scatter(lc, 1.f, den_gpu);\n      }\n    });\n  }).wait();\n\n  auto end = std::chrono::steady_clock::now();\n  time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n  \n\n  q.memcpy(den, den_gpu, sizeof(float) * domain.ni).wait();\n\n  \n\n  for (int i = 0; i < domain.ni; i++)\n    den[i] *= species->spwt / domain.dx;\n\n  \n\n  den[0] *= 2.0;\n  den[domain.ni - 1] *= 2.0;\n}\n\n\n\nvoid AddParticle(Species* species, double x, double v)\n{\n  \n\n  if (species->np > species->np_alloc - 1)\n  {\n    printf(\"Too many particles!\\n\");\n    exit(-1);\n  }\n\n  \n\n  species->part[species->np].x     = x;\n  species->part[species->np].v     = v;\n  species->part[species->np].alive = true;\n\n  \n\n  species->np++;\n}\n\n\n\nvoid ComputeRho(Species* ions, Species* electrons)\n{\n  double* rho = domain.rho;\n\n  for (int i = 0; i < domain.ni; i++)\n    rho[i] = ions->charge * domain.ndi[i] + electrons->charge * domain.nde[i];\n}\n\n\n\nbool SolvePotentialDirect(double* x, double* rho)\n{\n  \n\n  int ni     = domain.ni;\n  double dx2 = domain.dx * domain.dx;\n  int i;\n  double* a = new double[ni];\n  double* b = new double[ni];\n  double* c = new double[ni];\n\n  \n\n  for (i = 1; i < ni - 1; i++)\n  {\n    a[i] = 1;\n    b[i] = -2;\n    c[i] = 1;\n  }\n\n  \n\n  a[0]      = 0;\n  b[0]      = 1;\n  c[0]      = 0;\n  a[ni - 1] = 0;\n  b[ni - 1] = 1;\n  c[ni - 1] = 0;\n\n  \n\n  for (i = 1; i < domain.ni - 1; i++)\n    x[i] = -rho[i] * dx2 / EPS_0;\n\n  x[0]      = 0;\n  x[ni - 1] = 0;\n\n  \n\n  c[0] /= b[0]; \n\n  x[0] /= b[0]; \n\n  for (i = 1; i < ni; i++)\n  {\n    double id = (b[i] - c[i - 1] * a[i]); \n\n    c[i] /= id;                           \n\n    x[i] = (x[i] - x[i - 1] * a[i]) / id;\n  }\n\n  \n\n  for (i = ni - 2; i >= 0; i--)\n    x[i] = x[i] - c[i] * x[i + 1];\n\n  return true;\n}\n\n\n\nbool SolvePotential(double* phi, double* rho)\n{\n  double L2;\n  double dx2 = domain.dx * domain.dx; \n\n\n  \n\n  phi[0] = phi[domain.ni - 1] = 0;\n\n  \n\n  for (int solver_it = 0; solver_it < 40000; solver_it++)\n  {\n    \n\n    for (int i = 1; i < domain.ni - 1; i++)\n    {\n      \n\n      double g = 0.5 * (phi[i - 1] + phi[i + 1] + dx2 * rho[i] / EPS_0);\n      phi[i]   = phi[i] + 1.4 * (g - phi[i]);\n    }\n\n    \n\n    if (solver_it % 25 == 0)\n    {\n      double sum = 0;\n      for (int i = 1; i < domain.ni - 1; i++)\n      {\n        double R = -rho[i] / EPS_0 - (phi[i - 1] - 2 * phi[i] + phi[i + 1]) / dx2;\n        sum += R * R;\n      }\n      L2 = sqrt(sum) / domain.ni;\n      if (L2 < 1e-4)\n      {\n        return true;\n      }\n    }\n  }\n  printf(\"Gauss-Seidel solver failed to converge, L2=%.3g!\\n\", L2);\n  return false;\n}\n\n\n\nvoid ComputeEF(sycl::queue &q, double* phi, double* ef, double *ef_gpu)\n{\n  for (int i = 1; i < domain.ni - 1; i++)\n    ef[i] = -(phi[i + 1] - phi[i - 1]) / (2 * domain.dx); \n\n\n  \n\n  ef[0]             = -(phi[1] - phi[0]) / domain.dx;\n  ef[domain.ni - 1] = -(phi[domain.ni - 1] - phi[domain.ni - 2]) / domain.dx;\n\n  \n\n  q.memcpy(ef_gpu, ef, domain.ni * sizeof(double));\n}\n\n\n\nvoid PushSpecies(sycl::queue &q,\n                 Species* species,\n                 Particle *species_part_gpu,\n                 double *ef_gpu)\n{\n  \n\n  double qm = species->charge / species->mass;\n\n  int size = species->np_alloc;\n\n  \n\n  int nblocks = 1 + size / THREADS_PER_BLOCK;\n\n  sycl::range<1> gws (nblocks * THREADS_PER_BLOCK);\n  sycl::range<1> lws (THREADS_PER_BLOCK);\n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class pushParticle>(\n      sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n      \n\n      long p = item.get_global_id(0);\n\n      if (p < size && species_part_gpu[p].alive)\n      {\n        \n\n        Particle* part = &species_part_gpu[p];\n\n        \n\n        double lc = XtoL(part->x);\n\n        \n\n        double part_ef = gather(lc, ef_gpu);\n\n        \n\n        part->v += DT * qm * part_ef;\n\n        \n\n        part->x += DT * part->v;\n\n        \n\n        if (part->x < X0 || part->x >= XMAX)\n          part->alive = false;\n      }\n    });\n  });\n}\n\n\n\n\nvoid RewindSpecies(sycl::queue &q, Species* species, Particle *species_part_gpu, double *ef_gpu)\n{\n  \n\n  double qm = species->charge / species->mass;\n\n  int size = species->np_alloc;\n\n  \n\n  int nblocks = 1 + size / THREADS_PER_BLOCK;\n\n  sycl::range<1> gws (nblocks * THREADS_PER_BLOCK);\n  sycl::range<1> lws (THREADS_PER_BLOCK);\n\n  q.submit([&] (sycl::handler &cgh) {\n    cgh.parallel_for<class rewindParticle>(\n      sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n      \n\n      long p = item.get_global_id(0);\n\n      if (p < size && species_part_gpu[p].alive)\n      {\n        \n\n        Particle* part = &species_part_gpu[p];\n\n        \n\n        double lc = XtoL(part->x);\n\n        \n\n        double part_ef = gather(lc, ef_gpu);\n\n        \n\n        part->v -= 0.5 * DT * qm * part_ef;\n      }\n    });\n  });\n}\n\n\n\n\ndouble XtoL(double pos)\n{\n  double li = (pos - 0) / DX;\n  return li;\n}\n\n\n\n\nvoid scatter(double lc, float value, float* field)\n{\n  int i    = (int)lc;\n  float di = lc - i;\n\n  auto f1 = sycl::atomic_ref<float,\n            sycl::memory_order::relaxed,\n            sycl::memory_scope::device,\n            sycl::access::address_space::global_space> (field[i]);\n  f1.fetch_add(value * (1 - di));\n  auto f2 = sycl::atomic_ref<float,\n            sycl::memory_order::relaxed,\n            sycl::memory_scope::device,\n            sycl::access::address_space::global_space> (field[i+1]);\n  f2.fetch_add(value * di);\n}\n\n\n\ndouble gather(double lc, const double* field)\n{\n  int i     = (int)lc;\n  double di = lc - i;\n\n  \n\n  double val = field[i] * (1 - di) + field[i + 1] * (di);\n  return val;\n}\n\n\n\n\nvoid WriteResults(int ts)\n{\n  fprintf(file_res, \"ZONE I=%d T=ZONE_%06d\\n\", domain.ni, ts);\n  for (int i = 0; i < domain.ni; i++)\n  {\n    fprintf(file_res, \"%g %g %g %g %g %g\\n\", i * domain.dx, domain.nde[i], domain.ndi[i],\n        domain.rho[i], domain.phi[i], domain.ef[i]);\n  }\n\n  fflush(file_res);\n}\n"}}
{"kernel_name": "su3", "parallel_api": "cuda", "code": {"su3_nn_bench.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/resource.h>\n#include <math.h>\n#include <vector>\n#include <iostream>\n#include <string>\n#include <cassert>\n#include <cmath>\n#include <complex>\n#include <chrono>\ntypedef std::chrono::system_clock Clock;\n\n#ifndef ITERATIONS\n#  define ITERATIONS 100\n#endif\n#ifndef LDIM\n#  define LDIM 32       \n\n#endif\n#ifndef PRECISION\n#  define PRECISION 1  \n\n#endif\n\n\n\nunsigned int verbose=1;\nint          warmups=1;\n\n\nint  g_argc;\nchar **g_argv;\n\n#include \"lattice.hpp\"\n\n#ifndef MILC_COMPLEX\ntemplate<class T>\nbool almost_equal(thrust::complex<T> x, thrust::complex<T> y, double tol)\n{\n  if (std::isnan(x.real()) || std::isnan(x.imag())\n  ||  std::isnan(y.real()) || std::isnan(y.imag()) )\n\t  return (0);\n  return thrust::abs( x - y ) < tol ;\n}\n#else\ntemplate<class T>\nbool almost_equal(T x, T y, double tol)\n{\n  if (std::isnan(x) || std::isnan(y))\n\t  return (0);\n  return std::abs( x - y ) < tol ;\n}\n\n\n\ntemplate<class T>\nbool almost_equal(std::complex<T> x, std::complex<T> y, double tol)\n{\n  if (std::isnan(x.real()) || std::isnan(x.imag())\n  ||  std::isnan(y.real()) || std::isnan(y.imag()) )\n\t  return (0);\n  return std::abs( x - y ) < tol ;\n}\n#endif\n\n\n\nvoid init_link(su3_matrix *s, Complx val) {\n  for(int j=0; j<4; ++j) for(int k=0; k<3; ++k) for(int l=0; l<3; ++l) {\n    s[j].e[k][l] = val;\n  }\n}\n\n\n\nvoid make_lattice(site *s, size_t n, Complx val) {\n  int nx=n;\n  int ny=n;\n  int nz=n;\n  int nt=n;\n  for(int t=0;t<nt;t++) {\n    int i=t*nz*ny*nx;\n    for(int z=0;z<nz;z++)for(int y=0;y<ny;y++)for(int x=0;x<nx;x++,i++){\n      s[i].x=x; s[i].y=y; s[i].z=z; s[i].t=t;\n      s[i].index = x+nx*(y+ny*(z+nz*t));\n      if( (x+y+z+t)%2 == 0)\n        s[i].parity=EVEN;\n      else\n        s[i].parity=ODD;\n      init_link(&s[i].link[0], val);\n    }\n  }\n}\n\n\n\n#ifdef USE_THRUST\n#include <thrust/host_vector.h>\n#endif\n#include \"mat_nn_cuda.hpp\"\n\n\n\nint main(int argc, char **argv)\n{\n  int iterations = ITERATIONS;\n  size_t ldim = LDIM;\n  int threads_per_group = 128; \n\n  int device = -1;             \n\n\n  int opt;\n  g_argc = argc;\n  g_argv = argv;\n  \n\n\t\n\n  \n\n  \n\n  \n\n  while ((opt=getopt(argc, argv, \":hi:l:t:v:d:w:n:\")) != -1) {\n    switch (opt) {\n    case 'i':\n      iterations = atoi(optarg);\n      break;\n    case 'l':\n      ldim = atoi(optarg);\n      break;\n    case 't':\n      threads_per_group = atoi(optarg);\n      break;\n    case 'v':\n      verbose = atoi(optarg);\n      break;\n    case 'd':\n      device = atoi(optarg);\n      break;\n    case 'w':\n      warmups = atoi(optarg);\n      break;\n    case 'h':\n      fprintf(stderr, \"Usage: %s [-i iterations] [-l lattice dimension] \\\n[-t threads per workgroup] [-d device] [-v verbosity level [0,1,2,3]] [-w warmups]\\n\", argv[0]);\n      exit (1);\n    }\n  }\n\n  \n\n  size_t total_sites = ldim*ldim*ldim*ldim;\n#ifdef MILC_COMPLEX\n  std::vector<site> a(total_sites);\n  std::vector<su3_matrix> b(4);\n  std::vector<site> c(total_sites);\n#else\n  thrust::host_vector<site> a(total_sites);\n  thrust::host_vector<su3_matrix> b(4);\n  thrust::host_vector<site> c(total_sites);\n#endif\n\n  \n\n  make_lattice(a.data(), ldim, Complx{1.0,0.0});\n  init_link(b.data(), Complx{1.0/3.0,0.0});\n\n  if (verbose >= 1) {\n    printf(\"Number of sites = %zu^4\\n\", ldim);\n    printf(\"Executing %d iterations with %d warmups\\n\", iterations, warmups);\n    if (threads_per_group != 0)\n      printf(\"Threads per group = %d\\n\", threads_per_group);\n  }\n\n  \n\n  const double ttotal = su3_mat_nn(a, b, c, total_sites, iterations, threads_per_group, device);\n  if (verbose >= 1)\n    printf(\"Total kernel execution time = %f (s)\\n\", ttotal);\n\n  \n\n  \n\n  const double tflop = (double)iterations * total_sites * 864.0;\n  printf(\"Total GFLOP/s = %.3f\\n\", tflop / ttotal / 1.0e9);\n\n  const double memory_usage = (double)sizeof(site)*(a.capacity()+c.capacity())+sizeof(su3_matrix)*b.capacity();\n  printf(\"Total GByte/s (GPU memory)  = %.3f\\n\", iterations * memory_usage / ttotal / 1.0e9);\n  fflush(stdout);\n\n  \n\n  for (size_t i=0;i<total_sites;++i) for(int j=0;j<4;++j)  for(int k=0;k<3;++k)  for(int l=0;l<3;++l) {\n    Complx cc = {0.0, 0.0};\n    for(int m=0;m<3;m++) {\n      #ifdef MILC_COMPLEX\n        CMULSUM( a[i].link[j].e[k][m], b[j].e[m][l], cc)\n      #else\n        cc += a[i].link[j].e[k][m] * b[j].e[m][l];\n      #endif\n    }\n\n    #ifdef MILC_COMPLEX\n       assert(almost_equal(c[i].link[j].e[k][l].real, cc.real, 1E-6));\n       assert(almost_equal(c[i].link[j].e[k][l].imag, cc.imag, 1E-6));\n    #else\n       assert(almost_equal(c[i].link[j].e[k][l], cc, 1E-6));\n    #endif\n  }\n\n  \n\n  if (verbose >= 2) {\n    printf(\"Total allocation for matrices = %.3f MiB\\n\", memory_usage / 1048576.0);\n    struct rusage usage;\n    if (getrusage(RUSAGE_SELF, &usage) == 0)\n      printf(\"Approximate memory usage = %.3f MiB\\n\", (float)usage.ru_maxrss/1024.0);\n  }\n}\n"}}
{"kernel_name": "su3", "parallel_api": "hip", "code": {"su3_nn_bench.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/resource.h>\n#include <math.h>\n#include <vector>\n#include <iostream>\n#include <string>\n#include <cassert>\n#include <cmath>\n#include <complex>\n#include <chrono>\ntypedef std::chrono::system_clock Clock;\n\n#ifndef ITERATIONS\n#  define ITERATIONS 100\n#endif\n#ifndef LDIM\n#  define LDIM 32       \n\n#endif\n#ifndef PRECISION\n#  define PRECISION 2  \n\n#endif\n\n\n\nunsigned int verbose=1;\nint          warmups=1;\n\n\nint  g_argc;\nchar **g_argv;\n\n#include \"lattice.hpp\"\n\n#ifndef MILC_COMPLEX\ntemplate<class T>\nbool almost_equal(thrust::complex<T> x, thrust::complex<T> y, double tol)\n{\n  if (std::isnan(x.real()) || std::isnan(x.imag())\n  ||  std::isnan(y.real()) || std::isnan(y.imag()) )\n\t  return (0);\n  return thrust::abs( x - y ) < tol ;\n}\n#else\ntemplate<class T>\nbool almost_equal(T x, T y, double tol)\n{\n  if (std::isnan(x) || std::isnan(y))\n\t  return (0);\n  return std::abs( x - y ) < tol ;\n}\n\n\n\ntemplate<class T>\nbool almost_equal(std::complex<T> x, std::complex<T> y, double tol)\n{\n  if (std::isnan(x.real()) || std::isnan(x.imag())\n  ||  std::isnan(y.real()) || std::isnan(y.imag()) )\n\t  return (0);\n  return std::abs( x - y ) < tol ;\n}\n#endif\n\n\n\nvoid init_link(su3_matrix *s, Complx val) {\n  for(int j=0; j<4; ++j) for(int k=0; k<3; ++k) for(int l=0; l<3; ++l) {\n    s[j].e[k][l] = val;\n  }\n}\n\n\n\nvoid make_lattice(site *s, size_t n, Complx val) {\n  int nx=n;\n  int ny=n;\n  int nz=n;\n  int nt=n;\n  for(int t=0;t<nt;t++) {\n    int i=t*nz*ny*nx;\n    for(int z=0;z<nz;z++)for(int y=0;y<ny;y++)for(int x=0;x<nx;x++,i++){\n      s[i].x=x; s[i].y=y; s[i].z=z; s[i].t=t;\n      s[i].index = x+nx*(y+ny*(z+nz*t));\n      if( (x+y+z+t)%2 == 0)\n        s[i].parity=EVEN;\n      else\n        s[i].parity=ODD;\n      init_link(&s[i].link[0], val);\n    }\n  }\n}\n\n\n\n#ifdef USE_THRUST\n#include <thrust/host_vector.h>\n#endif\n#include \"mat_nn_cuda.hpp\"\n\n\n\nint main(int argc, char **argv)\n{\n  int iterations = ITERATIONS;\n  size_t ldim = LDIM;\n  int threads_per_group = 128; \n\n  int device = -1;             \n\n\n  int opt;\n  g_argc = argc;\n  g_argv = argv;\n  \n\n\t\n\n  \n\n  \n\n  \n\n  while ((opt=getopt(argc, argv, \":hi:l:t:v:d:w:n:\")) != -1) {\n    switch (opt) {\n    case 'i':\n      iterations = atoi(optarg);\n      break;\n    case 'l':\n      ldim = atoi(optarg);\n      break;\n    case 't':\n      threads_per_group = atoi(optarg);\n      break;\n    case 'v':\n      verbose = atoi(optarg);\n      break;\n    case 'd':\n      device = atoi(optarg);\n      break;\n    case 'w':\n      warmups = atoi(optarg);\n      break;\n    case 'h':\n      fprintf(stderr, \"Usage: %s [-i iterations] [-l lattice dimension] \\\n[-t threads per workgroup] [-d device] [-v verbosity level [0,1,2,3]] [-w warmups]\\n\", argv[0]);\n      exit (1);\n    }\n  }\n\n  \n\n  size_t total_sites = ldim*ldim*ldim*ldim;\n#ifdef MILC_COMPLEX\n  std::vector<site> a(total_sites);\n  std::vector<su3_matrix> b(4);\n  std::vector<site> c(total_sites);\n#else\n  thrust::host_vector<site> a(total_sites);\n  thrust::host_vector<su3_matrix> b(4);\n  thrust::host_vector<site> c(total_sites);\n#endif\n\n  \n\n  make_lattice(a.data(), ldim, Complx{1.0,0.0});\n  init_link(b.data(), Complx{1.0/3.0,0.0});\n\n  if (verbose >= 1) {\n    printf(\"Number of sites = %zu^4\\n\", ldim);\n    printf(\"Executing %d iterations with %d warmups\\n\", iterations, warmups);\n    if (threads_per_group != 0)\n      printf(\"Threads per group = %d\\n\", threads_per_group);\n  }\n\n  \n\n  const double ttotal = su3_mat_nn(a, b, c, total_sites, iterations, threads_per_group, device);\n  if (verbose >= 1)\n    printf(\"Total kernel execution time = %f (s)\\n\", ttotal);\n  \n\n  \n\n  const double tflop = (double)iterations * total_sites * 864.0;\n  printf(\"Total GFLOP/s = %.3f\\n\", tflop / ttotal / 1.0e9);\n\n  const double memory_usage = (double)sizeof(site)*(a.capacity()+c.capacity())+sizeof(su3_matrix)*b.capacity();\n  printf(\"Total GByte/s (GPU memory)  = %.3f\\n\", iterations * memory_usage / ttotal / 1.0e9);\n  fflush(stdout);\n\n  \n\n  for (size_t i=0;i<total_sites;++i) for(int j=0;j<4;++j)  for(int k=0;k<3;++k)  for(int l=0;l<3;++l) {\n    Complx cc = {0.0, 0.0};\n    for(int m=0;m<3;m++) {\n      #ifdef MILC_COMPLEX\n        CMULSUM( a[i].link[j].e[k][m], b[j].e[m][l], cc)\n      #else\n        cc += a[i].link[j].e[k][m] * b[j].e[m][l];\n      #endif\n    }\n\n    #ifdef MILC_COMPLEX\n       assert(almost_equal(c[i].link[j].e[k][l].real, cc.real, 1E-6));\n       assert(almost_equal(c[i].link[j].e[k][l].imag, cc.imag, 1E-6));\n    #else\n       assert(almost_equal(c[i].link[j].e[k][l], cc, 1E-6));\n    #endif\n  }\n\n  \n\n  if (verbose >= 2) {\n    printf(\"Total allocation for matrices = %.3f MiB\\n\", memory_usage / 1048576.0);\n    struct rusage usage;\n    if (getrusage(RUSAGE_SELF, &usage) == 0)\n      printf(\"Approximate memory usage = %.3f MiB\\n\", (float)usage.ru_maxrss/1024.0);\n  }\n}\n\n"}}
{"kernel_name": "su3", "parallel_api": "omp", "code": {"su3_nn_bench.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/resource.h>\n#include <math.h>\n#include <vector>\n#include <iostream>\n#include <string>\n#include <cassert>\n#include <cmath>\n#include <complex>\n#include <chrono>\ntypedef std::chrono::system_clock Clock;\n\n#ifndef ITERATIONS\n#  define ITERATIONS 100\n#endif\n#ifndef LDIM\n#  define LDIM 32       \n\n#endif\n#ifndef PRECISION\n#  define PRECISION 2  \n\n#endif\n\n\n\nunsigned int verbose=1;\nsize_t       warmups=1;\n\n\nint  g_argc;\nchar **g_argv;\n\n#include \"lattice.hpp\"\n\ntemplate<class T>\nbool almost_equal(T x, T y, double tol)\n{\n  if (std::isnan(x) || std::isnan(y))\n\t  return (0);\n  return std::abs( x - y ) < tol ;\n}\n\n\n\ntemplate<class T>\nbool almost_equal(std::complex<T> x, std::complex<T> y, double tol)\n{\n  if (std::isnan(x.real()) || std::isnan(x.imag())\n  ||  std::isnan(y.real()) || std::isnan(y.imag()) )\n\t  return (0);\n  return std::abs( x - y ) < tol ;\n}\n\n\n\nvoid init_link(su3_matrix *s, Complx val) {\n  for(int j=0; j<4; ++j) for(int k=0; k<3; ++k) for(int l=0; l<3; ++l) {\n    s[j].e[k][l] = val;\n  }\n}\n\n\n\nvoid make_lattice(site *s, size_t n, Complx val) {\n  int nx=n;\n  int ny=n;\n  int nz=n;\n  int nt=n;\n  for(int t=0;t<nt;t++) {\n    int i=t*nz*ny*nx;\n    for(int z=0;z<nz;z++)for(int y=0;y<ny;y++)for(int x=0;x<nx;x++,i++){\n      s[i].x=x; s[i].y=y; s[i].z=z; s[i].t=t;\n      s[i].index = x+nx*(y+ny*(z+nz*t));\n      if( (x+y+z+t)%2 == 0)\n        s[i].parity=EVEN;\n      else\n        s[i].parity=ODD;\n      init_link(&s[i].link[0], val);\n    }\n  }\n}\n\n\n\n#include \"mat_nn_openmp.hpp\"\n\n\n\nint main(int argc, char **argv)\n{\n  size_t iterations = ITERATIONS;\n  size_t ldim = LDIM;\n  size_t threads_per_group = 128; \n\n  int device = -1;                \n\n\n  int opt;\n  g_argc = argc;\n  g_argv = argv;\n  \n\n\t\n\n  \n\n  \n\n  \n\n  while ((opt=getopt(argc, argv, \":hi:l:t:v:d:w:n:\")) != -1) {\n    switch (opt) {\n    case 'i':\n      iterations = atoi(optarg);\n      break;\n    case 'l':\n      ldim = atoi(optarg);\n      break;\n    case 't':\n      threads_per_group = atoi(optarg);\n      break;\n    case 'v':\n      verbose = atoi(optarg);\n      break;\n    case 'd':\n      device = atoi(optarg);\n      break;\n    case 'w':\n      warmups = atoi(optarg);\n      break;\n    case 'h':\n      fprintf(stderr, \"Usage: %s [-i iterations] [-l lattice dimension] \\\n[-t threads per workgroup] [-d device] [-v verbosity level [0,1,2,3]] [-w warmups]\\n\", argv[0]);\n      exit (1);\n    }\n  }\n\n  \n\n  size_t total_sites = ldim*ldim*ldim*ldim;\n  std::vector<site> a(total_sites);\n  std::vector<su3_matrix> b(4);\n  std::vector<site> c(total_sites);\n\n  \n\n  make_lattice(a.data(), ldim, Complx{1.0,0.0});\n  init_link(b.data(), Complx{1.0/3.0,0.0});\n\n  if (verbose >= 1) {\n    printf(\"Number of sites = %zu^4\\n\", ldim);\n    printf(\"Executing %zu iterations with %zu warmups\\n\", iterations, warmups);\n    if (threads_per_group != 0)\n      printf(\"Threads per group = %zu\\n\", threads_per_group);\n  }\n\n  \n\n  const double ttotal = su3_mat_nn(a, b, c, total_sites, iterations, threads_per_group, device);\n  if (verbose >= 1)\n    printf(\"Total kernel execution time = %f (s)\\n\", ttotal);\n  \n\n  \n\n  const double tflop = (double)iterations * total_sites * 864.0;\n  printf(\"Total GFLOP/s = %.3f\\n\", tflop / ttotal / 1.0e9);\n\n  const double memory_usage = (double)sizeof(site)*(a.capacity()+c.capacity())+sizeof(su3_matrix)*b.capacity();\n  printf(\"Total GByte/s (GPU memory)  = %.3f\\n\", iterations * memory_usage / ttotal / 1.0e9);\n  fflush(stdout);\n\n  \n\n  for (int i=0;i<total_sites;++i) for(int j=0;j<4;++j)  for(int k=0;k<3;++k)  for(int l=0;l<3;++l) {\n    Complx cc = {0.0, 0.0};\n    for(int m=0;m<3;m++) {\n      #ifdef MILC_COMPLEX\n        CMULSUM( a[i].link[j].e[k][m], b[j].e[m][l], cc)\n      #else\n        cc += a[i].link[j].e[k][m] * b[j].e[m][l];\n      #endif\n    }\n\n    #ifdef MILC_COMPLEX\n       assert(almost_equal(c[i].link[j].e[k][l].real, cc.real, 1E-6));\n       assert(almost_equal(c[i].link[j].e[k][l].imag, cc.imag, 1E-6));\n    #else\n       assert(almost_equal(c[i].link[j].e[k][l], cc, 1E-6));\n    #endif\n  }\n\n  \n\n  if (verbose >= 2) {\n    printf(\"Total allocation for matrices = %.3f MiB\\n\", memory_usage / 1048576.0);\n    struct rusage usage;\n    if (getrusage(RUSAGE_SELF, &usage) == 0)\n      printf(\"Approximate memory usage = %.3f MiB\\n\", (float)usage.ru_maxrss/1024.0);\n  }\n}\n\n"}}
{"kernel_name": "su3", "parallel_api": "serial", "code": {"su3_nn_bench.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/resource.h>\n#include <math.h>\n#include <vector>\n#include <iostream>\n#include <string>\n#include <cassert>\n#include <cmath>\n#include <complex>\n#include <chrono>\ntypedef std::chrono::system_clock Clock;\n\n#ifndef ITERATIONS\n#  define ITERATIONS 100\n#endif\n#ifndef LDIM\n#  define LDIM 32       \n\n#endif\n#ifndef PRECISION\n#  define PRECISION 2  \n\n#endif\n\n\n\nunsigned int verbose=1;\nsize_t       warmups=1;\n\n\nint  g_argc;\nchar **g_argv;\n\n#include \"lattice.hpp\"\n\ntemplate<class T>\nbool almost_equal(T x, T y, double tol)\n{\n  if (std::isnan(x) || std::isnan(y))\n\t  return (0);\n  return std::abs( x - y ) < tol ;\n}\n\n\n\ntemplate<class T>\nbool almost_equal(std::complex<T> x, std::complex<T> y, double tol)\n{\n  if (std::isnan(x.real()) || std::isnan(x.imag())\n  ||  std::isnan(y.real()) || std::isnan(y.imag()) )\n\t  return (0);\n  return std::abs( x - y ) < tol ;\n}\n\n\n\nvoid init_link(su3_matrix *s, Complx val) {\n  for(int j=0; j<4; ++j) for(int k=0; k<3; ++k) for(int l=0; l<3; ++l) {\n    s[j].e[k][l] = val;\n  }\n}\n\n\n\nvoid make_lattice(site *s, size_t n, Complx val) {\n  int nx=n;\n  int ny=n;\n  int nz=n;\n  int nt=n;\n  for(int t=0;t<nt;t++) {\n    int i=t*nz*ny*nx;\n    for(int z=0;z<nz;z++)for(int y=0;y<ny;y++)for(int x=0;x<nx;x++,i++){\n      s[i].x=x; s[i].y=y; s[i].z=z; s[i].t=t;\n      s[i].index = x+nx*(y+ny*(z+nz*t));\n      if( (x+y+z+t)%2 == 0)\n        s[i].parity=EVEN;\n      else\n        s[i].parity=ODD;\n      init_link(&s[i].link[0], val);\n    }\n  }\n}\n\n\n\n#include \"mat_nn_openmp.hpp\"\n\n\n\nint main(int argc, char **argv)\n{\n  size_t iterations = ITERATIONS;\n  size_t ldim = LDIM;\n  size_t threads_per_group = 128; \n\n  int device = -1;                \n\n\n  int opt;\n  g_argc = argc;\n  g_argv = argv;\n  \n\n\t\n\n  \n\n  \n\n  \n\n  while ((opt=getopt(argc, argv, \":hi:l:t:v:d:w:n:\")) != -1) {\n    switch (opt) {\n    case 'i':\n      iterations = atoi(optarg);\n      break;\n    case 'l':\n      ldim = atoi(optarg);\n      break;\n    case 't':\n      threads_per_group = atoi(optarg);\n      break;\n    case 'v':\n      verbose = atoi(optarg);\n      break;\n    case 'd':\n      device = atoi(optarg);\n      break;\n    case 'w':\n      warmups = atoi(optarg);\n      break;\n    case 'h':\n      fprintf(stderr, \"Usage: %s [-i iterations] [-l lattice dimension] \\\n[-t threads per workgroup] [-d device] [-v verbosity level [0,1,2,3]] [-w warmups]\\n\", argv[0]);\n      exit (1);\n    }\n  }\n\n  \n\n  size_t total_sites = ldim*ldim*ldim*ldim;\n  std::vector<site> a(total_sites);\n  std::vector<su3_matrix> b(4);\n  std::vector<site> c(total_sites);\n\n  \n\n  make_lattice(a.data(), ldim, Complx{1.0,0.0});\n  init_link(b.data(), Complx{1.0/3.0,0.0});\n\n  if (verbose >= 1) {\n    printf(\"Number of sites = %zu^4\\n\", ldim);\n    printf(\"Executing %zu iterations with %zu warmups\\n\", iterations, warmups);\n    if (threads_per_group != 0)\n      printf(\"Threads per group = %zu\\n\", threads_per_group);\n  }\n\n  \n\n  const double ttotal = su3_mat_nn(a, b, c, total_sites, iterations, threads_per_group, device);\n  if (verbose >= 1)\n    printf(\"Total kernel execution time = %f (s)\\n\", ttotal);\n  \n\n  \n\n  const double tflop = (double)iterations * total_sites * 864.0;\n  printf(\"Total GFLOP/s = %.3f\\n\", tflop / ttotal / 1.0e9);\n\n  const double memory_usage = (double)sizeof(site)*(a.capacity()+c.capacity())+sizeof(su3_matrix)*b.capacity();\n  printf(\"Total GByte/s (GPU memory)  = %.3f\\n\", iterations * memory_usage / ttotal / 1.0e9);\n  fflush(stdout);\n\n  \n\n  for (int i=0;i<total_sites;++i) for(int j=0;j<4;++j)  for(int k=0;k<3;++k)  for(int l=0;l<3;++l) {\n    Complx cc = {0.0, 0.0};\n    for(int m=0;m<3;m++) {\n      #ifdef MILC_COMPLEX\n        CMULSUM( a[i].link[j].e[k][m], b[j].e[m][l], cc)\n      #else\n        cc += a[i].link[j].e[k][m] * b[j].e[m][l];\n      #endif\n    }\n\n    #ifdef MILC_COMPLEX\n       assert(almost_equal(c[i].link[j].e[k][l].real, cc.real, 1E-6));\n       assert(almost_equal(c[i].link[j].e[k][l].imag, cc.imag, 1E-6));\n    #else\n       assert(almost_equal(c[i].link[j].e[k][l], cc, 1E-6));\n    #endif\n  }\n\n  \n\n  if (verbose >= 2) {\n    printf(\"Total allocation for matrices = %.3f MiB\\n\", memory_usage / 1048576.0);\n    struct rusage usage;\n    if (getrusage(RUSAGE_SELF, &usage) == 0)\n      printf(\"Approximate memory usage = %.3f MiB\\n\", (float)usage.ru_maxrss/1024.0);\n  }\n}\n"}}
{"kernel_name": "su3", "parallel_api": "sycl", "code": {"su3_nn_bench.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/resource.h>\n#include <math.h>\n#include <vector>\n#include <iostream>\n#include <string>\n#include <cassert>\n#include <cmath>\n#include <complex>\n#include <chrono>\ntypedef std::chrono::system_clock Clock;\n\n#ifndef ITERATIONS\n#  define ITERATIONS 100\n#endif\n#ifndef LDIM\n#  define LDIM 32       \n\n#endif\n#ifndef PRECISION\n#  define PRECISION 2  \n\n#endif\n\n\n\nunsigned int verbose=1;\nsize_t       warmups=1;\n\n\nint  g_argc;\nchar **g_argv;\n\n#include \"lattice.hpp\"\n\ntemplate<class T>\nbool almost_equal(T x, T y, double tol)\n{\n  if (std::isnan(x) || std::isnan(y))\n\t  return (0);\n  return std::abs( x - y ) < tol ;\n}\n\n\n\ntemplate<class T>\nbool almost_equal(std::complex<T> x, std::complex<T> y, double tol)\n{\n  if (std::isnan(x.real()) || std::isnan(x.imag())\n  ||  std::isnan(y.real()) || std::isnan(y.imag()) )\n\t  return (0);\n  return std::abs( x - y ) < tol ;\n}\n\n\n\n\nvoid init_link(su3_matrix *s, Complx val) {\n  for(int j=0; j<4; ++j) for(int k=0; k<3; ++k) for(int l=0; l<3; ++l) {\n    s[j].e[k][l] = val;\n  }\n}\n\n\n\nvoid make_lattice(site *s, size_t n, Complx val) {\n  int nx=n;\n  int ny=n;\n  int nz=n;\n  int nt=n;\n  for(int t=0;t<nt;t++) {\n    int i=t*nz*ny*nx;\n    for(int z=0;z<nz;z++)for(int y=0;y<ny;y++)for(int x=0;x<nx;x++,i++){\n      s[i].x=x; s[i].y=y; s[i].z=z; s[i].t=t;\n      s[i].index = x+nx*(y+ny*(z+nz*t));\n      if( (x+y+z+t)%2 == 0)\n        s[i].parity=EVEN;\n      else\n        s[i].parity=ODD;\n      init_link(&s[i].link[0], val);\n    }\n  }\n}\n\n\n\n#include \"mat_nn_sycl.hpp\"\n\n\n\nint main(int argc, char **argv)\n{\n  size_t iterations = ITERATIONS;\n  size_t ldim = LDIM;\n  size_t threads_per_group = 128; \n\n  int device = -1;                \n\n\n  int opt;\n  g_argc = argc;\n  g_argv = argv;\n  \n\n\t\n\n  \n\n  \n\n  \n\n  while ((opt=getopt(argc, argv, \":hi:l:t:v:d:w:n:\")) != -1) {\n    switch (opt) {\n    case 'i':\n      iterations = atoi(optarg);\n      break;\n    case 'l':\n      ldim = atoi(optarg);\n      break;\n    case 't':\n      threads_per_group = atoi(optarg);\n      break;\n    case 'v':\n      verbose = atoi(optarg);\n      break;\n    case 'd':\n      device = atoi(optarg);\n      break;\n    case 'w':\n      warmups = atoi(optarg);\n      break;\n    case 'h':\n      fprintf(stderr, \"Usage: %s [-i iterations] [-l lattice dimension] \\\n[-t threads per workgroup] [-d device] [-v verbosity level [0,1,2,3]] [-w warmups]\\n\", argv[0]);\n      exit (1);\n    }\n  }\n\n  \n\n  size_t total_sites = ldim*ldim*ldim*ldim;\n  std::vector<site> a(total_sites);\n  std::vector<su3_matrix> b(4);\n  std::vector<site> c(total_sites);\n\n  \n\n  make_lattice(a.data(), ldim, Complx{1.0,0.0});\n  init_link(b.data(), Complx{1.0/3.0,0.0});\n\n  if (verbose >= 1) {\n    printf(\"Number of sites = %zu^4\\n\", ldim);\n    printf(\"Executing %zu iterations with %zu warmups\\n\", iterations, warmups);\n    if (threads_per_group != 0)\n      printf(\"Threads per group = %zu\\n\", threads_per_group);\n  }\n\n  \n\n  const double ttotal = su3_mat_nn(a, b, c, total_sites, iterations, threads_per_group, device);\n  if (verbose >= 1)\n    printf(\"Total kernel execution time = %f (s)\\n\", ttotal);\n  \n\n  \n\n  const double tflop = (double)iterations * total_sites * 864.0;\n  printf(\"Total GFLOP/s = %.3f\\n\", tflop / ttotal / 1.0e9);\n\n  const double memory_usage = (double)sizeof(site)*(a.capacity()+c.capacity())+sizeof(su3_matrix)*b.capacity();\n  printf(\"Total GByte/s (GPU memory)  = %.3f\\n\", iterations * memory_usage / ttotal / 1.0e9);\n  fflush(stdout);\n\n  \n\n  for (int i=0;i<total_sites;++i) for(int j=0;j<4;++j)  for(int k=0;k<3;++k)  for(int l=0;l<3;++l) {\n    Complx cc = {0.0, 0.0};\n    for(int m=0;m<3;m++) {\n      #ifdef MILC_COMPLEX\n        CMULSUM( a[i].link[j].e[k][m], b[j].e[m][l], cc)\n      #else\n        cc += a[i].link[j].e[k][m] * b[j].e[m][l];\n      #endif\n    }\n\n    #ifdef MILC_COMPLEX\n       assert(almost_equal(c[i].link[j].e[k][l].real, cc.real, 1E-6));\n       assert(almost_equal(c[i].link[j].e[k][l].imag, cc.imag, 1E-6));\n    #else\n       assert(almost_equal(c[i].link[j].e[k][l], cc, 1E-6));\n    #endif\n  }\n\n  \n\n  if (verbose >= 2) {\n    printf(\"Total allocation for matrices = %.3f MiB\\n\", memory_usage / 1048576.0);\n    struct rusage usage;\n    if (getrusage(RUSAGE_SELF, &usage) == 0)\n      printf(\"Approximate memory usage = %.3f MiB\\n\", (float)usage.ru_maxrss/1024.0);\n  }\n}\n\n"}}
{"kernel_name": "tensorT", "parallel_api": "cuda", "code": {"main.cu": "#include <cstdio>\n#include <cstdlib>\n#include <chrono>\n#include <cuda.h>\n\n#define TILE_SIZE 5900\n#define NTHREADS 256\n\n\n\nstatic const int d1 = 41, d2 = 13, d3 = 11, d4 = 9, d5 = 76, d6 = 50;\nstatic const int data_size = d1 * d2 * d3 * d4 * d5 * d6;\nstatic int repeat = 1;\n\nstatic const int shape_output[] = {d2, d3, d1};\nstatic const int shape_input[] = {d4, d5, d6};\nstatic const float shape_output_r[] = {1.f / d2, 1.f / d3, 1.f / d1};\nstatic const float shape_input_r[] = {1.f / d4, 1.f / d5, 1.f / d6};\nstatic const int stride_output_local[] = {d1, d1 * d2, 1};\nstatic const int stride_output_global[] = {1, d2, d2 * d3 * d4 * d6};\nstatic const int stride_input[] = {d2 * d3, d2 * d3 * d4 * d6 * d1, d2 * d3 * d4};\n\nvoid verify(double *input, double *output) {\n  int input_offset  = 2 + d1 * (2 + d2 * (2 + d3 * (2 + d4 * (0 + 2 * d5))));\n  int output_offset = 2 + d2 * (2 + d3 * (2 + d4 * (2 + d6 * (2 + 0 * d1))));\n  bool error = false;\n  for (size_t i = 0; i < d5; i++) {\n    if (input[input_offset + i * d1 * d2 * d3 * d4] != \n        output[output_offset + i * d2 * d3 * d4 * d6 * d1]) {\n      printf(\"FAIL\\n\");\n      error = true;\n      break;\n    }\n  }\n  if (!error) printf(\"PASS\\n\");\n}\n\n__global__ void tensor_transpose(\n    const int dim_input, \n    const int dim_output, \n    const int nblocks, \n    const int tile_size,\n    const int *shape_input, \n    const int *shape_output, \n    const float *shape_input_r, \n    const float *shape_output_r, \n    const int *stride_input,\n    const int *stride_output_local, \n    const int *stride_output_global,\n    const double *input, \n    double *output) \n{\n  __shared__ double tile[TILE_SIZE];\n\n  for (int block_idx = blockIdx.x; block_idx < nblocks; block_idx += gridDim.x) {\n    int it = block_idx, im = 0, offset1 = 0;\n    for (int i = 0; i < dim_input; i++) {\n      im = it * shape_input_r[i];  \n\n      offset1 += stride_input[i] * (it - im * shape_input[i]);\n      it = im;\n    }\n\n    for (int i = threadIdx.x; i < tile_size; i += blockDim.x) {\n      tile[i] = input[i + block_idx * tile_size];\n    }\n\n    __syncthreads();\n\n    for (int i = threadIdx.x; i < tile_size; i += blockDim.x) {\n      it = i;\n      int offset2 = 0, local_offset = 0;\n      for (int j = 0; j < dim_output; j++) {\n        im = it * shape_output_r[j];  \n\n        int tmp = it - im * shape_output[j];\n        offset2 += stride_output_global[j] * tmp;\n        local_offset += stride_output_local[j] * tmp;\n        it = im;\n      }\n      output[offset1 + offset2] = tile[local_offset];\n    }\n\n    __syncthreads();\n  }\n}\n\nint main(int argc, char **argv) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  repeat = atoi(argv[1]);\n\n  double *input = new double[data_size]();\n  double *output = new double[data_size]();\n\n  for (size_t i = 0; i < data_size; i++) {\n    input[i] = i;\n  }\n\n  const int nblocks = d4 * d5 * d6;\n  const int tile_size = d1 * d2 * d3;\n  const int dim_output = 3;\n  const int dim_input = 3;\n  double *d_output, *d_input;\n  int *d_shape_input, *d_shape_output;\n  float *d_shape_input_r, *d_shape_output_r;\n  int *d_stride_output_local, *d_stride_output_global;\n  int *d_stride_input;\n\n  cudaMalloc(&d_output, data_size * sizeof(double));\n  cudaMalloc(&d_input, data_size * sizeof(double));\n  cudaMalloc(&d_shape_input, dim_input * sizeof(int));\n  cudaMalloc(&d_shape_input_r, dim_input * sizeof(float));\n  cudaMalloc(&d_shape_output, dim_output * sizeof(int));\n  cudaMalloc(&d_shape_output_r, dim_output * sizeof(float));\n  cudaMalloc(&d_stride_input, dim_input * sizeof(int));\n  cudaMalloc(&d_stride_output_local, dim_output * sizeof(int));\n  cudaMalloc(&d_stride_output_global, dim_output * sizeof(int));\n\n  cudaMemcpy(d_input, input, \n      data_size * sizeof(double), cudaMemcpyHostToDevice );\n  cudaMemcpy(d_shape_input, shape_input, \n      dim_input * sizeof(int), cudaMemcpyHostToDevice );\n  cudaMemcpy(d_shape_input_r, shape_input_r, \n      dim_input * sizeof(float), cudaMemcpyHostToDevice );\n  cudaMemcpy(d_shape_output, shape_output, \n      dim_output * sizeof(int), cudaMemcpyHostToDevice );\n  cudaMemcpy(d_shape_output_r, shape_output_r, \n      dim_output * sizeof(float), cudaMemcpyHostToDevice );\n  cudaMemcpy(d_stride_input, stride_input, \n      dim_input * sizeof(int), cudaMemcpyHostToDevice );\n  cudaMemcpy(d_stride_output_local, stride_output_local, \n      dim_output * sizeof(int), cudaMemcpyHostToDevice );\n  cudaMemcpy(d_stride_output_global, stride_output_global, \n      dim_output * sizeof(int), cudaMemcpyHostToDevice );\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; ++i) {\n    tensor_transpose<<<nblocks, NTHREADS>>>(dim_input, \n        dim_output, \n        nblocks, \n        tile_size,\n        d_shape_input, \n        d_shape_output,\n        d_shape_input_r,\n        d_shape_output_r,\n        d_stride_input,\n        d_stride_output_local,\n        d_stride_output_global,\n        d_input, d_output);\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (ms)\\n\", (time * 1e-6f) / repeat);\n\n  cudaMemcpy(output, d_output, data_size * sizeof(double), cudaMemcpyDeviceToHost);\n  cudaFree(d_output);\n  cudaFree(d_input);\n  cudaFree(d_shape_input);\n  cudaFree(d_shape_input_r);\n  cudaFree(d_shape_output);\n  cudaFree(d_shape_output_r);\n  cudaFree(d_stride_input);\n  cudaFree(d_stride_output_local);\n  cudaFree(d_stride_output_global);\n\n  verify(input, output);\n\n  delete [] input;\n  delete [] output;\n  return 0;\n}\n"}}
{"kernel_name": "tensorT", "parallel_api": "hip", "code": {"main.cu": "#include <cstdio>\n#include <cstdlib>\n#include <chrono>\n#include <hip/hip_runtime.h>\n\n#define TILE_SIZE 5900\n#define NTHREADS 256\n\n\n\nstatic const int d1 = 41, d2 = 13, d3 = 11, d4 = 9, d5 = 76, d6 = 50;\nstatic const int data_size = d1 * d2 * d3 * d4 * d5 * d6;\nstatic int repeat = 1;\n\nstatic const int shape_output[] = {d2, d3, d1};\nstatic const int shape_input[] = {d4, d5, d6};\nstatic const float shape_output_r[] = {1.f / d2, 1.f / d3, 1.f / d1};\nstatic const float shape_input_r[] = {1.f / d4, 1.f / d5, 1.f / d6};\nstatic const int stride_output_local[] = {d1, d1 * d2, 1};\nstatic const int stride_output_global[] = {1, d2, d2 * d3 * d4 * d6};\nstatic const int stride_input[] = {d2 * d3, d2 * d3 * d4 * d6 * d1, d2 * d3 * d4};\n\nvoid verify(double *input, double *output) {\n  int input_offset  = 2 + d1 * (2 + d2 * (2 + d3 * (2 + d4 * (0 + 2 * d5))));\n  int output_offset = 2 + d2 * (2 + d3 * (2 + d4 * (2 + d6 * (2 + 0 * d1))));\n  bool error = false;\n  for (size_t i = 0; i < d5; i++) {\n    if (input[input_offset + i * d1 * d2 * d3 * d4] != \n        output[output_offset + i * d2 * d3 * d4 * d6 * d1]) {\n      printf(\"FAIL\\n\");\n      error = true;\n      break;\n    }\n  }\n  if (!error) printf(\"PASS\\n\");\n}\n\n__global__ void tensor_transpose(\n    const int dim_input, \n    const int dim_output, \n    const int nblocks, \n    const int tile_size,\n    const int *shape_input, \n    const int *shape_output, \n    const float *shape_input_r, \n    const float *shape_output_r, \n    const int *stride_input,\n    const int *stride_output_local, \n    const int *stride_output_global,\n    const double *input, \n    double *output) \n{\n  __shared__ double tile[TILE_SIZE];\n\n  for (int block_idx = blockIdx.x; block_idx < nblocks; block_idx += gridDim.x) {\n    int it = block_idx, im = 0, offset1 = 0;\n    for (int i = 0; i < dim_input; i++) {\n      im = it * shape_input_r[i];  \n\n      offset1 += stride_input[i] * (it - im * shape_input[i]);\n      it = im;\n    }\n\n    for (int i = threadIdx.x; i < tile_size; i += blockDim.x) {\n      tile[i] = input[i + block_idx * tile_size];\n    }\n\n    __syncthreads();\n\n    for (int i = threadIdx.x; i < tile_size; i += blockDim.x) {\n      it = i;\n      int offset2 = 0, local_offset = 0;\n      for (int j = 0; j < dim_output; j++) {\n        im = it * shape_output_r[j];  \n\n        int tmp = it - im * shape_output[j];\n        offset2 += stride_output_global[j] * tmp;\n        local_offset += stride_output_local[j] * tmp;\n        it = im;\n      }\n      output[offset1 + offset2] = tile[local_offset];\n    }\n\n    __syncthreads();\n  }\n}\n\nint main(int argc, char **argv) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  repeat = atoi(argv[1]);\n\n  double *input = new double[data_size]();\n  double *output = new double[data_size]();\n\n  for (size_t i = 0; i < data_size; i++) {\n    input[i] = i;\n  }\n\n  const int nblocks = d4 * d5 * d6;\n  const int tile_size = d1 * d2 * d3;\n  const int dim_output = 3;\n  const int dim_input = 3;\n  double *d_output, *d_input;\n  int *d_shape_input, *d_shape_output;\n  float *d_shape_input_r, *d_shape_output_r;\n  int *d_stride_output_local, *d_stride_output_global;\n  int *d_stride_input;\n\n  hipMalloc(&d_output, data_size * sizeof(double));\n  hipMalloc(&d_input, data_size * sizeof(double));\n  hipMalloc(&d_shape_input, dim_input * sizeof(int));\n  hipMalloc(&d_shape_input_r, dim_input * sizeof(float));\n  hipMalloc(&d_shape_output, dim_output * sizeof(int));\n  hipMalloc(&d_shape_output_r, dim_output * sizeof(float));\n  hipMalloc(&d_stride_input, dim_input * sizeof(int));\n  hipMalloc(&d_stride_output_local, dim_output * sizeof(int));\n  hipMalloc(&d_stride_output_global, dim_output * sizeof(int));\n\n  hipMemcpy(d_input, input, \n      data_size * sizeof(double), hipMemcpyHostToDevice );\n  hipMemcpy(d_shape_input, shape_input, \n      dim_input * sizeof(int), hipMemcpyHostToDevice );\n  hipMemcpy(d_shape_input_r, shape_input_r, \n      dim_input * sizeof(float), hipMemcpyHostToDevice );\n  hipMemcpy(d_shape_output, shape_output, \n      dim_output * sizeof(int), hipMemcpyHostToDevice );\n  hipMemcpy(d_shape_output_r, shape_output_r, \n      dim_output * sizeof(float), hipMemcpyHostToDevice );\n  hipMemcpy(d_stride_input, stride_input, \n      dim_input * sizeof(int), hipMemcpyHostToDevice );\n  hipMemcpy(d_stride_output_local, stride_output_local, \n      dim_output * sizeof(int), hipMemcpyHostToDevice );\n  hipMemcpy(d_stride_output_global, stride_output_global, \n      dim_output * sizeof(int), hipMemcpyHostToDevice );\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; ++i) {\n    hipLaunchKernelGGL(tensor_transpose, nblocks, NTHREADS, 0, 0, dim_input, \n        dim_output, \n        nblocks, \n        tile_size,\n        d_shape_input, \n        d_shape_output,\n        d_shape_input_r,\n        d_shape_output_r,\n        d_stride_input,\n        d_stride_output_local,\n        d_stride_output_global,\n        d_input, d_output);\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (ms)\\n\", (time * 1e-6f) / repeat);\n\n  hipMemcpy(output, d_output, data_size * sizeof(double), hipMemcpyDeviceToHost);\n  hipFree(d_output);\n  hipFree(d_input);\n  hipFree(d_shape_input);\n  hipFree(d_shape_input_r);\n  hipFree(d_shape_output);\n  hipFree(d_shape_output_r);\n  hipFree(d_stride_input);\n  hipFree(d_stride_output_local);\n  hipFree(d_stride_output_global);\n\n  verify(input, output);\n\n  delete [] input;\n  delete [] output;\n  return 0;\n}\n"}}
{"kernel_name": "tensorT", "parallel_api": "omp", "code": {"main.cpp": "#include <cstdio>\n#include <cstdlib>\n#include <chrono>\n#include <omp.h>\n\n#define TILE_SIZE 5900\n#define NTHREADS 256\n\n\n\nstatic const int d1 = 41, d2 = 13, d3 = 11, d4 = 9, d5 = 76, d6 = 50;\nstatic const int data_size = d1 * d2 * d3 * d4 * d5 * d6;\nstatic int repeat = 1;\n\nstatic const int shape_output[] = {d2, d3, d1};\nstatic const int shape_input[] = {d4, d5, d6};\nstatic const float shape_output_r[] = {1.0 / d2, 1.0 / d3, 1.0 / d1};\nstatic const float shape_input_r[] = {1.0 / d4, 1.0 / d5, 1.0 / d6};\nstatic const int stride_output_local[] = {d1, d1 * d2, 1};\nstatic const int stride_output_global[] = {1, d2, d2 * d3 * d4 * d6};\nstatic const int stride_input[] = {d2 * d3, d2 * d3 * d4 * d6 * d1, d2 * d3 * d4};\n\nvoid verify(double *input, double *output) {\n  int input_offset  = 2 + d1 * (2 + d2 * (2 + d3 * (2 + d4 * (0 + 2 * d5))));\n  int output_offset = 2 + d2 * (2 + d3 * (2 + d4 * (2 + d6 * (2 + 0 * d1))));\n  bool error = false;\n  for (size_t i = 0; i < d5; i++) {\n    if (input[input_offset + i * d1 * d2 * d3 * d4] != \n        output[output_offset + i * d2 * d3 * d4 * d6 * d1]) {\n      printf(\"FAIL\\n\");\n      error = true;\n      break;\n    }\n  }\n  if (!error) printf(\"PASS\\n\");\n}\n\nint main(int argc, char **argv) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  repeat = atoi(argv[1]);\n\n  double *input = new double[data_size]();\n  double *output = new double[data_size]();\n\n  for (size_t i = 0; i < data_size; i++) {\n    input[i] = i;\n  }\n\n  const int nblocks = d4 * d5 * d6;\n  const int tile_size = d1 * d2 * d3;\n  const int dim_output = 3;\n  const int dim_input = 3;\n\n#pragma omp target data map(to: input[0:data_size],  \\\n                                shape_input[0:dim_input], \\\n                                shape_input_r[0:dim_input], \\\n                                shape_output[0:dim_output], \\\n                                shape_output_r[0:dim_output], \\\n                                stride_input[0:dim_input], \\\n                                stride_output_local[0:dim_output], \\\n                                stride_output_global[0:dim_output]) \\\n                        map(from: output[0:data_size])\n  {\n    auto start = std::chrono::steady_clock::now();\n\n    for (size_t i = 0; i < repeat; ++i) {\n\n      #pragma omp target teams num_teams(nblocks) thread_limit(NTHREADS)\n      {\n        double tile[TILE_SIZE];\n        #pragma omp parallel \n        {\n          for (int block_idx = omp_get_team_num(); block_idx < nblocks; block_idx += omp_get_num_teams()) {\n            int it = block_idx, im = 0, offset1 = 0;\n            for (int i = 0; i < dim_input; i++) {\n              im = it * shape_input_r[i];\n              offset1 += stride_input[i] * (it - im * shape_input[i]);\n              it = im;\n            }\n\n            for (int i = omp_get_thread_num(); i < tile_size; i += omp_get_num_threads()) {\n              tile[i] = input[i + block_idx * tile_size];\n            }\n\n            #pragma omp barrier\n\n            for (int i = omp_get_thread_num(); i < tile_size; i += omp_get_num_threads()) {\n              it = i;\n              int offset2 = 0, local_offset = 0;\n              for (int j = 0; j < dim_output; j++) {\n                im = it * shape_output_r[j];\n                int tmp = it - im * shape_output[j];\n                offset2 += stride_output_global[j] * tmp;\n                local_offset += stride_output_local[j] * tmp;\n                it = im;\n              }\n              output[offset1 + offset2] = tile[local_offset];\n            }\n            #pragma omp barrier\n          }\n        }\n      }\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (ms)\\n\", (time * 1e-6f) / repeat);\n  }\n\n  verify(input, output);\n  delete [] input;\n  delete [] output;\n  return 0;\n}\n"}}
{"kernel_name": "tensorT", "parallel_api": "serial", "code": {"main.cpp": "#include <cstdio>\n#include <cstdlib>\n#include <chrono>\n\n#define TILE_SIZE 5900\n#define NTHREADS 256\n\n\n\nstatic const int d1 = 41, d2 = 13, d3 = 11, d4 = 9, d5 = 76, d6 = 50;\nstatic const int data_size = d1 * d2 * d3 * d4 * d5 * d6;\nstatic int repeat = 1;\n\nstatic const int shape_output[] = {d2, d3, d1};\nstatic const int shape_input[] = {d4, d5, d6};\nstatic const float shape_output_r[] = {1.0 / d2, 1.0 / d3, 1.0 / d1};\nstatic const float shape_input_r[] = {1.0 / d4, 1.0 / d5, 1.0 / d6};\nstatic const int stride_output_local[] = {d1, d1 * d2, 1};\nstatic const int stride_output_global[] = {1, d2, d2 * d3 * d4 * d6};\nstatic const int stride_input[] = {d2 * d3, d2 * d3 * d4 * d6 * d1, d2 * d3 * d4};\n\nvoid verify(double *input, double *output) {\n  int input_offset  = 2 + d1 * (2 + d2 * (2 + d3 * (2 + d4 * (0 + 2 * d5))));\n  int output_offset = 2 + d2 * (2 + d3 * (2 + d4 * (2 + d6 * (2 + 0 * d1))));\n  bool error = false;\n  for (size_t i = 0; i < d5; i++) {\n    if (input[input_offset + i * d1 * d2 * d3 * d4] != \n        output[output_offset + i * d2 * d3 * d4 * d6 * d1]) {\n      printf(\"FAIL\\n\");\n      error = true;\n      break;\n    }\n  }\n  if (!error) printf(\"PASS\\n\");\n}\n\nint main(int argc, char **argv) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  repeat = atoi(argv[1]);\n\n  double *input = new double[data_size]();\n  double *output = new double[data_size]();\n\n  for (size_t i = 0; i < data_size; i++) {\n    input[i] = i;\n  }\n\n  const int nblocks = d4 * d5 * d6;\n  const int tile_size = d1 * d2 * d3;\n  const int dim_output = 3;\n  const int dim_input = 3;\n\n  {\n    auto start = std::chrono::steady_clock::now();\n\n    for (size_t i = 0; i < repeat; ++i) {\n\n            {\n        double tile[TILE_SIZE];\n                {\n          for (int block_idx = omp_get_team_num(); block_idx < nblocks; block_idx += omp_get_num_teams()) {\n            int it = block_idx, im = 0, offset1 = 0;\n            for (int i = 0; i < dim_input; i++) {\n              im = it * shape_input_r[i];\n              offset1 += stride_input[i] * (it - im * shape_input[i]);\n              it = im;\n            }\n\n            for (int i = omp_get_thread_num(); i < tile_size; i += omp_get_num_threads()) {\n              tile[i] = input[i + block_idx * tile_size];\n            }\n\n            \n            for (int i = omp_get_thread_num(); i < tile_size; i += omp_get_num_threads()) {\n              it = i;\n              int offset2 = 0, local_offset = 0;\n              for (int j = 0; j < dim_output; j++) {\n                im = it * shape_output_r[j];\n                int tmp = it - im * shape_output[j];\n                offset2 += stride_output_global[j] * tmp;\n                local_offset += stride_output_local[j] * tmp;\n                it = im;\n              }\n              output[offset1 + offset2] = tile[local_offset];\n            }\n                      }\n        }\n      }\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (ms)\\n\", (time * 1e-6f) / repeat);\n  }\n\n  verify(input, output);\n  delete [] input;\n  delete [] output;\n  return 0;\n}"}}
{"kernel_name": "tensorT", "parallel_api": "sycl", "code": {"main.cpp": "#include <cstdio>\n#include <cstdlib>\n#include <chrono>\n#include <sycl/sycl.hpp>\n\n#define TILE_SIZE 5900\n#define NTHREADS 256\n\n\n\nstatic const int d1 = 41, d2 = 13, d3 = 11, d4 = 9, d5 = 76, d6 = 50;\nstatic const int data_size = d1 * d2 * d3 * d4 * d5 * d6;\nstatic int repeat = 1;\n\nstatic const int shape_output[] = {d2, d3, d1};\nstatic const int shape_input[] = {d4, d5, d6};\nstatic const float shape_output_r[] = {1.0 / d2, 1.0 / d3, 1.0 / d1};\nstatic const float shape_input_r[] = {1.0 / d4, 1.0 / d5, 1.0 / d6};\nstatic const int stride_output_local[] = {d1, d1 * d2, 1};\nstatic const int stride_output_global[] = {1, d2, d2 * d3 * d4 * d6};\nstatic const int stride_input[] = {d2 * d3, d2 * d3 * d4 * d6 * d1, d2 * d3 * d4};\n\nvoid verify(double *input, double *output) {\n  int input_offset  = 2 + d1 * (2 + d2 * (2 + d3 * (2 + d4 * (0 + 2 * d5))));\n  int output_offset = 2 + d2 * (2 + d3 * (2 + d4 * (2 + d6 * (2 + 0 * d1))));\n  bool error = false;\n  for (size_t i = 0; i < d5; i++) {\n    if (input[input_offset + i * d1 * d2 * d3 * d4] !=\n        output[output_offset + i * d2 * d3 * d4 * d6 * d1]) {\n      printf(\"FAIL\\n\");\n      error = true;\n      break;\n    }\n  }\n  if (!error) printf(\"PASS\\n\");\n}\n\nint main(int argv, char **argc) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  repeat = atoi(argv[1]);\n\n  double *input = new double[data_size]();\n  double *output = new double[data_size]();\n\n  for (size_t i = 0; i < data_size; i++) {\n    input[i] = i;\n  }\n\n  const int nblocks = d4 * d5 * d6;\n  const int tile_size = d1 * d2 * d3;\n  const int dim_output = 3;\n  const int dim_input = 3;\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  double *d_output = sycl::malloc_device<double>(data_size, q);\n\n  double *d_input = sycl::malloc_device<double>(data_size, q);\n  q.memcpy(d_input, input, data_size * sizeof(double));\n\n  int *d_shape_input = sycl::malloc_device<int>(dim_input, q);\n  q.memcpy(d_shape_input, shape_input, dim_input * sizeof(int));\n\n  float *d_shape_input_r = sycl::malloc_device<float>(dim_output, q);\n  q.memcpy(d_shape_input_r, shape_input_r, dim_input * sizeof(float));\n\n  int *d_shape_output = sycl::malloc_device<int>(dim_output, q);\n  q.memcpy(d_shape_output, shape_output, dim_output * sizeof(int));\n\n  float *d_shape_output_r = sycl::malloc_device<float>(dim_output, q);\n  q.memcpy(d_shape_output_r, shape_output_r, dim_output * sizeof(float));\n\n  int *d_stride_input = sycl::malloc_device<int>(dim_input, q);\n  q.memcpy(d_stride_input, stride_input, dim_input * sizeof(int));\n\n  int *d_stride_output_local = sycl::malloc_device<int>(dim_output, q);\n  q.memcpy(d_stride_output_local, stride_output_local, dim_output * sizeof(int));\n\n  int *d_stride_output_global = sycl::malloc_device<int>(dim_output, q);\n  q.memcpy(d_stride_output_global, stride_output_global, dim_output * sizeof(int));\n\n  sycl::range<1> gws (nblocks * NTHREADS);\n  sycl::range<1> lws (NTHREADS);\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  for (size_t i = 0; i < repeat; ++i) {\n    q.submit([&] (sycl::handler &cgh) {\n      sycl::local_accessor<double, 1> tile(sycl::range<1>(TILE_SIZE), cgh);\n      cgh.parallel_for<class transpose>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        for (int block_idx = item.get_group(0); block_idx < nblocks;\n                 block_idx += item.get_group_range(0)) {\n          int it = block_idx, im = 0, offset1 = 0;\n          for (int i = 0; i < dim_input; i++) {\n            im = it * d_shape_input_r[i];\n            offset1 += d_stride_input[i] * (it - im * d_shape_input[i]);\n            it = im;\n          }\n\n          for (int i = item.get_local_id(0); i < tile_size; i += item.get_local_range(0)) {\n            tile[i] = d_input[i + block_idx * tile_size];\n          }\n\n          item.barrier(sycl::access::fence_space::local_space);\n\n          for (int i = item.get_local_id(0); i < tile_size; i += item.get_local_range(0)) {\n            it = i;\n            int offset2 = 0, local_offset = 0;\n            for (int j = 0; j < dim_output; j++) {\n              im = it * d_shape_output_r[j];\n              int tmp = it - im * d_shape_output[j];\n              offset2 += d_stride_output_global[j] * tmp;\n              local_offset += d_stride_output_local[j] * tmp;\n              it = im;\n            }\n            d_output[offset1 + offset2] = tile[local_offset];\n          }\n          item.barrier(sycl::access::fence_space::local_space);\n        }\n      });\n    });\n  }\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (ms)\\n\", (time * 1e-6f) / repeat);\n\n  q.memcpy(output, d_output, data_size * sizeof(double)).wait();\n  sycl::free(d_output, q);\n  sycl::free(d_input, q);\n  sycl::free(d_shape_input, q);\n  sycl::free(d_shape_input_r, q);\n  sycl::free(d_shape_output, q);\n  sycl::free(d_shape_output_r, q);\n  sycl::free(d_stride_input, q);\n  sycl::free(d_stride_output_local, q);\n  sycl::free(d_stride_output_global, q);\n\n  verify(input, output);\n  delete [] input;\n  delete [] output;\n  return 0;\n}\n"}}
{"kernel_name": "testSNAP", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <chrono>\n#include <cmath>\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#include <iostream>\n#include <cuda.h>\n#include \"snap.h\"\n#include \"utils.cu\"\n\n#if REFDATA_TWOJ == 14\n#include \"refdata_2J14_W.h\"\n#elif REFDATA_TWOJ == 8\n#include \"refdata_2J8_W.h\"\n#elif REFDATA_TWOJ == 4\n#include \"refdata_2J4_W.h\"\n#else\n#include \"refdata_2J2_W.h\"\n#endif\n\n\nint nsteps = 1; \n\n\n__global__ void reset_ulisttot(COMPLEX *ulisttot, const int ulisttot_size) \n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < ulisttot_size) ulisttot[i] = {0.0, 0.0};\n}\n\n__global__ void set_ulisttot(\n    COMPLEX *__restrict__ ulisttot,\n    const int*__restrict__ idxu_block, \n    const int num_atoms,\n    const int twojmax,\n    const double wself) \n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  if (natom < num_atoms) \n    for (int j = 0; j <= twojmax; j++) {\n      int jju = idxu_block[j];\n      for (int ma = 0; ma <= j; ma++) {\n        ulisttot[INDEX_2D(natom, jju)] = { wself, 0.0 };\n        jju += j + 2;\n      }\n    }\n}\n\n__global__ void update_ulisttot(\n    const double*__restrict__ rij, \n    const double*__restrict__ rcutij,\n    const double*__restrict__ wj, \n    const int*__restrict__ ulist_parity, \n    const int*__restrict__ idxu_block, \n    const double*__restrict__ rootpqarray, \n    COMPLEX *__restrict__ ulist, \n    COMPLEX *__restrict__ ulisttot, \n    const int num_atoms,\n    const int num_nbor,\n    const int switch_flag, \n    const int twojmax, \n    const int jdimpq)\n{\n\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int nbor = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (natom < num_atoms && nbor < num_nbor) {\n    double x = rij[ULIST_INDEX(natom, nbor, 0)];\n    double y = rij[ULIST_INDEX(natom, nbor, 1)];\n    double z = rij[ULIST_INDEX(natom, nbor, 2)];\n    double rsq = x * x + y * y + z * z;\n    double r = sqrt(rsq);\n\n    double theta0 = (r - rmin0) * rfac0 * MY_PI / (rcutij[INDEX_2D(natom, nbor)] - rmin0);\n    double z0 = r / tan(theta0);\n\n    double rootpq;\n    int jju, jjup;\n\n    \n\n\n    double r0inv = 1.0 / sqrt(r * r + z0 * z0);\n    double a_r = r0inv * z0;\n    double a_i = -r0inv * z;\n    double b_r = r0inv * y;\n    double b_i = -r0inv * x;\n\n    double sfac;\n\n    sfac = compute_sfac(r, rcutij[INDEX_2D(natom, nbor)], switch_flag);\n    sfac *= wj[INDEX_2D(natom, nbor)];\n\n    \n\n    \n\n\n    \n\n    \n\n\n    \n\n    \n\n\n    \n\n    \n\n    ulist[ULIST_INDEX(natom, nbor, 0)].re = 1.0;\n    ulist[ULIST_INDEX(natom, nbor, 0)].im = 0.0;\n\n    \n\n    jju = 1;\n    for (int j = 1; j <= twojmax; j++) {\n      int deljju = j + 1;\n      for (int mb = 0; 2 * mb <= j; mb++) {\n        ulist[ULIST_INDEX(natom, nbor, jju)].re = 0.0;\n        ulist[ULIST_INDEX(natom, nbor, jju)].im = 0.0;\n        jju += deljju;\n      }\n      int ncolhalf = deljju / 2;\n      jju += deljju * ncolhalf;\n    }\n\n    jju = 1;\n    jjup = 0;\n    for (int j = 1; j <= twojmax; j++) {\n      int deljju = j + 1;\n      int deljjup = j;\n      int mb_max = (j + 1) / 2;\n      int ma_max = j;\n      int m_max = ma_max * mb_max;\n\n      \n\n      for (int m_iter = 0; m_iter < m_max; ++m_iter) {\n        int mb = m_iter / ma_max;\n        int ma = m_iter % ma_max;\n        double up_r = ulist[ULIST_INDEX(natom, nbor, jjup)].re;\n        double up_i = ulist[ULIST_INDEX(natom, nbor, jjup)].im;\n\n        rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, j - mb)];\n        ulist[ULIST_INDEX(natom, nbor, jju)].re += rootpq * (a_r * up_r + a_i * up_i);\n        ulist[ULIST_INDEX(natom, nbor, jju)].im += rootpq * (a_r * up_i - a_i * up_r);\n\n        rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, j - mb)];\n        ulist[ULIST_INDEX(natom, nbor, jju+1)].re = -rootpq * (b_r * up_r + b_i * up_i);\n        ulist[ULIST_INDEX(natom, nbor, jju+1)].im = -rootpq * (b_r * up_i - b_i * up_r);\n\n        \n\n\n        if (2 * (mb + 1) == j) {\n          rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, mb + 1)];\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju)].re += rootpq * (b_r * up_r - b_i * up_i);\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju)].im += rootpq * (b_r * up_i + b_i * up_r);\n\n          rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, mb + 1)];\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].re = rootpq * (a_r * up_r - a_i * up_i);\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].im = rootpq * (a_r * up_i + a_i * up_r);\n        }\n\n        jju++;\n        jjup++;\n\n        if (ma == ma_max - 1)\n          jju++;\n      }\n\n      \n\n      \n\n      \n\n      \n\n      int jjui = idxu_block[j];\n      int jjuip = jjui + (j + 1) * (j + 1) - 1;\n      for (int mb = 0; 2 * mb < j; mb++) {\n        for (int ma = 0; ma <= j; ma++) {\n          ulist[ULIST_INDEX(natom, nbor, jjuip)].re = ulist_parity[jjui] * ulist[ULIST_INDEX(natom, nbor, jjui)].re;\n          ulist[ULIST_INDEX(natom, nbor, jjuip)].im = ulist_parity[jjui] * -ulist[ULIST_INDEX(natom, nbor, jjui)].im;\n          jjui++;\n          jjuip--;\n        }\n      }\n\n      \n\n      \n\n      if (j % 2 == 0)\n        jju += deljju;\n      int ncolhalf = deljju / 2;\n      jju += deljju * ncolhalf;\n      int ncolhalfp = deljjup / 2;\n      jjup += deljjup * ncolhalfp;\n    }\n\n    sfac = compute_sfac(r, rcutij[INDEX_2D(natom, nbor)], switch_flag);\n    sfac *= wj[INDEX_2D(natom, nbor)];\n\n    for (int j = 0; j <= twojmax; j++) {\n      int jju = idxu_block[j];\n      for (int mb = 0; mb <= j; mb++)\n        for (int ma = 0; ma <= j; ma++) {\n          atomicAdd(&(ulisttot[INDEX_2D(natom, jju)].re), sfac * ulist[ULIST_INDEX(natom, nbor, jju)].re);\n          atomicAdd(&(ulisttot[INDEX_2D(natom, jju)].im), sfac * ulist[ULIST_INDEX(natom, nbor, jju)].im);\n          jju++;\n        }\n    }\n  }\n}\n\n__global__ void reset_ylist(COMPLEX *ylist, const int ylist_size) \n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < ylist_size) ylist[i] = {0.0, 0.0};\n}\n\n__global__ void compute_yi (\n    const int*__restrict__ idxz,\n    const double*__restrict__ idxzbeta,\n    const double*__restrict__ cglist,\n    const int*__restrict__ idxcg_block,\n    const int*__restrict__ idxu_block,\n    const int*__restrict__ idxdu_block,\n    const COMPLEX*__restrict__ ulisttot,\n          COMPLEX*__restrict__ ylist,\n    const int num_atoms,\n    const int idxz_max,\n    const int jdim) \n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int jjz = blockIdx.y * blockDim.y + threadIdx.y;\n  if (jjz < idxz_max && natom < num_atoms) {\n    const int j1 = idxz[IDXZ_INDEX(jjz, 0)];\n    const int j2 = idxz[IDXZ_INDEX(jjz, 1)];\n    const int j = idxz[IDXZ_INDEX(jjz, 2)];\n    const int ma1min = idxz[IDXZ_INDEX(jjz, 3)];\n    const int ma2max = idxz[IDXZ_INDEX(jjz, 4)];\n    const int na = idxz[IDXZ_INDEX(jjz, 5)];\n    const int mb1min = idxz[IDXZ_INDEX(jjz, 6)];\n    const int mb2max = idxz[IDXZ_INDEX(jjz, 7)];\n    const int nb = idxz[IDXZ_INDEX(jjz, 8)];\n\n    const double betaj = idxzbeta[jjz];\n\n    const double* cgblock = cglist + idxcg_block[j1 + jdim*j2 + jdim*jdim*j];\n\n    int mb = (2 * (mb1min + mb2max) - j1 - j2 + j) / 2;\n    int ma = (2 * (ma1min + ma2max) - j1 - j2 + j) / 2;\n    const int jjdu = idxdu_block[j] + (j + 1) * mb + ma;\n\n    int jju1 = idxu_block[j1] + (j1 + 1) * mb1min;\n    int jju2 = idxu_block[j2] + (j2 + 1) * mb2max;\n    int icgb = mb1min * (j2 + 1) + mb2max;\n\n    double ztmp_r = 0.0;\n    double ztmp_i = 0.0;\n\n    \n\n    \n\n    \n\n\n    for (int ib = 0; ib < nb; ib++) {\n\n      double suma1_r = 0.0;\n      double suma1_i = 0.0;\n\n      int ma1 = ma1min;\n      int ma2 = ma2max;\n      int icga = ma1min * (j2 + 1) + ma2max;\n\n      \n\n      \n\n      \n\n\n      for (int ia = 0; ia < na; ia++) {\n        suma1_r += cgblock[icga] *\n          (ulisttot[INDEX_2D(natom, jju1 + ma1)].re * ulisttot[INDEX_2D(natom, jju2 + ma2)].re -\n           ulisttot[INDEX_2D(natom, jju1 + ma1)].im * ulisttot[INDEX_2D(natom, jju2 + ma2)].im);\n\n        suma1_i += cgblock[icga] *\n          (ulisttot[INDEX_2D(natom, jju1 + ma1)].re * ulisttot[INDEX_2D(natom, jju2 + ma2)].im +\n           ulisttot[INDEX_2D(natom, jju1 + ma1)].im * ulisttot[INDEX_2D(natom, jju2 + ma2)].re);\n\n        ma1++;\n        ma2--;\n        icga += j2;\n      } \n\n\n      ztmp_r += cgblock[icgb] * suma1_r;\n      ztmp_i += cgblock[icgb] * suma1_i;\n      jju1 += j1 + 1;\n      jju2 -= j2 + 1;\n      icgb += j2;\n    } \n\n\n    \n\n\n    atomicAdd(&(ylist[INDEX_2D(natom, jjdu)].re), betaj * ztmp_r);\n    atomicAdd(&(ylist[INDEX_2D(natom, jjdu)].im), betaj * ztmp_i);\n\n  } \n\n}\n\n__global__ void compute_duidrj (\n    const double *__restrict__ wj,\n    const double *__restrict__ rij,\n    const double *__restrict__ rcutij,\n    const double*__restrict__ rootpqarray,\n    const COMPLEX*__restrict__ ulist,\n          COMPLEX*__restrict__ dulist,\n    const int num_atoms,\n    const int num_nbor,\n    const int twojmax,\n    const int idxdu_max,\n    const int jdimpq,\n    const int switch_flag)\n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int nbor = blockIdx.y * blockDim.y + threadIdx.y;\n  if (natom < num_atoms && nbor < num_nbor) {\n    double wj_in = wj[INDEX_2D(natom, nbor)];\n    double rcut = rcutij[INDEX_2D(natom, nbor)];\n\n    double x = rij[ULIST_INDEX(natom, nbor, 0)];\n    double y = rij[ULIST_INDEX(natom, nbor, 1)];\n    double z = rij[ULIST_INDEX(natom, nbor, 2)];\n    double rsq = x * x + y * y + z * z;\n    double r = sqrt(rsq);\n    double rscale0 = rfac0 * MY_PI / (rcut - rmin0);\n    double theta0 = (r - rmin0) * rscale0;\n    double cs = cos(theta0);\n    double sn = sin(theta0);\n    double z0 = r * cs / sn;\n    double dz0dr = z0 / r - (r * rscale0) * (rsq + z0 * z0) / rsq;\n\n    compute_duarray(natom, nbor, num_atoms, num_nbor, \n        twojmax, idxdu_max, jdimpq, switch_flag,\n        x, y, z, z0, r, dz0dr, wj_in, rcut,\n        rootpqarray,\n        ulist,\n        dulist);\n  }\n}\n\n__global__ void compute_deidrj(\n    const int*__restrict__ idxdu_block,\n    const COMPLEX*__restrict__ dulist,\n    const COMPLEX*__restrict__ ylist,\n    double*__restrict__ dedr,\n    const int num_atoms,\n    const int num_nbor,\n    const int twojmax,\n    const int idxdu_max)\n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int nbor = blockIdx.y * blockDim.y + threadIdx.y;\n  if (natom < num_atoms && nbor < num_nbor) {\n    for (int k = 0; k < 3; k++)\n      dedr[ULIST_INDEX(natom, nbor, k)] = 0.0;\n\n    for (int j = 0; j <= twojmax; j++) {\n      int jjdu = idxdu_block[j];\n\n      for (int mb = 0; 2 * mb < j; mb++)\n        for (int ma = 0; ma <= j; ma++) {\n\n          double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n          double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n          for (int k = 0; k < 3; k++)\n            dedr[ULIST_INDEX(natom, nbor, k)] +=\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n          jjdu++;\n        } \n\n\n      \n\n\n      if (j % 2 == 0) {\n\n        int mb = j / 2;\n        for (int ma = 0; ma < mb; ma++) {\n          double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n          double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n          for (int k = 0; k < 3; k++)\n            dedr[ULIST_INDEX(natom, nbor, k)] +=\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n          jjdu++;\n        }\n\n        double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n        double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n        for (int k = 0; k < 3; k++)\n          dedr[ULIST_INDEX(natom, nbor, k)] +=\n            (dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n             dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i) *\n            0.5;\n        jjdu++;\n\n      } \n\n\n    } \n\n\n    for (int k = 0; k < 3; k++)\n      dedr[ULIST_INDEX(natom, nbor, k)] *= 2.0;\n  }\n}\n\nint main(int argc, char* argv[])\n{\n  options(argc, argv);\n\n  const int switch_flag = 1;     \n\n\n  \n\n  double elapsed_ui = 0.0, \n         elapsed_yi = 0.0, \n         elapsed_duidrj = 0.0,\n         elapsed_deidrj = 0.0;\n\n  const int ninside = refdata.ninside;\n  const int ncoeff = refdata.ncoeff;\n  const int nlocal = refdata.nlocal;\n  const int nghost = refdata.nghost;\n  const int ntotal = nlocal + nghost;\n  const int twojmax = refdata.twojmax;\n  const double rcutfac = refdata.rcutfac;\n\n  const double wself = 1.0;\n  const int num_atoms = nlocal; \n  const int num_nbor = ninside; \n\n  \n\n  double* coeffi = (double*) malloc (sizeof(double) * (ncoeff+1));\n\n  for (int icoeff = 0; icoeff < ncoeff + 1; icoeff++)\n    coeffi[icoeff] = refdata.coeff[icoeff];\n\n  double* beta = coeffi + 1;\n\n  \n\n  const int jdim = twojmax + 1;\n\n  \n\n\n  int *idxcg_block = (int*) malloc(sizeof(int) * jdim * jdim * jdim);\n\n  int idxcg_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        idxcg_block[j1 + j2 *jdim + jdim*jdim*j] = idxcg_count;\n        for (int m1 = 0; m1 <= j1; m1++)\n          for (int m2 = 0; m2 <= j2; m2++)\n            idxcg_count++;\n      }\n  const int idxcg_max = idxcg_count;\n\n  \n\n  \n\n  \n\n\n  int* idxu_block = (int*) malloc (sizeof(int) * jdim);\n  int idxu_count = 0;\n\n  for (int j = 0; j <= twojmax; j++) {\n    idxu_block[j] = idxu_count;\n    for (int mb = 0; mb <= j; mb++)\n      for (int ma = 0; ma <= j; ma++)\n        idxu_count++;\n  }\n  const int idxu_max = idxu_count;\n\n  \n\n  \n\n  \n\n\n  \n\n  int* ulist_parity = (int*) malloc (sizeof(int) * idxu_max);\n  idxu_count = 0;\n  for (int j = 0; j <= twojmax; j++) {\n    int mbpar = 1;\n    for (int mb = 0; mb <= j; mb++) {\n      int mapar = mbpar;\n      for (int ma = 0; ma <= j; ma++) {\n        ulist_parity[idxu_count] = mapar;\n        mapar = -mapar;\n        idxu_count++;\n      }\n      mbpar = -mbpar;\n    }\n  }\n\n  \n\n  \n\n  \n\n  \n\n\n  int* idxdu_block = (int*) malloc (sizeof(int) * jdim);\n  int idxdu_count = 0;\n\n  for (int j = 0; j <= twojmax; j++) {\n    idxdu_block[j] = idxdu_count;\n    for (int mb = 0; 2 * mb <= j; mb++)\n      for (int ma = 0; ma <= j; ma++)\n        idxdu_count++;\n  }\n  const int idxdu_max = idxdu_count;\n\n  \n\n\n  int idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        if (j >= j1)\n          idxb_count++;\n\n  const int idxb_max = idxb_count;\n  SNA_BINDICES* idxb = (SNA_BINDICES*) malloc (sizeof(SNA_BINDICES) * idxb_max);\n\n  idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        if (j >= j1) {\n          idxb[idxb_count].j1 = j1;\n          idxb[idxb_count].j2 = j2;\n          idxb[idxb_count].j = j;\n          idxb_count++;\n        }\n\n  \n\n\n  int* idxb_block = (int*) malloc (sizeof(int) * jdim * jdim * jdim);\n  idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        if (j < j1)\n          continue;\n        idxb_block[j1*jdim*jdim+j2*jdim+j] = idxb_count;\n        idxb_count++;\n      }\n\n\n  \n\n\n  int idxz_count = 0;\n\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        for (int mb = 0; 2 * mb <= j; mb++)\n          for (int ma = 0; ma <= j; ma++)\n            idxz_count++;\n\n  const int idxz_max = idxz_count;\n  \n\n  int* idxz = (int*) malloc (sizeof(int) * idxz_max * 9);\n\n  \n\n  double* idxzbeta = (double*) malloc (sizeof(double) * idxz_max);\n\n  \n\n  int* idxz_block = (int*) malloc (sizeof(int) * jdim * jdim * jdim);\n\n  idxz_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        idxz_block[j1*jdim*jdim+j2*jdim+j] = idxz_count;\n\n        \n\n        \n\n        \n\n        \n\n\n        double betaj;\n        if (j >= j1) {\n          const int jjb = idxb_block[j1*jdim*jdim+j2*jdim+j];\n          if (j1 == j) {\n            if (j2 == j) {\n              betaj = 3 * beta[jjb];\n            }\n            else {\n              betaj = 2 * beta[jjb];\n            }\n          } else {\n            betaj = beta[jjb];\n          }\n        } else if (j >= j2) {\n          const int jjb = idxb_block[j*jdim*jdim+j2*jdim+j1];\n          if (j2 == j) {\n            betaj = 2 * beta[jjb] * (j1 + 1) / (j + 1.0);\n          }\n          else {\n            betaj = beta[jjb] * (j1 + 1) / (j + 1.0);\n          }\n        } else {\n          const int jjb = idxb_block[j2*jdim*jdim+j*jdim+j1];\n          betaj = beta[jjb] * (j1 + 1) / (j + 1.0);\n        }\n\n        for (int mb = 0; 2 * mb <= j; mb++)\n          for (int ma = 0; ma <= j; ma++) {\n\n            idxz[IDXZ_INDEX(idxz_count, 0)] = j1;\n            idxz[IDXZ_INDEX(idxz_count, 1)] = j2;\n            idxz[IDXZ_INDEX(idxz_count, 2)] = j;\n\n            int ma1min = MAX(0, (2 * ma - j - j2 + j1) / 2);\n            idxz[IDXZ_INDEX(idxz_count, 3)] = ma1min;\n            idxz[IDXZ_INDEX(idxz_count, 4)] = (2 * ma - j - (2 * ma1min - j1) + j2) / 2;\n            idxz[IDXZ_INDEX(idxz_count, 5)] =\n              MIN(j1, (2 * ma - j + j2 + j1) / 2) - ma1min + 1;\n\n            int mb1min = MAX(0, (2 * mb - j - j2 + j1) / 2);\n            idxz[IDXZ_INDEX(idxz_count, 6)] = mb1min;\n            idxz[IDXZ_INDEX(idxz_count, 7)] = (2 * mb - j - (2 * mb1min - j1) + j2) / 2;\n            idxz[IDXZ_INDEX(idxz_count, 8)] =\n              MIN(j1, (2 * mb - j + j2 + j1) / 2) - mb1min + 1;\n\n            idxzbeta[idxz_count] = betaj;\n\n            idxz_count++;\n          }\n      }\n  \n\n\n\n  if (compute_ncoeff(twojmax) != ncoeff) {\n    printf(\"ERROR: ncoeff from SNA does not match reference data\\n\");\n    exit(1);\n  }\n\n  \n\n\n  double *rij    = (double*) malloc(sizeof(double) * (num_atoms * num_nbor * 3));\n  double *inside = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n  double *wj     = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n  double *rcutij = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n\n  const int jdimpq = twojmax + 2;\n  double* rootpqarray = (double*) malloc(sizeof(double) * jdimpq * jdimpq);\n  double* cglist = (double*) malloc (sizeof(double) * idxcg_max);\n  double* dedr = (double*) malloc (sizeof(double) * num_atoms * num_nbor * 3); \n\n  COMPLEX* ulist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * num_nbor * idxu_max); \n  COMPLEX* ylist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * idxdu_max);\n  COMPLEX* ulisttot = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * idxu_max);\n  COMPLEX* dulist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * num_nbor * 3 * idxdu_max);\n\n  \n\n  for (int p = 1; p <= twojmax; p++)\n    for (int q = 1; q <= twojmax; q++)\n      rootpqarray[ROOTPQ_INDEX(p, q)] = sqrt(static_cast<double>(p) / q);\n\n  \n\n  double sum, dcg, sfaccg;\n  int m, aa2, bb2, cc2;\n  int ifac;\n\n  idxcg_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        for (int m1 = 0; m1 <= j1; m1++) {\n          aa2 = 2 * m1 - j1;\n\n          for (int m2 = 0; m2 <= j2; m2++) {\n\n            \n\n\n            bb2 = 2 * m2 - j2;\n            m = (aa2 + bb2 + j) / 2;\n\n            if (m < 0 || m > j) {\n              cglist[idxcg_count] = 0.0;\n              idxcg_count++;\n              continue;\n            }\n\n            sum = 0.0;\n\n            for (int z = MAX(0, MAX(-(j - j2 + aa2) / 2, -(j - j1 - bb2) / 2));\n                z <=\n                MIN((j1 + j2 - j) / 2, MIN((j1 - aa2) / 2, (j2 + bb2) / 2));\n                z++) {\n              ifac = z % 2 ? -1 : 1;\n              sum += ifac / (factorial(z) * factorial((j1 + j2 - j) / 2 - z) *\n                  factorial((j1 - aa2) / 2 - z) *\n                  factorial((j2 + bb2) / 2 - z) *\n                  factorial((j - j2 + aa2) / 2 + z) *\n                  factorial((j - j1 - bb2) / 2 + z));\n            }\n\n            cc2 = 2 * m - j;\n            dcg = deltacg(j1, j2, j);\n            sfaccg = sqrt(\n                factorial((j1 + aa2) / 2) * factorial((j1 - aa2) / 2) *\n                factorial((j2 + bb2) / 2) * factorial((j2 - bb2) / 2) *\n                factorial((j + cc2) / 2) * factorial((j - cc2) / 2) * (j + 1));\n\n            cglist[idxcg_count] = sum * dcg * sfaccg;\n            idxcg_count++;\n          }\n        }\n      }\n\n  double* f = (double*) malloc (sizeof(double) * ntotal * 3);\n\n  \n\n  double sumsqferr = 0.0;\n\n  int* d_idxu_block;\n  cudaMalloc((void**)&d_idxu_block, sizeof(int)*jdim);\n  cudaMemcpy(d_idxu_block, idxu_block, sizeof(int)*jdim, cudaMemcpyHostToDevice);\n\n  int* d_ulist_parity;\n  cudaMalloc((void**)&d_ulist_parity, sizeof(int)*idxu_max);\n  cudaMemcpy(d_ulist_parity, ulist_parity, sizeof(int)*idxu_max, cudaMemcpyHostToDevice);\n\n  double* d_rootpqarray;\n  cudaMalloc((void**)&d_rootpqarray, sizeof(double)*jdimpq*jdimpq);\n  cudaMemcpy(d_rootpqarray, rootpqarray, sizeof(double)*jdimpq*jdimpq, cudaMemcpyHostToDevice); \n\n  int* d_idxz;\n  cudaMalloc((void**)&d_idxz, sizeof(int)*idxz_max*9);\n  cudaMemcpy(d_idxz, idxz, sizeof(int)*idxz_max*9, cudaMemcpyHostToDevice);\n\n  double* d_idxzbeta;\n  cudaMalloc((void**)&d_idxzbeta, sizeof(double)*idxz_max);\n  cudaMemcpy(d_idxzbeta, idxzbeta, sizeof(double)*idxz_max, cudaMemcpyHostToDevice);\n\n  int* d_idxcg_block;\n  cudaMalloc((void**)&d_idxcg_block, sizeof(int)*jdim*jdim*jdim);\n  cudaMemcpy(d_idxcg_block, idxcg_block, sizeof(int)*jdim*jdim*jdim, cudaMemcpyHostToDevice);\n\n  int* d_idxdu_block;\n  cudaMalloc((void**)&d_idxdu_block, sizeof(int)*jdim);\n  cudaMemcpy(d_idxdu_block, idxdu_block, sizeof(int)*jdim, cudaMemcpyHostToDevice);\n\n  double* d_cglist;\n  cudaMalloc((void**)&d_cglist, sizeof(double)*idxcg_max);\n  cudaMemcpy(d_cglist, cglist, sizeof(double)*idxcg_max, cudaMemcpyHostToDevice);\n\n  COMPLEX* d_dulist;\n  cudaMalloc((void**)&d_dulist, sizeof(COMPLEX)*num_atoms*num_nbor*3*idxdu_max);\n  cudaMemcpy(d_dulist, dulist, sizeof(COMPLEX)*num_atoms*num_nbor*3*idxdu_max, cudaMemcpyHostToDevice);\n\n  COMPLEX* d_ulist;\n  cudaMalloc((void**)&d_ulist, sizeof(COMPLEX)*num_atoms*num_nbor*idxu_max);\n  cudaMemcpy(d_ulist, ulist, sizeof(COMPLEX)*num_atoms*num_nbor*idxu_max, cudaMemcpyHostToDevice);\n\n  double* d_dedr;\n  cudaMalloc((void**)&d_dedr, sizeof(double)*num_atoms*num_nbor*3);\n  cudaMemcpy(d_dedr, dedr, sizeof(double)*num_atoms*num_nbor*3, cudaMemcpyHostToDevice);\n\n  COMPLEX* d_ulisttot;\n  cudaMalloc((void**)&d_ulisttot, sizeof(COMPLEX)*num_atoms*idxu_max);\n\n  COMPLEX* d_ylist;\n  cudaMalloc((void**)&d_ylist, sizeof(COMPLEX)*num_atoms*idxdu_max);\n\n  double *d_rij;\n  cudaMalloc((void**)&d_rij, sizeof(double)*num_atoms*num_nbor*3);\n\n  double *d_rcutij;\n  cudaMalloc((void**)&d_rcutij, sizeof(double)*num_atoms*num_nbor);\n\n  double *d_wj;\n  cudaMalloc((void**)&d_wj, sizeof(double)*num_atoms*num_nbor);\n\n\n  \n\n\n  auto begin = myclock::now();\n\n  for (int istep = 0; istep < nsteps; istep++) {\n\n    time_point<system_clock> start, end;\n    duration<double> elapsed;\n\n    for (int j = 0; j < ntotal * 3; j++) {\n      f[j] = 0.0;\n    }\n\n    int jt = 0, jjt = 0;\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int nbor = 0; nbor < num_nbor; nbor++) {\n        \n\n        rij[ULIST_INDEX(natom, nbor, 0)] = refdata.rij[jt++];\n        rij[ULIST_INDEX(natom, nbor, 1)] = refdata.rij[jt++];\n        rij[ULIST_INDEX(natom, nbor, 2)] = refdata.rij[jt++];\n        inside[INDEX_2D(natom, nbor)] = refdata.jlist[jjt++];\n        wj[INDEX_2D(natom, nbor)] = 1.0;\n        rcutij[INDEX_2D(natom, nbor)] = rcutfac;\n      }\n    }\n\n    cudaMemcpy(d_rij, rij, sizeof(double)*num_atoms*num_nbor*3, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_rcutij, rcutij, sizeof(double)*num_atoms*num_nbor, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_wj, wj, sizeof(double)*num_atoms*num_nbor, cudaMemcpyHostToDevice);\n\n    \n\n    start = system_clock::now();\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    dim3 grid_k1 ((num_atoms*idxu_max+255)/256);\n    dim3 block_k1 (256);\n    reset_ulisttot<<<grid_k1, block_k1>>>(d_ulisttot, num_atoms*idxu_max);\n\n\n    dim3 grid_k2 ((num_atoms+255)/256);\n    dim3 block_k2 (256);\n    set_ulisttot<<<grid_k2, block_k2>>>(d_ulisttot, d_idxu_block, num_atoms, twojmax, wself);\n\n\n    dim3 grid_k3 ((num_atoms+15)/16, (num_nbor+15)/16);\n    dim3 block_k3 (16, 16);\n\n    update_ulisttot<<<grid_k3, block_k3>>>(\n        d_rij, \n        d_rcutij,\n        d_wj,\n        d_ulist_parity,\n        d_idxu_block,\n        d_rootpqarray,\n        d_ulist,\n        d_ulisttot,\n        num_atoms,\n        num_nbor,\n        switch_flag,\n        twojmax,\n        jdimpq);\n\n    cudaDeviceSynchronize();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_ui += elapsed.count();\n\n    start = system_clock::now();\n\n    \n\n    dim3 grid_k4 ((num_atoms*idxdu_max+255)/256);\n    dim3 block_k4 (256);\n\n    reset_ylist<<<grid_k4, block_k4>>>(d_ylist, num_atoms*idxdu_max);\n\n    dim3 grid_k5 ((num_atoms+15)/16, (idxz_max+15)/16);\n    dim3 block_k5 (16, 16);\n\n    compute_yi<<<grid_k5, block_k5>>>(\n        d_idxz,\n        d_idxzbeta,\n        d_cglist,\n        d_idxcg_block,\n        d_idxu_block,\n        d_idxdu_block,\n        d_ulisttot,\n        d_ylist,\n        num_atoms,\n        idxz_max,\n        jdim);\n\n    cudaDeviceSynchronize();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_yi += elapsed.count();\n\n    \n\n    start = system_clock::now();\n\n    dim3 grid_k6 ((num_atoms+15)/16, (num_nbor+15)/16);\n    dim3 block_k6 (16, 16);\n    compute_duidrj<<<grid_k6, block_k6>>>(\n        d_wj,\n        d_rij,\n        d_rcutij,\n        d_rootpqarray,\n        d_ulist,\n        d_dulist,\n        num_atoms,\n        num_nbor,\n        twojmax,\n        idxdu_max,\n        jdimpq,\n        switch_flag);\n\n    cudaDeviceSynchronize();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_duidrj += elapsed.count();\n\n    start = system_clock::now();\n\n    \n\n    dim3 grid_k7 ((num_atoms+15)/16, (num_nbor+15)/16);\n    dim3 block_k7 (16, 16);\n\n    compute_deidrj<<<grid_k7, block_k7>>>( \n        d_idxdu_block,\n        d_dulist,\n        d_ylist,\n        d_dedr,\n        num_atoms,\n        num_nbor,\n        twojmax,\n        idxdu_max);\n\n    cudaDeviceSynchronize();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_deidrj += elapsed.count();\n\n    cudaMemcpy(dedr, d_dedr, sizeof(double)*num_atoms*num_nbor*3, cudaMemcpyDeviceToHost);\n\n    \n\n    \n\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int nbor = 0; nbor < num_nbor; nbor++) {\n        int j = inside[INDEX_2D(natom, nbor)];\n        f[F_INDEX(natom, 0)] += dedr[ULIST_INDEX(natom, nbor, 0)];\n        f[F_INDEX(natom, 1)] += dedr[ULIST_INDEX(natom, nbor, 1)];\n        f[F_INDEX(natom, 2)] += dedr[ULIST_INDEX(natom, nbor, 2)];\n        f[F_INDEX(j, 0)] -= dedr[ULIST_INDEX(natom, nbor, 0)];\n        f[F_INDEX(j, 1)] -= dedr[ULIST_INDEX(natom, nbor, 1)];\n        f[F_INDEX(j, 2)] -= dedr[ULIST_INDEX(natom, nbor, 2)];\n\n      } \n\n    }   \n\n    \n\n    jt = 0;\n    for (int j = 0; j < ntotal; j++) {\n      double ferrx = f[F_INDEX(j, 0)] - refdata.fj[jt++];\n      double ferry = f[F_INDEX(j, 1)] - refdata.fj[jt++];\n      double ferrz = f[F_INDEX(j, 2)] - refdata.fj[jt++];\n      sumsqferr += ferrx * ferrx + ferry * ferry + ferrz * ferrz;\n    }\n  }\n  auto stop = myclock::now();\n  myduration elapsed = stop - begin;\n  double duration = elapsed.count(); \n\n  printf(\"-----------------------\\n\");\n  printf(\"Summary of TestSNAP run\\n\");\n  printf(\"-----------------------\\n\");\n  printf(\"natoms = %d \\n\", nlocal);\n  printf(\"nghostatoms = %d \\n\", nghost);\n  printf(\"nsteps = %d \\n\", nsteps);\n  printf(\"nneighs = %d \\n\", ninside);\n  printf(\"twojmax = %d \\n\", twojmax);\n  printf(\"duration = %g [sec]\\n\", duration);\n\n  \n\n  double ktime = elapsed_ui + elapsed_yi + elapsed_duidrj + elapsed_deidrj;\n  printf(\"step time = %g [msec/step]\\n\", 1000.0 * duration / nsteps);\n  printf(\"\\n Individual kernel timings for each step\\n\");\n  printf(\"   compute_ui = %g [msec/step]\\n\", 1000.0 * elapsed_ui / nsteps);\n  printf(\"   compute_yi = %g [msec/step]\\n\", 1000.0 * elapsed_yi / nsteps);\n  printf(\"   compute_duidrj = %g [msec/step]\\n\", 1000.0 * elapsed_duidrj / nsteps);\n  printf(\"   compute_deidrj = %g [msec/step]\\n\", 1000.0 * elapsed_deidrj / nsteps);\n  printf(\"   Total kernel time = %g [msec/step]\\n\", 1000.0 * ktime / nsteps);\n  printf(\"   Percentage of step time = %g%%\\n\\n\", ktime / duration * 100.0);\n  printf(\"grind time = %g [msec/atom-step]\\n\", 1000.0 * duration / (nlocal * nsteps));\n  printf(\"RMS |Fj| deviation %g [eV/A]\\n\", sqrt(sumsqferr / (ntotal * nsteps)));\n\n  cudaFree(d_idxu_block);\n  cudaFree(d_ulist_parity);\n  cudaFree(d_rootpqarray);\n  cudaFree(d_idxz);\n  cudaFree(d_idxzbeta);\n  cudaFree(d_idxcg_block);\n  cudaFree(d_idxdu_block);\n  cudaFree(d_cglist);\n  cudaFree(d_dulist);\n  cudaFree(d_ulist);\n  cudaFree(d_dedr);\n  cudaFree(d_ulisttot);\n  cudaFree(d_ylist);\n  cudaFree(d_rij);\n  cudaFree(d_rcutij);\n  cudaFree(d_wj);\n\n  free(coeffi);\n  free(idxcg_block);\n  free(idxu_block);\n  free(ulist_parity);\n  free(idxdu_block);\n  free(idxb);\n  free(idxb_block);\n  free(idxz);\n  free(idxzbeta);\n  free(idxz_block);\n  free(rij);\n  free(inside);\n  free(wj);\n  free(rcutij);\n  free(rootpqarray);\n  free(cglist);\n  free(dedr);\n  free(ulist);\n  free(ylist);\n  free(ulisttot);\n  free(dulist);\n  free(f);\n\n  return 0;\n}\n"}}
{"kernel_name": "testSNAP", "parallel_api": "hip", "code": {"main.cu": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <chrono>\n#include <cmath>\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#include <iostream>\n#include <hip/hip_runtime.h>\n#include \"snap.h\"\n#include \"utils.cu\"\n\n#if REFDATA_TWOJ == 14\n#include \"refdata_2J14_W.h\"\n#elif REFDATA_TWOJ == 8\n#include \"refdata_2J8_W.h\"\n#elif REFDATA_TWOJ == 4\n#include \"refdata_2J4_W.h\"\n#else\n#include \"refdata_2J2_W.h\"\n#endif\n\n\nint nsteps = 1; \n\n\n__global__ void reset_ulisttot(COMPLEX *ulisttot, const int ulisttot_size) \n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < ulisttot_size) ulisttot[i] = {0.0, 0.0};\n}\n\n__global__ void set_ulisttot(\n    COMPLEX *__restrict__ ulisttot,\n    const int*__restrict__ idxu_block, \n    const int num_atoms,\n    const int twojmax,\n    const double wself) \n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  if (natom < num_atoms) \n    for (int j = 0; j <= twojmax; j++) {\n      int jju = idxu_block[j];\n      for (int ma = 0; ma <= j; ma++) {\n        ulisttot[INDEX_2D(natom, jju)] = { wself, 0.0 };\n        jju += j + 2;\n      }\n    }\n}\n\n__global__ void update_ulisttot(\n    const double*__restrict__ rij, \n    const double*__restrict__ rcutij,\n    const double*__restrict__ wj, \n    const int*__restrict__ ulist_parity, \n    const int*__restrict__ idxu_block, \n    const double*__restrict__ rootpqarray, \n    COMPLEX *__restrict__ ulist, \n    COMPLEX *__restrict__ ulisttot, \n    const int num_atoms,\n    const int num_nbor,\n    const int switch_flag, \n    const int twojmax, \n    const int jdimpq)\n{\n\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int nbor = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (natom < num_atoms && nbor < num_nbor) {\n    double x = rij[ULIST_INDEX(natom, nbor, 0)];\n    double y = rij[ULIST_INDEX(natom, nbor, 1)];\n    double z = rij[ULIST_INDEX(natom, nbor, 2)];\n    double rsq = x * x + y * y + z * z;\n    double r = sqrt(rsq);\n\n    double theta0 = (r - rmin0) * rfac0 * MY_PI / (rcutij[INDEX_2D(natom, nbor)] - rmin0);\n    double z0 = r / tan(theta0);\n\n    double rootpq;\n    int jju, jjup;\n\n    \n\n\n    double r0inv = 1.0 / sqrt(r * r + z0 * z0);\n    double a_r = r0inv * z0;\n    double a_i = -r0inv * z;\n    double b_r = r0inv * y;\n    double b_i = -r0inv * x;\n\n    double sfac;\n\n    sfac = compute_sfac(r, rcutij[INDEX_2D(natom, nbor)], switch_flag);\n    sfac *= wj[INDEX_2D(natom, nbor)];\n\n    \n\n    \n\n\n    \n\n    \n\n\n    \n\n    \n\n\n    \n\n    \n\n    ulist[ULIST_INDEX(natom, nbor, 0)].re = 1.0;\n    ulist[ULIST_INDEX(natom, nbor, 0)].im = 0.0;\n\n    \n\n    jju = 1;\n    for (int j = 1; j <= twojmax; j++) {\n      int deljju = j + 1;\n      for (int mb = 0; 2 * mb <= j; mb++) {\n        ulist[ULIST_INDEX(natom, nbor, jju)].re = 0.0;\n        ulist[ULIST_INDEX(natom, nbor, jju)].im = 0.0;\n        jju += deljju;\n      }\n      int ncolhalf = deljju / 2;\n      jju += deljju * ncolhalf;\n    }\n\n    jju = 1;\n    jjup = 0;\n    for (int j = 1; j <= twojmax; j++) {\n      int deljju = j + 1;\n      int deljjup = j;\n      int mb_max = (j + 1) / 2;\n      int ma_max = j;\n      int m_max = ma_max * mb_max;\n\n      \n\n      for (int m_iter = 0; m_iter < m_max; ++m_iter) {\n        int mb = m_iter / ma_max;\n        int ma = m_iter % ma_max;\n        double up_r = ulist[ULIST_INDEX(natom, nbor, jjup)].re;\n        double up_i = ulist[ULIST_INDEX(natom, nbor, jjup)].im;\n\n        rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, j - mb)];\n        ulist[ULIST_INDEX(natom, nbor, jju)].re += rootpq * (a_r * up_r + a_i * up_i);\n        ulist[ULIST_INDEX(natom, nbor, jju)].im += rootpq * (a_r * up_i - a_i * up_r);\n\n        rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, j - mb)];\n        ulist[ULIST_INDEX(natom, nbor, jju+1)].re = -rootpq * (b_r * up_r + b_i * up_i);\n        ulist[ULIST_INDEX(natom, nbor, jju+1)].im = -rootpq * (b_r * up_i - b_i * up_r);\n\n        \n\n\n        if (2 * (mb + 1) == j) {\n          rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, mb + 1)];\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju)].re += rootpq * (b_r * up_r - b_i * up_i);\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju)].im += rootpq * (b_r * up_i + b_i * up_r);\n\n          rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, mb + 1)];\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].re = rootpq * (a_r * up_r - a_i * up_i);\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].im = rootpq * (a_r * up_i + a_i * up_r);\n        }\n\n        jju++;\n        jjup++;\n\n        if (ma == ma_max - 1)\n          jju++;\n      }\n\n      \n\n      \n\n      \n\n      \n\n      int jjui = idxu_block[j];\n      int jjuip = jjui + (j + 1) * (j + 1) - 1;\n      for (int mb = 0; 2 * mb < j; mb++) {\n        for (int ma = 0; ma <= j; ma++) {\n          ulist[ULIST_INDEX(natom, nbor, jjuip)].re = ulist_parity[jjui] * ulist[ULIST_INDEX(natom, nbor, jjui)].re;\n          ulist[ULIST_INDEX(natom, nbor, jjuip)].im = ulist_parity[jjui] * -ulist[ULIST_INDEX(natom, nbor, jjui)].im;\n          jjui++;\n          jjuip--;\n        }\n      }\n\n      \n\n      \n\n      if (j % 2 == 0)\n        jju += deljju;\n      int ncolhalf = deljju / 2;\n      jju += deljju * ncolhalf;\n      int ncolhalfp = deljjup / 2;\n      jjup += deljjup * ncolhalfp;\n    }\n\n    sfac = compute_sfac(r, rcutij[INDEX_2D(natom, nbor)], switch_flag);\n    sfac *= wj[INDEX_2D(natom, nbor)];\n\n    for (int j = 0; j <= twojmax; j++) {\n      int jju = idxu_block[j];\n      for (int mb = 0; mb <= j; mb++)\n        for (int ma = 0; ma <= j; ma++) {\n          atomicAdd(&(ulisttot[INDEX_2D(natom, jju)].re), sfac * ulist[ULIST_INDEX(natom, nbor, jju)].re);\n          atomicAdd(&(ulisttot[INDEX_2D(natom, jju)].im), sfac * ulist[ULIST_INDEX(natom, nbor, jju)].im);\n          jju++;\n        }\n    }\n  }\n}\n\n__global__ void reset_ylist(COMPLEX *ylist, const int ylist_size) \n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < ylist_size) ylist[i] = {0.0, 0.0};\n}\n\n__global__ void compute_yi (\n    const int*__restrict__ idxz,\n    const double*__restrict__ idxzbeta,\n    const double*__restrict__ cglist,\n    const int*__restrict__ idxcg_block,\n    const int*__restrict__ idxu_block,\n    const int*__restrict__ idxdu_block,\n    const COMPLEX*__restrict__ ulisttot,\n          COMPLEX*__restrict__ ylist,\n    const int num_atoms,\n    const int idxz_max,\n    const int jdim) \n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int jjz = blockIdx.y * blockDim.y + threadIdx.y;\n  if (jjz < idxz_max && natom < num_atoms) {\n    const int j1 = idxz[IDXZ_INDEX(jjz, 0)];\n    const int j2 = idxz[IDXZ_INDEX(jjz, 1)];\n    const int j = idxz[IDXZ_INDEX(jjz, 2)];\n    const int ma1min = idxz[IDXZ_INDEX(jjz, 3)];\n    const int ma2max = idxz[IDXZ_INDEX(jjz, 4)];\n    const int na = idxz[IDXZ_INDEX(jjz, 5)];\n    const int mb1min = idxz[IDXZ_INDEX(jjz, 6)];\n    const int mb2max = idxz[IDXZ_INDEX(jjz, 7)];\n    const int nb = idxz[IDXZ_INDEX(jjz, 8)];\n\n    const double betaj = idxzbeta[jjz];\n\n    const double* cgblock = cglist + idxcg_block[j1 + jdim*j2 + jdim*jdim*j];\n\n    int mb = (2 * (mb1min + mb2max) - j1 - j2 + j) / 2;\n    int ma = (2 * (ma1min + ma2max) - j1 - j2 + j) / 2;\n    const int jjdu = idxdu_block[j] + (j + 1) * mb + ma;\n\n    int jju1 = idxu_block[j1] + (j1 + 1) * mb1min;\n    int jju2 = idxu_block[j2] + (j2 + 1) * mb2max;\n    int icgb = mb1min * (j2 + 1) + mb2max;\n\n    double ztmp_r = 0.0;\n    double ztmp_i = 0.0;\n\n    \n\n    \n\n    \n\n\n    for (int ib = 0; ib < nb; ib++) {\n\n      double suma1_r = 0.0;\n      double suma1_i = 0.0;\n\n      int ma1 = ma1min;\n      int ma2 = ma2max;\n      int icga = ma1min * (j2 + 1) + ma2max;\n\n      \n\n      \n\n      \n\n\n      for (int ia = 0; ia < na; ia++) {\n        suma1_r += cgblock[icga] *\n          (ulisttot[INDEX_2D(natom, jju1 + ma1)].re * ulisttot[INDEX_2D(natom, jju2 + ma2)].re -\n           ulisttot[INDEX_2D(natom, jju1 + ma1)].im * ulisttot[INDEX_2D(natom, jju2 + ma2)].im);\n\n        suma1_i += cgblock[icga] *\n          (ulisttot[INDEX_2D(natom, jju1 + ma1)].re * ulisttot[INDEX_2D(natom, jju2 + ma2)].im +\n           ulisttot[INDEX_2D(natom, jju1 + ma1)].im * ulisttot[INDEX_2D(natom, jju2 + ma2)].re);\n\n        ma1++;\n        ma2--;\n        icga += j2;\n      } \n\n\n      ztmp_r += cgblock[icgb] * suma1_r;\n      ztmp_i += cgblock[icgb] * suma1_i;\n      jju1 += j1 + 1;\n      jju2 -= j2 + 1;\n      icgb += j2;\n    } \n\n\n    \n\n\n    atomicAdd(&(ylist[INDEX_2D(natom, jjdu)].re), betaj * ztmp_r);\n    atomicAdd(&(ylist[INDEX_2D(natom, jjdu)].im), betaj * ztmp_i);\n\n  } \n\n}\n\n__global__ void compute_duidrj (\n    const double *__restrict__ wj,\n    const double *__restrict__ rij,\n    const double *__restrict__ rcutij,\n    const double*__restrict__ rootpqarray,\n    const COMPLEX*__restrict__ ulist,\n          COMPLEX*__restrict__ dulist,\n    const int num_atoms,\n    const int num_nbor,\n    const int twojmax,\n    const int idxdu_max,\n    const int jdimpq,\n    const int switch_flag)\n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int nbor = blockIdx.y * blockDim.y + threadIdx.y;\n  if (natom < num_atoms && nbor < num_nbor) {\n    double wj_in = wj[INDEX_2D(natom, nbor)];\n    double rcut = rcutij[INDEX_2D(natom, nbor)];\n\n    double x = rij[ULIST_INDEX(natom, nbor, 0)];\n    double y = rij[ULIST_INDEX(natom, nbor, 1)];\n    double z = rij[ULIST_INDEX(natom, nbor, 2)];\n    double rsq = x * x + y * y + z * z;\n    double r = sqrt(rsq);\n    double rscale0 = rfac0 * MY_PI / (rcut - rmin0);\n    double theta0 = (r - rmin0) * rscale0;\n    double cs = cos(theta0);\n    double sn = sin(theta0);\n    double z0 = r * cs / sn;\n    double dz0dr = z0 / r - (r * rscale0) * (rsq + z0 * z0) / rsq;\n\n    compute_duarray(natom, nbor, num_atoms, num_nbor, \n        twojmax, idxdu_max, jdimpq, switch_flag,\n        x, y, z, z0, r, dz0dr, wj_in, rcut,\n        rootpqarray,\n        ulist,\n        dulist);\n  }\n}\n\n__global__ void compute_deidrj(\n    const int*__restrict__ idxdu_block,\n    const COMPLEX*__restrict__ dulist,\n    const COMPLEX*__restrict__ ylist,\n    double*__restrict__ dedr,\n    const int num_atoms,\n    const int num_nbor,\n    const int twojmax,\n    const int idxdu_max)\n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int nbor = blockIdx.y * blockDim.y + threadIdx.y;\n  if (natom < num_atoms && nbor < num_nbor) {\n    for (int k = 0; k < 3; k++)\n      dedr[ULIST_INDEX(natom, nbor, k)] = 0.0;\n\n    for (int j = 0; j <= twojmax; j++) {\n      int jjdu = idxdu_block[j];\n\n      for (int mb = 0; 2 * mb < j; mb++)\n        for (int ma = 0; ma <= j; ma++) {\n\n          double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n          double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n          for (int k = 0; k < 3; k++)\n            dedr[ULIST_INDEX(natom, nbor, k)] +=\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n          jjdu++;\n        } \n\n\n      \n\n\n      if (j % 2 == 0) {\n\n        int mb = j / 2;\n        for (int ma = 0; ma < mb; ma++) {\n          double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n          double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n          for (int k = 0; k < 3; k++)\n            dedr[ULIST_INDEX(natom, nbor, k)] +=\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n          jjdu++;\n        }\n\n        double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n        double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n        for (int k = 0; k < 3; k++)\n          dedr[ULIST_INDEX(natom, nbor, k)] +=\n            (dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n             dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i) *\n            0.5;\n        jjdu++;\n\n      } \n\n\n    } \n\n\n    for (int k = 0; k < 3; k++)\n      dedr[ULIST_INDEX(natom, nbor, k)] *= 2.0;\n  }\n}\n\nint main(int argc, char* argv[])\n{\n  options(argc, argv);\n\n  const int switch_flag = 1;     \n\n\n  \n\n  double elapsed_ui = 0.0, \n         elapsed_yi = 0.0, \n         elapsed_duidrj = 0.0,\n         elapsed_deidrj = 0.0;\n\n  const int ninside = refdata.ninside;\n  const int ncoeff = refdata.ncoeff;\n  const int nlocal = refdata.nlocal;\n  const int nghost = refdata.nghost;\n  const int ntotal = nlocal + nghost;\n  const int twojmax = refdata.twojmax;\n  const double rcutfac = refdata.rcutfac;\n\n  const double wself = 1.0;\n  const int num_atoms = nlocal; \n  const int num_nbor = ninside; \n\n  \n\n  double* coeffi = (double*) malloc (sizeof(double) * (ncoeff+1));\n\n  for (int icoeff = 0; icoeff < ncoeff + 1; icoeff++)\n    coeffi[icoeff] = refdata.coeff[icoeff];\n\n  double* beta = coeffi + 1;\n\n  \n\n  const int jdim = twojmax + 1;\n\n  \n\n\n  int *idxcg_block = (int*) malloc(sizeof(int) * jdim * jdim * jdim);\n\n  int idxcg_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        idxcg_block[j1 + j2 *jdim + jdim*jdim*j] = idxcg_count;\n        for (int m1 = 0; m1 <= j1; m1++)\n          for (int m2 = 0; m2 <= j2; m2++)\n            idxcg_count++;\n      }\n  const int idxcg_max = idxcg_count;\n\n  \n\n  \n\n  \n\n\n  int* idxu_block = (int*) malloc (sizeof(int) * jdim);\n  int idxu_count = 0;\n\n  for (int j = 0; j <= twojmax; j++) {\n    idxu_block[j] = idxu_count;\n    for (int mb = 0; mb <= j; mb++)\n      for (int ma = 0; ma <= j; ma++)\n        idxu_count++;\n  }\n  const int idxu_max = idxu_count;\n\n  \n\n  \n\n  \n\n\n  \n\n  int* ulist_parity = (int*) malloc (sizeof(int) * idxu_max);\n  idxu_count = 0;\n  for (int j = 0; j <= twojmax; j++) {\n    int mbpar = 1;\n    for (int mb = 0; mb <= j; mb++) {\n      int mapar = mbpar;\n      for (int ma = 0; ma <= j; ma++) {\n        ulist_parity[idxu_count] = mapar;\n        mapar = -mapar;\n        idxu_count++;\n      }\n      mbpar = -mbpar;\n    }\n  }\n\n  \n\n  \n\n  \n\n  \n\n\n  int* idxdu_block = (int*) malloc (sizeof(int) * jdim);\n  int idxdu_count = 0;\n\n  for (int j = 0; j <= twojmax; j++) {\n    idxdu_block[j] = idxdu_count;\n    for (int mb = 0; 2 * mb <= j; mb++)\n      for (int ma = 0; ma <= j; ma++)\n        idxdu_count++;\n  }\n  const int idxdu_max = idxdu_count;\n\n  \n\n\n  int idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        if (j >= j1)\n          idxb_count++;\n\n  const int idxb_max = idxb_count;\n  SNA_BINDICES* idxb = (SNA_BINDICES*) malloc (sizeof(SNA_BINDICES) * idxb_max);\n\n  idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        if (j >= j1) {\n          idxb[idxb_count].j1 = j1;\n          idxb[idxb_count].j2 = j2;\n          idxb[idxb_count].j = j;\n          idxb_count++;\n        }\n\n  \n\n\n  int* idxb_block = (int*) malloc (sizeof(int) * jdim * jdim * jdim);\n  idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        if (j < j1)\n          continue;\n        idxb_block[j1*jdim*jdim+j2*jdim+j] = idxb_count;\n        idxb_count++;\n      }\n\n\n  \n\n\n  int idxz_count = 0;\n\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        for (int mb = 0; 2 * mb <= j; mb++)\n          for (int ma = 0; ma <= j; ma++)\n            idxz_count++;\n\n  const int idxz_max = idxz_count;\n  \n\n  int* idxz = (int*) malloc (sizeof(int) * idxz_max * 9);\n\n  \n\n  double* idxzbeta = (double*) malloc (sizeof(double) * idxz_max);\n\n  \n\n  int* idxz_block = (int*) malloc (sizeof(int) * jdim * jdim * jdim);\n\n  idxz_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        idxz_block[j1*jdim*jdim+j2*jdim+j] = idxz_count;\n\n        \n\n        \n\n        \n\n        \n\n\n        double betaj;\n        if (j >= j1) {\n          const int jjb = idxb_block[j1*jdim*jdim+j2*jdim+j];\n          if (j1 == j) {\n            if (j2 == j) {\n              betaj = 3 * beta[jjb];\n            }\n            else {\n              betaj = 2 * beta[jjb];\n            }\n          } else {\n            betaj = beta[jjb];\n          }\n        } else if (j >= j2) {\n          const int jjb = idxb_block[j*jdim*jdim+j2*jdim+j1];\n          if (j2 == j) {\n            betaj = 2 * beta[jjb] * (j1 + 1) / (j + 1.0);\n          }\n          else {\n            betaj = beta[jjb] * (j1 + 1) / (j + 1.0);\n          }\n        } else {\n          const int jjb = idxb_block[j2*jdim*jdim+j*jdim+j1];\n          betaj = beta[jjb] * (j1 + 1) / (j + 1.0);\n        }\n\n        for (int mb = 0; 2 * mb <= j; mb++)\n          for (int ma = 0; ma <= j; ma++) {\n\n            idxz[IDXZ_INDEX(idxz_count, 0)] = j1;\n            idxz[IDXZ_INDEX(idxz_count, 1)] = j2;\n            idxz[IDXZ_INDEX(idxz_count, 2)] = j;\n\n            int ma1min = MAX(0, (2 * ma - j - j2 + j1) / 2);\n            idxz[IDXZ_INDEX(idxz_count, 3)] = ma1min;\n            idxz[IDXZ_INDEX(idxz_count, 4)] = (2 * ma - j - (2 * ma1min - j1) + j2) / 2;\n            idxz[IDXZ_INDEX(idxz_count, 5)] =\n              MIN(j1, (2 * ma - j + j2 + j1) / 2) - ma1min + 1;\n\n            int mb1min = MAX(0, (2 * mb - j - j2 + j1) / 2);\n            idxz[IDXZ_INDEX(idxz_count, 6)] = mb1min;\n            idxz[IDXZ_INDEX(idxz_count, 7)] = (2 * mb - j - (2 * mb1min - j1) + j2) / 2;\n            idxz[IDXZ_INDEX(idxz_count, 8)] =\n              MIN(j1, (2 * mb - j + j2 + j1) / 2) - mb1min + 1;\n\n            idxzbeta[idxz_count] = betaj;\n\n            idxz_count++;\n          }\n      }\n  \n\n\n\n  if (compute_ncoeff(twojmax) != ncoeff) {\n    printf(\"ERROR: ncoeff from SNA does not match reference data\\n\");\n    exit(1);\n  }\n\n  \n\n\n  double *rij    = (double*) malloc(sizeof(double) * (num_atoms * num_nbor * 3));\n  double *inside = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n  double *wj     = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n  double *rcutij = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n\n  const int jdimpq = twojmax + 2;\n  double* rootpqarray = (double*) malloc(sizeof(double) * jdimpq * jdimpq);\n  double* cglist = (double*) malloc (sizeof(double) * idxcg_max);\n  double* dedr = (double*) malloc (sizeof(double) * num_atoms * num_nbor * 3); \n\n  COMPLEX* ulist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * num_nbor * idxu_max); \n  COMPLEX* ylist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * idxdu_max);\n  COMPLEX* ulisttot = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * idxu_max);\n  COMPLEX* dulist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * num_nbor * 3 * idxdu_max);\n\n  \n\n  for (int p = 1; p <= twojmax; p++)\n    for (int q = 1; q <= twojmax; q++)\n      rootpqarray[ROOTPQ_INDEX(p, q)] = sqrt(static_cast<double>(p) / q);\n\n  \n\n  double sum, dcg, sfaccg;\n  int m, aa2, bb2, cc2;\n  int ifac;\n\n  idxcg_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        for (int m1 = 0; m1 <= j1; m1++) {\n          aa2 = 2 * m1 - j1;\n\n          for (int m2 = 0; m2 <= j2; m2++) {\n\n            \n\n\n            bb2 = 2 * m2 - j2;\n            m = (aa2 + bb2 + j) / 2;\n\n            if (m < 0 || m > j) {\n              cglist[idxcg_count] = 0.0;\n              idxcg_count++;\n              continue;\n            }\n\n            sum = 0.0;\n\n            for (int z = MAX(0, MAX(-(j - j2 + aa2) / 2, -(j - j1 - bb2) / 2));\n                z <=\n                MIN((j1 + j2 - j) / 2, MIN((j1 - aa2) / 2, (j2 + bb2) / 2));\n                z++) {\n              ifac = z % 2 ? -1 : 1;\n              sum += ifac / (factorial(z) * factorial((j1 + j2 - j) / 2 - z) *\n                  factorial((j1 - aa2) / 2 - z) *\n                  factorial((j2 + bb2) / 2 - z) *\n                  factorial((j - j2 + aa2) / 2 + z) *\n                  factorial((j - j1 - bb2) / 2 + z));\n            }\n\n            cc2 = 2 * m - j;\n            dcg = deltacg(j1, j2, j);\n            sfaccg = sqrt(\n                factorial((j1 + aa2) / 2) * factorial((j1 - aa2) / 2) *\n                factorial((j2 + bb2) / 2) * factorial((j2 - bb2) / 2) *\n                factorial((j + cc2) / 2) * factorial((j - cc2) / 2) * (j + 1));\n\n            cglist[idxcg_count] = sum * dcg * sfaccg;\n            idxcg_count++;\n          }\n        }\n      }\n\n  double* f = (double*) malloc (sizeof(double) * ntotal * 3);\n\n  \n\n  double sumsqferr = 0.0;\n\n  int* d_idxu_block;\n  hipMalloc((void**)&d_idxu_block, sizeof(int)*jdim);\n  hipMemcpy(d_idxu_block, idxu_block, sizeof(int)*jdim, hipMemcpyHostToDevice);\n\n  int* d_ulist_parity;\n  hipMalloc((void**)&d_ulist_parity, sizeof(int)*idxu_max);\n  hipMemcpy(d_ulist_parity, ulist_parity, sizeof(int)*idxu_max, hipMemcpyHostToDevice);\n\n  double* d_rootpqarray;\n  hipMalloc((void**)&d_rootpqarray, sizeof(double)*jdimpq*jdimpq);\n  hipMemcpy(d_rootpqarray, rootpqarray, sizeof(double)*jdimpq*jdimpq, hipMemcpyHostToDevice); \n\n  int* d_idxz;\n  hipMalloc((void**)&d_idxz, sizeof(int)*idxz_max*9);\n  hipMemcpy(d_idxz, idxz, sizeof(int)*idxz_max*9, hipMemcpyHostToDevice);\n\n  double* d_idxzbeta;\n  hipMalloc((void**)&d_idxzbeta, sizeof(double)*idxz_max);\n  hipMemcpy(d_idxzbeta, idxzbeta, sizeof(double)*idxz_max, hipMemcpyHostToDevice);\n\n  int* d_idxcg_block;\n  hipMalloc((void**)&d_idxcg_block, sizeof(int)*jdim*jdim*jdim);\n  hipMemcpy(d_idxcg_block, idxcg_block, sizeof(int)*jdim*jdim*jdim, hipMemcpyHostToDevice);\n\n  int* d_idxdu_block;\n  hipMalloc((void**)&d_idxdu_block, sizeof(int)*jdim);\n  hipMemcpy(d_idxdu_block, idxdu_block, sizeof(int)*jdim, hipMemcpyHostToDevice);\n\n  double* d_cglist;\n  hipMalloc((void**)&d_cglist, sizeof(double)*idxcg_max);\n  hipMemcpy(d_cglist, cglist, sizeof(double)*idxcg_max, hipMemcpyHostToDevice);\n\n  COMPLEX* d_dulist;\n  hipMalloc((void**)&d_dulist, sizeof(COMPLEX)*num_atoms*num_nbor*3*idxdu_max);\n  hipMemcpy(d_dulist, dulist, sizeof(COMPLEX)*num_atoms*num_nbor*3*idxdu_max, hipMemcpyHostToDevice);\n\n  COMPLEX* d_ulist;\n  hipMalloc((void**)&d_ulist, sizeof(COMPLEX)*num_atoms*num_nbor*idxu_max);\n  hipMemcpy(d_ulist, ulist, sizeof(COMPLEX)*num_atoms*num_nbor*idxu_max, hipMemcpyHostToDevice);\n\n  double* d_dedr;\n  hipMalloc((void**)&d_dedr, sizeof(double)*num_atoms*num_nbor*3);\n  hipMemcpy(d_dedr, dedr, sizeof(double)*num_atoms*num_nbor*3, hipMemcpyHostToDevice);\n\n  COMPLEX* d_ulisttot;\n  hipMalloc((void**)&d_ulisttot, sizeof(COMPLEX)*num_atoms*idxu_max);\n\n  COMPLEX* d_ylist;\n  hipMalloc((void**)&d_ylist, sizeof(COMPLEX)*num_atoms*idxdu_max);\n\n  double *d_rij;\n  hipMalloc((void**)&d_rij, sizeof(double)*num_atoms*num_nbor*3);\n\n  double *d_rcutij;\n  hipMalloc((void**)&d_rcutij, sizeof(double)*num_atoms*num_nbor);\n\n  double *d_wj;\n  hipMalloc((void**)&d_wj, sizeof(double)*num_atoms*num_nbor);\n\n\n  \n\n\n  auto begin = myclock::now();\n\n  for (int istep = 0; istep < nsteps; istep++) {\n\n    time_point<system_clock> start, end;\n    duration<double> elapsed;\n\n    for (int j = 0; j < ntotal * 3; j++) {\n      f[j] = 0.0;\n    }\n\n    int jt = 0, jjt = 0;\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int nbor = 0; nbor < num_nbor; nbor++) {\n        \n\n        rij[ULIST_INDEX(natom, nbor, 0)] = refdata.rij[jt++];\n        rij[ULIST_INDEX(natom, nbor, 1)] = refdata.rij[jt++];\n        rij[ULIST_INDEX(natom, nbor, 2)] = refdata.rij[jt++];\n        inside[INDEX_2D(natom, nbor)] = refdata.jlist[jjt++];\n        wj[INDEX_2D(natom, nbor)] = 1.0;\n        rcutij[INDEX_2D(natom, nbor)] = rcutfac;\n      }\n    }\n\n    hipMemcpy(d_rij, rij, sizeof(double)*num_atoms*num_nbor*3, hipMemcpyHostToDevice);\n    hipMemcpy(d_rcutij, rcutij, sizeof(double)*num_atoms*num_nbor, hipMemcpyHostToDevice);\n    hipMemcpy(d_wj, wj, sizeof(double)*num_atoms*num_nbor, hipMemcpyHostToDevice);\n\n    \n\n    start = system_clock::now();\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    dim3 grid_k1 ((num_atoms*idxu_max+255)/256);\n    dim3 block_k1 (256);\n    hipLaunchKernelGGL(reset_ulisttot, grid_k1, block_k1, 0, 0, d_ulisttot, num_atoms*idxu_max);\n\n\n    dim3 grid_k2 ((num_atoms+255)/256);\n    dim3 block_k2 (256);\n    hipLaunchKernelGGL(set_ulisttot, grid_k2, block_k2, 0, 0, d_ulisttot, d_idxu_block, num_atoms, twojmax, wself);\n\n\n    dim3 grid_k3 ((num_atoms+15)/16, (num_nbor+15)/16);\n    dim3 block_k3 (16, 16);\n\n    hipLaunchKernelGGL(update_ulisttot, grid_k3, block_k3, 0, 0, \n        d_rij, \n        d_rcutij,\n        d_wj,\n        d_ulist_parity,\n        d_idxu_block,\n        d_rootpqarray,\n        d_ulist,\n        d_ulisttot,\n        num_atoms,\n        num_nbor,\n        switch_flag,\n        twojmax,\n        jdimpq);\n\n    hipDeviceSynchronize();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_ui += elapsed.count();\n\n    start = system_clock::now();\n\n    \n\n    dim3 grid_k4 ((num_atoms*idxdu_max+255)/256);\n    dim3 block_k4 (256);\n\n    hipLaunchKernelGGL(reset_ylist, grid_k4, block_k4, 0, 0, d_ylist, num_atoms*idxdu_max);\n\n    dim3 grid_k5 ((num_atoms+15)/16, (idxz_max+15)/16);\n    dim3 block_k5 (16, 16);\n\n    hipLaunchKernelGGL(compute_yi, grid_k5, block_k5, 0, 0, \n        d_idxz,\n        d_idxzbeta,\n        d_cglist,\n        d_idxcg_block,\n        d_idxu_block,\n        d_idxdu_block,\n        d_ulisttot,\n        d_ylist,\n        num_atoms,\n        idxz_max,\n        jdim);\n\n    hipDeviceSynchronize();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_yi += elapsed.count();\n\n    \n\n    start = system_clock::now();\n\n    dim3 grid_k6 ((num_atoms+15)/16, (num_nbor+15)/16);\n    dim3 block_k6 (16, 16);\n    hipLaunchKernelGGL(compute_duidrj, grid_k6, block_k6, 0, 0, \n        d_wj,\n        d_rij,\n        d_rcutij,\n        d_rootpqarray,\n        d_ulist,\n        d_dulist,\n        num_atoms,\n        num_nbor,\n        twojmax,\n        idxdu_max,\n        jdimpq,\n        switch_flag);\n\n    hipDeviceSynchronize();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_duidrj += elapsed.count();\n\n    start = system_clock::now();\n\n    \n\n    dim3 grid_k7 ((num_atoms+15)/16, (num_nbor+15)/16);\n    dim3 block_k7 (16, 16);\n\n    hipLaunchKernelGGL(compute_deidrj, grid_k7, block_k7, 0, 0,  \n        d_idxdu_block,\n        d_dulist,\n        d_ylist,\n        d_dedr,\n        num_atoms,\n        num_nbor,\n        twojmax,\n        idxdu_max);\n\n    hipDeviceSynchronize();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_deidrj += elapsed.count();\n\n    hipMemcpy(dedr, d_dedr, sizeof(double)*num_atoms*num_nbor*3, hipMemcpyDeviceToHost);\n\n    \n\n    \n\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int nbor = 0; nbor < num_nbor; nbor++) {\n        int j = inside[INDEX_2D(natom, nbor)];\n        f[F_INDEX(natom, 0)] += dedr[ULIST_INDEX(natom, nbor, 0)];\n        f[F_INDEX(natom, 1)] += dedr[ULIST_INDEX(natom, nbor, 1)];\n        f[F_INDEX(natom, 2)] += dedr[ULIST_INDEX(natom, nbor, 2)];\n        f[F_INDEX(j, 0)] -= dedr[ULIST_INDEX(natom, nbor, 0)];\n        f[F_INDEX(j, 1)] -= dedr[ULIST_INDEX(natom, nbor, 1)];\n        f[F_INDEX(j, 2)] -= dedr[ULIST_INDEX(natom, nbor, 2)];\n\n      } \n\n    }   \n\n    \n\n    jt = 0;\n    for (int j = 0; j < ntotal; j++) {\n      double ferrx = f[F_INDEX(j, 0)] - refdata.fj[jt++];\n      double ferry = f[F_INDEX(j, 1)] - refdata.fj[jt++];\n      double ferrz = f[F_INDEX(j, 2)] - refdata.fj[jt++];\n      sumsqferr += ferrx * ferrx + ferry * ferry + ferrz * ferrz;\n    }\n  }\n  auto stop = myclock::now();\n  myduration elapsed = stop - begin;\n  double duration = elapsed.count(); \n\n  printf(\"-----------------------\\n\");\n  printf(\"Summary of TestSNAP run\\n\");\n  printf(\"-----------------------\\n\");\n  printf(\"natoms = %d \\n\", nlocal);\n  printf(\"nghostatoms = %d \\n\", nghost);\n  printf(\"nsteps = %d \\n\", nsteps);\n  printf(\"nneighs = %d \\n\", ninside);\n  printf(\"twojmax = %d \\n\", twojmax);\n  printf(\"duration = %g [sec]\\n\", duration);\n\n  \n\n  double ktime = elapsed_ui + elapsed_yi + elapsed_duidrj + elapsed_deidrj;\n  printf(\"step time = %g [msec/step]\\n\", 1000.0 * duration / nsteps);\n  printf(\"\\n Individual kernel timings for each step\\n\");\n  printf(\"   compute_ui = %g [msec/step]\\n\", 1000.0 * elapsed_ui / nsteps);\n  printf(\"   compute_yi = %g [msec/step]\\n\", 1000.0 * elapsed_yi / nsteps);\n  printf(\"   compute_duidrj = %g [msec/step]\\n\", 1000.0 * elapsed_duidrj / nsteps);\n  printf(\"   compute_deidrj = %g [msec/step]\\n\", 1000.0 * elapsed_deidrj / nsteps);\n  printf(\"   Total kernel time = %g [msec/step]\\n\", 1000.0 * ktime / nsteps);\n  printf(\"   Percentage of step time = %g%%\\n\\n\", ktime / duration * 100.0);\n  printf(\"grind time = %g [msec/atom-step]\\n\", 1000.0 * duration / (nlocal * nsteps));\n  printf(\"RMS |Fj| deviation %g [eV/A]\\n\", sqrt(sumsqferr / (ntotal * nsteps)));\n\n  hipFree(d_idxu_block);\n  hipFree(d_ulist_parity);\n  hipFree(d_rootpqarray);\n  hipFree(d_idxz);\n  hipFree(d_idxzbeta);\n  hipFree(d_idxcg_block);\n  hipFree(d_idxdu_block);\n  hipFree(d_cglist);\n  hipFree(d_dulist);\n  hipFree(d_ulist);\n  hipFree(d_dedr);\n  hipFree(d_ulisttot);\n  hipFree(d_ylist);\n  hipFree(d_rij);\n  hipFree(d_rcutij);\n  hipFree(d_wj);\n\n  free(coeffi);\n  free(idxcg_block);\n  free(idxu_block);\n  free(ulist_parity);\n  free(idxdu_block);\n  free(idxb);\n  free(idxb_block);\n  free(idxz);\n  free(idxzbeta);\n  free(idxz_block);\n  free(rij);\n  free(inside);\n  free(wj);\n  free(rcutij);\n  free(rootpqarray);\n  free(cglist);\n  free(dedr);\n  free(ulist);\n  free(ylist);\n  free(ulisttot);\n  free(dulist);\n  free(f);\n\n  return 0;\n}\n"}}
{"kernel_name": "testSNAP", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <chrono>\n#include <cmath>\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#include <iostream>\n#include \"snap.h\"\n#include \"utils.cpp\"\n\n#if REFDATA_TWOJ == 14\n#include \"refdata_2J14_W.h\"\n#elif REFDATA_TWOJ == 8\n#include \"refdata_2J8_W.h\"\n#elif REFDATA_TWOJ == 4\n#include \"refdata_2J4_W.h\"\n#else\n#include \"refdata_2J2_W.h\"\n#endif\n\n\nint nsteps = 1; \n\n\nint main(int argc, char* argv[])\n{\n  options(argc, argv);\n\n  const int switch_flag = 1;     \n\n\n  \n\n  double elapsed_ui = 0.0, \n         elapsed_yi = 0.0, \n         elapsed_duidrj = 0.0,\n         elapsed_deidrj = 0.0;\n\n  const int ninside = refdata.ninside;\n  const int ncoeff = refdata.ncoeff;\n  const int nlocal = refdata.nlocal;\n  const int nghost = refdata.nghost;\n  const int ntotal = nlocal + nghost;\n  const int twojmax = refdata.twojmax;\n  const double rcutfac = refdata.rcutfac;\n\n  const double wself = 1.0;\n  const int num_atoms = nlocal; \n  const int num_nbor = ninside; \n\n  \n\n  double* coeffi = (double*) malloc (sizeof(double) * (ncoeff+1));\n\n  for (int icoeff = 0; icoeff < ncoeff + 1; icoeff++)\n    coeffi[icoeff] = refdata.coeff[icoeff];\n\n  double* beta = coeffi + 1;\n\n  \n\n  const int jdim = twojmax + 1;\n\n  \n\n\n  int *idxcg_block = (int*) malloc(sizeof(int) * jdim * jdim * jdim);\n\n  int idxcg_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        idxcg_block[j1 + j2 *jdim + jdim*jdim*j] = idxcg_count;\n        for (int m1 = 0; m1 <= j1; m1++)\n          for (int m2 = 0; m2 <= j2; m2++)\n            idxcg_count++;\n      }\n  const int idxcg_max = idxcg_count;\n\n  \n\n  \n\n  \n\n\n  int* idxu_block = (int*) malloc (sizeof(int) * jdim);\n  int idxu_count = 0;\n\n  for (int j = 0; j <= twojmax; j++) {\n    idxu_block[j] = idxu_count;\n    for (int mb = 0; mb <= j; mb++)\n      for (int ma = 0; ma <= j; ma++)\n        idxu_count++;\n  }\n  const int idxu_max = idxu_count;\n\n  \n\n  \n\n  \n\n\n  \n\n  int* ulist_parity = (int*) malloc (sizeof(int) * idxu_max);\n  idxu_count = 0;\n  for (int j = 0; j <= twojmax; j++) {\n    int mbpar = 1;\n    for (int mb = 0; mb <= j; mb++) {\n      int mapar = mbpar;\n      for (int ma = 0; ma <= j; ma++) {\n        ulist_parity[idxu_count] = mapar;\n        mapar = -mapar;\n        idxu_count++;\n      }\n      mbpar = -mbpar;\n    }\n  }\n\n  \n\n  \n\n  \n\n  \n\n\n  int* idxdu_block = (int*) malloc (sizeof(int) * jdim);\n  int idxdu_count = 0;\n\n  for (int j = 0; j <= twojmax; j++) {\n    idxdu_block[j] = idxdu_count;\n    for (int mb = 0; 2 * mb <= j; mb++)\n      for (int ma = 0; ma <= j; ma++)\n        idxdu_count++;\n  }\n  const int idxdu_max = idxdu_count;\n\n  \n\n\n  int idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        if (j >= j1)\n          idxb_count++;\n\n  const int idxb_max = idxb_count;\n  SNA_BINDICES* idxb = (SNA_BINDICES*) malloc (sizeof(SNA_BINDICES) * idxb_max);\n\n  idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        if (j >= j1) {\n          idxb[idxb_count].j1 = j1;\n          idxb[idxb_count].j2 = j2;\n          idxb[idxb_count].j = j;\n          idxb_count++;\n        }\n\n  \n\n\n  int* idxb_block = (int*) malloc (sizeof(int) * jdim * jdim * jdim);\n  idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        if (j < j1)\n          continue;\n        idxb_block[j1*jdim*jdim+j2*jdim+j] = idxb_count;\n        idxb_count++;\n      }\n\n\n  \n\n\n  int idxz_count = 0;\n\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        for (int mb = 0; 2 * mb <= j; mb++)\n          for (int ma = 0; ma <= j; ma++)\n            idxz_count++;\n\n  const int idxz_max = idxz_count;\n  \n\n  int* idxz = (int*) malloc (sizeof(int) * idxz_max * 9);\n\n  \n\n  double* idxzbeta = (double*) malloc (sizeof(double) * idxz_max);\n\n  \n\n  int* idxz_block = (int*) malloc (sizeof(int) * jdim * jdim * jdim);\n\n  idxz_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        idxz_block[j1*jdim*jdim+j2*jdim+j] = idxz_count;\n\n        \n\n        \n\n        \n\n        \n\n\n        double betaj;\n        if (j >= j1) {\n          const int jjb = idxb_block[j1*jdim*jdim+j2*jdim+j];\n          if (j1 == j) {\n            if (j2 == j) {\n              betaj = 3 * beta[jjb];\n            }\n            else {\n              betaj = 2 * beta[jjb];\n            }\n          } else {\n            betaj = beta[jjb];\n          }\n        } else if (j >= j2) {\n          const int jjb = idxb_block[j*jdim*jdim+j2*jdim+j1];\n          if (j2 == j) {\n            betaj = 2 * beta[jjb] * (j1 + 1) / (j + 1.0);\n          }\n          else {\n            betaj = beta[jjb] * (j1 + 1) / (j + 1.0);\n          }\n        } else {\n          const int jjb = idxb_block[j2*jdim*jdim+j*jdim+j1];\n          betaj = beta[jjb] * (j1 + 1) / (j + 1.0);\n        }\n\n        for (int mb = 0; 2 * mb <= j; mb++)\n          for (int ma = 0; ma <= j; ma++) {\n\n            idxz[IDXZ_INDEX(idxz_count, 0)] = j1;\n            idxz[IDXZ_INDEX(idxz_count, 1)] = j2;\n            idxz[IDXZ_INDEX(idxz_count, 2)] = j;\n\n            int ma1min = MAX(0, (2 * ma - j - j2 + j1) / 2);\n            idxz[IDXZ_INDEX(idxz_count, 3)] = ma1min;\n            idxz[IDXZ_INDEX(idxz_count, 4)] = (2 * ma - j - (2 * ma1min - j1) + j2) / 2;\n            idxz[IDXZ_INDEX(idxz_count, 5)] =\n              MIN(j1, (2 * ma - j + j2 + j1) / 2) - ma1min + 1;\n\n            int mb1min = MAX(0, (2 * mb - j - j2 + j1) / 2);\n            idxz[IDXZ_INDEX(idxz_count, 6)] = mb1min;\n            idxz[IDXZ_INDEX(idxz_count, 7)] = (2 * mb - j - (2 * mb1min - j1) + j2) / 2;\n            idxz[IDXZ_INDEX(idxz_count, 8)] =\n              MIN(j1, (2 * mb - j + j2 + j1) / 2) - mb1min + 1;\n\n            idxzbeta[idxz_count] = betaj;\n\n            idxz_count++;\n          }\n      }\n  \n\n\n\n  if (compute_ncoeff(twojmax) != ncoeff) {\n    printf(\"ERROR: ncoeff from SNA does not match reference data\\n\");\n    exit(1);\n  }\n\n  \n\n\n  double *rij    = (double*) malloc(sizeof(double) * (num_atoms * num_nbor * 3));\n  double *inside = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n  double *wj     = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n  double *rcutij = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n\n  const int jdimpq = twojmax + 2;\n  double* rootpqarray = (double*) malloc(sizeof(double) * jdimpq * jdimpq);\n  double* cglist = (double*) malloc (sizeof(double) * idxcg_max);\n  double* dedr = (double*) malloc (sizeof(double) * num_atoms * num_nbor * 3); \n\n  COMPLEX* ulist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * num_nbor * idxu_max); \n  COMPLEX* ylist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * idxdu_max);\n  COMPLEX* ulisttot = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * idxu_max);\n  COMPLEX* dulist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * num_nbor * 3 * idxdu_max);\n\n  \n\n  for (int p = 1; p <= twojmax; p++)\n    for (int q = 1; q <= twojmax; q++)\n      rootpqarray[ROOTPQ_INDEX(p, q)] = sqrt(static_cast<double>(p) / q);\n\n  \n\n  double sum, dcg, sfaccg;\n  int m, aa2, bb2, cc2;\n  int ifac;\n\n  idxcg_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        for (int m1 = 0; m1 <= j1; m1++) {\n          aa2 = 2 * m1 - j1;\n\n          for (int m2 = 0; m2 <= j2; m2++) {\n\n            \n\n\n            bb2 = 2 * m2 - j2;\n            m = (aa2 + bb2 + j) / 2;\n\n            if (m < 0 || m > j) {\n              cglist[idxcg_count] = 0.0;\n              idxcg_count++;\n              continue;\n            }\n\n            sum = 0.0;\n\n            for (int z = MAX(0, MAX(-(j - j2 + aa2) / 2, -(j - j1 - bb2) / 2));\n                z <=\n                MIN((j1 + j2 - j) / 2, MIN((j1 - aa2) / 2, (j2 + bb2) / 2));\n                z++) {\n              ifac = z % 2 ? -1 : 1;\n              sum += ifac / (factorial(z) * factorial((j1 + j2 - j) / 2 - z) *\n                  factorial((j1 - aa2) / 2 - z) *\n                  factorial((j2 + bb2) / 2 - z) *\n                  factorial((j - j2 + aa2) / 2 + z) *\n                  factorial((j - j1 - bb2) / 2 + z));\n            }\n\n            cc2 = 2 * m - j;\n            dcg = deltacg(j1, j2, j);\n            sfaccg = sqrt(\n                factorial((j1 + aa2) / 2) * factorial((j1 - aa2) / 2) *\n                factorial((j2 + bb2) / 2) * factorial((j2 - bb2) / 2) *\n                factorial((j + cc2) / 2) * factorial((j - cc2) / 2) * (j + 1));\n\n            cglist[idxcg_count] = sum * dcg * sfaccg;\n            idxcg_count++;\n          }\n        }\n      }\n\n  double* f = (double*) malloc (sizeof(double) * ntotal * 3);\n\n  \n\n  double sumsqferr = 0.0;\n\n#if defined(OPENMP_TARGET)\n#pragma omp target data map(to: idxu_block[0:jdim], \\\n                                ulist_parity[0:idxu_max], \\\n                                rootpqarray[0:jdimpq * jdimpq], \\\n                                idxz[0:idxz_max*9], \\\n                                idxzbeta[0:idxz_max], \\\n                                idxcg_block[0:jdim*jdim*jdim], \\\n                                idxdu_block[0:jdim], \\\n                                cglist[0:idxcg_max], \\\n                                ulist[0:num_atoms * num_nbor * idxu_max], \\\n                                dulist[0: num_atoms * num_nbor * 3 * idxdu_max], \\\n                                dedr[0:num_atoms * num_nbor * 3]) \\\n                       map(alloc: ulisttot[0:num_atoms * idxu_max], \\\n                                  ylist[0:num_atoms * idxdu_max], \\\n                                  rij[0:num_atoms*num_nbor*3], \\\n                                  rcutij[0:num_atoms*num_nbor], \\\n                                  wj[0:num_atoms*num_nbor])\n{\n#endif\n\n  \n\n\n  auto begin = myclock::now();\n  for (int istep = 0; istep < nsteps; istep++) {\n\n    time_point<system_clock> start, end;\n    duration<double> elapsed;\n\n    for (int j = 0; j < ntotal * 3; j++) {\n      f[j] = 0.0;\n    }\n\n    int jt = 0, jjt = 0;\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int nbor = 0; nbor < num_nbor; nbor++) {\n        rij[ULIST_INDEX(natom, nbor, 0)] = refdata.rij[jt++];\n        rij[ULIST_INDEX(natom, nbor, 1)] = refdata.rij[jt++];\n        rij[ULIST_INDEX(natom, nbor, 2)] = refdata.rij[jt++];\n        inside[INDEX_2D(natom, nbor)] = refdata.jlist[jjt++];\n        wj[INDEX_2D(natom, nbor)] = 1.0;\n        rcutij[INDEX_2D(natom, nbor)] = rcutfac;\n      }\n    }\n\n#if defined(OPENMP_TARGET)\n#pragma omp target update to(rij[0:num_atoms*num_nbor*3])\n#pragma omp target update to(rcutij[0:num_atoms*num_nbor])\n#pragma omp target update to(wj[0:num_atoms*num_nbor])\n#endif\n\n    \n\n    start = system_clock::now();\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n#if defined(OPENMP_TARGET)\n#pragma omp target teams distribute parallel for\n#else\n#pragma omp parallel for default(none) shared(ulisttot, num_atoms, idxu_max)\n#endif\n    for (int i = 0; i < num_atoms * idxu_max; ++i)\n      ulisttot[i] = { 0.0, 0.0 };\n\n#if (OPENMP_TARGET)\n#pragma omp target teams distribute parallel for\n#else\n#pragma omp parallel for default(none) shared(ulisttot, wself, idxu_block, num_atoms, twojmax)\n#endif\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int j = 0; j <= twojmax; j++) {\n        int jju = idxu_block[j];\n        for (int ma = 0; ma <= j; ma++) {\n          ulisttot[INDEX_2D(natom, jju)] = { wself, 0.0 };\n          jju += j + 2;\n        }\n      }\n    }\n\n#if (OPENMP_TARGET)\n#pragma omp target teams distribute parallel for collapse(2)\n#else\n#pragma omp parallel for collapse(2) default(none) \\\n    shared(rcutij, rij, wj, rootpqarray, ulist_parity, idxu_block, ulist, ulisttot, \\\n           num_atoms, num_nbor, twojmax, jdimpq, idxu_max, switch_flag)\n#endif\n    for (int nbor = 0; nbor < num_nbor; nbor++) {\n      for (int natom = 0; natom < num_atoms; natom++) {\n        double x = rij[ULIST_INDEX(natom, nbor, 0)];\n        double y = rij[ULIST_INDEX(natom, nbor, 1)];\n        double z = rij[ULIST_INDEX(natom, nbor, 2)];\n        double rsq = x * x + y * y + z * z;\n        double r = sqrt(rsq);\n\n        double theta0 = (r - rmin0) * rfac0 * MY_PI / (rcutij[INDEX_2D(natom, nbor)] - rmin0);\n        double z0 = r / tan(theta0);\n\n        double rootpq;\n        int jju, jjup;\n\n        \n\n\n        double r0inv = 1.0 / sqrt(r * r + z0 * z0);\n        double a_r = r0inv * z0;\n        double a_i = -r0inv * z;\n        double b_r = r0inv * y;\n        double b_i = -r0inv * x;\n\n        double sfac;\n\n        sfac = compute_sfac(r, rcutij[INDEX_2D(natom, nbor)], switch_flag);\n        sfac *= wj[INDEX_2D(natom, nbor)];\n\n        \n\n        \n\n\n        \n\n        \n\n\n        \n\n        \n\n\n        \n\n        \n\n        ulist[ULIST_INDEX(natom, nbor, 0)].re = 1.0;\n        ulist[ULIST_INDEX(natom, nbor, 0)].im = 0.0;\n\n        \n\n        jju = 1;\n        for (int j = 1; j <= twojmax; j++) {\n          int deljju = j + 1;\n          for (int mb = 0; 2 * mb <= j; mb++) {\n            ulist[ULIST_INDEX(natom, nbor, jju)].re = 0.0;\n            ulist[ULIST_INDEX(natom, nbor, jju)].im = 0.0;\n            jju += deljju;\n          }\n          int ncolhalf = deljju / 2;\n          jju += deljju * ncolhalf;\n        }\n\n        jju = 1;\n        jjup = 0;\n        for (int j = 1; j <= twojmax; j++) {\n          int deljju = j + 1;\n          int deljjup = j;\n          int mb_max = (j + 1) / 2;\n          int ma_max = j;\n          int m_max = ma_max * mb_max;\n\n          \n\n          for (int m_iter = 0; m_iter < m_max; ++m_iter) {\n            int mb = m_iter / ma_max;\n            int ma = m_iter % ma_max;\n            double up_r = ulist[ULIST_INDEX(natom, nbor, jjup)].re;\n            double up_i = ulist[ULIST_INDEX(natom, nbor, jjup)].im;\n\n            rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, j - mb)];\n            ulist[ULIST_INDEX(natom, nbor, jju)].re += rootpq * (a_r * up_r + a_i * up_i);\n            ulist[ULIST_INDEX(natom, nbor, jju)].im += rootpq * (a_r * up_i - a_i * up_r);\n\n            rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, j - mb)];\n            ulist[ULIST_INDEX(natom, nbor, jju+1)].re = -rootpq * (b_r * up_r + b_i * up_i);\n            ulist[ULIST_INDEX(natom, nbor, jju+1)].im = -rootpq * (b_r * up_i - b_i * up_r);\n\n            \n\n\n            if (2 * (mb + 1) == j) {\n              rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, mb + 1)];\n              ulist[ULIST_INDEX(natom, nbor, jju+deljju)].re += rootpq * (b_r * up_r - b_i * up_i);\n              ulist[ULIST_INDEX(natom, nbor, jju+deljju)].im += rootpq * (b_r * up_i + b_i * up_r);\n\n              rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, mb + 1)];\n              ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].re = rootpq * (a_r * up_r - a_i * up_i);\n              ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].im = rootpq * (a_r * up_i + a_i * up_r);\n            }\n\n            jju++;\n            jjup++;\n\n            if (ma == ma_max - 1)\n              jju++;\n          }\n\n          \n\n          \n\n          \n\n          \n\n          int jjui = idxu_block[j];\n          int jjuip = jjui + (j + 1) * (j + 1) - 1;\n          for (int mb = 0; 2 * mb < j; mb++) {\n            for (int ma = 0; ma <= j; ma++) {\n              ulist[ULIST_INDEX(natom, nbor, jjuip)].re = ulist_parity[jjui] * ulist[ULIST_INDEX(natom, nbor, jjui)].re;\n              ulist[ULIST_INDEX(natom, nbor, jjuip)].im = ulist_parity[jjui] * -ulist[ULIST_INDEX(natom, nbor, jjui)].im;\n              jjui++;\n              jjuip--;\n            }\n          }\n\n          \n\n          \n\n          if (j % 2 == 0)\n            jju += deljju;\n          int ncolhalf = deljju / 2;\n          jju += deljju * ncolhalf;\n          int ncolhalfp = deljjup / 2;\n          jjup += deljjup * ncolhalfp;\n        }\n\n\n        sfac = compute_sfac(r, rcutij[INDEX_2D(natom, nbor)], switch_flag);\n        sfac *= wj[INDEX_2D(natom, nbor)];\n\n        for (int j = 0; j <= twojmax; j++) {\n          int jju = idxu_block[j];\n          for (int mb = 0; mb <= j; mb++)\n            for (int ma = 0; ma <= j; ma++) {\n#pragma omp atomic\n              ulisttot[INDEX_2D(natom, jju)].re += sfac * ulist[ULIST_INDEX(natom, nbor, jju)].re;\n#pragma omp atomic\n              ulisttot[INDEX_2D(natom, jju)].im += sfac * ulist[ULIST_INDEX(natom, nbor, jju)].im;\n\n              jju++;\n            }\n        }\n      }\n    }\n\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_ui += elapsed.count();\n\n    start = system_clock::now();\n\n    \n\n\n    \n\n#if defined(OPENMP_TARGET)\n#pragma omp target teams distribute parallel for\n#else\n#pragma omp parallel for default(none) shared(num_atoms, idxdu_max, ylist)\n#endif\n    for (int i = 0; i < num_atoms * idxdu_max; i++)\n      ylist[i] = { 0.0, 0.0 };\n\n#if defined(OPENMP_TARGET)\n#pragma omp target teams distribute parallel for collapse(2)\n#else\n#pragma omp parallel for collapse(2) default(none) shared(idxz,                \\\n    idxzbeta,            \\\n    idxcg_block,         \\\n    idxdu_block,         \\\n    idxu_block,          \\\n    cglist,              \\\n    ulisttot,            \\\n    ylist, \\\n    jdim, num_atoms, idxz_max)\n#endif\n    for (int jjz = 0; jjz < idxz_max; jjz++)\n      for (int natom = 0; natom < num_atoms; natom++)\n          {\n            const int j1 = idxz[IDXZ_INDEX(jjz, 0)];\n            const int j2 = idxz[IDXZ_INDEX(jjz, 1)];\n            const int j = idxz[IDXZ_INDEX(jjz, 2)];\n            const int ma1min = idxz[IDXZ_INDEX(jjz, 3)];\n            const int ma2max = idxz[IDXZ_INDEX(jjz, 4)];\n            const int na = idxz[IDXZ_INDEX(jjz, 5)];\n            const int mb1min = idxz[IDXZ_INDEX(jjz, 6)];\n            const int mb2max = idxz[IDXZ_INDEX(jjz, 7)];\n            const int nb = idxz[IDXZ_INDEX(jjz, 8)];\n\n            const double betaj = idxzbeta[jjz];\n\n            \n\n            const double* cgblock = cglist + idxcg_block[j1 + jdim*j2 + jdim*jdim*j];\n\n            int mb = (2 * (mb1min + mb2max) - j1 - j2 + j) / 2;\n            int ma = (2 * (ma1min + ma2max) - j1 - j2 + j) / 2;\n            const int jjdu = idxdu_block[j] + (j + 1) * mb + ma;\n\n            int jju1 = idxu_block[j1] + (j1 + 1) * mb1min;\n            int jju2 = idxu_block[j2] + (j2 + 1) * mb2max;\n            int icgb = mb1min * (j2 + 1) + mb2max;\n\n            double ztmp_r = 0.0;\n            double ztmp_i = 0.0;\n\n            \n\n            \n\n            \n\n\n            for (int ib = 0; ib < nb; ib++) {\n\n              double suma1_r = 0.0;\n              double suma1_i = 0.0;\n\n              int ma1 = ma1min;\n              int ma2 = ma2max;\n              int icga = ma1min * (j2 + 1) + ma2max;\n\n              \n\n              \n\n              \n\n\n              for (int ia = 0; ia < na; ia++) {\n                suma1_r +=\n                  cgblock[icga] *\n                  (ulisttot[INDEX_2D(natom, jju1 + ma1)].re * ulisttot[INDEX_2D(natom, jju2 + ma2)].re -\n                   ulisttot[INDEX_2D(natom, jju1 + ma1)].im * ulisttot[INDEX_2D(natom, jju2 + ma2)].im);\n\n                suma1_i +=\n                  cgblock[icga] *\n                  (ulisttot[INDEX_2D(natom, jju1 + ma1)].re * ulisttot[INDEX_2D(natom, jju2 + ma2)].im +\n                   ulisttot[INDEX_2D(natom, jju1 + ma1)].im * ulisttot[INDEX_2D(natom, jju2 + ma2)].re);\n\n                ma1++;\n                ma2--;\n                icga += j2;\n              } \n\n\n              ztmp_r += cgblock[icgb] * suma1_r;\n              ztmp_i += cgblock[icgb] * suma1_i;\n              jju1 += j1 + 1;\n              jju2 -= j2 + 1;\n              icgb += j2;\n            } \n\n\n            \n\n\n#pragma omp atomic\n            ylist[INDEX_2D(natom, jjdu)].re += betaj * ztmp_r;\n#pragma omp atomic\n            ylist[INDEX_2D(natom, jjdu)].im += betaj * ztmp_i;\n\n          } \n\n\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_yi += elapsed.count();\n\n    \n\n    start = system_clock::now();\n#if defined(OPENMP_TARGET)\n#pragma omp target teams distribute parallel for collapse(2)\n#else\n#pragma omp parallel default(none) shared(rij, wj, rcutij, rootpqarray, dulist, ulist, \\\n                                          num_atoms, num_nbor, twojmax, idxdu_max, jdimpq, switch_flag)\n#pragma omp for collapse(2)\n#endif\n    for (int nbor = 0; nbor < num_nbor; nbor++) {\n      for (int natom = 0; natom < num_atoms; natom++) {\n        double wj_in = wj[INDEX_2D(natom, nbor)];\n        double rcut = rcutij[INDEX_2D(natom, nbor)];\n\n        double x = rij[ULIST_INDEX(natom, nbor, 0)];\n        double y = rij[ULIST_INDEX(natom, nbor, 1)];\n        double z = rij[ULIST_INDEX(natom, nbor, 2)];\n        double rsq = x * x + y * y + z * z;\n        double r = sqrt(rsq);\n        double rscale0 = rfac0 * MY_PI / (rcut - rmin0);\n        double theta0 = (r - rmin0) * rscale0;\n        double cs = cos(theta0);\n        double sn = sin(theta0);\n        double z0 = r * cs / sn;\n        double dz0dr = z0 / r - (r * rscale0) * (rsq + z0 * z0) / rsq;\n\n        compute_duarray(natom, nbor, num_atoms, num_nbor, twojmax, \n                        idxdu_max, jdimpq, switch_flag,\n                        x, y, z, z0, r, dz0dr, wj_in, rcut,\n                        rootpqarray, ulist, dulist);\n      }\n    }\n\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_duidrj += elapsed.count();\n\n    start = system_clock::now();\n    \n\n#if (OPENMP_TARGET)\n#pragma omp target teams distribute parallel for collapse(2)\n#else\n#pragma omp parallel for collapse(2)\n#endif\n    for (int nbor = 0; nbor < num_nbor; nbor++) {\n      for (int natom = 0; natom < num_atoms; natom++) {\n        for (int k = 0; k < 3; k++)\n          dedr[ULIST_INDEX(natom, nbor, k)] = 0.0;\n\n        for (int j = 0; j <= twojmax; j++) {\n          int jjdu = idxdu_block[j];\n\n          for (int mb = 0; 2 * mb < j; mb++)\n            for (int ma = 0; ma <= j; ma++) {\n\n              double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n              double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n              for (int k = 0; k < 3; k++)\n                dedr[ULIST_INDEX(natom, nbor, k)] +=\n                  dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n                  dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n              jjdu++;\n            } \n\n\n          \n\n\n          if (j % 2 == 0) {\n\n            int mb = j / 2;\n            for (int ma = 0; ma < mb; ma++) {\n              double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n              double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n              for (int k = 0; k < 3; k++)\n                dedr[ULIST_INDEX(natom, nbor, k)] +=\n                  dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n                  dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n              jjdu++;\n            }\n\n            double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n            double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n            for (int k = 0; k < 3; k++)\n              dedr[ULIST_INDEX(natom, nbor, k)] +=\n                (dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n                 dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i) *\n                0.5;\n            jjdu++;\n\n          } \n\n\n        } \n\n\n        for (int k = 0; k < 3; k++)\n          dedr[ULIST_INDEX(natom, nbor, k)] *= 2.0;\n\n      } \n\n    }   \n\n\n#if defined(OPENMP_TARGET)\n#pragma omp target update from(dedr[0:num_atoms * num_nbor * 3])\n#endif\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_deidrj += elapsed.count();\n\n    \n\n    \n\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int nbor = 0; nbor < num_nbor; nbor++) {\n        int j = inside[INDEX_2D(natom, nbor)];\n        f[F_INDEX(natom, 0)] += dedr[ULIST_INDEX(natom, nbor, 0)];\n        f[F_INDEX(natom, 1)] += dedr[ULIST_INDEX(natom, nbor, 1)];\n        f[F_INDEX(natom, 2)] += dedr[ULIST_INDEX(natom, nbor, 2)];\n        f[F_INDEX(j, 0)] -= dedr[ULIST_INDEX(natom, nbor, 0)];\n        f[F_INDEX(j, 1)] -= dedr[ULIST_INDEX(natom, nbor, 1)];\n        f[F_INDEX(j, 2)] -= dedr[ULIST_INDEX(natom, nbor, 2)];\n\n      } \n\n    }   \n\n    \n\n    jt = 0;\n    for (int j = 0; j < ntotal; j++) {\n      double ferrx = f[F_INDEX(j, 0)] - refdata.fj[jt++];\n      double ferry = f[F_INDEX(j, 1)] - refdata.fj[jt++];\n      double ferrz = f[F_INDEX(j, 2)] - refdata.fj[jt++];\n      sumsqferr += ferrx * ferrx + ferry * ferry + ferrz * ferrz;\n    }\n\n  }\n  auto stop = myclock::now();\n  myduration elapsed = stop - begin;\n  double duration = elapsed.count(); \n\n  printf(\"-----------------------\\n\");\n  printf(\"Summary of TestSNAP run\\n\");\n  printf(\"-----------------------\\n\");\n  printf(\"natoms = %d \\n\", nlocal);\n  printf(\"nghostatoms = %d \\n\", nghost);\n  printf(\"nsteps = %d \\n\", nsteps);\n  printf(\"nneighs = %d \\n\", ninside);\n  printf(\"twojmax = %d \\n\", twojmax);\n  printf(\"duration = %g [sec]\\n\", duration);\n\n  \n\n  double ktime = elapsed_ui + elapsed_yi + elapsed_duidrj + elapsed_deidrj;\n  printf(\"step time = %g [msec/step]\\n\", 1000.0 * duration / nsteps);\n  printf(\"\\n Individual kernel timings for each step\\n\");\n  printf(\"   compute_ui = %g [msec/step]\\n\", 1000.0 * elapsed_ui / nsteps);\n  printf(\"   compute_yi = %g [msec/step]\\n\", 1000.0 * elapsed_yi / nsteps);\n  printf(\"   compute_duidrj = %g [msec/step]\\n\", 1000.0 * elapsed_duidrj / nsteps);\n  printf(\"   compute_deidrj = %g [msec/step]\\n\", 1000.0 * elapsed_deidrj / nsteps);\n  printf(\"   Total kernel time = %g [msec/step]\\n\", 1000.0 * ktime / nsteps);\n  printf(\"   Percentage of step time = %g%%\\n\\n\", ktime / duration * 100.0);\n  printf(\"grind time = %g [msec/atom-step]\\n\", 1000.0 * duration / (nlocal * nsteps));\n  printf(\"RMS |Fj| deviation %g [eV/A]\\n\", sqrt(sumsqferr / (ntotal * nsteps)));\n\n#if defined(OPENMP_TARGET)\n}\n#endif\n\n\n  free(coeffi);\n  free(idxcg_block);\n  free(idxu_block);\n  free(ulist_parity);\n  free(idxdu_block);\n  free(idxb);\n  free(idxb_block);\n  free(idxz);\n  free(idxzbeta);\n  free(idxz_block);\n  free(rij);\n  free(inside);\n  free(wj);\n  free(rcutij);\n  free(rootpqarray);\n  free(cglist);\n  free(dedr);\n  free(ulist);\n  free(ylist);\n  free(ulisttot);\n  free(dulist);\n  free(f);\n\n  return 0;\n}\n"}}
{"kernel_name": "testSNAP", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <chrono>\n#include <cmath>\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#include <iostream>\n#include \"snap.h\"\n#include \"utils.cpp\"\n\n#if REFDATA_TWOJ == 14\n#include \"refdata_2J14_W.h\"\n#elif REFDATA_TWOJ == 8\n#include \"refdata_2J8_W.h\"\n#elif REFDATA_TWOJ == 4\n#include \"refdata_2J4_W.h\"\n#else\n#include \"refdata_2J2_W.h\"\n#endif\n\n\nint nsteps = 1; \n\n\nint main(int argc, char* argv[])\n{\n  options(argc, argv);\n\n  const int switch_flag = 1;     \n\n\n  \n\n  double elapsed_ui = 0.0, \n         elapsed_yi = 0.0, \n         elapsed_duidrj = 0.0,\n         elapsed_deidrj = 0.0;\n\n  const int ninside = refdata.ninside;\n  const int ncoeff = refdata.ncoeff;\n  const int nlocal = refdata.nlocal;\n  const int nghost = refdata.nghost;\n  const int ntotal = nlocal + nghost;\n  const int twojmax = refdata.twojmax;\n  const double rcutfac = refdata.rcutfac;\n\n  const double wself = 1.0;\n  const int num_atoms = nlocal; \n  const int num_nbor = ninside; \n\n  \n\n  double* coeffi = (double*) malloc (sizeof(double) * (ncoeff+1));\n\n  for (int icoeff = 0; icoeff < ncoeff + 1; icoeff++)\n    coeffi[icoeff] = refdata.coeff[icoeff];\n\n  double* beta = coeffi + 1;\n\n  \n\n  const int jdim = twojmax + 1;\n\n  \n\n\n  int *idxcg_block = (int*) malloc(sizeof(int) * jdim * jdim * jdim);\n\n  int idxcg_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        idxcg_block[j1 + j2 *jdim + jdim*jdim*j] = idxcg_count;\n        for (int m1 = 0; m1 <= j1; m1++)\n          for (int m2 = 0; m2 <= j2; m2++)\n            idxcg_count++;\n      }\n  const int idxcg_max = idxcg_count;\n\n  \n\n  \n\n  \n\n\n  int* idxu_block = (int*) malloc (sizeof(int) * jdim);\n  int idxu_count = 0;\n\n  for (int j = 0; j <= twojmax; j++) {\n    idxu_block[j] = idxu_count;\n    for (int mb = 0; mb <= j; mb++)\n      for (int ma = 0; ma <= j; ma++)\n        idxu_count++;\n  }\n  const int idxu_max = idxu_count;\n\n  \n\n  \n\n  \n\n\n  \n\n  int* ulist_parity = (int*) malloc (sizeof(int) * idxu_max);\n  idxu_count = 0;\n  for (int j = 0; j <= twojmax; j++) {\n    int mbpar = 1;\n    for (int mb = 0; mb <= j; mb++) {\n      int mapar = mbpar;\n      for (int ma = 0; ma <= j; ma++) {\n        ulist_parity[idxu_count] = mapar;\n        mapar = -mapar;\n        idxu_count++;\n      }\n      mbpar = -mbpar;\n    }\n  }\n\n  \n\n  \n\n  \n\n  \n\n\n  int* idxdu_block = (int*) malloc (sizeof(int) * jdim);\n  int idxdu_count = 0;\n\n  for (int j = 0; j <= twojmax; j++) {\n    idxdu_block[j] = idxdu_count;\n    for (int mb = 0; 2 * mb <= j; mb++)\n      for (int ma = 0; ma <= j; ma++)\n        idxdu_count++;\n  }\n  const int idxdu_max = idxdu_count;\n\n  \n\n\n  int idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        if (j >= j1)\n          idxb_count++;\n\n  const int idxb_max = idxb_count;\n  SNA_BINDICES* idxb = (SNA_BINDICES*) malloc (sizeof(SNA_BINDICES) * idxb_max);\n\n  idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        if (j >= j1) {\n          idxb[idxb_count].j1 = j1;\n          idxb[idxb_count].j2 = j2;\n          idxb[idxb_count].j = j;\n          idxb_count++;\n        }\n\n  \n\n\n  int* idxb_block = (int*) malloc (sizeof(int) * jdim * jdim * jdim);\n  idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        if (j < j1)\n          continue;\n        idxb_block[j1*jdim*jdim+j2*jdim+j] = idxb_count;\n        idxb_count++;\n      }\n\n\n  \n\n\n  int idxz_count = 0;\n\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        for (int mb = 0; 2 * mb <= j; mb++)\n          for (int ma = 0; ma <= j; ma++)\n            idxz_count++;\n\n  const int idxz_max = idxz_count;\n  \n\n  int* idxz = (int*) malloc (sizeof(int) * idxz_max * 9);\n\n  \n\n  double* idxzbeta = (double*) malloc (sizeof(double) * idxz_max);\n\n  \n\n  int* idxz_block = (int*) malloc (sizeof(int) * jdim * jdim * jdim);\n\n  idxz_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        idxz_block[j1*jdim*jdim+j2*jdim+j] = idxz_count;\n\n        \n\n        \n\n        \n\n        \n\n\n        double betaj;\n        if (j >= j1) {\n          const int jjb = idxb_block[j1*jdim*jdim+j2*jdim+j];\n          if (j1 == j) {\n            if (j2 == j) {\n              betaj = 3 * beta[jjb];\n            }\n            else {\n              betaj = 2 * beta[jjb];\n            }\n          } else {\n            betaj = beta[jjb];\n          }\n        } else if (j >= j2) {\n          const int jjb = idxb_block[j*jdim*jdim+j2*jdim+j1];\n          if (j2 == j) {\n            betaj = 2 * beta[jjb] * (j1 + 1) / (j + 1.0);\n          }\n          else {\n            betaj = beta[jjb] * (j1 + 1) / (j + 1.0);\n          }\n        } else {\n          const int jjb = idxb_block[j2*jdim*jdim+j*jdim+j1];\n          betaj = beta[jjb] * (j1 + 1) / (j + 1.0);\n        }\n\n        for (int mb = 0; 2 * mb <= j; mb++)\n          for (int ma = 0; ma <= j; ma++) {\n\n            idxz[IDXZ_INDEX(idxz_count, 0)] = j1;\n            idxz[IDXZ_INDEX(idxz_count, 1)] = j2;\n            idxz[IDXZ_INDEX(idxz_count, 2)] = j;\n\n            int ma1min = MAX(0, (2 * ma - j - j2 + j1) / 2);\n            idxz[IDXZ_INDEX(idxz_count, 3)] = ma1min;\n            idxz[IDXZ_INDEX(idxz_count, 4)] = (2 * ma - j - (2 * ma1min - j1) + j2) / 2;\n            idxz[IDXZ_INDEX(idxz_count, 5)] =\n              MIN(j1, (2 * ma - j + j2 + j1) / 2) - ma1min + 1;\n\n            int mb1min = MAX(0, (2 * mb - j - j2 + j1) / 2);\n            idxz[IDXZ_INDEX(idxz_count, 6)] = mb1min;\n            idxz[IDXZ_INDEX(idxz_count, 7)] = (2 * mb - j - (2 * mb1min - j1) + j2) / 2;\n            idxz[IDXZ_INDEX(idxz_count, 8)] =\n              MIN(j1, (2 * mb - j + j2 + j1) / 2) - mb1min + 1;\n\n            idxzbeta[idxz_count] = betaj;\n\n            idxz_count++;\n          }\n      }\n  \n\n\n\n  if (compute_ncoeff(twojmax) != ncoeff) {\n    printf(\"ERROR: ncoeff from SNA does not match reference data\\n\");\n    exit(1);\n  }\n\n  \n\n\n  double *rij    = (double*) malloc(sizeof(double) * (num_atoms * num_nbor * 3));\n  double *inside = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n  double *wj     = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n  double *rcutij = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n\n  const int jdimpq = twojmax + 2;\n  double* rootpqarray = (double*) malloc(sizeof(double) * jdimpq * jdimpq);\n  double* cglist = (double*) malloc (sizeof(double) * idxcg_max);\n  double* dedr = (double*) malloc (sizeof(double) * num_atoms * num_nbor * 3); \n\n  COMPLEX* ulist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * num_nbor * idxu_max); \n  COMPLEX* ylist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * idxdu_max);\n  COMPLEX* ulisttot = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * idxu_max);\n  COMPLEX* dulist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * num_nbor * 3 * idxdu_max);\n\n  \n\n  for (int p = 1; p <= twojmax; p++)\n    for (int q = 1; q <= twojmax; q++)\n      rootpqarray[ROOTPQ_INDEX(p, q)] = sqrt(static_cast<double>(p) / q);\n\n  \n\n  double sum, dcg, sfaccg;\n  int m, aa2, bb2, cc2;\n  int ifac;\n\n  idxcg_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        for (int m1 = 0; m1 <= j1; m1++) {\n          aa2 = 2 * m1 - j1;\n\n          for (int m2 = 0; m2 <= j2; m2++) {\n\n            \n\n\n            bb2 = 2 * m2 - j2;\n            m = (aa2 + bb2 + j) / 2;\n\n            if (m < 0 || m > j) {\n              cglist[idxcg_count] = 0.0;\n              idxcg_count++;\n              continue;\n            }\n\n            sum = 0.0;\n\n            for (int z = MAX(0, MAX(-(j - j2 + aa2) / 2, -(j - j1 - bb2) / 2));\n                z <=\n                MIN((j1 + j2 - j) / 2, MIN((j1 - aa2) / 2, (j2 + bb2) / 2));\n                z++) {\n              ifac = z % 2 ? -1 : 1;\n              sum += ifac / (factorial(z) * factorial((j1 + j2 - j) / 2 - z) *\n                  factorial((j1 - aa2) / 2 - z) *\n                  factorial((j2 + bb2) / 2 - z) *\n                  factorial((j - j2 + aa2) / 2 + z) *\n                  factorial((j - j1 - bb2) / 2 + z));\n            }\n\n            cc2 = 2 * m - j;\n            dcg = deltacg(j1, j2, j);\n            sfaccg = sqrt(\n                factorial((j1 + aa2) / 2) * factorial((j1 - aa2) / 2) *\n                factorial((j2 + bb2) / 2) * factorial((j2 - bb2) / 2) *\n                factorial((j + cc2) / 2) * factorial((j - cc2) / 2) * (j + 1));\n\n            cglist[idxcg_count] = sum * dcg * sfaccg;\n            idxcg_count++;\n          }\n        }\n      }\n\n  double* f = (double*) malloc (sizeof(double) * ntotal * 3);\n\n  \n\n  double sumsqferr = 0.0;\n\n#if defined(OPENMP_TARGET)\n{\n\n  \n\n\n  auto begin = myclock::now();\n  for (int istep = 0; istep < nsteps; istep++) {\n\n    time_point<system_clock> start, end;\n    duration<double> elapsed;\n\n    for (int j = 0; j < ntotal * 3; j++) {\n      f[j] = 0.0;\n    }\n\n    int jt = 0, jjt = 0;\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int nbor = 0; nbor < num_nbor; nbor++) {\n        rij[ULIST_INDEX(natom, nbor, 0)] = refdata.rij[jt++];\n        rij[ULIST_INDEX(natom, nbor, 1)] = refdata.rij[jt++];\n        rij[ULIST_INDEX(natom, nbor, 2)] = refdata.rij[jt++];\n        inside[INDEX_2D(natom, nbor)] = refdata.jlist[jjt++];\n        wj[INDEX_2D(natom, nbor)] = 1.0;\n        rcutij[INDEX_2D(natom, nbor)] = rcutfac;\n      }\n    }\n\n#if defined(OPENMP_TARGET)\n#endif\n\n    \n\n    start = system_clock::now();\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n#if defined(OPENMP_TARGET)\n#else\n#endif\n    for (int i = 0; i < num_atoms * idxu_max; ++i)\n      ulisttot[i] = { 0.0, 0.0 };\n\n#if (OPENMP_TARGET)\n#else\n#endif\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int j = 0; j <= twojmax; j++) {\n        int jju = idxu_block[j];\n        for (int ma = 0; ma <= j; ma++) {\n          ulisttot[INDEX_2D(natom, jju)] = { wself, 0.0 };\n          jju += j + 2;\n        }\n      }\n    }\n\n#if (OPENMP_TARGET)\n#else\n#endif\n    for (int nbor = 0; nbor < num_nbor; nbor++) {\n      for (int natom = 0; natom < num_atoms; natom++) {\n        double x = rij[ULIST_INDEX(natom, nbor, 0)];\n        double y = rij[ULIST_INDEX(natom, nbor, 1)];\n        double z = rij[ULIST_INDEX(natom, nbor, 2)];\n        double rsq = x * x + y * y + z * z;\n        double r = sqrt(rsq);\n\n        double theta0 = (r - rmin0) * rfac0 * MY_PI / (rcutij[INDEX_2D(natom, nbor)] - rmin0);\n        double z0 = r / tan(theta0);\n\n        double rootpq;\n        int jju, jjup;\n\n        \n\n\n        double r0inv = 1.0 / sqrt(r * r + z0 * z0);\n        double a_r = r0inv * z0;\n        double a_i = -r0inv * z;\n        double b_r = r0inv * y;\n        double b_i = -r0inv * x;\n\n        double sfac;\n\n        sfac = compute_sfac(r, rcutij[INDEX_2D(natom, nbor)], switch_flag);\n        sfac *= wj[INDEX_2D(natom, nbor)];\n\n        \n\n        \n\n\n        \n\n        \n\n\n        \n\n        \n\n\n        \n\n        \n\n        ulist[ULIST_INDEX(natom, nbor, 0)].re = 1.0;\n        ulist[ULIST_INDEX(natom, nbor, 0)].im = 0.0;\n\n        \n\n        jju = 1;\n        for (int j = 1; j <= twojmax; j++) {\n          int deljju = j + 1;\n          for (int mb = 0; 2 * mb <= j; mb++) {\n            ulist[ULIST_INDEX(natom, nbor, jju)].re = 0.0;\n            ulist[ULIST_INDEX(natom, nbor, jju)].im = 0.0;\n            jju += deljju;\n          }\n          int ncolhalf = deljju / 2;\n          jju += deljju * ncolhalf;\n        }\n\n        jju = 1;\n        jjup = 0;\n        for (int j = 1; j <= twojmax; j++) {\n          int deljju = j + 1;\n          int deljjup = j;\n          int mb_max = (j + 1) / 2;\n          int ma_max = j;\n          int m_max = ma_max * mb_max;\n\n          \n\n          for (int m_iter = 0; m_iter < m_max; ++m_iter) {\n            int mb = m_iter / ma_max;\n            int ma = m_iter % ma_max;\n            double up_r = ulist[ULIST_INDEX(natom, nbor, jjup)].re;\n            double up_i = ulist[ULIST_INDEX(natom, nbor, jjup)].im;\n\n            rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, j - mb)];\n            ulist[ULIST_INDEX(natom, nbor, jju)].re += rootpq * (a_r * up_r + a_i * up_i);\n            ulist[ULIST_INDEX(natom, nbor, jju)].im += rootpq * (a_r * up_i - a_i * up_r);\n\n            rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, j - mb)];\n            ulist[ULIST_INDEX(natom, nbor, jju+1)].re = -rootpq * (b_r * up_r + b_i * up_i);\n            ulist[ULIST_INDEX(natom, nbor, jju+1)].im = -rootpq * (b_r * up_i - b_i * up_r);\n\n            \n\n\n            if (2 * (mb + 1) == j) {\n              rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, mb + 1)];\n              ulist[ULIST_INDEX(natom, nbor, jju+deljju)].re += rootpq * (b_r * up_r - b_i * up_i);\n              ulist[ULIST_INDEX(natom, nbor, jju+deljju)].im += rootpq * (b_r * up_i + b_i * up_r);\n\n              rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, mb + 1)];\n              ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].re = rootpq * (a_r * up_r - a_i * up_i);\n              ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].im = rootpq * (a_r * up_i + a_i * up_r);\n            }\n\n            jju++;\n            jjup++;\n\n            if (ma == ma_max - 1)\n              jju++;\n          }\n\n          \n\n          \n\n          \n\n          \n\n          int jjui = idxu_block[j];\n          int jjuip = jjui + (j + 1) * (j + 1) - 1;\n          for (int mb = 0; 2 * mb < j; mb++) {\n            for (int ma = 0; ma <= j; ma++) {\n              ulist[ULIST_INDEX(natom, nbor, jjuip)].re = ulist_parity[jjui] * ulist[ULIST_INDEX(natom, nbor, jjui)].re;\n              ulist[ULIST_INDEX(natom, nbor, jjuip)].im = ulist_parity[jjui] * -ulist[ULIST_INDEX(natom, nbor, jjui)].im;\n              jjui++;\n              jjuip--;\n            }\n          }\n\n          \n\n          \n\n          if (j % 2 == 0)\n            jju += deljju;\n          int ncolhalf = deljju / 2;\n          jju += deljju * ncolhalf;\n          int ncolhalfp = deljjup / 2;\n          jjup += deljjup * ncolhalfp;\n        }\n\n\n        sfac = compute_sfac(r, rcutij[INDEX_2D(natom, nbor)], switch_flag);\n        sfac *= wj[INDEX_2D(natom, nbor)];\n\n        for (int j = 0; j <= twojmax; j++) {\n          int jju = idxu_block[j];\n          for (int mb = 0; mb <= j; mb++)\n            for (int ma = 0; ma <= j; ma++) {\n              ulisttot[INDEX_2D(natom, jju)].re += sfac * ulist[ULIST_INDEX(natom, nbor, jju)].re;\n              ulisttot[INDEX_2D(natom, jju)].im += sfac * ulist[ULIST_INDEX(natom, nbor, jju)].im;\n\n              jju++;\n            }\n        }\n      }\n    }\n\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_ui += elapsed.count();\n\n    start = system_clock::now();\n\n    \n\n\n    \n\n#if defined(OPENMP_TARGET)\n#else\n#endif\n    for (int i = 0; i < num_atoms * idxdu_max; i++)\n      ylist[i] = { 0.0, 0.0 };\n\n#if defined(OPENMP_TARGET)\n#else\n#endif\n    for (int jjz = 0; jjz < idxz_max; jjz++)\n      for (int natom = 0; natom < num_atoms; natom++)\n          {\n            const int j1 = idxz[IDXZ_INDEX(jjz, 0)];\n            const int j2 = idxz[IDXZ_INDEX(jjz, 1)];\n            const int j = idxz[IDXZ_INDEX(jjz, 2)];\n            const int ma1min = idxz[IDXZ_INDEX(jjz, 3)];\n            const int ma2max = idxz[IDXZ_INDEX(jjz, 4)];\n            const int na = idxz[IDXZ_INDEX(jjz, 5)];\n            const int mb1min = idxz[IDXZ_INDEX(jjz, 6)];\n            const int mb2max = idxz[IDXZ_INDEX(jjz, 7)];\n            const int nb = idxz[IDXZ_INDEX(jjz, 8)];\n\n            const double betaj = idxzbeta[jjz];\n\n            \n\n            const double* cgblock = cglist + idxcg_block[j1 + jdim*j2 + jdim*jdim*j];\n\n            int mb = (2 * (mb1min + mb2max) - j1 - j2 + j) / 2;\n            int ma = (2 * (ma1min + ma2max) - j1 - j2 + j) / 2;\n            const int jjdu = idxdu_block[j] + (j + 1) * mb + ma;\n\n            int jju1 = idxu_block[j1] + (j1 + 1) * mb1min;\n            int jju2 = idxu_block[j2] + (j2 + 1) * mb2max;\n            int icgb = mb1min * (j2 + 1) + mb2max;\n\n            double ztmp_r = 0.0;\n            double ztmp_i = 0.0;\n\n            \n\n            \n\n            \n\n\n            for (int ib = 0; ib < nb; ib++) {\n\n              double suma1_r = 0.0;\n              double suma1_i = 0.0;\n\n              int ma1 = ma1min;\n              int ma2 = ma2max;\n              int icga = ma1min * (j2 + 1) + ma2max;\n\n              \n\n              \n\n              \n\n\n              for (int ia = 0; ia < na; ia++) {\n                suma1_r +=\n                  cgblock[icga] *\n                  (ulisttot[INDEX_2D(natom, jju1 + ma1)].re * ulisttot[INDEX_2D(natom, jju2 + ma2)].re -\n                   ulisttot[INDEX_2D(natom, jju1 + ma1)].im * ulisttot[INDEX_2D(natom, jju2 + ma2)].im);\n\n                suma1_i +=\n                  cgblock[icga] *\n                  (ulisttot[INDEX_2D(natom, jju1 + ma1)].re * ulisttot[INDEX_2D(natom, jju2 + ma2)].im +\n                   ulisttot[INDEX_2D(natom, jju1 + ma1)].im * ulisttot[INDEX_2D(natom, jju2 + ma2)].re);\n\n                ma1++;\n                ma2--;\n                icga += j2;\n              } \n\n\n              ztmp_r += cgblock[icgb] * suma1_r;\n              ztmp_i += cgblock[icgb] * suma1_i;\n              jju1 += j1 + 1;\n              jju2 -= j2 + 1;\n              icgb += j2;\n            } \n\n\n            \n\n\n            ylist[INDEX_2D(natom, jjdu)].re += betaj * ztmp_r;\n            ylist[INDEX_2D(natom, jjdu)].im += betaj * ztmp_i;\n\n          } \n\n\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_yi += elapsed.count();\n\n    \n\n    start = system_clock::now();\n#if defined(OPENMP_TARGET)\n#else\n#endif\n    for (int nbor = 0; nbor < num_nbor; nbor++) {\n      for (int natom = 0; natom < num_atoms; natom++) {\n        double wj_in = wj[INDEX_2D(natom, nbor)];\n        double rcut = rcutij[INDEX_2D(natom, nbor)];\n\n        double x = rij[ULIST_INDEX(natom, nbor, 0)];\n        double y = rij[ULIST_INDEX(natom, nbor, 1)];\n        double z = rij[ULIST_INDEX(natom, nbor, 2)];\n        double rsq = x * x + y * y + z * z;\n        double r = sqrt(rsq);\n        double rscale0 = rfac0 * MY_PI / (rcut - rmin0);\n        double theta0 = (r - rmin0) * rscale0;\n        double cs = cos(theta0);\n        double sn = sin(theta0);\n        double z0 = r * cs / sn;\n        double dz0dr = z0 / r - (r * rscale0) * (rsq + z0 * z0) / rsq;\n\n        compute_duarray(natom, nbor, num_atoms, num_nbor, twojmax, \n                        idxdu_max, jdimpq, switch_flag,\n                        x, y, z, z0, r, dz0dr, wj_in, rcut,\n                        rootpqarray, ulist, dulist);\n      }\n    }\n\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_duidrj += elapsed.count();\n\n    start = system_clock::now();\n    \n\n#if (OPENMP_TARGET)\n#else\n#endif\n    for (int nbor = 0; nbor < num_nbor; nbor++) {\n      for (int natom = 0; natom < num_atoms; natom++) {\n        for (int k = 0; k < 3; k++)\n          dedr[ULIST_INDEX(natom, nbor, k)] = 0.0;\n\n        for (int j = 0; j <= twojmax; j++) {\n          int jjdu = idxdu_block[j];\n\n          for (int mb = 0; 2 * mb < j; mb++)\n            for (int ma = 0; ma <= j; ma++) {\n\n              double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n              double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n              for (int k = 0; k < 3; k++)\n                dedr[ULIST_INDEX(natom, nbor, k)] +=\n                  dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n                  dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n              jjdu++;\n            } \n\n\n          \n\n\n          if (j % 2 == 0) {\n\n            int mb = j / 2;\n            for (int ma = 0; ma < mb; ma++) {\n              double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n              double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n              for (int k = 0; k < 3; k++)\n                dedr[ULIST_INDEX(natom, nbor, k)] +=\n                  dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n                  dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n              jjdu++;\n            }\n\n            double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n            double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n            for (int k = 0; k < 3; k++)\n              dedr[ULIST_INDEX(natom, nbor, k)] +=\n                (dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n                 dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i) *\n                0.5;\n            jjdu++;\n\n          } \n\n\n        } \n\n\n        for (int k = 0; k < 3; k++)\n          dedr[ULIST_INDEX(natom, nbor, k)] *= 2.0;\n\n      } \n\n    }   \n\n\n#if defined(OPENMP_TARGET)\n#endif\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_deidrj += elapsed.count();\n\n    \n\n    \n\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int nbor = 0; nbor < num_nbor; nbor++) {\n        int j = inside[INDEX_2D(natom, nbor)];\n        f[F_INDEX(natom, 0)] += dedr[ULIST_INDEX(natom, nbor, 0)];\n        f[F_INDEX(natom, 1)] += dedr[ULIST_INDEX(natom, nbor, 1)];\n        f[F_INDEX(natom, 2)] += dedr[ULIST_INDEX(natom, nbor, 2)];\n        f[F_INDEX(j, 0)] -= dedr[ULIST_INDEX(natom, nbor, 0)];\n        f[F_INDEX(j, 1)] -= dedr[ULIST_INDEX(natom, nbor, 1)];\n        f[F_INDEX(j, 2)] -= dedr[ULIST_INDEX(natom, nbor, 2)];\n\n      } \n\n    }   \n\n    \n\n    jt = 0;\n    for (int j = 0; j < ntotal; j++) {\n      double ferrx = f[F_INDEX(j, 0)] - refdata.fj[jt++];\n      double ferry = f[F_INDEX(j, 1)] - refdata.fj[jt++];\n      double ferrz = f[F_INDEX(j, 2)] - refdata.fj[jt++];\n      sumsqferr += ferrx * ferrx + ferry * ferry + ferrz * ferrz;\n    }\n\n  }\n  auto stop = myclock::now();\n  myduration elapsed = stop - begin;\n  double duration = elapsed.count(); \n\n  printf(\"-----------------------\\n\");\n  printf(\"Summary of TestSNAP run\\n\");\n  printf(\"-----------------------\\n\");\n  printf(\"natoms = %d \\n\", nlocal);\n  printf(\"nghostatoms = %d \\n\", nghost);\n  printf(\"nsteps = %d \\n\", nsteps);\n  printf(\"nneighs = %d \\n\", ninside);\n  printf(\"twojmax = %d \\n\", twojmax);\n  printf(\"duration = %g [sec]\\n\", duration);\n\n  \n\n  double ktime = elapsed_ui + elapsed_yi + elapsed_duidrj + elapsed_deidrj;\n  printf(\"step time = %g [msec/step]\\n\", 1000.0 * duration / nsteps);\n  printf(\"\\n Individual kernel timings for each step\\n\");\n  printf(\"   compute_ui = %g [msec/step]\\n\", 1000.0 * elapsed_ui / nsteps);\n  printf(\"   compute_yi = %g [msec/step]\\n\", 1000.0 * elapsed_yi / nsteps);\n  printf(\"   compute_duidrj = %g [msec/step]\\n\", 1000.0 * elapsed_duidrj / nsteps);\n  printf(\"   compute_deidrj = %g [msec/step]\\n\", 1000.0 * elapsed_deidrj / nsteps);\n  printf(\"   Total kernel time = %g [msec/step]\\n\", 1000.0 * ktime / nsteps);\n  printf(\"   Percentage of step time = %g%%\\n\\n\", ktime / duration * 100.0);\n  printf(\"grind time = %g [msec/atom-step]\\n\", 1000.0 * duration / (nlocal * nsteps));\n  printf(\"RMS |Fj| deviation %g [eV/A]\\n\", sqrt(sumsqferr / (ntotal * nsteps)));\n\n#if defined(OPENMP_TARGET)\n}\n#endif\n\n\n  free(coeffi);\n  free(idxcg_block);\n  free(idxu_block);\n  free(ulist_parity);\n  free(idxdu_block);\n  free(idxb);\n  free(idxb_block);\n  free(idxz);\n  free(idxzbeta);\n  free(idxz_block);\n  free(rij);\n  free(inside);\n  free(wj);\n  free(rcutij);\n  free(rootpqarray);\n  free(cglist);\n  free(dedr);\n  free(ulist);\n  free(ylist);\n  free(ulisttot);\n  free(dulist);\n  free(f);\n\n  return 0;\n}"}}
{"kernel_name": "testSNAP", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#include <chrono>\n#include <cmath>\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#include <iostream>\n#include <sycl/sycl.hpp>\n#include \"snap.h\"\n#include \"utils.cpp\"\n\n#if REFDATA_TWOJ == 14\n#include \"refdata_2J14_W.h\"\n#elif REFDATA_TWOJ == 8\n#include \"refdata_2J8_W.h\"\n#elif REFDATA_TWOJ == 4\n#include \"refdata_2J4_W.h\"\n#else\n#include \"refdata_2J2_W.h\"\n#endif\n\nint nsteps = 1; \n\n\ninline double atomicAdd( double &var, double operand )\n{\n  auto atm = sycl::atomic_ref<double,\n    sycl::memory_order::relaxed,\n    sycl::memory_scope::device,\n    sycl::access::address_space::global_space>(var);\n  return atm.fetch_add(operand);\n}\n\nint main(int argc, char* argv[])\n{\n  options(argc, argv);\n\n  const int switch_flag = 1;     \n\n\n  \n\n  double elapsed_ui = 0.0,\n         elapsed_yi = 0.0,\n         elapsed_duidrj = 0.0,\n         elapsed_deidrj = 0.0;\n\n  const int ninside = refdata.ninside;\n  const int ncoeff = refdata.ncoeff;\n  const int nlocal = refdata.nlocal;\n  const int nghost = refdata.nghost;\n  const int ntotal = nlocal + nghost;\n  const int twojmax = refdata.twojmax;\n  const double rcutfac = refdata.rcutfac;\n\n  const double wself = 1.0;\n  const int num_atoms = nlocal;\n  const int num_nbor = ninside;\n\n  \n\n  double* coeffi = (double*) malloc (sizeof(double) * (ncoeff+1));\n\n  for (int icoeff = 0; icoeff < ncoeff + 1; icoeff++)\n    coeffi[icoeff] = refdata.coeff[icoeff];\n\n  double* beta = coeffi + 1;\n\n  \n\n  const int jdim = twojmax + 1;\n\n  \n\n\n  int *idxcg_block = (int*) malloc(sizeof(int) * jdim * jdim * jdim);\n\n  int idxcg_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        idxcg_block[j1 + j2 *jdim + jdim*jdim*j] = idxcg_count;\n        for (int m1 = 0; m1 <= j1; m1++)\n          for (int m2 = 0; m2 <= j2; m2++)\n            idxcg_count++;\n      }\n  const int idxcg_max = idxcg_count;\n\n  \n\n  \n\n  \n\n\n  int* idxu_block = (int*) malloc (sizeof(int) * jdim);\n  int idxu_count = 0;\n\n  for (int j = 0; j <= twojmax; j++) {\n    idxu_block[j] = idxu_count;\n    for (int mb = 0; mb <= j; mb++)\n      for (int ma = 0; ma <= j; ma++)\n        idxu_count++;\n  }\n  const int idxu_max = idxu_count;\n\n  \n\n  \n\n  \n\n\n  \n\n  int* ulist_parity = (int*) malloc (sizeof(int) * idxu_max);\n  idxu_count = 0;\n  for (int j = 0; j <= twojmax; j++) {\n    int mbpar = 1;\n    for (int mb = 0; mb <= j; mb++) {\n      int mapar = mbpar;\n      for (int ma = 0; ma <= j; ma++) {\n        ulist_parity[idxu_count] = mapar;\n        mapar = -mapar;\n        idxu_count++;\n      }\n      mbpar = -mbpar;\n    }\n  }\n\n  \n\n  \n\n  \n\n  \n\n\n  int* idxdu_block = (int*) malloc (sizeof(int) * jdim);\n  int idxdu_count = 0;\n\n  for (int j = 0; j <= twojmax; j++) {\n    idxdu_block[j] = idxdu_count;\n    for (int mb = 0; 2 * mb <= j; mb++)\n      for (int ma = 0; ma <= j; ma++)\n        idxdu_count++;\n  }\n  const int idxdu_max = idxdu_count;\n\n  \n\n\n  int idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        if (j >= j1)\n          idxb_count++;\n\n  const int idxb_max = idxb_count;\n  SNA_BINDICES* idxb = (SNA_BINDICES*) malloc (sizeof(SNA_BINDICES) * idxb_max);\n\n  idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        if (j >= j1) {\n          idxb[idxb_count].j1 = j1;\n          idxb[idxb_count].j2 = j2;\n          idxb[idxb_count].j = j;\n          idxb_count++;\n        }\n\n  \n\n\n  int* idxb_block = (int*) malloc (sizeof(int) * jdim * jdim * jdim);\n  idxb_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        if (j < j1)\n          continue;\n        idxb_block[j1*jdim*jdim+j2*jdim+j] = idxb_count;\n        idxb_count++;\n      }\n\n\n  \n\n\n  int idxz_count = 0;\n\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2)\n        for (int mb = 0; 2 * mb <= j; mb++)\n          for (int ma = 0; ma <= j; ma++)\n            idxz_count++;\n\n  const int idxz_max = idxz_count;\n  \n\n  int* idxz = (int*) malloc (sizeof(int) * idxz_max * 9);\n\n  \n\n  double* idxzbeta = (double*) malloc (sizeof(double) * idxz_max);\n\n  \n\n  int* idxz_block = (int*) malloc (sizeof(int) * jdim * jdim * jdim);\n\n  idxz_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        idxz_block[j1*jdim*jdim+j2*jdim+j] = idxz_count;\n\n        \n\n        \n\n        \n\n        \n\n\n        double betaj;\n        if (j >= j1) {\n          const int jjb = idxb_block[j1*jdim*jdim+j2*jdim+j];\n          if (j1 == j) {\n            if (j2 == j) {\n              betaj = 3 * beta[jjb];\n            }\n            else {\n              betaj = 2 * beta[jjb];\n            }\n          } else {\n            betaj = beta[jjb];\n          }\n        } else if (j >= j2) {\n          const int jjb = idxb_block[j*jdim*jdim+j2*jdim+j1];\n          if (j2 == j) {\n            betaj = 2 * beta[jjb] * (j1 + 1) / (j + 1.0);\n          }\n          else {\n            betaj = beta[jjb] * (j1 + 1) / (j + 1.0);\n          }\n        } else {\n          const int jjb = idxb_block[j2*jdim*jdim+j*jdim+j1];\n          betaj = beta[jjb] * (j1 + 1) / (j + 1.0);\n        }\n\n        for (int mb = 0; 2 * mb <= j; mb++)\n          for (int ma = 0; ma <= j; ma++) {\n\n            idxz[IDXZ_INDEX(idxz_count, 0)] = j1;\n            idxz[IDXZ_INDEX(idxz_count, 1)] = j2;\n            idxz[IDXZ_INDEX(idxz_count, 2)] = j;\n\n            int ma1min = MAX(0, (2 * ma - j - j2 + j1) / 2);\n            idxz[IDXZ_INDEX(idxz_count, 3)] = ma1min;\n            idxz[IDXZ_INDEX(idxz_count, 4)] = (2 * ma - j - (2 * ma1min - j1) + j2) / 2;\n            idxz[IDXZ_INDEX(idxz_count, 5)] =\n              MIN(j1, (2 * ma - j + j2 + j1) / 2) - ma1min + 1;\n\n            int mb1min = MAX(0, (2 * mb - j - j2 + j1) / 2);\n            idxz[IDXZ_INDEX(idxz_count, 6)] = mb1min;\n            idxz[IDXZ_INDEX(idxz_count, 7)] = (2 * mb - j - (2 * mb1min - j1) + j2) / 2;\n            idxz[IDXZ_INDEX(idxz_count, 8)] =\n              MIN(j1, (2 * mb - j + j2 + j1) / 2) - mb1min + 1;\n\n            idxzbeta[idxz_count] = betaj;\n\n            idxz_count++;\n          }\n      }\n  \n\n\n\n  if (compute_ncoeff(twojmax) != ncoeff) {\n    printf(\"ERROR: ncoeff from SNA does not match reference data\\n\");\n    exit(1);\n  }\n\n  \n\n\n  double *rij    = (double*) malloc(sizeof(double) * (num_atoms * num_nbor * 3));\n  double *inside = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n  double *wj     = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n  double *rcutij = (double*) malloc(sizeof(double) * (num_atoms * num_nbor));\n\n  const int jdimpq = twojmax + 2;\n  double* rootpqarray = (double*) malloc(sizeof(double) * jdimpq * jdimpq);\n  double* cglist = (double*) malloc (sizeof(double) * idxcg_max);\n  double* dedr = (double*) malloc (sizeof(double) * num_atoms * num_nbor * 3);\n\n  COMPLEX* ulist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * num_nbor * idxu_max);\n  COMPLEX* ylist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * idxdu_max);\n  COMPLEX* ulisttot = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * idxu_max);\n  COMPLEX* dulist = (COMPLEX*) malloc (sizeof(COMPLEX) * num_atoms * num_nbor * 3 * idxdu_max);\n\n  \n\n  for (int p = 1; p <= twojmax; p++)\n    for (int q = 1; q <= twojmax; q++)\n      rootpqarray[ROOTPQ_INDEX(p, q)] = sqrt(static_cast<double>(p) / q);\n\n  \n\n  double sum, dcg, sfaccg;\n  int m, aa2, bb2, cc2;\n  int ifac;\n\n  idxcg_count = 0;\n  for (int j1 = 0; j1 <= twojmax; j1++)\n    for (int j2 = 0; j2 <= j1; j2++)\n      for (int j = abs(j1 - j2); j <= MIN(twojmax, j1 + j2); j += 2) {\n        for (int m1 = 0; m1 <= j1; m1++) {\n          aa2 = 2 * m1 - j1;\n\n          for (int m2 = 0; m2 <= j2; m2++) {\n\n            \n\n\n            bb2 = 2 * m2 - j2;\n            m = (aa2 + bb2 + j) / 2;\n\n            if (m < 0 || m > j) {\n              cglist[idxcg_count] = 0.0;\n              idxcg_count++;\n              continue;\n            }\n\n            sum = 0.0;\n\n            for (int z = MAX(0, MAX(-(j - j2 + aa2) / 2, -(j - j1 - bb2) / 2));\n                z <=\n                MIN((j1 + j2 - j) / 2, MIN((j1 - aa2) / 2, (j2 + bb2) / 2));\n                z++) {\n              ifac = z % 2 ? -1 : 1;\n              sum += ifac / (factorial(z) * factorial((j1 + j2 - j) / 2 - z) *\n                  factorial((j1 - aa2) / 2 - z) *\n                  factorial((j2 + bb2) / 2 - z) *\n                  factorial((j - j2 + aa2) / 2 + z) *\n                  factorial((j - j1 - bb2) / 2 + z));\n            }\n\n            cc2 = 2 * m - j;\n            dcg = deltacg(j1, j2, j);\n            sfaccg = sqrt(\n                factorial((j1 + aa2) / 2) * factorial((j1 - aa2) / 2) *\n                factorial((j2 + bb2) / 2) * factorial((j2 - bb2) / 2) *\n                factorial((j + cc2) / 2) * factorial((j - cc2) / 2) * (j + 1));\n\n            cglist[idxcg_count] = sum * dcg * sfaccg;\n            idxcg_count++;\n          }\n        }\n      }\n\n  double* f = (double*) malloc (sizeof(double) * ntotal * 3);\n\n  \n\n  double sumsqferr = 0.0;\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  int *d_idxu_block = sycl::malloc_device<int>(jdim, q);\n  q.memcpy(d_idxu_block, idxu_block, sizeof(int)*jdim);\n\n  int *d_ulist_parity = sycl::malloc_device<int>(idxu_max, q);\n  q.memcpy(d_ulist_parity, ulist_parity, sizeof(int)*idxu_max);\n\n  double *d_rootpqarray = sycl::malloc_device<double>(jdimpq * jdimpq, q);\n  q.memcpy(d_rootpqarray, rootpqarray, sizeof(double)*jdimpq*jdimpq);\n\n  int *d_idxz = sycl::malloc_device<int>(idxz_max * 9, q);\n  q.memcpy(d_idxz, idxz, sizeof(int)*idxz_max*9);\n\n  double *d_idxzbeta = sycl::malloc_device<double>(idxz_max, q);\n  q.memcpy(d_idxzbeta, idxzbeta, sizeof(double)*idxz_max);\n\n  int *d_idxcg_block = sycl::malloc_device<int>(jdim * jdim * jdim, q);\n  q.memcpy(d_idxcg_block, idxcg_block, sizeof(int)*jdim*jdim*jdim);\n\n  int *d_idxdu_block = sycl::malloc_device<int>(jdim, q);\n  q.memcpy(d_idxdu_block, idxdu_block, sizeof(int)*jdim);\n\n  double *d_cglist = sycl::malloc_device<double>(idxcg_max, q);\n  q.memcpy(d_cglist, cglist, sizeof(double)*idxcg_max);\n\n  COMPLEX *d_dulist = sycl::malloc_device<COMPLEX>(num_atoms * num_nbor * 3 * idxdu_max, q);\n  q.memcpy(d_dulist, dulist, sizeof(COMPLEX)*num_atoms*num_nbor*3*idxdu_max);\n\n  COMPLEX *d_ulist = sycl::malloc_device<COMPLEX>(num_atoms * num_nbor * idxu_max, q);\n  q.memcpy(d_ulist, ulist, sizeof(COMPLEX)*num_atoms*num_nbor*idxu_max);\n\n  double *d_dedr = sycl::malloc_device<double>(num_atoms * num_nbor * 3, q);\n  q.memcpy(d_dedr, dedr, sizeof(double)*num_atoms*num_nbor*3);\n\n  COMPLEX *d_ulisttot = sycl::malloc_device<COMPLEX>(num_atoms * idxu_max, q);\n  COMPLEX *d_ylist = sycl::malloc_device<COMPLEX>(num_atoms * idxdu_max, q);\n  double *d_rij = sycl::malloc_device<double>(num_atoms*num_nbor*3, q);\n  double *d_rcutij = sycl::malloc_device<double>(num_atoms*num_nbor, q);\n  double *d_wj = sycl::malloc_device<double>(num_atoms*num_nbor, q);\n\n  \n\n\n  auto begin = myclock::now();\n  for (int istep = 0; istep < nsteps; istep++) {\n\n    time_point<system_clock> start, end;\n    duration<double> elapsed;\n\n    for (int j = 0; j < ntotal * 3; j++) {\n      f[j] = 0.0;\n    }\n\n    int jt = 0, jjt = 0;\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int nbor = 0; nbor < num_nbor; nbor++) {\n        rij[ULIST_INDEX(natom, nbor, 0)] = refdata.rij[jt++];\n        rij[ULIST_INDEX(natom, nbor, 1)] = refdata.rij[jt++];\n        rij[ULIST_INDEX(natom, nbor, 2)] = refdata.rij[jt++];\n        inside[INDEX_2D(natom, nbor)] = refdata.jlist[jjt++];\n        wj[INDEX_2D(natom, nbor)] = 1.0;\n        rcutij[INDEX_2D(natom, nbor)] = rcutfac;\n      }\n    }\n\n    q.memcpy(d_rij, rij, sizeof(double)*num_atoms*num_nbor*3);\n    q.memcpy(d_rcutij, rcutij, sizeof(double)*num_atoms*num_nbor);\n    q.memcpy(d_wj, wj, sizeof(double)*num_atoms*num_nbor);\n    q.wait();\n\n    \n\n    start = system_clock::now();\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    sycl::range<1> gws_k1 ((num_atoms*idxu_max+255)/256*256);\n    sycl::range<1> lws_k1 (256);\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class reset_ulisttot>(\n        sycl::nd_range<1>(gws_k1, lws_k1), [=] (sycl::nd_item<1> item) {\n        int i = item.get_global_id(0);\n        if (i < num_atoms*idxu_max) d_ulisttot[i] = {0.0, 0.0};\n      });\n    });\n\n    sycl::range<1> gws_k2 ((num_atoms+255)/256*256);\n    sycl::range<1> lws_k2 (256);\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class set_ulisttot>(\n        sycl::nd_range<1>(gws_k2, lws_k2), [=] (sycl::nd_item<1> item) {\n        int natom = item.get_global_id(0);\n        if (natom < num_atoms)\n          for (int j = 0; j <= twojmax; j++) {\n            int jju = d_idxu_block[j];\n            for (int ma = 0; ma <= j; ma++) {\n              d_ulisttot[INDEX_2D(natom, jju)] = { wself, 0.0 };\n              jju += j + 2;\n            }\n          }\n      });\n    });\n\n    sycl::range<2> gws_k3 ((num_nbor+15)/16*16, (num_atoms+15)/16*16);\n    sycl::range<2> lws_k3 (16, 16);\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class update_ulisttot>(sycl::nd_range<2>(gws_k3, lws_k3), [=] (sycl::nd_item<2> item) {\n\tint nbor = item.get_global_id(0);\n\tint natom = item.get_global_id(1);\n        if (natom < num_atoms && nbor < num_nbor) {\n          double x = d_rij[ULIST_INDEX(natom, nbor, 0)];\n          double y = d_rij[ULIST_INDEX(natom, nbor, 1)];\n          double z = d_rij[ULIST_INDEX(natom, nbor, 2)];\n          double rsq = x * x + y * y + z * z;\n          double r = sycl::sqrt(rsq);\n\n          double theta0 = (r - rmin0) * rfac0 * MY_PI / (d_rcutij[INDEX_2D(natom, nbor)] - rmin0);\n          double z0 = r / sycl::tan(theta0);\n\n          double rootpq;\n          int jju, jjup;\n\n          \n\n\n          double r0inv = 1.0 / sycl::sqrt(r * r + z0 * z0);\n          double a_r = r0inv * z0;\n          double a_i = -r0inv * z;\n          double b_r = r0inv * y;\n          double b_i = -r0inv * x;\n\n          double sfac;\n\n          sfac = compute_sfac(r, d_rcutij[INDEX_2D(natom, nbor)], switch_flag);\n          sfac *= d_wj[INDEX_2D(natom, nbor)];\n\n          \n\n          \n\n\n          \n\n          \n\n\n          \n\n          \n\n\n          \n\n          \n\n          d_ulist[ULIST_INDEX(natom, nbor, 0)].re = 1.0;\n          d_ulist[ULIST_INDEX(natom, nbor, 0)].im = 0.0;\n\n          \n\n          jju = 1;\n          for (int j = 1; j <= twojmax; j++) {\n            int deljju = j + 1;\n            for (int mb = 0; 2 * mb <= j; mb++) {\n              d_ulist[ULIST_INDEX(natom, nbor, jju)].re = 0.0;\n              d_ulist[ULIST_INDEX(natom, nbor, jju)].im = 0.0;\n              jju += deljju;\n            }\n            int ncolhalf = deljju / 2;\n            jju += deljju * ncolhalf;\n          }\n\n          jju = 1;\n          jjup = 0;\n          for (int j = 1; j <= twojmax; j++) {\n            int deljju = j + 1;\n            int deljjup = j;\n            int mb_max = (j + 1) / 2;\n            int ma_max = j;\n            int m_max = ma_max * mb_max;\n\n            \n\n            for (int m_iter = 0; m_iter < m_max; ++m_iter) {\n              int mb = m_iter / ma_max;\n              int ma = m_iter % ma_max;\n              double up_r = d_ulist[ULIST_INDEX(natom, nbor, jjup)].re;\n              double up_i = d_ulist[ULIST_INDEX(natom, nbor, jjup)].im;\n\n              rootpq = d_rootpqarray[ROOTPQ_INDEX(j - ma, j - mb)];\n              d_ulist[ULIST_INDEX(natom, nbor, jju)].re += rootpq * (a_r * up_r + a_i * up_i);\n              d_ulist[ULIST_INDEX(natom, nbor, jju)].im += rootpq * (a_r * up_i - a_i * up_r);\n\n              rootpq = d_rootpqarray[ROOTPQ_INDEX(ma + 1, j - mb)];\n              d_ulist[ULIST_INDEX(natom, nbor, jju+1)].re = -rootpq * (b_r * up_r + b_i * up_i);\n              d_ulist[ULIST_INDEX(natom, nbor, jju+1)].im = -rootpq * (b_r * up_i - b_i * up_r);\n\n              \n\n\n              if (2 * (mb + 1) == j) {\n                rootpq = d_rootpqarray[ROOTPQ_INDEX(j - ma, mb + 1)];\n                d_ulist[ULIST_INDEX(natom, nbor, jju+deljju)].re += rootpq * (b_r * up_r - b_i * up_i);\n                d_ulist[ULIST_INDEX(natom, nbor, jju+deljju)].im += rootpq * (b_r * up_i + b_i * up_r);\n\n                rootpq = d_rootpqarray[ROOTPQ_INDEX(ma + 1, mb + 1)];\n                d_ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].re = rootpq * (a_r * up_r - a_i * up_i);\n                d_ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].im = rootpq * (a_r * up_i + a_i * up_r);\n              }\n\n              jju++;\n              jjup++;\n\n              if (ma == ma_max - 1)\n                jju++;\n            }\n\n            \n\n            \n\n            \n\n            \n\n            int jjui = d_idxu_block[j];\n            int jjuip = jjui + (j + 1) * (j + 1) - 1;\n            for (int mb = 0; 2 * mb < j; mb++) {\n              for (int ma = 0; ma <= j; ma++) {\n                d_ulist[ULIST_INDEX(natom, nbor, jjuip)].re = d_ulist_parity[jjui] * d_ulist[ULIST_INDEX(natom, nbor, jjui)].re;\n                d_ulist[ULIST_INDEX(natom, nbor, jjuip)].im = d_ulist_parity[jjui] * -d_ulist[ULIST_INDEX(natom, nbor, jjui)].im;\n                jjui++;\n                jjuip--;\n              }\n            }\n\n            \n\n            \n\n            if (j % 2 == 0)\n              jju += deljju;\n            int ncolhalf = deljju / 2;\n            jju += deljju * ncolhalf;\n            int ncolhalfp = deljjup / 2;\n            jjup += deljjup * ncolhalfp;\n          }\n\n          sfac = compute_sfac(r, d_rcutij[INDEX_2D(natom, nbor)], switch_flag);\n          sfac *= d_wj[INDEX_2D(natom, nbor)];\n\n          for (int j = 0; j <= twojmax; j++) {\n            int jju = d_idxu_block[j];\n            for (int mb = 0; mb <= j; mb++)\n              for (int ma = 0; ma <= j; ma++) {\n                atomicAdd(d_ulisttot[INDEX_2D(natom, jju)].re, sfac * d_ulist[ULIST_INDEX(natom, nbor, jju)].re);\n                atomicAdd(d_ulisttot[INDEX_2D(natom, jju)].im, sfac * d_ulist[ULIST_INDEX(natom, nbor, jju)].im);\n                jju++;\n              }\n          }\n        }\n      });\n    });\n\n    q.wait();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_ui += elapsed.count();\n\n    start = system_clock::now();\n\n    \n\n\n    \n\n    sycl::range<1> gws_k4 ((num_atoms*idxdu_max+255)/256*256);\n    sycl::range<1> lws_k4 (256);\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class reset_ylist>(\n        sycl::nd_range<1>(gws_k4, lws_k4), [=] (sycl::nd_item<1> item) {\n        int i = item.get_global_id(0);\n        if (i < num_atoms*idxdu_max) d_ylist[i] = {0.0, 0.0};\n      });\n    });\n\n    sycl::range<2> gws_k5 ((idxz_max+15)/16*16, (num_atoms+15)/16*16);\n    sycl::range<2> lws_k5 (16, 16);\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class compute_yi>(\n        sycl::nd_range<2>(gws_k5, lws_k5), [=] (sycl::nd_item<2> item) {\n        int jjz = item.get_global_id(0);\n        int natom = item.get_global_id(1);\n        if (jjz < idxz_max && natom < num_atoms) {\n          const int j1 = d_idxz[IDXZ_INDEX(jjz, 0)];\n          const int j2 = d_idxz[IDXZ_INDEX(jjz, 1)];\n          const int j = d_idxz[IDXZ_INDEX(jjz, 2)];\n          const int ma1min = d_idxz[IDXZ_INDEX(jjz, 3)];\n          const int ma2max = d_idxz[IDXZ_INDEX(jjz, 4)];\n          const int na = d_idxz[IDXZ_INDEX(jjz, 5)];\n          const int mb1min = d_idxz[IDXZ_INDEX(jjz, 6)];\n          const int mb2max = d_idxz[IDXZ_INDEX(jjz, 7)];\n          const int nb = d_idxz[IDXZ_INDEX(jjz, 8)];\n\n          const double betaj = d_idxzbeta[jjz];\n\n          \n\n          const double* cgblock = d_cglist + d_idxcg_block[j1 + jdim*j2 + jdim*jdim*j];\n\n          int mb = (2 * (mb1min + mb2max) - j1 - j2 + j) / 2;\n          int ma = (2 * (ma1min + ma2max) - j1 - j2 + j) / 2;\n          const int jjdu = d_idxdu_block[j] + (j + 1) * mb + ma;\n\n          int jju1 = d_idxu_block[j1] + (j1 + 1) * mb1min;\n          int jju2 = d_idxu_block[j2] + (j2 + 1) * mb2max;\n          int icgb = mb1min * (j2 + 1) + mb2max;\n\n          double ztmp_r = 0.0;\n          double ztmp_i = 0.0;\n\n          \n\n          \n\n          \n\n\n          for (int ib = 0; ib < nb; ib++) {\n\n            double suma1_r = 0.0;\n            double suma1_i = 0.0;\n\n            int ma1 = ma1min;\n            int ma2 = ma2max;\n            int icga = ma1min * (j2 + 1) + ma2max;\n\n            \n\n            \n\n            \n\n\n            for (int ia = 0; ia < na; ia++) {\n              suma1_r += cgblock[icga] *\n                (d_ulisttot[INDEX_2D(natom, jju1 + ma1)].re * d_ulisttot[INDEX_2D(natom, jju2 + ma2)].re -\n                 d_ulisttot[INDEX_2D(natom, jju1 + ma1)].im * d_ulisttot[INDEX_2D(natom, jju2 + ma2)].im);\n\n              suma1_i += cgblock[icga] *\n                (d_ulisttot[INDEX_2D(natom, jju1 + ma1)].re * d_ulisttot[INDEX_2D(natom, jju2 + ma2)].im +\n                 d_ulisttot[INDEX_2D(natom, jju1 + ma1)].im * d_ulisttot[INDEX_2D(natom, jju2 + ma2)].re);\n\n              ma1++;\n              ma2--;\n              icga += j2;\n           } \n\n\n           ztmp_r += cgblock[icgb] * suma1_r;\n           ztmp_i += cgblock[icgb] * suma1_i;\n           jju1 += j1 + 1;\n           jju2 -= j2 + 1;\n           icgb += j2;\n          } \n\n\n            \n\n\n          atomicAdd(d_ylist[INDEX_2D(natom, jjdu)].re, betaj * ztmp_r);\n          atomicAdd(d_ylist[INDEX_2D(natom, jjdu)].im, betaj * ztmp_i);\n\n        } \n\n      });\n    });\n\n    q.wait();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_yi += elapsed.count();\n\n    \n\n    start = system_clock::now();\n\n    sycl::range<2> gws_k6 ((num_nbor+15)/16*16, (num_atoms+15)/16*16);\n    sycl::range<2> lws_k6 (16, 16);\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class compute_duidrj>(\n        sycl::nd_range<2>(gws_k6, lws_k6), [=] (sycl::nd_item<2> item) {\n\tint nbor = item.get_global_id(0);\n\tint natom = item.get_global_id(1);\n        if (natom < num_atoms && nbor < num_nbor) {\n          double wj_in = d_wj[INDEX_2D(natom, nbor)];\n          double rcut = d_rcutij[INDEX_2D(natom, nbor)];\n\n          double x = d_rij[ULIST_INDEX(natom, nbor, 0)];\n          double y = d_rij[ULIST_INDEX(natom, nbor, 1)];\n          double z = d_rij[ULIST_INDEX(natom, nbor, 2)];\n          double rsq = x * x + y * y + z * z;\n          double r = sycl::sqrt(rsq);\n          double rscale0 = rfac0 * MY_PI / (rcut - rmin0);\n          double theta0 = (r - rmin0) * rscale0;\n          double cs = sycl::cos(theta0);\n          double sn = sycl::sin(theta0);\n          double z0 = r * cs / sn;\n          double dz0dr = z0 / r - (r * rscale0) * (rsq + z0 * z0) / rsq;\n\n          compute_duarray(natom, nbor, num_atoms, num_nbor, twojmax,\n                          idxdu_max, jdimpq, switch_flag,\n                          x, y, z, z0, r, dz0dr, wj_in, rcut,\n                          d_rootpqarray,\n                          d_ulist,\n                          d_dulist);\n         }\n      });\n    });\n\n    q.wait();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_duidrj += elapsed.count();\n\n    start = system_clock::now();\n    \n\n    sycl::range<2> gws_k7 ((num_nbor+15)/16*16, (num_atoms+15)/16*16);\n    sycl::range<2> lws_k7 (16, 16);\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class compute_deidrj>(\n        sycl::nd_range<2>(gws_k7, lws_k7), [=] (sycl::nd_item<2> item) {\n\tint nbor = item.get_global_id(0);\n\tint natom = item.get_global_id(1);\n        if (natom < num_atoms && nbor < num_nbor) {\n          for (int k = 0; k < 3; k++)\n            d_dedr[ULIST_INDEX(natom, nbor, k)] = 0.0;\n\n          for (int j = 0; j <= twojmax; j++) {\n            int jjdu = d_idxdu_block[j];\n\n            for (int mb = 0; 2 * mb < j; mb++)\n              for (int ma = 0; ma <= j; ma++) {\n\n                double jjjmambyarray_r = d_ylist[INDEX_2D(natom, jjdu)].re;\n                double jjjmambyarray_i = d_ylist[INDEX_2D(natom, jjdu)].im;\n\n                for (int k = 0; k < 3; k++)\n                  d_dedr[ULIST_INDEX(natom, nbor, k)] +=\n                    d_dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n                    d_dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n                jjdu++;\n              } \n\n\n            \n\n\n            if (j % 2 == 0) {\n\n              int mb = j / 2;\n              for (int ma = 0; ma < mb; ma++) {\n                double jjjmambyarray_r = d_ylist[INDEX_2D(natom, jjdu)].re;\n                double jjjmambyarray_i = d_ylist[INDEX_2D(natom, jjdu)].im;\n\n                for (int k = 0; k < 3; k++)\n                  d_dedr[ULIST_INDEX(natom, nbor, k)] +=\n                    d_dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n                    d_dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n                jjdu++;\n              }\n\n              double jjjmambyarray_r = d_ylist[INDEX_2D(natom, jjdu)].re;\n              double jjjmambyarray_i = d_ylist[INDEX_2D(natom, jjdu)].im;\n\n              for (int k = 0; k < 3; k++)\n                d_dedr[ULIST_INDEX(natom, nbor, k)] +=\n                  (d_dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n                   d_dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i) *\n                  0.5;\n              jjdu++;\n\n            } \n\n\n          } \n\n\n          for (int k = 0; k < 3; k++)\n            d_dedr[ULIST_INDEX(natom, nbor, k)] *= 2.0;\n        }\n      });\n    });\n\n    q.wait();\n    end = system_clock::now();\n    elapsed = end - start;\n    elapsed_deidrj += elapsed.count();\n\n    q.memcpy(dedr, d_dedr, sizeof(double)*num_atoms*num_nbor*3).wait();\n\n    \n\n    \n\n    for (int natom = 0; natom < num_atoms; natom++) {\n      for (int nbor = 0; nbor < num_nbor; nbor++) {\n        int j = inside[INDEX_2D(natom, nbor)];\n        f[F_INDEX(natom, 0)] += dedr[ULIST_INDEX(natom, nbor, 0)];\n        f[F_INDEX(natom, 1)] += dedr[ULIST_INDEX(natom, nbor, 1)];\n        f[F_INDEX(natom, 2)] += dedr[ULIST_INDEX(natom, nbor, 2)];\n        f[F_INDEX(j, 0)] -= dedr[ULIST_INDEX(natom, nbor, 0)];\n        f[F_INDEX(j, 1)] -= dedr[ULIST_INDEX(natom, nbor, 1)];\n        f[F_INDEX(j, 2)] -= dedr[ULIST_INDEX(natom, nbor, 2)];\n\n      } \n\n    }   \n\n    \n\n    jt = 0;\n    for (int j = 0; j < ntotal; j++) {\n      double ferrx = f[F_INDEX(j, 0)] - refdata.fj[jt++];\n      double ferry = f[F_INDEX(j, 1)] - refdata.fj[jt++];\n      double ferrz = f[F_INDEX(j, 2)] - refdata.fj[jt++];\n      sumsqferr += ferrx * ferrx + ferry * ferry + ferrz * ferrz;\n    }\n  }\n  auto stop = myclock::now();\n  myduration elapsed = stop - begin;\n  double duration = elapsed.count();\n\n  printf(\"-----------------------\\n\");\n  printf(\"Summary of TestSNAP run\\n\");\n  printf(\"-----------------------\\n\");\n  printf(\"natoms = %d \\n\", nlocal);\n  printf(\"nghostatoms = %d \\n\", nghost);\n  printf(\"nsteps = %d \\n\", nsteps);\n  printf(\"nneighs = %d \\n\", ninside);\n  printf(\"twojmax = %d \\n\", twojmax);\n  printf(\"duration = %g [sec]\\n\", duration);\n\n  \n\n  double ktime = elapsed_ui + elapsed_yi + elapsed_duidrj + elapsed_deidrj;\n  printf(\"step time = %g [msec/step]\\n\", 1000.0 * duration / nsteps);\n  printf(\"\\n Individual kernel timings for each step\\n\");\n  printf(\"   compute_ui = %g [msec/step]\\n\", 1000.0 * elapsed_ui / nsteps);\n  printf(\"   compute_yi = %g [msec/step]\\n\", 1000.0 * elapsed_yi / nsteps);\n  printf(\"   compute_duidrj = %g [msec/step]\\n\", 1000.0 * elapsed_duidrj / nsteps);\n  printf(\"   compute_deidrj = %g [msec/step]\\n\", 1000.0 * elapsed_deidrj / nsteps);\n  printf(\"   Total kernel time = %g [msec/step]\\n\", 1000.0 * ktime / nsteps);\n  printf(\"   Percentage of step time = %g%%\\n\\n\", ktime / duration * 100.0);\n  printf(\"grind time = %g [msec/atom-step]\\n\", 1000.0 * duration / (nlocal * nsteps));\n  printf(\"RMS |Fj| deviation %g [eV/A]\\n\", sqrt(sumsqferr / (ntotal * nsteps)));\n\n  sycl::free(d_idxu_block, q);\n  sycl::free(d_ulist_parity, q);\n  sycl::free(d_rootpqarray, q);\n  sycl::free(d_idxz, q);\n  sycl::free(d_idxzbeta, q);\n  sycl::free(d_idxcg_block, q);\n  sycl::free(d_idxdu_block, q);\n  sycl::free(d_cglist, q);\n  sycl::free(d_dulist, q);\n  sycl::free(d_ulist, q);\n  sycl::free(d_dedr, q);\n  sycl::free(d_ulisttot, q);\n  sycl::free(d_ylist, q);\n  sycl::free(d_rij, q);\n  sycl::free(d_rcutij, q);\n  sycl::free(d_wj, q);\n\n  free(coeffi);\n  free(idxcg_block);\n  free(idxu_block);\n  free(ulist_parity);\n  free(idxdu_block);\n  free(idxb);\n  free(idxb_block);\n  free(idxz);\n  free(idxzbeta);\n  free(idxz_block);\n  free(rij);\n  free(inside);\n  free(wj);\n  free(rcutij);\n  free(rootpqarray);\n  free(cglist);\n  free(dedr);\n  free(ulist);\n  free(ylist);\n  free(ulisttot);\n  free(dulist);\n  free(f);\n\n  return 0;\n}\n"}}
{"kernel_name": "tissue", "parallel_api": "cuda", "code": {"main.cu": "\n\n \n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <cuda.h>\n\nvoid reference(\n    const   int *d_tisspoints,\n    const float *d_gtt,\n    const float *d_gbartt,\n          float *d_ct,\n    const float *d_ctprev,\n    const float *d_qt,\n    int nnt, int nntDev, int step, int isp)\n{\n  for (int i = 0; i < step * nnt; i++) {\n    int jtp,ixyz,ix,iy,iz,jx,jy,jz,istep;\n    int nnt2 = 2*nnt;\n    float p = 0.f;\n\n    int itp = i/step;\n    int itp1 = i%step;\n    if(itp < nnt) {\n      ix = d_tisspoints[itp];\n      iy = d_tisspoints[itp+nnt];\n      iz = d_tisspoints[itp+nnt2];\n      for(jtp=itp1; jtp<nnt; jtp+=step){\n        jx = d_tisspoints[jtp];\n        jy = d_tisspoints[jtp+nnt];\n        jz = d_tisspoints[jtp+nnt2];\n        ixyz = abs(jx-ix) + abs(jy-iy) + abs(jz-iz) + (isp-1)*nntDev;\n        p += d_gtt[ixyz]*d_ctprev[jtp] + d_gbartt[ixyz]*d_qt[jtp];\n      }\n      if(itp1 == 0) d_ct[itp] = p;\n    }\n\n    for(istep=1; istep<step; istep++)\n      if(itp1 == istep && itp < nnt) d_ct[itp] += p;\n  }\n}\n\n__global__ void tissue(\n    const   int *__restrict__ d_tisspoints,\n    const float *__restrict__ d_gtt,\n    const float *__restrict__ d_gbartt,\n          float *__restrict__ d_ct,\n    const float *__restrict__ d_ctprev,\n    const float *__restrict__ d_qt,\n    int nnt, int nntDev, int step, int isp)\n{\n  int jtp,ixyz,ix,iy,iz,jx,jy,jz,istep;\n  int nnt2 = 2*nnt;\n  float p = 0.f;\n\n  const int i = blockDim.x * blockIdx.x + threadIdx.x;\n  const int itp = i/step;\n  const int itp1 = i%step;\n  if(itp < nnt) {\n    ix = d_tisspoints[itp];\n    iy = d_tisspoints[itp+nnt];\n    iz = d_tisspoints[itp+nnt2];\n    for(jtp = itp1; jtp < nnt; jtp += step) {\n      jx = d_tisspoints[jtp];\n      jy = d_tisspoints[jtp+nnt];\n      jz = d_tisspoints[jtp+nnt2];\n      ixyz = abs(jx-ix) + abs(jy-iy) + abs(jz-iz) + (isp-1)*nntDev;\n      p += d_gtt[ixyz]*d_ctprev[jtp] + d_gbartt[ixyz]*d_qt[jtp];\n    }\n    if(itp1 == 0) d_ct[itp] = p;\n  }\n  \n\n  for(istep=1; istep<step; istep++)\n    if(itp1 == istep && itp < nnt) d_ct[itp] += p;\n}\n\nint main(int argc, char** argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <dimension of a 3D grid> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int dim = atoi(argv[1]);\n  if (dim > 32) {\n    printf(\"Maximum dimension is 32\\n\");\n    return 1;\n  }\n  const int repeat = atoi(argv[2]);\n\n  int *d_tisspoints;\n  float *d_gtt;\n  float *d_gbartt;\n  float *d_ct;\n  float *d_ctprev;\n  float *d_qt;\n  const int nnt = dim * dim * dim;\n  const int nntDev = 32*32*32;  \n\n  const int nsp = 2;\n\n    int* h_tisspoints = (int*) malloc (3*nntDev*sizeof(int));\n  float* h_gtt = (float*) malloc (nsp*nntDev*sizeof(float));\n  float* h_gbartt = (float*) malloc (nsp*nntDev*sizeof(float));\n  float* h_ct = (float*) malloc (nntDev*sizeof(float));\n  float* h_ctprev = (float*) malloc (nntDev*sizeof(float));\n  float* h_qt = (float*) malloc (nntDev*sizeof(float));\n  float* h_ct_gold = (float*) malloc (nntDev*sizeof(float));\n\n  \n\n  for (int i = 0; i < 3 * nntDev; i++) {\n    h_tisspoints[i] = rand() % (nntDev / 3);\n  }\n  for (int i = 0; i < nsp * nntDev; i++) {\n    h_gtt[i] = rand() / (float)RAND_MAX;\n    h_gbartt[i] = rand() / (float)RAND_MAX;\n  }\n  for (int i = 0; i < nntDev; i++) {\n    h_ct[i] = h_ct_gold[i] = 0;\n    h_ctprev[i] = rand() / (float)RAND_MAX;\n    h_qt[i] = rand() / (float)RAND_MAX;\n  }\n\n  cudaMalloc((void **)&d_tisspoints, 3*nntDev*sizeof(int));\n  cudaMalloc((void **)&d_gtt, nsp*nntDev*sizeof(float));\n  cudaMalloc((void **)&d_gbartt, nsp*nntDev*sizeof(float));\n  cudaMalloc((void **)&d_ct, nntDev*sizeof(float));\n  cudaMalloc((void **)&d_ctprev, nntDev*sizeof(float));\n  cudaMalloc((void **)&d_qt, nntDev*sizeof(float));\n\n  cudaMemcpy(d_tisspoints, h_tisspoints, 3*nntDev*sizeof(int), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_gtt, h_gtt, nsp*nntDev*sizeof(float), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_gbartt, h_gbartt, nsp*nntDev*sizeof(float), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_ct, h_ct, nntDev*sizeof(float), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_ctprev, h_ctprev, nntDev*sizeof(float), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_qt, h_qt, nntDev*sizeof(float), cudaMemcpyHostToDevice);\n\n  int threadsPerBlock = 256;\n  int step = 4; \n\n  int blocksPerGrid = (step*nnt + threadsPerBlock - 1) / threadsPerBlock;\n\n  \n\n  for (int i = 0; i < 2; i++) {\n    tissue<<<blocksPerGrid, threadsPerBlock>>>(\n        d_tisspoints,d_gtt,d_gbartt,d_ct,d_ctprev,d_qt,nnt,nntDev,step,1);\n\n    tissue<<<blocksPerGrid, threadsPerBlock>>>(\n        d_tisspoints,d_gtt,d_gbartt,d_ct,d_ctprev,d_qt,nnt,nntDev,step,2);\n  }\n\n  \n\n  for (int i = 0; i < 2; i++) {\n    reference(h_tisspoints,h_gtt,h_gbartt,h_ct_gold,h_ctprev,h_qt,nnt,nntDev,step,1);\n    reference(h_tisspoints,h_gtt,h_gbartt,h_ct_gold,h_ctprev,h_qt,nnt,nntDev,step,2);\n  }\n\n  bool ok = true;\n  cudaMemcpy(h_ct, d_ct, nntDev*sizeof(float), cudaMemcpyDeviceToHost);\n  for (int i = 0; i < nntDev; i++) {\n    if (fabsf(h_ct[i] - h_ct_gold[i]) > 1e-2f) {\n      printf(\"@%d: %f %f\\n\", i, h_ct[i], h_ct_gold[i]);\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  \n\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    tissue<<<blocksPerGrid, threadsPerBlock>>>(\n        d_tisspoints,d_gtt,d_gbartt,d_ct,d_ctprev,d_qt,nnt,nntDev,step,1);\n\n    tissue<<<blocksPerGrid, threadsPerBlock>>>(\n        d_tisspoints,d_gtt,d_gbartt,d_ct,d_ctprev,d_qt,nnt,nntDev,step,2);\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  free(h_tisspoints);\n  free(h_gtt);\n  free(h_gbartt);\n  free(h_ct);\n  free(h_ct_gold);\n  free(h_ctprev);\n  free(h_qt);\n  cudaFree(d_tisspoints);\n  cudaFree(d_gtt);\n  cudaFree(d_gbartt);\n  cudaFree(d_ct);\n  cudaFree(d_ctprev);\n  cudaFree(d_qt);\n\n  return 0;\n}\n"}}
{"kernel_name": "tissue", "parallel_api": "hip", "code": {"main.cu": "\n\n \n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n\nvoid reference(\n    const   int *d_tisspoints,\n    const float *d_gtt,\n    const float *d_gbartt,\n          float *d_ct,\n    const float *d_ctprev,\n    const float *d_qt,\n    int nnt, int nntDev, int step, int isp)\n{\n  for (int i = 0; i < step * nnt; i++) {\n    int jtp,ixyz,ix,iy,iz,jx,jy,jz,istep;\n    int nnt2 = 2*nnt;\n    float p = 0.f;\n\n    int itp = i/step;\n    int itp1 = i%step;\n    if(itp < nnt) {\n      ix = d_tisspoints[itp];\n      iy = d_tisspoints[itp+nnt];\n      iz = d_tisspoints[itp+nnt2];\n      for(jtp=itp1; jtp<nnt; jtp+=step){\n        jx = d_tisspoints[jtp];\n        jy = d_tisspoints[jtp+nnt];\n        jz = d_tisspoints[jtp+nnt2];\n        ixyz = abs(jx-ix) + abs(jy-iy) + abs(jz-iz) + (isp-1)*nntDev;\n        p += d_gtt[ixyz]*d_ctprev[jtp] + d_gbartt[ixyz]*d_qt[jtp];\n      }\n      if(itp1 == 0) d_ct[itp] = p;\n    }\n\n    for(istep=1; istep<step; istep++)\n      if(itp1 == istep && itp < nnt) d_ct[itp] += p;\n  }\n}\n\n__global__ void tissue(\n    const   int *__restrict__ d_tisspoints,\n    const float *__restrict__ d_gtt,\n    const float *__restrict__ d_gbartt,\n          float *__restrict__ d_ct,\n    const float *__restrict__ d_ctprev,\n    const float *__restrict__ d_qt,\n    int nnt, int nntDev, int step, int isp)\n{\n  int jtp,ixyz,ix,iy,iz,jx,jy,jz,istep;\n  int nnt2 = 2*nnt;\n  float p = 0.f;\n\n  const int i = blockDim.x * blockIdx.x + threadIdx.x;\n  const int itp = i/step;\n  const int itp1 = i%step;\n  if(itp < nnt) {\n    ix = d_tisspoints[itp];\n    iy = d_tisspoints[itp+nnt];\n    iz = d_tisspoints[itp+nnt2];\n    for(jtp = itp1; jtp < nnt; jtp += step) {\n      jx = d_tisspoints[jtp];\n      jy = d_tisspoints[jtp+nnt];\n      jz = d_tisspoints[jtp+nnt2];\n      ixyz = abs(jx-ix) + abs(jy-iy) + abs(jz-iz) + (isp-1)*nntDev;\n      p += d_gtt[ixyz]*d_ctprev[jtp] + d_gbartt[ixyz]*d_qt[jtp];\n    }\n    if(itp1 == 0) d_ct[itp] = p;\n  }\n  \n\n  for(istep=1; istep<step; istep++)\n    if(itp1 == istep && itp < nnt) d_ct[itp] += p;\n}\n\nint main(int argc, char** argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <dimension of a 3D grid> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int dim = atoi(argv[1]);\n  if (dim > 32) {\n    printf(\"Maximum dimension is 32\\n\");\n    return 1;\n  }\n  const int repeat = atoi(argv[2]);\n\n  int *d_tisspoints;\n  float *d_gtt;\n  float *d_gbartt;\n  float *d_ct;\n  float *d_ctprev;\n  float *d_qt;\n  const int nnt = dim * dim * dim;\n  const int nntDev = 32*32*32;  \n\n  const int nsp = 2;\n\n    int* h_tisspoints = (int*) malloc (3*nntDev*sizeof(int));\n  float* h_gtt = (float*) malloc (nsp*nntDev*sizeof(float));\n  float* h_gbartt = (float*) malloc (nsp*nntDev*sizeof(float));\n  float* h_ct = (float*) malloc (nntDev*sizeof(float));\n  float* h_ctprev = (float*) malloc (nntDev*sizeof(float));\n  float* h_qt = (float*) malloc (nntDev*sizeof(float));\n  float* h_ct_gold = (float*) malloc (nntDev*sizeof(float));\n\n  \n\n  for (int i = 0; i < 3 * nntDev; i++) {\n    h_tisspoints[i] = rand() % (nntDev / 3);\n  }\n  for (int i = 0; i < nsp * nntDev; i++) {\n    h_gtt[i] = rand() / (float)RAND_MAX;\n    h_gbartt[i] = rand() / (float)RAND_MAX;\n  }\n  for (int i = 0; i < nntDev; i++) {\n    h_ct[i] = h_ct_gold[i] = 0;\n    h_ctprev[i] = rand() / (float)RAND_MAX;\n    h_qt[i] = rand() / (float)RAND_MAX;\n  }\n\n  hipMalloc((void **)&d_tisspoints, 3*nntDev*sizeof(int));\n  hipMalloc((void **)&d_gtt, nsp*nntDev*sizeof(float));\n  hipMalloc((void **)&d_gbartt, nsp*nntDev*sizeof(float));\n  hipMalloc((void **)&d_ct, nntDev*sizeof(float));\n  hipMalloc((void **)&d_ctprev, nntDev*sizeof(float));\n  hipMalloc((void **)&d_qt, nntDev*sizeof(float));\n\n  hipMemcpy(d_tisspoints, h_tisspoints, 3*nntDev*sizeof(int), hipMemcpyHostToDevice);\n  hipMemcpy(d_gtt, h_gtt, nsp*nntDev*sizeof(float), hipMemcpyHostToDevice);\n  hipMemcpy(d_gbartt, h_gbartt, nsp*nntDev*sizeof(float), hipMemcpyHostToDevice);\n  hipMemcpy(d_ct, h_ct, nntDev*sizeof(float), hipMemcpyHostToDevice);\n  hipMemcpy(d_ctprev, h_ctprev, nntDev*sizeof(float), hipMemcpyHostToDevice);\n  hipMemcpy(d_qt, h_qt, nntDev*sizeof(float), hipMemcpyHostToDevice);\n\n  int threadsPerBlock = 256;\n  int step = 4; \n\n  int blocksPerGrid = (step*nnt + threadsPerBlock - 1) / threadsPerBlock;\n\n  \n\n  for (int i = 0; i < 2; i++) {\n    hipLaunchKernelGGL(tissue, blocksPerGrid, threadsPerBlock, 0, 0, \n        d_tisspoints,d_gtt,d_gbartt,d_ct,d_ctprev,d_qt,nnt,nntDev,step,1);\n\n    hipLaunchKernelGGL(tissue, blocksPerGrid, threadsPerBlock, 0, 0, \n        d_tisspoints,d_gtt,d_gbartt,d_ct,d_ctprev,d_qt,nnt,nntDev,step,2);\n  }\n\n  \n\n  for (int i = 0; i < 2; i++) {\n    reference(h_tisspoints,h_gtt,h_gbartt,h_ct_gold,h_ctprev,h_qt,nnt,nntDev,step,1);\n    reference(h_tisspoints,h_gtt,h_gbartt,h_ct_gold,h_ctprev,h_qt,nnt,nntDev,step,2);\n  }\n\n  bool ok = true;\n  hipMemcpy(h_ct, d_ct, nntDev*sizeof(float), hipMemcpyDeviceToHost);\n  for (int i = 0; i < nntDev; i++) {\n    if (fabsf(h_ct[i] - h_ct_gold[i]) > 1e-2f) {\n      printf(\"@%d: %f %f\\n\", i, h_ct[i], h_ct_gold[i]);\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  \n\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    hipLaunchKernelGGL(tissue, blocksPerGrid, threadsPerBlock, 0, 0, \n        d_tisspoints,d_gtt,d_gbartt,d_ct,d_ctprev,d_qt,nnt,nntDev,step,1);\n\n    hipLaunchKernelGGL(tissue, blocksPerGrid, threadsPerBlock, 0, 0, \n        d_tisspoints,d_gtt,d_gbartt,d_ct,d_ctprev,d_qt,nnt,nntDev,step,2);\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  free(h_tisspoints);\n  free(h_gtt);\n  free(h_gbartt);\n  free(h_ct);\n  free(h_ct_gold);\n  free(h_ctprev);\n  free(h_qt);\n  hipFree(d_tisspoints);\n  hipFree(d_gtt);\n  hipFree(d_gbartt);\n  hipFree(d_ct);\n  hipFree(d_ctprev);\n  hipFree(d_qt);\n\n  return 0;\n}\n"}}
{"kernel_name": "tissue", "parallel_api": "omp", "code": {"main.cpp": "\n\n \n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <omp.h>\n\nvoid reference(\n    const   int *__restrict d_tisspoints,\n    const float *__restrict d_gtt,\n    const float *__restrict d_gbartt,\n          float *__restrict d_ct,\n    const float *__restrict d_ctprev,\n    const float *__restrict d_qt,\n    int nnt, int nntDev, int step, int isp)\n{\n  for (int i = 0; i < step * nnt; i++) {\n    int jtp,ixyz,ix,iy,iz,jx,jy,jz,istep;\n    int nnt2 = 2*nnt;\n    float p = 0.f;\n\n    int itp = i/step;\n    int itp1 = i%step;\n    if(itp < nnt) {\n      ix = d_tisspoints[itp];\n      iy = d_tisspoints[itp+nnt];\n      iz = d_tisspoints[itp+nnt2];\n      for(jtp=itp1; jtp<nnt; jtp+=step){\n        jx = d_tisspoints[jtp];\n        jy = d_tisspoints[jtp+nnt];\n        jz = d_tisspoints[jtp+nnt2];\n        ixyz = abs(jx-ix) + abs(jy-iy) + abs(jz-iz) + (isp-1)*nntDev;\n        p += d_gtt[ixyz]*d_ctprev[jtp] + d_gbartt[ixyz]*d_qt[jtp];\n      }\n      if(itp1 == 0) d_ct[itp] = p;\n    }\n\n    for(istep=1; istep<step; istep++)\n      if(itp1 == istep && itp < nnt) d_ct[itp] += p;\n  }\n}\n\nvoid tissue(\n    const   int *__restrict d_tisspoints,\n    const float *__restrict d_gtt,\n    const float *__restrict d_gbartt,\n          float *__restrict d_ct,\n    const float *__restrict d_ctprev,\n    const float *__restrict d_qt,\n    int nnt, int nntDev, int step, int isp)\n{\n  #pragma omp target teams distribute parallel for thread_limit(256)\n  for (int i = 0; i < step * nnt; i++) {\n    int jtp,ixyz,ix,iy,iz,jx,jy,jz,istep;\n    int nnt2 = 2*nnt;\n    float p = 0.f;\n\n    const int itp = i/step;\n    const int itp1 = i%step;\n    if(itp < nnt) {\n      ix = d_tisspoints[itp];\n      iy = d_tisspoints[itp+nnt];\n      iz = d_tisspoints[itp+nnt2];\n      for(jtp = itp1; jtp < nnt; jtp += step) {\n        jx = d_tisspoints[jtp];\n        jy = d_tisspoints[jtp+nnt];\n        jz = d_tisspoints[jtp+nnt2];\n        ixyz = abs(jx-ix) + abs(jy-iy) + abs(jz-iz) + (isp-1)*nntDev;\n        p += d_gtt[ixyz]*d_ctprev[jtp] + d_gbartt[ixyz]*d_qt[jtp];\n      }\n      if(itp1 == 0) d_ct[itp] = p;\n    }\n    \n\n    for(istep=1; istep<step; istep++)\n      if(itp1 == istep && itp < nnt) d_ct[itp] += p;\n  }\n}\n\nint main(int argc, char** argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <dimension of a 3D grid> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int dim = atoi(argv[1]);\n  if (dim > 32) {\n    printf(\"Maximum dimension is 32\\n\");\n    return 1;\n  }\n  const int repeat = atoi(argv[2]);\n\n  const int nnt = dim * dim * dim;\n  const int nntDev = 32*32*32;  \n\n  const int nsp = 2;\n\n    int* h_tisspoints = (int*) malloc (3*nntDev*sizeof(int));\n  float* h_gtt = (float*) malloc (nsp*nntDev*sizeof(float));\n  float* h_gbartt = (float*) malloc (nsp*nntDev*sizeof(float));\n  float* h_ct = (float*) malloc (nntDev*sizeof(float));\n  float* h_ctprev = (float*) malloc (nntDev*sizeof(float));\n  float* h_qt = (float*) malloc (nntDev*sizeof(float));\n  float* h_ct_gold = (float*) malloc (nntDev*sizeof(float));\n\n  \n\n  for (int i = 0; i < 3 * nntDev; i++) {\n    h_tisspoints[i] = rand() % (nntDev / 3);\n  }\n  for (int i = 0; i < nsp * nntDev; i++) {\n    h_gtt[i] = rand() / (float)RAND_MAX;\n    h_gbartt[i] = rand() / (float)RAND_MAX;\n  }\n  for (int i = 0; i < nntDev; i++) {\n    h_ct[i] = h_ct_gold[i] = 0;\n    h_ctprev[i] = rand() / (float)RAND_MAX;\n    h_qt[i] = rand() / (float)RAND_MAX;\n  }\n\n  int step = 4; \n\n\n  #pragma omp target data map (to: h_tisspoints[0:3*nntDev],\\\n                                   h_gtt[0:nsp*nntDev],\\\n                                   h_gbartt[0:nsp*nntDev],\\\n                                   h_ctprev[0:nntDev],\\\n                                   h_qt[0:nntDev], \\\n                                   h_ct[0:nntDev])\n  {\n    \n\n    for (int i = 0; i < 2; i++) {\n      tissue(h_tisspoints,h_gtt,h_gbartt,h_ct,h_ctprev,h_qt,nnt,nntDev,step,1);\n      tissue(h_tisspoints,h_gtt,h_gbartt,h_ct,h_ctprev,h_qt,nnt,nntDev,step,2);\n    }\n\n    \n\n    for (int i = 0; i < 2; i++) {\n      reference(h_tisspoints,h_gtt,h_gbartt,h_ct_gold,h_ctprev,h_qt,nnt,nntDev,step,1);\n      reference(h_tisspoints,h_gtt,h_gbartt,h_ct_gold,h_ctprev,h_qt,nnt,nntDev,step,2);\n    }\n\n    bool ok = true;\n    #pragma omp target update from (h_ct[0:nntDev])\n    for (int i = 0; i < nntDev; i++) {\n      if (fabsf(h_ct[i] - h_ct_gold[i]) > 1e-2) {\n        printf(\"@%d: %f %f\\n\", i, h_ct[i], h_ct_gold[i]);\n        ok = false;\n        break;\n      }\n    }\n\n    printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n    \n\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      tissue(h_tisspoints,h_gtt,h_gbartt,h_ct,h_ctprev,h_qt,nnt,nntDev,step,1);\n      tissue(h_tisspoints,h_gtt,h_gbartt,h_ct,h_ctprev,h_qt,nnt,nntDev,step,2);\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n  }\n\n  free(h_tisspoints);\n  free(h_gtt);\n  free(h_gbartt);\n  free(h_ct);\n  free(h_ct_gold);\n  free(h_ctprev);\n  free(h_qt);\n\n  return 0;\n}\n"}}
{"kernel_name": "tissue", "parallel_api": "serial", "code": {"main.cpp": "\n\n \n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n\nvoid reference(\n    const   int *__restrict d_tisspoints,\n    const float *__restrict d_gtt,\n    const float *__restrict d_gbartt,\n          float *__restrict d_ct,\n    const float *__restrict d_ctprev,\n    const float *__restrict d_qt,\n    int nnt, int nntDev, int step, int isp)\n{\n  for (int i = 0; i < step * nnt; i++) {\n    int jtp,ixyz,ix,iy,iz,jx,jy,jz,istep;\n    int nnt2 = 2*nnt;\n    float p = 0.f;\n\n    int itp = i/step;\n    int itp1 = i%step;\n    if(itp < nnt) {\n      ix = d_tisspoints[itp];\n      iy = d_tisspoints[itp+nnt];\n      iz = d_tisspoints[itp+nnt2];\n      for(jtp=itp1; jtp<nnt; jtp+=step){\n        jx = d_tisspoints[jtp];\n        jy = d_tisspoints[jtp+nnt];\n        jz = d_tisspoints[jtp+nnt2];\n        ixyz = abs(jx-ix) + abs(jy-iy) + abs(jz-iz) + (isp-1)*nntDev;\n        p += d_gtt[ixyz]*d_ctprev[jtp] + d_gbartt[ixyz]*d_qt[jtp];\n      }\n      if(itp1 == 0) d_ct[itp] = p;\n    }\n\n    for(istep=1; istep<step; istep++)\n      if(itp1 == istep && itp < nnt) d_ct[itp] += p;\n  }\n}\n\nvoid tissue(\n    const   int *__restrict d_tisspoints,\n    const float *__restrict d_gtt,\n    const float *__restrict d_gbartt,\n          float *__restrict d_ct,\n    const float *__restrict d_ctprev,\n    const float *__restrict d_qt,\n    int nnt, int nntDev, int step, int isp)\n{\n    for (int i = 0; i < step * nnt; i++) {\n    int jtp,ixyz,ix,iy,iz,jx,jy,jz,istep;\n    int nnt2 = 2*nnt;\n    float p = 0.f;\n\n    const int itp = i/step;\n    const int itp1 = i%step;\n    if(itp < nnt) {\n      ix = d_tisspoints[itp];\n      iy = d_tisspoints[itp+nnt];\n      iz = d_tisspoints[itp+nnt2];\n      for(jtp = itp1; jtp < nnt; jtp += step) {\n        jx = d_tisspoints[jtp];\n        jy = d_tisspoints[jtp+nnt];\n        jz = d_tisspoints[jtp+nnt2];\n        ixyz = abs(jx-ix) + abs(jy-iy) + abs(jz-iz) + (isp-1)*nntDev;\n        p += d_gtt[ixyz]*d_ctprev[jtp] + d_gbartt[ixyz]*d_qt[jtp];\n      }\n      if(itp1 == 0) d_ct[itp] = p;\n    }\n    \n\n    for(istep=1; istep<step; istep++)\n      if(itp1 == istep && itp < nnt) d_ct[itp] += p;\n  }\n}\n\nint main(int argc, char** argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <dimension of a 3D grid> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int dim = atoi(argv[1]);\n  if (dim > 32) {\n    printf(\"Maximum dimension is 32\\n\");\n    return 1;\n  }\n  const int repeat = atoi(argv[2]);\n\n  const int nnt = dim * dim * dim;\n  const int nntDev = 32*32*32;  \n\n  const int nsp = 2;\n\n    int* h_tisspoints = (int*) malloc (3*nntDev*sizeof(int));\n  float* h_gtt = (float*) malloc (nsp*nntDev*sizeof(float));\n  float* h_gbartt = (float*) malloc (nsp*nntDev*sizeof(float));\n  float* h_ct = (float*) malloc (nntDev*sizeof(float));\n  float* h_ctprev = (float*) malloc (nntDev*sizeof(float));\n  float* h_qt = (float*) malloc (nntDev*sizeof(float));\n  float* h_ct_gold = (float*) malloc (nntDev*sizeof(float));\n\n  \n\n  for (int i = 0; i < 3 * nntDev; i++) {\n    h_tisspoints[i] = rand() % (nntDev / 3);\n  }\n  for (int i = 0; i < nsp * nntDev; i++) {\n    h_gtt[i] = rand() / (float)RAND_MAX;\n    h_gbartt[i] = rand() / (float)RAND_MAX;\n  }\n  for (int i = 0; i < nntDev; i++) {\n    h_ct[i] = h_ct_gold[i] = 0;\n    h_ctprev[i] = rand() / (float)RAND_MAX;\n    h_qt[i] = rand() / (float)RAND_MAX;\n  }\n\n  int step = 4; \n\n\n    {\n    \n\n    for (int i = 0; i < 2; i++) {\n      tissue(h_tisspoints,h_gtt,h_gbartt,h_ct,h_ctprev,h_qt,nnt,nntDev,step,1);\n      tissue(h_tisspoints,h_gtt,h_gbartt,h_ct,h_ctprev,h_qt,nnt,nntDev,step,2);\n    }\n\n    \n\n    for (int i = 0; i < 2; i++) {\n      reference(h_tisspoints,h_gtt,h_gbartt,h_ct_gold,h_ctprev,h_qt,nnt,nntDev,step,1);\n      reference(h_tisspoints,h_gtt,h_gbartt,h_ct_gold,h_ctprev,h_qt,nnt,nntDev,step,2);\n    }\n\n    bool ok = true;\n        for (int i = 0; i < nntDev; i++) {\n      if (fabsf(h_ct[i] - h_ct_gold[i]) > 1e-2) {\n        printf(\"@%d: %f %f\\n\", i, h_ct[i], h_ct_gold[i]);\n        ok = false;\n        break;\n      }\n    }\n\n    printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n    \n\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      tissue(h_tisspoints,h_gtt,h_gbartt,h_ct,h_ctprev,h_qt,nnt,nntDev,step,1);\n      tissue(h_tisspoints,h_gtt,h_gbartt,h_ct,h_ctprev,h_qt,nnt,nntDev,step,2);\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n  }\n\n  free(h_tisspoints);\n  free(h_gtt);\n  free(h_gbartt);\n  free(h_ct);\n  free(h_ct_gold);\n  free(h_ctprev);\n  free(h_qt);\n\n  return 0;\n}"}}
{"kernel_name": "tissue", "parallel_api": "sycl", "code": {"main.cpp": "\n\n \n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n\nvoid reference(\n    const   int *d_tisspoints,\n    const float *d_gtt,\n    const float *d_gbartt,\n          float *d_ct,\n    const float *d_ctprev,\n    const float *d_qt,\n    int nnt, int nntDev, int step, int isp)\n{\n  for (int i = 0; i < step * nnt; i++) {\n    int jtp,ixyz,ix,iy,iz,jx,jy,jz,istep;\n    int nnt2 = 2*nnt;\n    float p = 0.f;\n\n    int itp = i/step;\n    int itp1 = i%step;\n    if(itp < nnt) {\n      ix = d_tisspoints[itp];\n      iy = d_tisspoints[itp+nnt];\n      iz = d_tisspoints[itp+nnt2];\n      for(jtp=itp1; jtp<nnt; jtp+=step){\n        jx = d_tisspoints[jtp];\n        jy = d_tisspoints[jtp+nnt];\n        jz = d_tisspoints[jtp+nnt2];\n        ixyz = abs(jx-ix) + abs(jy-iy) + abs(jz-iz) + (isp-1)*nntDev;\n        p += d_gtt[ixyz]*d_ctprev[jtp] + d_gbartt[ixyz]*d_qt[jtp];\n      }\n      if(itp1 == 0) d_ct[itp] = p;\n    }\n\n    for(istep=1; istep<step; istep++)\n      if(itp1 == istep && itp < nnt) d_ct[itp] += p;\n  }\n}\n\nvoid tissue(\n    sycl::nd_item<1> &item,\n    const   int *__restrict d_tisspoints,\n    const float *__restrict d_gtt,\n    const float *__restrict d_gbartt,\n          float *__restrict d_ct,\n    const float *__restrict d_ctprev,\n    const float *__restrict d_qt,\n    int nnt, int nntDev, int step, int isp)\n{\n  int jtp,ixyz,ix,iy,iz,jx,jy,jz,istep;\n  int nnt2 = 2*nnt;\n  float p = 0.f;\n\n  const int i = item.get_global_id(0);\n  const int itp = i/step;\n  const int itp1 = i%step;\n  if(itp < nnt) {\n    ix = d_tisspoints[itp];\n    iy = d_tisspoints[itp+nnt];\n    iz = d_tisspoints[itp+nnt2];\n    for(jtp = itp1; jtp < nnt; jtp += step) {\n      jx = d_tisspoints[jtp];\n      jy = d_tisspoints[jtp+nnt];\n      jz = d_tisspoints[jtp+nnt2];\n      ixyz = sycl::abs(jx-ix) + sycl::abs(jy-iy) + sycl::abs(jz-iz) + (isp-1)*nntDev;\n      p += d_gtt[ixyz]*d_ctprev[jtp] + d_gbartt[ixyz]*d_qt[jtp];\n    }\n    if(itp1 == 0) d_ct[itp] = p;\n  }\n  \n\n  for(istep=1; istep<step; istep++)\n    if(itp1 == istep && itp < nnt) d_ct[itp] += p;\n}\n\nint main(int argc, char** argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <dimension of a 3D grid> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int dim = atoi(argv[1]);\n  if (dim > 32) {\n    printf(\"Maximum dimension is 32\\n\");\n    return 1;\n  }\n  const int repeat = atoi(argv[2]);\n\n  const int nnt = dim * dim * dim;\n  const int nntDev = 32*32*32;  \n\n  const int nsp = 2;\n\n    int* h_tisspoints = (int*) malloc (3*nntDev*sizeof(int));\n  float* h_gtt = (float*) malloc (nsp*nntDev*sizeof(float));\n  float* h_gbartt = (float*) malloc (nsp*nntDev*sizeof(float));\n  float* h_ct = (float*) malloc (nntDev*sizeof(float));\n  float* h_ctprev = (float*) malloc (nntDev*sizeof(float));\n  float* h_qt = (float*) malloc (nntDev*sizeof(float));\n  float* h_ct_gold = (float*) malloc (nntDev*sizeof(float));\n\n  \n\n  for (int i = 0; i < 3 * nntDev; i++) {\n    h_tisspoints[i] = rand() % (nntDev / 3);\n  }\n  for (int i = 0; i < nsp * nntDev; i++) {\n    h_gtt[i] = rand() / (float)RAND_MAX;\n    h_gbartt[i] = rand() / (float)RAND_MAX;\n  }\n  for (int i = 0; i < nntDev; i++) {\n    h_ct[i] = h_ct_gold[i] = 0;\n    h_ctprev[i] = rand() / (float)RAND_MAX;\n    h_qt[i] = rand() / (float)RAND_MAX;\n  }\n\n  int step = 4; \n\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n    int *d_tisspoints = sycl::malloc_device<int>(3*nntDev, q);\n  float *d_gtt = sycl::malloc_device<float>(nsp*nntDev, q);\n  float *d_gbartt = sycl::malloc_device<float>(nsp*nntDev, q);\n  float *d_ct = sycl::malloc_device<float>(nntDev, q);\n  float *d_ctprev = sycl::malloc_device<float>(nntDev, q);\n  float *d_qt = sycl::malloc_device<float>(nntDev, q);\n\n  q.memcpy(d_tisspoints, h_tisspoints, 3*nntDev*sizeof(int));\n  q.memcpy(d_gtt, h_gtt, nsp*nntDev*sizeof(float));\n  q.memcpy(d_gbartt, h_gbartt, nsp*nntDev*sizeof(float));\n  q.memcpy(d_ct, h_ct, nntDev*sizeof(float));\n  q.memcpy(d_ctprev, h_ctprev, nntDev*sizeof(float));\n  q.memcpy(d_qt, h_qt, nntDev*sizeof(float));\n\n  sycl::range<1> lws (256);\n  sycl::range<1> gws ((step*nnt + 255) / 256 * 256);\n\n  \n\n  for (int i = 0; i < 2; i++) {\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class warmup>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        tissue(item, d_tisspoints, d_gtt, d_gbartt, d_ct, d_ctprev, d_qt,\n               nnt, nntDev, step, 1);\n      });\n    });\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class warmup2>(sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        tissue(item, d_tisspoints, d_gtt, d_gbartt, d_ct, d_ctprev, d_qt,\n               nnt, nntDev, step, 2);\n      });\n    });\n  }\n\n  for (int i = 0; i < 2; i++) {\n    reference(h_tisspoints,h_gtt,h_gbartt,h_ct_gold,h_ctprev,h_qt,nnt,nntDev,step,1);\n    reference(h_tisspoints,h_gtt,h_gbartt,h_ct_gold,h_ctprev,h_qt,nnt,nntDev,step,2);\n  }\n\n  bool ok = true;\n  q.memcpy(h_ct, d_ct, nntDev*sizeof(float)).wait();\n\n  for (int i = 0; i < nntDev; i++) {\n    if (fabsf(h_ct[i] - h_ct_gold[i]) > 1e-2) {\n      printf(\"@%d: %f %f\\n\", i, h_ct[i], h_ct_gold[i]);\n      ok = false;\n      break;\n    }\n  }\n\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  \n\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class timing>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        tissue(item, d_tisspoints, d_gtt, d_gbartt, d_ct, d_ctprev, d_qt,\n               nnt, nntDev, step, 1);\n      });\n    });\n\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class timing2>(\n        sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n        tissue(item, d_tisspoints, d_gtt, d_gbartt, d_ct, d_ctprev, d_qt,\n               nnt, nntDev, step, 2);\n      });\n    });\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  free(h_tisspoints);\n  free(h_gtt);\n  free(h_gbartt);\n  free(h_ct);\n  free(h_ct_gold);\n  free(h_ctprev);\n  free(h_qt);\n  sycl::free(d_tisspoints, q);\n  sycl::free(d_gtt, q);\n  sycl::free(d_gbartt, q);\n  sycl::free(d_ct, q);\n  sycl::free(d_ctprev, q);\n  sycl::free(d_qt, q);\n\n  return 0;\n}\n"}}
{"kernel_name": "tsa", "parallel_api": "cuda", "code": {"main.cu": "#include <complex>\n#include <cmath>\n#include <cstdio>\n#include <cstring>\n#include <chrono>\n#include <cuda.h>\n#include \"kernels.h\"\n#include \"reference.h\"\n\ntemplate <typename T>\nvoid init_p(T *p_real, T *p_imag, int width, int height) {\n  double s = 64.0;\n  for (int j = 1; j <= height; j++) {\n    for (int i = 1; i <= width; i++) {\n      \n\n      std::complex<T> tmp = std::complex<T>(\n        exp(-(pow(i - 180.0, 2.0) + pow(j - 300.0, 2.0)) / (2.0 * pow(s, 2.0))), 0.0) *\n        exp(std::complex<T>(0.0, 0.4 * (i + j - 480.0)));\n\n      p_real[(j-1) * width + i-1] = real(tmp);\n      p_imag[(j-1) * width + i-1] = imag(tmp);\n    }\n  }\n}\n\ntemplate <typename T>\nvoid tsa(int width, int height, int repeat) {\n\n  T * p_real = new T[width * height];\n  T * p_imag = new T[width * height];\n  T * h_real = new T[width * height];\n  T * h_imag = new T[width * height];\n\n  \n\n  init_p(p_real, p_imag, width, height);\n\n  \n\n  T a = cos(0.02);\n  T b = sin(0.02);\n\n  \n\n  memcpy(h_imag, p_imag, sizeof(T)*width*height);\n  memcpy(h_real, p_real, sizeof(T)*width*height);\n  reference(h_real, h_imag, a, b, width, height, repeat);\n\n  \n\n  const int BLOCK_X = 16;\n  \n\n  const int BLOCK_Y = sizeof(T) == 8 ? 32 : 96;\n  \n\n  const int STRIDE_Y = 16;\n\n  \n\n  const int MARGIN_X = 3;\n  const int MARGIN_Y = 4;\n\n  \n\n  const int STEPS = 1;\n\n  dim3 grids ((width + (BLOCK_X - 2 * STEPS * MARGIN_X) - 1) / (BLOCK_X - 2 * STEPS * MARGIN_X),\n              (height + (BLOCK_Y - 2 * STEPS * MARGIN_Y) - 1) / (BLOCK_Y - 2 * STEPS * MARGIN_Y));\n  dim3 blocks (BLOCK_X, STRIDE_Y);\n  int sense = 0;\n\n  \n\n  T *d_real[2];\n  T *d_imag[2];\n\n  cudaMalloc((void**)(&d_real[0]), width * height * sizeof(T));\n  cudaMalloc((void**)(&d_real[1]), width * height * sizeof(T));\n  cudaMalloc((void**)(&d_imag[0]), width * height * sizeof(T));\n  cudaMalloc((void**)(&d_imag[1]), width * height * sizeof(T));\n  cudaMemcpy(d_real[0], p_real, width * height * sizeof(T), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_imag[0], p_imag, width * height * sizeof(T), cudaMemcpyHostToDevice);\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    kernel<T, STEPS, BLOCK_X, BLOCK_Y, MARGIN_X, MARGIN_Y, STRIDE_Y>\n          <<<grids, blocks>>>(a, b, width, height,\n           d_real[sense], d_imag[sense], d_real[1-sense], d_imag[1-sense]);\n    sense = 1 - sense; \n\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  cudaMemcpy(p_real, d_real[sense], width * height * sizeof(T), cudaMemcpyDeviceToHost);\n  cudaMemcpy(p_imag, d_imag[sense], width * height * sizeof(T), cudaMemcpyDeviceToHost);\n\n  \n\n  bool ok = true;\n  for (int i = 0; i < width * height; i++) {\n    if (fabs(p_real[i] - h_real[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n    if (fabs(p_imag[i] - h_imag[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  delete[] p_real;\n  delete[] p_imag;\n  delete[] h_real;\n  delete[] h_imag;\n  cudaFree(d_real[0]);\n  cudaFree(d_real[1]);\n  cudaFree(d_imag[0]);\n  cudaFree(d_imag[1]);\n}\n\nint main(int argc, char** argv) {\n  if (argc != 4) {\n    printf(\"Usage: %s <matrix width> <matrix height> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  int width = atoi(argv[1]);   \n\n  int height = atoi(argv[2]);  \n\n  int repeat = atoi(argv[3]);  \n\n\n  printf(\"TSA in float32\\n\");\n  tsa<float>(width, height, repeat);\n\n  printf(\"\\n\");\n\n  printf(\"TSA in float64\\n\");\n  tsa<double>(width, height, repeat);\n  return 0;\n}\n"}}
{"kernel_name": "tsa", "parallel_api": "hip", "code": {"main.cu": "#include <complex>\n#include <cmath>\n#include <cstdio>\n#include <cstring>\n#include <chrono>\n#include <hip/hip_runtime.h>\n#include \"kernels.h\"\n#include \"reference.h\"\n\ntemplate <typename T>\nvoid init_p(T *p_real, T *p_imag, int width, int height) {\n  double s = 64.0;\n  for (int j = 1; j <= height; j++) {\n    for (int i = 1; i <= width; i++) {\n      \n\n      std::complex<T> tmp = std::complex<T>(\n        exp(-(pow(i - 180.0, 2.0) + pow(j - 300.0, 2.0)) / (2.0 * pow(s, 2.0))), 0.0) *\n        exp(std::complex<T>(0.0, 0.4 * (i + j - 480.0)));\n\n      p_real[(j-1) * width + i-1] = real(tmp);\n      p_imag[(j-1) * width + i-1] = imag(tmp);\n    }\n  }\n}\n\ntemplate <typename T>\nvoid tsa(int width, int height, int repeat) {\n\n  T * p_real = new T[width * height];\n  T * p_imag = new T[width * height];\n  T * h_real = new T[width * height];\n  T * h_imag = new T[width * height];\n\n  \n\n  init_p(p_real, p_imag, width, height);\n\n  \n\n  T a = cos(0.02);\n  T b = sin(0.02);\n\n  \n\n  memcpy(h_imag, p_imag, sizeof(T)*width*height);\n  memcpy(h_real, p_real, sizeof(T)*width*height);\n  reference(h_real, h_imag, a, b, width, height, repeat);\n\n  \n\n  const int BLOCK_X = 16;\n  \n\n  const int BLOCK_Y = sizeof(T) == 8 ? 32 : 96;\n  \n\n  const int STRIDE_Y = 16;\n\n  \n\n  const int MARGIN_X = 3;\n  const int MARGIN_Y = 4;\n\n  \n\n  const int STEPS = 1;\n\n  dim3 grids ((width + (BLOCK_X - 2 * STEPS * MARGIN_X) - 1) / (BLOCK_X - 2 * STEPS * MARGIN_X),\n              (height + (BLOCK_Y - 2 * STEPS * MARGIN_Y) - 1) / (BLOCK_Y - 2 * STEPS * MARGIN_Y));\n  dim3 blocks (BLOCK_X, STRIDE_Y);\n  int sense = 0;\n\n  \n\n  T *d_real[2];\n  T *d_imag[2];\n\n  hipMalloc((void**)(&d_real[0]), width * height * sizeof(T));\n  hipMalloc((void**)(&d_real[1]), width * height * sizeof(T));\n  hipMalloc((void**)(&d_imag[0]), width * height * sizeof(T));\n  hipMalloc((void**)(&d_imag[1]), width * height * sizeof(T));\n  hipMemcpy(d_real[0], p_real, width * height * sizeof(T), hipMemcpyHostToDevice);\n  hipMemcpy(d_imag[0], p_imag, width * height * sizeof(T), hipMemcpyHostToDevice);\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    hipLaunchKernelGGL(HIP_KERNEL_NAME(kernel<T, STEPS, BLOCK_X, BLOCK_Y, MARGIN_X, MARGIN_Y, STRIDE_Y>), grids, blocks, 0, 0, a, b, width, height,\n           d_real[sense], d_imag[sense], d_real[1-sense], d_imag[1-sense]);\n    sense = 1 - sense; \n\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  hipMemcpy(p_real, d_real[sense], width * height * sizeof(T), hipMemcpyDeviceToHost);\n  hipMemcpy(p_imag, d_imag[sense], width * height * sizeof(T), hipMemcpyDeviceToHost);\n\n  \n\n  bool ok = true;\n  for (int i = 0; i < width * height; i++) {\n    if (fabs(p_real[i] - h_real[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n    if (fabs(p_imag[i] - h_imag[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  delete[] p_real;\n  delete[] p_imag;\n  delete[] h_real;\n  delete[] h_imag;\n  hipFree(d_real[0]);\n  hipFree(d_real[1]);\n  hipFree(d_imag[0]);\n  hipFree(d_imag[1]);\n}\n\nint main(int argc, char** argv) {\n  if (argc != 4) {\n    printf(\"Usage: %s <matrix width> <matrix height> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  int width = atoi(argv[1]);   \n\n  int height = atoi(argv[2]);  \n\n  int repeat = atoi(argv[3]);  \n\n\n  printf(\"TSA in float32\\n\");\n  tsa<float>(width, height, repeat);\n\n  printf(\"\\n\");\n\n  printf(\"TSA in float64\\n\");\n  tsa<double>(width, height, repeat);\n  return 0;\n}\n"}}
{"kernel_name": "tsa", "parallel_api": "omp", "code": {"main.cpp": "#include <complex>\n#include <cmath>\n#include <cstdio>\n#include <cstring>\n#include <chrono>\n#include <omp.h>\n#include \"kernels.h\"\n#include \"reference.h\"\n\ntemplate <typename T>\nstatic void init_p(T *p_real, T *p_imag, int width, int height) {\n  double s = 64.0;\n  for (int j = 1; j <= height; j++) {\n    for (int i = 1; i <= width; i++) {\n      \n\n      std::complex<T> tmp = std::complex<T>(\n        exp(-(pow(i - 180.0, 2.0) + pow(j - 300.0, 2.0)) / (2.0 * pow(s, 2.0))), 0.0) *\n        exp(std::complex<T>(0.0, 0.4 * (i + j - 480.0)));\n\n      p_real[(j-1) * width + i-1] = real(tmp);\n      p_imag[(j-1) * width + i-1] = imag(tmp);\n    }\n  }\n}\n\ntemplate <typename T>\nvoid tsa(int width, int height, int repeat) {\n\n  T * p_real = new T[width * height];\n  T * p_imag = new T[width * height];\n  T * h_real = new T[width * height];\n  T * h_imag = new T[width * height];\n\n  \n\n  init_p(p_real, p_imag, width, height);\n\n  \n\n  T a = cos(0.02);\n  T b = sin(0.02);\n\n  \n\n  memcpy(h_imag, p_imag, sizeof(T)*width*height);\n  memcpy(h_real, p_real, sizeof(T)*width*height);\n  reference(h_real, h_imag, a, b, width, height, repeat);\n\n  \n\n  static const int BLOCK_X = 16;\n  \n\n  static const int BLOCK_Y = sizeof(T) == 8 ? 32 : 96;\n  \n\n  static const int STRIDE_Y = 16;\n\n  \n\n  static const int MARGIN_X = 3;\n  static const int MARGIN_Y = 4;\n\n  \n\n  static const int STEPS = 1;\n\n  const int teamX = (width + (BLOCK_X - 2 * STEPS * MARGIN_X) - 1) / (BLOCK_X - 2 * STEPS * MARGIN_X);\n  const int teamY = (height + (BLOCK_Y - 2 * STEPS * MARGIN_Y) - 1) / (BLOCK_Y - 2 * STEPS * MARGIN_Y);\n  int sense = 0;\n\n  \n\n  T *d_real[2];\n  T *d_imag[2];\n\n  d_real[0] = p_real;\n  d_real[1] = new T[width * height];\n  d_imag[0] = p_imag;\n  d_imag[1] = new T[width * height]; \n\n  #pragma omp target data map (to: d_real[0][0:width*height], d_imag[0][0:width*height]) \\\n                          map(alloc: d_real[1][0:width*height], d_imag[1][0:width*height])\n  {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      kernel<T, STEPS, BLOCK_X, BLOCK_Y, MARGIN_X, MARGIN_Y, STRIDE_Y>\n          (teamX, teamY, a, b, width, height,\n           d_real[sense], d_imag[sense], d_real[1-sense], d_imag[1-sense]);\n      sense = 1 - sense; \n\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n    #pragma omp target update from (d_real[sense][0:width*height])\n    #pragma omp target update from (d_imag[sense][0:width*height])\n  }\n\n  \n\n  bool ok = true;\n  T *t_real = d_real[sense];\n  T *t_imag = d_imag[sense];\n  for (int i = 0; i < width * height; i++) {\n    if (fabs(t_real[i] - h_real[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n    if (fabs(t_imag[i] - h_imag[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  delete[] d_real[0];\n  delete[] d_imag[0];\n  delete[] d_real[1];\n  delete[] d_imag[1];\n  delete[] h_real;\n  delete[] h_imag;\n}\n\nint main(int argc, char** argv) {\n  if (argc != 4) {\n    printf(\"Usage: %s <matrix width> <matrix height> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  int width = atoi(argv[1]);   \n\n  int height = atoi(argv[2]);  \n\n  int repeat = atoi(argv[3]);  \n\n\n  printf(\"TSA in float32\\n\");\n  tsa<float>(width, height, repeat);\n\n  printf(\"\\n\");\n\n  printf(\"TSA in float64\\n\");\n  tsa<double>(width, height, repeat);\n  return 0;\n}\n"}}
{"kernel_name": "tsa", "parallel_api": "serial", "code": {"main.cpp": "#include <complex>\n#include <cmath>\n#include <cstdio>\n#include <cstring>\n#include <chrono>\n#include \"kernels.h\"\n#include \"reference.h\"\n\ntemplate <typename T>\nstatic void init_p(T *p_real, T *p_imag, int width, int height) {\n  double s = 64.0;\n  for (int j = 1; j <= height; j++) {\n    for (int i = 1; i <= width; i++) {\n      \n\n      std::complex<T> tmp = std::complex<T>(\n        exp(-(pow(i - 180.0, 2.0) + pow(j - 300.0, 2.0)) / (2.0 * pow(s, 2.0))), 0.0) *\n        exp(std::complex<T>(0.0, 0.4 * (i + j - 480.0)));\n\n      p_real[(j-1) * width + i-1] = real(tmp);\n      p_imag[(j-1) * width + i-1] = imag(tmp);\n    }\n  }\n}\n\ntemplate <typename T>\nvoid tsa(int width, int height, int repeat) {\n\n  T * p_real = new T[width * height];\n  T * p_imag = new T[width * height];\n  T * h_real = new T[width * height];\n  T * h_imag = new T[width * height];\n\n  \n\n  init_p(p_real, p_imag, width, height);\n\n  \n\n  T a = cos(0.02);\n  T b = sin(0.02);\n\n  \n\n  memcpy(h_imag, p_imag, sizeof(T)*width*height);\n  memcpy(h_real, p_real, sizeof(T)*width*height);\n  reference(h_real, h_imag, a, b, width, height, repeat);\n\n  \n\n  static const int BLOCK_X = 16;\n  \n\n  static const int BLOCK_Y = sizeof(T) == 8 ? 32 : 96;\n  \n\n  static const int STRIDE_Y = 16;\n\n  \n\n  static const int MARGIN_X = 3;\n  static const int MARGIN_Y = 4;\n\n  \n\n  static const int STEPS = 1;\n\n  const int teamX = (width + (BLOCK_X - 2 * STEPS * MARGIN_X) - 1) / (BLOCK_X - 2 * STEPS * MARGIN_X);\n  const int teamY = (height + (BLOCK_Y - 2 * STEPS * MARGIN_Y) - 1) / (BLOCK_Y - 2 * STEPS * MARGIN_Y);\n  int sense = 0;\n\n  \n\n  T *d_real[2];\n  T *d_imag[2];\n\n  d_real[0] = p_real;\n  d_real[1] = new T[width * height];\n  d_imag[0] = p_imag;\n  d_imag[1] = new T[width * height]; \n\n    {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      kernel<T, STEPS, BLOCK_X, BLOCK_Y, MARGIN_X, MARGIN_Y, STRIDE_Y>\n          (teamX, teamY, a, b, width, height,\n           d_real[sense], d_imag[sense], d_real[1-sense], d_imag[1-sense]);\n      sense = 1 - sense; \n\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n          }\n\n  \n\n  bool ok = true;\n  T *t_real = d_real[sense];\n  T *t_imag = d_imag[sense];\n  for (int i = 0; i < width * height; i++) {\n    if (fabs(t_real[i] - h_real[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n    if (fabs(t_imag[i] - h_imag[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  delete[] d_real[0];\n  delete[] d_imag[0];\n  delete[] d_real[1];\n  delete[] d_imag[1];\n  delete[] h_real;\n  delete[] h_imag;\n}\n\nint main(int argc, char** argv) {\n  if (argc != 4) {\n    printf(\"Usage: %s <matrix width> <matrix height> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  int width = atoi(argv[1]);   \n\n  int height = atoi(argv[2]);  \n\n  int repeat = atoi(argv[3]);  \n\n\n  printf(\"TSA in float32\\n\");\n  tsa<float>(width, height, repeat);\n\n  printf(\"\\n\");\n\n  printf(\"TSA in float64\\n\");\n  tsa<double>(width, height, repeat);\n  return 0;\n}"}}
{"kernel_name": "tsa", "parallel_api": "sycl", "code": {"main.cpp": "#include <complex>\n#include <cmath>\n#include <cstdio>\n#include <cstring>\n#include <chrono>\n#include <vector>\n#include <sycl/sycl.hpp>\n#include \"kernels.h\"\n#include \"reference.h\"\n\n\n\ntemplate<typename T, int STEPS, int BLOCK_X, int BLOCK_Y, int MARGIN_X, int MARGIN_Y, int STRIDE_Y>\nclass k;\n\ntemplate <typename T>\nvoid init_p(T *p_real, T *p_imag, int width, int height) {\n  double s = 64.0;\n  for (int j = 1; j <= height; j++) {\n    for (int i = 1; i <= width; i++) {\n      \n\n      std::complex<T> tmp = std::complex<T>(\n        exp(-(pow(i - 180.0, 2.0) + pow(j - 300.0, 2.0)) / (2.0 * pow(s, 2.0))), 0.0) *\n        exp(std::complex<T>(0.0, 0.4 * (i + j - 480.0)));\n\n      p_real[(j-1) * width + i-1] = real(tmp);\n      p_imag[(j-1) * width + i-1] = imag(tmp);\n    }\n  }\n}\n\ntemplate <typename T>\nvoid tsa(sycl::queue &q, int width, int height, int repeat) {\n\n  T * p_real = new T[width * height];\n  T * p_imag = new T[width * height];\n  T * h_real = new T[width * height];\n  T * h_imag = new T[width * height];\n\n  \n\n  init_p(p_real, p_imag, width, height);\n\n  \n\n  T a = cos(0.02);\n  T b = sin(0.02);\n\n  \n\n  memcpy(h_imag, p_imag, sizeof(T)*width*height);\n  memcpy(h_real, p_real, sizeof(T)*width*height);\n  reference(h_real, h_imag, a, b, width, height, repeat);\n\n  \n\n  const int BLOCK_X = 16;\n  \n\n  const int BLOCK_Y = sizeof(T) == 8 ? 32 : 96;\n  \n\n  const int STRIDE_Y = 16;\n\n  \n\n  const int MARGIN_X = 3;\n  const int MARGIN_Y = 4;\n\n  \n\n  const int STEPS = 1;\n\n  sycl::range<2> gws ((height + (BLOCK_Y - 2 * STEPS * MARGIN_Y) - 1) /\n                      (BLOCK_Y - 2 * STEPS * MARGIN_Y) * STRIDE_Y,\n                      (width + (BLOCK_X - 2 * STEPS * MARGIN_X) - 1) /\n                      (BLOCK_X - 2 * STEPS * MARGIN_X)  * BLOCK_X);\n\n  sycl::range<2> lws (STRIDE_Y, BLOCK_X);\n\n  int sense = 0;\n\n  T *d_real[2];\n  T *d_imag[2];\n\n  \n\n  d_real[0] = sycl::malloc_device<T>(width * height, q);\n  d_real[1] = sycl::malloc_device<T>(width * height, q);\n  d_imag[0] = sycl::malloc_device<T>(width * height, q);\n  d_imag[1] = sycl::malloc_device<T>(width * height, q);\n  q.memcpy(d_real[0], p_real, width * height * sizeof(T));\n  q.memcpy(d_imag[0], p_imag, width * height * sizeof(T));\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class k<T, STEPS, BLOCK_X, BLOCK_Y, MARGIN_X, MARGIN_Y, STRIDE_Y>>(\n        sycl::nd_range<2>(gws, lws), [=] (sycl::nd_item<2> item) {\n        tsa_kernel<T, STEPS, BLOCK_X, BLOCK_Y, MARGIN_X, MARGIN_Y, STRIDE_Y>\n          (item, a, b, width, height,\n           d_real[sense], d_imag[sense], d_real[1-sense], d_imag[1-sense]);\n      });\n    });\n    sense = 1 - sense; \n\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  q.memcpy(p_real, d_real[sense], width * height * sizeof(T));\n  q.memcpy(p_imag, d_imag[sense], width * height * sizeof(T));\n\n  q.wait();\n\n  \n\n  bool ok = true;\n  for (int i = 0; i < width * height; i++) {\n    if (fabs(p_real[i] - h_real[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n    if (fabs(p_imag[i] - h_imag[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  delete[] p_real;\n  delete[] p_imag;\n  delete[] h_real;\n  delete[] h_imag;\n  sycl::free(d_real[0], q);\n  sycl::free(d_real[1], q);\n  sycl::free(d_imag[0], q);\n  sycl::free(d_imag[1], q);\n}\n\nint main(int argc, char** argv) {\n  if (argc != 4) {\n    printf(\"Usage: %s <matrix width> <matrix height> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  int width = atoi(argv[1]);   \n\n  int height = atoi(argv[2]);  \n\n  int repeat = atoi(argv[3]);  \n\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  printf(\"TSA in float32\\n\");\n  tsa<float>(q, width, height, repeat);\n\n  printf(\"\\n\");\n\n  printf(\"TSA in float64\\n\");\n  tsa<double>(q, width, height, repeat);\n  return 0;\n}\n"}}
{"kernel_name": "vanGenuchten", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <cuda.h>\n#include \"reference.h\"\n\n__global__ \nvoid vanGenuchten(\n  const double *__restrict__ Ksat,\n  const double *__restrict__ psi,\n        double *__restrict__ C,\n        double *__restrict__ theta,\n        double *__restrict__ K,\n  const int size)\n{\n  double Se, _theta, _psi, lambda, m, t;\n\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < size)\n  {\n    lambda = n - 1.0;\n    m = lambda/n;\n\n    \n\n    _psi = psi[i] * 100.0;\n    if ( _psi < 0.0 )\n      _theta = (theta_S - theta_R) / pow(1.0 + pow((alpha*(-_psi)),n), m) + theta_R;\n    else\n      _theta = theta_S;\n\n    theta[i] = _theta;\n\n    \n\n    Se = (_theta - theta_R)/(theta_S - theta_R);\n\n    \n\n    t = 1.0 - pow(1.0-pow(Se,1.0/m), m);\n    K[i] = Ksat[i] * sqrt(Se) * t * t;\n\n  \n\n  \n\n  if (_psi < 0.0)\n    C[i] = 100.0 * alpha * n * (1.0/n-1.0)*pow(alpha*abs(_psi), n-1.0)\n      * (theta_R-theta_S) * pow(pow(alpha*abs(_psi), n)+1.0, 1.0/n-2.0);\n  else\n    C[i] = 0.0;\n  }\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: ./%s <dimX> <dimY> <dimZ> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int dimX = atoi(argv[1]);\n  const int dimY = atoi(argv[2]);\n  const int dimZ = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int size = dimX * dimY * dimZ;\n  const int size_byte = size * sizeof(double);\n\n  double *Ksat, *psi, *C, *theta, *K;\n  double *C_ref, *theta_ref, *K_ref;\n  \n  Ksat = new double[size];\n  psi = new double[size];\n  C = new double[size];\n  theta = new double[size];\n  K = new double[size];\n\n  C_ref = new double[size];\n  theta_ref = new double[size];\n  K_ref = new double[size];\n\n  \n\n  for (int i = 0; i < size; i++) {\n    Ksat[i] = 1e-6 +  (1.0 - 1e-6) * i / size; \n    psi[i] = -100.0 + 101.0 * i / size;\n  }\n\n  \n\n  reference(Ksat, psi, C_ref, theta_ref, K_ref, size);\n\n  double *d_Ksat, *d_psi, *d_C, *d_theta, *d_K;\n  cudaMalloc((void**)&d_Ksat, size_byte); \n  cudaMalloc((void**)&d_psi, size_byte); \n  cudaMalloc((void**)&d_C, size_byte); \n  cudaMalloc((void**)&d_theta, size_byte); \n  cudaMalloc((void**)&d_K, size_byte); \n\n  cudaMemcpy(d_Ksat, Ksat, size_byte, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_psi, psi, size_byte, cudaMemcpyHostToDevice);\n\n  dim3 grids ((size+255)/256);\n  dim3 blocks (256);\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++)\n    vanGenuchten <<< grids, blocks >>> (d_Ksat, d_psi, d_C, d_theta, d_K, size);\n\n  cudaDeviceSynchronize();\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  cudaMemcpy(C, d_C, size_byte, cudaMemcpyDeviceToHost);\n  cudaMemcpy(theta, d_theta, size_byte, cudaMemcpyDeviceToHost);\n  cudaMemcpy(K, d_K, size_byte, cudaMemcpyDeviceToHost);\n\n  bool ok = true;\n  for (int i = 0; i < size; i++) {\n    if (fabs(C[i] - C_ref[i]) > 1e-3 || \n        fabs(theta[i] - theta_ref[i]) > 1e-3 ||\n        fabs(K[i] - K_ref[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  cudaFree(d_Ksat);\n  cudaFree(d_psi);\n  cudaFree(d_C);\n  cudaFree(d_theta);\n  cudaFree(d_K);\n\n  delete(Ksat);\n  delete(psi);\n  delete(C);\n  delete(theta);\n  delete(K);\n  delete(C_ref);\n  delete(theta_ref);\n  delete(K_ref);\n\n  return 0;\n}\n"}}
{"kernel_name": "vanGenuchten", "parallel_api": "hip", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n#include \"reference.h\"\n\n__global__ \nvoid vanGenuchten(\n  const double *__restrict__ Ksat,\n  const double *__restrict__ psi,\n        double *__restrict__ C,\n        double *__restrict__ theta,\n        double *__restrict__ K,\n  const int size)\n{\n  double Se, _theta, _psi, lambda, m, t;\n\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < size)\n  {\n    lambda = n - 1.0;\n    m = lambda/n;\n\n    \n\n    _psi = psi[i] * 100.0;\n    if ( _psi < 0.0 )\n      _theta = (theta_S - theta_R) / pow(1.0 + pow((alpha*(-_psi)),n), m) + theta_R;\n    else\n      _theta = theta_S;\n\n    theta[i] = _theta;\n\n    \n\n    Se = (_theta - theta_R)/(theta_S - theta_R);\n\n    \n\n    t = 1.0 - pow(1.0-pow(Se,1.0/m), m);\n    K[i] = Ksat[i] * sqrt(Se) * t * t;\n\n    \n\n    \n\n    if (_psi < 0.0)\n      C[i] = 100.0 * alpha * n * (1.0/n-1.0)*pow(alpha*abs(_psi), n-1.0)\n        * (theta_R-theta_S) * pow(pow(alpha*abs(_psi), n)+1.0, 1.0/n-2.0);\n    else\n      C[i] = 0.0;\n  }\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: ./%s <dimX> <dimY> <dimZ> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int dimX = atoi(argv[1]);\n  const int dimY = atoi(argv[2]);\n  const int dimZ = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int size = dimX * dimY * dimZ;\n  const int size_byte = size * sizeof(double);\n\n  double *Ksat, *psi, *C, *theta, *K;\n  double *C_ref, *theta_ref, *K_ref;\n  \n  Ksat = new double[size];\n  psi = new double[size];\n  C = new double[size];\n  theta = new double[size];\n  K = new double[size];\n\n  C_ref = new double[size];\n  theta_ref = new double[size];\n  K_ref = new double[size];\n\n  \n\n  for (int i = 0; i < size; i++) {\n    Ksat[i] = 1e-6 +  (1.0 - 1e-6) * i / size; \n    psi[i] = -100.0 + 101.0 * i / size;\n  }\n\n  \n\n  reference(Ksat, psi, C_ref, theta_ref, K_ref, size);\n\n  double *d_Ksat, *d_psi, *d_C, *d_theta, *d_K;\n  hipMalloc((void**)&d_Ksat, size_byte); \n  hipMalloc((void**)&d_psi, size_byte); \n  hipMalloc((void**)&d_C, size_byte); \n  hipMalloc((void**)&d_theta, size_byte); \n  hipMalloc((void**)&d_K, size_byte); \n\n  hipMemcpy(d_Ksat, Ksat, size_byte, hipMemcpyHostToDevice);\n  hipMemcpy(d_psi, psi, size_byte, hipMemcpyHostToDevice);\n\n  dim3 grids ((size+255)/256);\n  dim3 blocks (256);\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++)\n    hipLaunchKernelGGL(vanGenuchten, grids, blocks , 0, 0, d_Ksat, d_psi, d_C, d_theta, d_K, size);\n\n  hipDeviceSynchronize();\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  hipMemcpy(C, d_C, size_byte, hipMemcpyDeviceToHost);\n  hipMemcpy(theta, d_theta, size_byte, hipMemcpyDeviceToHost);\n  hipMemcpy(K, d_K, size_byte, hipMemcpyDeviceToHost);\n\n  bool ok = true;\n  for (int i = 0; i < size; i++) {\n    if (fabs(C[i] - C_ref[i]) > 1e-3 || \n        fabs(theta[i] - theta_ref[i]) > 1e-3 ||\n        fabs(K[i] - K_ref[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  hipFree(d_Ksat);\n  hipFree(d_psi);\n  hipFree(d_C);\n  hipFree(d_theta);\n  hipFree(d_K);\n\n  delete(Ksat);\n  delete(psi);\n  delete(C);\n  delete(theta);\n  delete(K);\n  delete(C_ref);\n  delete(theta_ref);\n  delete(K_ref);\n\n  return 0;\n}\n"}}
{"kernel_name": "vanGenuchten", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <omp.h>\n#include \"reference.h\"\n\nvoid vanGenuchten(\n  const double *__restrict Ksat,\n  const double *__restrict psi,\n        double *__restrict C,\n        double *__restrict theta,\n        double *__restrict K,\n  const int size)\n{\n  #pragma omp target teams distribute parallel for thread_limit(256)\n  for (int i = 0; i < size; i++) {\n\n    double Se, _theta, _psi, lambda, m, t;\n\n    lambda = n - 1.0;\n    m = lambda/n;\n\n    \n\n    _psi = psi[i] * 100.0;\n    if ( _psi < 0.0 )\n      _theta = (theta_S - theta_R) / pow(1.0 + pow((alpha*(-_psi)),n), m) + theta_R;\n    else\n      _theta = theta_S;\n\n    theta[i] = _theta;\n\n    \n\n    Se = (_theta - theta_R)/(theta_S - theta_R);\n\n    \n\n    t = 1.0 - pow(1.0-pow(Se,1.0/m), m);\n    K[i] = Ksat[i] * sqrt(Se) * t * t;\n\n    \n\n    \n\n    if (_psi < 0.0)\n      C[i] = 100 * alpha * n * (1.0/n-1.0)*pow(alpha*abs(_psi), n-1.0)\n        * (theta_R-theta_S) * pow(pow(alpha*abs(_psi), n)+1.0, 1.0/n-2.0);\n    else\n      C[i] = 0.0;\n  }\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: ./%s <dimX> <dimY> <dimZ> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int dimX = atoi(argv[1]);\n  const int dimY = atoi(argv[2]);\n  const int dimZ = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int size = dimX * dimY * dimZ;\n\n  double *Ksat, *psi, *C, *theta, *K;\n  double *C_ref, *theta_ref, *K_ref;\n  \n  Ksat = new double[size];\n  psi = new double[size];\n  C = new double[size];\n  theta = new double[size];\n  K = new double[size];\n\n  C_ref = new double[size];\n  theta_ref = new double[size];\n  K_ref = new double[size];\n\n  \n\n  for (int i = 0; i < size; i++) {\n    Ksat[i] = 1e-6 +  (1.0 - 1e-6) * i / size; \n    psi[i] = -100.0 + 101.0 * i / size;\n  }\n\n  \n\n  reference(Ksat, psi, C_ref, theta_ref, K_ref, size);\n\n  #pragma omp target data map(to: Ksat[0:size], psi[0:size]) \\\n                          map(from: C[0:size], theta[0:size], K[0:size])\n  {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++)\n      vanGenuchten(Ksat, psi, C, theta, K, size);\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n  }\n\n  bool ok = true;\n  for (int i = 0; i < size; i++) {\n    if (fabs(C[i] - C_ref[i]) > 1e-3 || \n        fabs(theta[i] - theta_ref[i]) > 1e-3 ||\n        fabs(K[i] - K_ref[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  delete(Ksat);\n  delete(psi);\n  delete(C);\n  delete(theta);\n  delete(K);\n  delete(C_ref);\n  delete(theta_ref);\n  delete(K_ref);\n\n  return 0;\n}\n"}}
{"kernel_name": "vanGenuchten", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include \"reference.h\"\n\nvoid vanGenuchten(\n  const double *__restrict Ksat,\n  const double *__restrict psi,\n        double *__restrict C,\n        double *__restrict theta,\n        double *__restrict K,\n  const int size)\n{\n    for (int i = 0; i < size; i++) {\n\n    double Se, _theta, _psi, lambda, m, t;\n\n    lambda = n - 1.0;\n    m = lambda/n;\n\n    \n\n    _psi = psi[i] * 100.0;\n    if ( _psi < 0.0 )\n      _theta = (theta_S - theta_R) / pow(1.0 + pow((alpha*(-_psi)),n), m) + theta_R;\n    else\n      _theta = theta_S;\n\n    theta[i] = _theta;\n\n    \n\n    Se = (_theta - theta_R)/(theta_S - theta_R);\n\n    \n\n    t = 1.0 - pow(1.0-pow(Se,1.0/m), m);\n    K[i] = Ksat[i] * sqrt(Se) * t * t;\n\n    \n\n    \n\n    if (_psi < 0.0)\n      C[i] = 100 * alpha * n * (1.0/n-1.0)*pow(alpha*abs(_psi), n-1.0)\n        * (theta_R-theta_S) * pow(pow(alpha*abs(_psi), n)+1.0, 1.0/n-2.0);\n    else\n      C[i] = 0.0;\n  }\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: ./%s <dimX> <dimY> <dimZ> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int dimX = atoi(argv[1]);\n  const int dimY = atoi(argv[2]);\n  const int dimZ = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int size = dimX * dimY * dimZ;\n\n  double *Ksat, *psi, *C, *theta, *K;\n  double *C_ref, *theta_ref, *K_ref;\n  \n  Ksat = new double[size];\n  psi = new double[size];\n  C = new double[size];\n  theta = new double[size];\n  K = new double[size];\n\n  C_ref = new double[size];\n  theta_ref = new double[size];\n  K_ref = new double[size];\n\n  \n\n  for (int i = 0; i < size; i++) {\n    Ksat[i] = 1e-6 +  (1.0 - 1e-6) * i / size; \n    psi[i] = -100.0 + 101.0 * i / size;\n  }\n\n  \n\n  reference(Ksat, psi, C_ref, theta_ref, K_ref, size);\n\n    {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++)\n      vanGenuchten(Ksat, psi, C, theta, K, size);\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n  }\n\n  bool ok = true;\n  for (int i = 0; i < size; i++) {\n    if (fabs(C[i] - C_ref[i]) > 1e-3 || \n        fabs(theta[i] - theta_ref[i]) > 1e-3 ||\n        fabs(K[i] - K_ref[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  delete(Ksat);\n  delete(psi);\n  delete(C);\n  delete(theta);\n  delete(K);\n  delete(C_ref);\n  delete(theta_ref);\n  delete(K_ref);\n\n  return 0;\n}"}}
{"kernel_name": "vanGenuchten", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n#include \"reference.h\"\n\nvoid vanGenuchten(\n  const double *__restrict__ Ksat,\n  const double *__restrict__ psi,\n        double *__restrict__ C,\n        double *__restrict__ theta,\n        double *__restrict__ K,\n  const int size,\n  sycl::nd_item<1> &item)\n{\n  double Se, _theta, _psi, lambda, m;\n\n  int i = item.get_global_id(0);\n  if (i < size)\n  {\n    lambda = n - 1.0;\n    m = lambda/n;\n\n    \n\n    _psi = psi[i] * 100.0;\n    if ( _psi < 0.0 )\n      _theta = (theta_S - theta_R) / sycl::pow(\n               1.0 + sycl::pow((alpha * (-_psi)), n), m) + theta_R;\n    else\n      _theta = theta_S;\n\n    theta[i] = _theta;\n\n   \n\n   Se = (_theta - theta_R)/(theta_S - theta_R);\n\n   \n\n   double t = 1.0 - sycl::pow(1.0 - sycl::pow(Se, 1.0 / m), m);\n   K[i] = Ksat[i] * sycl::sqrt(Se) * t * t;\n\n   \n\n   \n\n   if (_psi < 0.0)\n     C[i] = 100.0 * alpha * n * (1.0 / n - 1.0) *\n            sycl::pow(alpha * sycl::fabs(_psi), n - 1.0) *\n            (theta_R - theta_S) *\n            sycl::pow(sycl::pow(alpha * sycl::fabs(_psi), n) + 1.0,\n                           1.0 / n - 2.0);\n   else\n     C[i] = 0.0;\n  }\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: ./%s <dimX> <dimY> <dimZ> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int dimX = atoi(argv[1]);\n  const int dimY = atoi(argv[2]);\n  const int dimZ = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int size = dimX * dimY * dimZ;\n  const int size_byte = size * sizeof(double);\n\n  double *Ksat, *psi, *C, *theta, *K;\n  double *C_ref, *theta_ref, *K_ref;\n\n  Ksat = new double[size];\n  psi = new double[size];\n  C = new double[size];\n  theta = new double[size];\n  K = new double[size];\n\n  C_ref = new double[size];\n  theta_ref = new double[size];\n  K_ref = new double[size];\n\n  \n\n  for (int i = 0; i < size; i++) {\n    Ksat[i] = 1e-6 +  (1.0 - 1e-6) * i / size;\n    psi[i] = -100.0 + 101.0 * i / size;\n  }\n\n  \n\n  reference(Ksat, psi, C_ref, theta_ref, K_ref, size);\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  double *d_Ksat, *d_psi, *d_C, *d_theta, *d_K;\n  d_Ksat = (double *)sycl::malloc_device(size_byte, q);\n  d_psi = (double *)sycl::malloc_device(size_byte, q);\n  d_C = (double *)sycl::malloc_device(size_byte, q);\n  d_theta = (double *)sycl::malloc_device(size_byte, q);\n  d_K = (double *)sycl::malloc_device(size_byte, q);\n\n  q.memcpy(d_Ksat, Ksat, size_byte);\n  q.memcpy(d_psi, psi, size_byte);\n\n  sycl::range<1> gws ((size + 255) / 256 * 256);\n  sycl::range<1> lws (256);\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++)\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for(sycl::nd_range<1>(gws, lws), [=](sycl::nd_item<1> item) {\n        vanGenuchten(d_Ksat, d_psi, d_C, d_theta, d_K, size, item);\n      });\n    });\n\n  q.wait();\n\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (s)\\n\", (time * 1e-9f) / repeat);\n\n  q.memcpy(C, d_C, size_byte);\n  q.memcpy(theta, d_theta, size_byte);\n  q.memcpy(K, d_K, size_byte);\n\n  q.wait();\n\n  bool ok = true;\n  for (int i = 0; i < size; i++) {\n    if (fabs(C[i] - C_ref[i]) > 1e-3 ||\n        fabs(theta[i] - theta_ref[i]) > 1e-3 ||\n        fabs(K[i] - K_ref[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  sycl::free(d_Ksat, q);\n  sycl::free(d_psi, q);\n  sycl::free(d_C, q);\n  sycl::free(d_theta, q);\n  sycl::free(d_K, q);\n\n  delete(Ksat);\n  delete(psi);\n  delete(C);\n  delete(theta);\n  delete(K);\n  delete(C_ref);\n  delete(theta_ref);\n  delete(K_ref);\n\n  return 0;\n}\n"}}
{"kernel_name": "wlcpow", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <random>\n#include <chrono>\n#include <cuda.h>\n#include \"utils.h\"\n\n__global__ \nvoid bond_wlcpowallvisc(\n             r64* __restrict__ force_x,\n             r64* __restrict__ force_y,\n             r64* __restrict__ force_z,\n    const float4* __restrict__ coord_merged,\n    const float4* __restrict__ veloc,\n    const int*  __restrict__ nbond,\n    const int2* __restrict__ bonds,\n    const r64* __restrict__ bond_r0,\n    const r32* __restrict__ temp_global,\n    const r32* __restrict__ r0_global,\n    const r32* __restrict__ mu_targ_global,\n    const r32* __restrict__ qp_global,\n    const r32* __restrict__ gamc_global,\n    const r32* __restrict__ gamt_global,\n    const r32* __restrict__ sigc_global,\n    const r32* __restrict__ sigt_global,\n    const float3 period,\n    const int padding,\n    const int n_type,\n    const int n_local )\n{\n  extern __shared__ r32 shared_data[];\n  r32* temp    = &shared_data[0];\n  r32* r0      = &shared_data[1*(n_type+1)];\n  r32* mu_targ = &shared_data[2*(n_type+1)];\n  r32* qp      = &shared_data[3*(n_type+1)];\n  r32* gamc    = &shared_data[4*(n_type+1)];\n  r32* gamt    = &shared_data[5*(n_type+1)];\n  r32* sigc    = &shared_data[6*(n_type+1)];\n  r32* sigt    = &shared_data[7*(n_type+1)];\n\n  for ( int i = threadIdx.x; i < n_type + 1; i += blockDim.x ) {\n    temp[i]    = temp_global[i];\n    r0[i]      = r0_global[i];\n    mu_targ[i] = mu_targ_global[i];\n    qp[i]      = qp_global[i];\n    gamc[i]    = gamc_global[i];\n    gamt[i]    = gamt_global[i];\n    sigc[i]    = sigc_global[i];\n    sigt[i]    = sigt_global[i];\n  }\n  __syncthreads();\n\n  for( int i = blockIdx.x * blockDim.x + threadIdx.x;\n           i < n_local ; i += gridDim.x * blockDim.x ) {\n    int n = nbond[i];\n    float4 coord1 = coord_merged[i];\n    float4 veloc1 = veloc[i];\n    r32 fxi = 0.f, fyi = 0.f, fzi = 0.f;\n\n    for( int p = 0; p < n; p++ ) {\n      int j = bonds[ i + p*padding ].x;\n      int type = bonds[ i + p*padding ].y;\n      float4 coord2 = coord_merged[j];\n      r32 delx = minimum_image( coord1.x - coord2.x, period.x );\n      r32 dely = minimum_image( coord1.y - coord2.y, period.y );\n      r32 delz = minimum_image( coord1.z - coord2.z, period.z );\n      float4 veloc2 = veloc[j];\n      r32 dvx = veloc1.x - veloc2.x;\n      r32 dvy = veloc1.y - veloc2.y;\n      r32 dvz = veloc1.z - veloc2.z;\n\n      r32 l0 = bond_r0[ i + p*padding ];\n      r32 ra = sqrtf(delx*delx + dely*dely + delz*delz);\n      r32 lmax = l0*r0[type];\n      r32 rr = 1.0f/r0[type];\n      r32 sr = (1.0f-rr)*(1.0f-rr);\n      r32 kph = powf(l0,qp[type])*temp[type]*(0.25f/sr-0.25f+rr);\n      \n\n      r32 mu = 0.433f*(   \n\n\t       temp[type]*(-0.25f/sr + 0.25f + \n               0.5f*rr/(sr*(1.0f-rr)))/(lmax*rr) +\n               kph*(qp[type]+1.0f)/powf(l0,qp[type]+1.0f));\n      r32 lambda = mu/mu_targ[type];\n      kph = kph/lambda;\n      rr = ra/lmax;\n      r32 rlogarg = powf(ra,qp[type]+1.0f);\n      r32 vv = (delx*dvx + dely*dvy + delz*dvz)/ra;\n\n      if (rr >= 0.99) rr = 0.99f;\n      if (rlogarg < 0.01) rlogarg = 0.01f;\n\n      float4 wrr;\n      r32 ww[3][3];\n\n      for (int tes=0; tes<3; tes++) {\n        for (int see=0; see<3; see++) {\n          int v1 = __float_as_int(veloc1.w);\n          int v2 = __float_as_int(veloc2.w);\n          ww[tes][see] = gaussian_TEA_fast<4>(v1 > v2, v1+tes, v2+see);\n        }\n      }\n\n      wrr.w = (ww[0][0]+ww[1][1]+ww[2][2])/3.0f;\n      wrr.x = (ww[0][0]-wrr.w)*delx + 0.5f*(ww[0][1]+ww[1][0])*dely + 0.5f*(ww[0][2]+ww[2][0])*delz;\n      wrr.y = 0.5f*(ww[1][0]+ww[0][1])*delx + (ww[1][1]-wrr.w)*dely + 0.5f*(ww[1][2]+ww[2][1])*delz;\n      wrr.z = 0.5f*(ww[2][0]+ww[0][2])*delx + 0.5f*(ww[2][1]+ww[1][2])*dely + (ww[2][2]-wrr.w)*delz;\n\n      r32 fforce = - temp[type]*(0.25f/(1.0f-rr)/(1.0f-rr)-0.25f+rr)/lambda/ra + kph/rlogarg + (sigc[type]*wrr.w - gamc[type]*vv)/ra;\n      r32 fxij = delx*fforce - gamt[type]*dvx + sigt[type]*wrr.x/ra;\n      r32 fyij = dely*fforce - gamt[type]*dvy + sigt[type]*wrr.y/ra;\n      r32 fzij = delz*fforce - gamt[type]*dvz + sigt[type]*wrr.z/ra;\n\n      fxi += fxij;\n      fyi += fyij;\n      fzi += fzij;\n    }\n    force_x[i] += fxi;\n    force_y[i] += fyi;\n    force_z[i] += fzi;\n  }\n}\n\ntemplate <typename T>\nT* resize (int n) {\n  return (T*) malloc (sizeof(T) * n);\n}\n\ntemplate <typename T>\nT* grow (int n) {\n  T* p;\n  cudaMalloc((void**)&p, sizeof(T) * n);\n  return p;\n}\n\ntemplate <typename T>\nvoid upload(T* d, T* h, int n) {\n  cudaMemcpy(d, h, sizeof(T) * n, cudaMemcpyHostToDevice); \n}\n\ntemplate <typename T>\nvoid reset(T* d, int n) {\n  cudaMemset(d, (T)0, sizeof(T) * n);\n}\n\ntemplate <typename T>\nvoid download(T* h, T* d, int n) {\n  cudaMemcpy(h, d, sizeof(T) * n, cudaMemcpyDeviceToHost); \n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: ./%s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int repeat = atoi(argv[1]);\n\n  int i;\n\n  \n\n\n  float3 period = {0.5f, 0.5f, 0.5f};\n  int padding = 1;\n  int n_type = 32;\n  int n = 1e6;  \n\n\n  float4 *coord_merged = resize<float4>(n+1);\n  float4 *veloc = resize<float4>(n+1);\n  int *nbond = resize<int>(n+1);\n\n  \n\n  int2 *bonds = resize<int2>(n+n+1);\n  r64 *bond_r0 = resize<r64>(n+n+1);\n\n  r64 *force_x = resize<r64>(n+1);\n  r64 *force_y = resize<r64>(n+1);\n  r64 *force_z = resize<r64>(n+1);\n\n  r32 *bond_l0 = resize<r32>(n+1);\n  r32 *temp = resize<r32>(n+1);\n  r32 *mu_targ = resize<r32>(n+1);\n  r32 *qp = resize<r32>(n+1);\n  r32 *gamc = resize<r32>(n+1);\n  r32 *gamt = resize<r32>(n+1);\n  r32 *sigc = resize<r32>(n+1);\n  r32 *sigt = resize<r32>(n+1);\n\n  std::mt19937 g (19937);\n  std::uniform_real_distribution<r64> dist_r64(0.1, 0.9);\n  std::uniform_real_distribution<r32> dist_r32(0.1, 0.9);\n  std::uniform_int_distribution<i32> dist_i32(0, n_type);\n\n  for (i = 0; i < n + n + 1; i++) {\n    bond_r0[i] = dist_r64(g) + 0.001;\n    \n\n    bonds[i] = { (i+1)%(n+1), \n                 dist_i32(g) };\n  }\n\n  for (i = 0; i < n + 1; i++) {\n    nbond[i] = dist_i32(g);\n    coord_merged[i] = {dist_r32(g), dist_r32(g), dist_r32(g), 0};\n    r32 vx = dist_r32(g), vy = dist_r32(g), vz = dist_r32(g);\n    veloc[i] = {vx, vy, vz, sqrtf(vx*vx+vy*vy+vz*vz)};\n    bond_l0[i] = dist_r32(g);\n    gamt[i] = dist_r32(g);\n    gamc[i] = ((dist_i32(g) % 4) + 4) * gamt[i]; \n\n    temp[i] = dist_r32(g);\n    mu_targ[i] = dist_r32(g);\n    qp[i] = dist_r32(g);\n    sigc[i] = sqrt(2.0*temp[i]*(3.0*gamc[i]-gamt[i]));\n    sigt[i] = 2.0*sqrt(gamt[i]*temp[i]);\n  }\n\n  float4 *dev_coord_merged = grow<float4>(n + 1);\n  float4 *dev_veloc = grow<float4>(n + 1);\n  int *dev_nbond = grow<int>(n + 1);\n  int2 *dev_bonds = grow<int2>(n + n + 1);\n  r64 *dev_bond_r0 = grow<r64>(n + n + 1);\n\n  r64 *dev_force_x = grow<r64>(n + 1);\n  r64 *dev_force_y = grow<r64>(n + 1);\n  r64 *dev_force_z = grow<r64>(n + 1);\n\n  r32 *dev_bond_l0 = grow<r32>(n + 1);\n  r32 *dev_temp = grow<r32>(n + 1);\n  r32 *dev_mu_targ = grow<r32>(n + 1);\n  r32 *dev_qp = grow<r32>(n + 1);\n  r32 *dev_gamc = grow<r32>(n + 1);\n  r32 *dev_gamt = grow<r32>(n + 1);\n  r32 *dev_sigc = grow<r32>(n + 1);\n  r32 *dev_sigt = grow<r32>(n + 1);\n\n  reset (dev_force_x, n+1);\n  reset (dev_force_y, n+1);\n  reset (dev_force_z, n+1);\n  upload (dev_coord_merged, coord_merged, n+1);\n  upload (dev_veloc, veloc, n+1);\n  upload (dev_nbond, nbond, n+1);\n  upload (dev_bonds, bonds, n+n+1);\n  upload (dev_bond_r0, bond_r0, n+n+1);\n  upload (dev_temp, temp, n+1);\n  upload (dev_bond_l0, bond_l0, n+1);\n  upload (dev_mu_targ, mu_targ, n+1);\n  upload (dev_qp, qp, n+1);\n  upload (dev_gamc, gamc, n+1);\n  upload (dev_gamt, gamt, n+1);\n  upload (dev_sigc, sigc, n+1);\n  upload (dev_sigt, sigt, n+1);\n\n  dim3 grids ((n+127)/128);\n  dim3 blocks (128);\n\n  const int sm_size = (n_type+1) * 8 * sizeof(r32);\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  for (i = 0; i < repeat; i++) {\n    bond_wlcpowallvisc <<<grids, blocks, sm_size, 0>>> (\n      dev_force_x,\n      dev_force_y,\n      dev_force_z,\n      dev_coord_merged,\n      dev_veloc,\n      dev_nbond,\n      dev_bonds,\n      dev_bond_r0,\n      dev_temp,\n      dev_bond_l0,\n      dev_mu_targ,\n      dev_qp,\n      dev_gamc,\n      dev_gamt,\n      dev_sigc,\n      dev_sigt,\n      period,\n      padding,\n      n_type,\n      n);\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", time * 1e-3f / repeat);\n\n  download (force_x, dev_force_x, n+1);\n  download (force_y, dev_force_y, n+1);\n  download (force_z, dev_force_z, n+1);\n\n  \n\n  for (i = 0; i < n+1; i++) {\n    bool r = (isnan(force_x[i]) || isnan(force_y[i]) || isnan(force_z[i]));\n    if (r) printf(\"There are NaN numbers at index %d\\n\", i);\n  }\n\n  double force_x_sum = 0, force_y_sum = 0, force_z_sum = 0;\n  for (i = 0; i < n+1; i++) {\n    force_x_sum += force_x[i];\n    force_y_sum += force_y[i];\n    force_z_sum += force_z[i];\n  }\n  \n\n  printf(\"checksum: forceX=%lf forceY=%lf forceZ=%lf\\n\",\n         force_x_sum/(n+1), force_y_sum/(n+1), force_z_sum/(n+1));\n  \n#ifdef DEBUG\n  for (i = 0; i < 16; i++) {\n    printf(\"%d %lf %lf %lf\\n\", i, force_x[i], force_y[i], force_z[i]);\n  }\n#endif\n\n  free(coord_merged);\n  free(veloc);\n  free(force_x);\n  free(force_y);\n  free(force_z);\n  free(nbond);\n  free(bonds);\n  free(bond_r0);\n  free(bond_l0);\n  free(temp);\n  free(mu_targ);\n  free(qp);\n  free(gamc);\n  free(gamt);\n  free(sigc);\n  free(sigt);\n\n  cudaFree(dev_coord_merged);\n  cudaFree(dev_veloc);\n  cudaFree(dev_force_x);\n  cudaFree(dev_force_y);\n  cudaFree(dev_force_z);\n  cudaFree(dev_nbond);\n  cudaFree(dev_bonds);\n  cudaFree(dev_bond_r0);\n  cudaFree(dev_bond_l0);\n  cudaFree(dev_temp);\n  cudaFree(dev_mu_targ);\n  cudaFree(dev_qp);\n  cudaFree(dev_gamc);\n  cudaFree(dev_gamt);\n  cudaFree(dev_sigc);\n  cudaFree(dev_sigt);\n  return 0;\n}\n"}}
{"kernel_name": "wlcpow", "parallel_api": "hip", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <random>\n#include <chrono>\n#include <hip/hip_runtime.h>\n#include \"utils.h\"\n\n__global__ \nvoid bond_wlcpowallvisc(\n             r64* __restrict__ force_x,\n             r64* __restrict__ force_y,\n             r64* __restrict__ force_z,\n    const float4* __restrict__ coord_merged,\n    const float4* __restrict__ veloc,\n    const int*  __restrict__ nbond,\n    const int2* __restrict__ bonds,\n    const r64* __restrict__ bond_r0,\n    const r32* __restrict__ temp_global,\n    const r32* __restrict__ r0_global,\n    const r32* __restrict__ mu_targ_global,\n    const r32* __restrict__ qp_global,\n    const r32* __restrict__ gamc_global,\n    const r32* __restrict__ gamt_global,\n    const r32* __restrict__ sigc_global,\n    const r32* __restrict__ sigt_global,\n    const float3 period,\n    const int padding,\n    const int n_type,\n    const int n_local )\n{\n  extern __shared__ r32 shared_data[];\n  r32* temp    = &shared_data[0];\n  r32* r0      = &shared_data[1*(n_type+1)];\n  r32* mu_targ = &shared_data[2*(n_type+1)];\n  r32* qp      = &shared_data[3*(n_type+1)];\n  r32* gamc    = &shared_data[4*(n_type+1)];\n  r32* gamt    = &shared_data[5*(n_type+1)];\n  r32* sigc    = &shared_data[6*(n_type+1)];\n  r32* sigt    = &shared_data[7*(n_type+1)];\n\n  for ( int i = threadIdx.x; i < n_type + 1; i += blockDim.x ) {\n    temp[i]    = temp_global[i];\n    r0[i]      = r0_global[i];\n    mu_targ[i] = mu_targ_global[i];\n    qp[i]      = qp_global[i];\n    gamc[i]    = gamc_global[i];\n    gamt[i]    = gamt_global[i];\n    sigc[i]    = sigc_global[i];\n    sigt[i]    = sigt_global[i];\n  }\n  __syncthreads();\n\n  for( int i = blockIdx.x * blockDim.x + threadIdx.x;\n           i < n_local ; i += gridDim.x * blockDim.x ) {\n    int n = nbond[i];\n    float4 coord1 = coord_merged[i];\n    float4 veloc1 = veloc[i];\n    r32 fxi = 0.f, fyi = 0.f, fzi = 0.f;\n\n    for( int p = 0; p < n; p++ ) {\n      int j = bonds[ i + p*padding ].x;\n      int type = bonds[ i + p*padding ].y;\n      float4 coord2 = coord_merged[j];\n      r32 delx = minimum_image( coord1.x - coord2.x, period.x );\n      r32 dely = minimum_image( coord1.y - coord2.y, period.y );\n      r32 delz = minimum_image( coord1.z - coord2.z, period.z );\n      float4 veloc2 = veloc[j];\n      r32 dvx = veloc1.x - veloc2.x;\n      r32 dvy = veloc1.y - veloc2.y;\n      r32 dvz = veloc1.z - veloc2.z;\n\n      r32 l0 = bond_r0[ i + p*padding ];\n      r32 ra = sqrtf(delx*delx + dely*dely + delz*delz);\n      r32 lmax = l0*r0[type];\n      r32 rr = 1.0f/r0[type];\n      r32 sr = (1.0f-rr)*(1.0f-rr);\n      r32 kph = powf(l0,qp[type])*temp[type]*(0.25f/sr-0.25f+rr);\n      \n\n      r32 mu = 0.433f*(   \n\n\t       temp[type]*(-0.25f/sr + 0.25f + \n               0.5f*rr/(sr*(1.0f-rr)))/(lmax*rr) +\n               kph*(qp[type]+1.0f)/powf(l0,qp[type]+1.0f));\n      r32 lambda = mu/mu_targ[type];\n      kph = kph/lambda;\n      rr = ra/lmax;\n      r32 rlogarg = powf(ra,qp[type]+1.0f);\n      r32 vv = (delx*dvx + dely*dvy + delz*dvz)/ra;\n\n      if (rr >= 0.99) rr = 0.99f;\n      if (rlogarg < 0.01) rlogarg = 0.01f;\n\n      float4 wrr;\n      r32 ww[3][3];\n\n      for (int tes=0; tes<3; tes++) {\n        for (int see=0; see<3; see++) {\n          int v1 = __float_as_int(veloc1.w);\n          int v2 = __float_as_int(veloc2.w);\n          ww[tes][see] = gaussian_TEA_fast<4>(v1 > v2, v1+tes, v2+see);\n        }\n      }\n\n      wrr.w = (ww[0][0]+ww[1][1]+ww[2][2])/3.0f;\n      wrr.x = (ww[0][0]-wrr.w)*delx + 0.5f*(ww[0][1]+ww[1][0])*dely + 0.5f*(ww[0][2]+ww[2][0])*delz;\n      wrr.y = 0.5f*(ww[1][0]+ww[0][1])*delx + (ww[1][1]-wrr.w)*dely + 0.5f*(ww[1][2]+ww[2][1])*delz;\n      wrr.z = 0.5f*(ww[2][0]+ww[0][2])*delx + 0.5f*(ww[2][1]+ww[1][2])*dely + (ww[2][2]-wrr.w)*delz;\n\n      r32 fforce = - temp[type]*(0.25f/(1.0f-rr)/(1.0f-rr)-0.25f+rr)/lambda/ra + kph/rlogarg + (sigc[type]*wrr.w - gamc[type]*vv)/ra;\n      r32 fxij = delx*fforce - gamt[type]*dvx + sigt[type]*wrr.x/ra;\n      r32 fyij = dely*fforce - gamt[type]*dvy + sigt[type]*wrr.y/ra;\n      r32 fzij = delz*fforce - gamt[type]*dvz + sigt[type]*wrr.z/ra;\n\n      fxi += fxij;\n      fyi += fyij;\n      fzi += fzij;\n    }\n    force_x[i] += fxi;\n    force_y[i] += fyi;\n    force_z[i] += fzi;\n  }\n}\n\ntemplate <typename T>\nT* resize (int n) {\n  return (T*) malloc (sizeof(T) * n);\n}\n\ntemplate <typename T>\nT* grow (int n) {\n  T* p;\n  hipMalloc((void**)&p, sizeof(T) * n);\n  return p;\n}\n\ntemplate <typename T>\nvoid upload(T* d, T* h, int n) {\n  hipMemcpy(d, h, sizeof(T) * n, hipMemcpyHostToDevice); \n}\n\ntemplate <typename T>\nvoid reset(T* d, int n) {\n  hipMemset(d, (T)0, sizeof(T) * n);\n}\n\ntemplate <typename T>\nvoid download(T* h, T* d, int n) {\n  hipMemcpy(h, d, sizeof(T) * n, hipMemcpyDeviceToHost); \n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: ./%s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int repeat = atoi(argv[1]);\n\n  int i;\n\n  \n\n\n  float3 period = {0.5f, 0.5f, 0.5f};\n  int padding = 1;\n  int n_type = 32;\n  int n = 1e6;  \n\n\n  float4 *coord_merged = resize<float4>(n+1);\n  float4 *veloc = resize<float4>(n+1);\n  int *nbond = resize<int>(n+1);\n\n  \n\n  int2 *bonds = resize<int2>(n+n+1);\n  r64 *bond_r0 = resize<r64>(n+n+1);\n\n  r64 *force_x = resize<r64>(n+1);\n  r64 *force_y = resize<r64>(n+1);\n  r64 *force_z = resize<r64>(n+1);\n\n  r32 *bond_l0 = resize<r32>(n+1);\n  r32 *temp = resize<r32>(n+1);\n  r32 *mu_targ = resize<r32>(n+1);\n  r32 *qp = resize<r32>(n+1);\n  r32 *gamc = resize<r32>(n+1);\n  r32 *gamt = resize<r32>(n+1);\n  r32 *sigc = resize<r32>(n+1);\n  r32 *sigt = resize<r32>(n+1);\n\n  std::mt19937 g (19937);\n  std::uniform_real_distribution<r64> dist_r64(0.1, 0.9);\n  std::uniform_real_distribution<r32> dist_r32(0.1, 0.9);\n  std::uniform_int_distribution<i32> dist_i32(0, n_type);\n\n  for (i = 0; i < n + n + 1; i++) {\n    bond_r0[i] = dist_r64(g) + 0.001;\n    \n\n    bonds[i] = { (i+1)%(n+1), \n                 dist_i32(g) };\n  }\n\n  for (i = 0; i < n + 1; i++) {\n    nbond[i] = dist_i32(g);\n    coord_merged[i] = {dist_r32(g), dist_r32(g), dist_r32(g), 0};\n    r32 vx = dist_r32(g), vy = dist_r32(g), vz = dist_r32(g);\n    veloc[i] = {vx, vy, vz, sqrtf(vx*vx+vy*vy+vz*vz)};\n    bond_l0[i] = dist_r32(g);\n    gamt[i] = dist_r32(g);\n    gamc[i] = ((dist_i32(g) % 4) + 4) * gamt[i]; \n\n    temp[i] = dist_r32(g);\n    mu_targ[i] = dist_r32(g);\n    qp[i] = dist_r32(g);\n    sigc[i] = sqrt(2.0*temp[i]*(3.0*gamc[i]-gamt[i]));\n    sigt[i] = 2.0*sqrt(gamt[i]*temp[i]);\n  }\n\n  float4 *dev_coord_merged = grow<float4>(n + 1);\n  float4 *dev_veloc = grow<float4>(n + 1);\n  int *dev_nbond = grow<int>(n + 1);\n  int2 *dev_bonds = grow<int2>(n + n + 1);\n  r64 *dev_bond_r0 = grow<r64>(n + n + 1);\n\n  r64 *dev_force_x = grow<r64>(n + 1);\n  r64 *dev_force_y = grow<r64>(n + 1);\n  r64 *dev_force_z = grow<r64>(n + 1);\n\n  r32 *dev_bond_l0 = grow<r32>(n + 1);\n  r32 *dev_temp = grow<r32>(n + 1);\n  r32 *dev_mu_targ = grow<r32>(n + 1);\n  r32 *dev_qp = grow<r32>(n + 1);\n  r32 *dev_gamc = grow<r32>(n + 1);\n  r32 *dev_gamt = grow<r32>(n + 1);\n  r32 *dev_sigc = grow<r32>(n + 1);\n  r32 *dev_sigt = grow<r32>(n + 1);\n\n  reset (dev_force_x, n+1);\n  reset (dev_force_y, n+1);\n  reset (dev_force_z, n+1);\n  upload (dev_coord_merged, coord_merged, n+1);\n  upload (dev_veloc, veloc, n+1);\n  upload (dev_nbond, nbond, n+1);\n  upload (dev_bonds, bonds, n+n+1);\n  upload (dev_bond_r0, bond_r0, n+n+1);\n  upload (dev_temp, temp, n+1);\n  upload (dev_bond_l0, bond_l0, n+1);\n  upload (dev_mu_targ, mu_targ, n+1);\n  upload (dev_qp, qp, n+1);\n  upload (dev_gamc, gamc, n+1);\n  upload (dev_gamt, gamt, n+1);\n  upload (dev_sigc, sigc, n+1);\n  upload (dev_sigt, sigt, n+1);\n\n  dim3 grids ((n+127)/128);\n  dim3 blocks (128);\n\n  const int sm_size = (n_type+1) * 8 * sizeof(r32);\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  for (i = 0; i < repeat; i++) {\n    hipLaunchKernelGGL(bond_wlcpowallvisc, grids, blocks, sm_size, 0, \n      dev_force_x,\n      dev_force_y,\n      dev_force_z,\n      dev_coord_merged,\n      dev_veloc,\n      dev_nbond,\n      dev_bonds,\n      dev_bond_r0,\n      dev_temp,\n      dev_bond_l0,\n      dev_mu_targ,\n      dev_qp,\n      dev_gamc,\n      dev_gamt,\n      dev_sigc,\n      dev_sigt,\n      period,\n      padding,\n      n_type,\n      n);\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", time * 1e-3f / repeat);\n\n  download (force_x, dev_force_x, n+1);\n  download (force_y, dev_force_y, n+1);\n  download (force_z, dev_force_z, n+1);\n\n  \n\n  for (i = 0; i < n+1; i++) {\n    bool r = (isnan(force_x[i]) || isnan(force_y[i]) || isnan(force_z[i]));\n    if (r) printf(\"There are NaN numbers at index %d\\n\", i);\n  }\n\n  double force_x_sum = 0, force_y_sum = 0, force_z_sum = 0;\n  for (i = 0; i < n+1; i++) {\n    force_x_sum += force_x[i];\n    force_y_sum += force_y[i];\n    force_z_sum += force_z[i];\n  }\n  \n\n  printf(\"checksum: forceX=%lf forceY=%lf forceZ=%lf\\n\",\n         force_x_sum/(n+1), force_y_sum/(n+1), force_z_sum/(n+1));\n  \n#ifdef DEBUG\n  for (i = 0; i < 16; i++) {\n    printf(\"%d %lf %lf %lf\\n\", i, force_x[i], force_y[i], force_z[i]);\n  }\n#endif\n\n  free(coord_merged);\n  free(veloc);\n  free(force_x);\n  free(force_y);\n  free(force_z);\n  free(nbond);\n  free(bonds);\n  free(bond_r0);\n  free(bond_l0);\n  free(temp);\n  free(mu_targ);\n  free(qp);\n  free(gamc);\n  free(gamt);\n  free(sigc);\n  free(sigt);\n\n  hipFree(dev_coord_merged);\n  hipFree(dev_veloc);\n  hipFree(dev_force_x);\n  hipFree(dev_force_y);\n  hipFree(dev_force_z);\n  hipFree(dev_nbond);\n  hipFree(dev_bonds);\n  hipFree(dev_bond_r0);\n  hipFree(dev_bond_l0);\n  hipFree(dev_temp);\n  hipFree(dev_mu_targ);\n  hipFree(dev_qp);\n  hipFree(dev_gamc);\n  hipFree(dev_gamt);\n  hipFree(dev_sigc);\n  hipFree(dev_sigt);\n  return 0;\n}\n"}}
{"kernel_name": "wlcpow", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <random>\n#include <chrono>\n#include <omp.h>\n#include \"utils.h\"\n\n#define n_type 32\n\nvoid bond_wlcpowallvisc(\n             r64* __restrict force_x,\n             r64* __restrict force_y,\n             r64* __restrict force_z,\n    const float4* __restrict coord_merged,\n    const float4* __restrict veloc,\n    const int*  __restrict nbond,\n    const int2* __restrict bonds,\n    const r64* __restrict bond_r0,\n    const r32* __restrict temp_global,\n    const r32* __restrict r0_global,\n    const r32* __restrict mu_targ_global,\n    const r32* __restrict qp_global,\n    const r32* __restrict gamc_global,\n    const r32* __restrict gamt_global,\n    const r32* __restrict sigc_global,\n    const r32* __restrict sigt_global,\n    const float3 period,\n    const int padding,\n    const int n_local,\n    const int teams,\n    const int blocks )\n{\n  #pragma omp target teams num_teams(teams) thread_limit(blocks)\n  {\n    r32 shared_data[(n_type + 1) * 8];  \n\n    #pragma omp parallel \n    {\n      int threadIdx_x = omp_get_thread_num();\n      int blockIdx_x = omp_get_team_num();\n\n      r32* temp    = &shared_data[0];\n      r32* r0      = &shared_data[1*(n_type+1)];\n      r32* mu_targ = &shared_data[2*(n_type+1)];\n      r32* qp      = &shared_data[3*(n_type+1)];\n      r32* gamc    = &shared_data[4*(n_type+1)];\n      r32* gamt    = &shared_data[5*(n_type+1)];\n      r32* sigc    = &shared_data[6*(n_type+1)];\n      r32* sigt    = &shared_data[7*(n_type+1)];\n\n      for ( int i = threadIdx_x; i < n_type + 1; i += blocks ) {\n        temp[i]    = temp_global[i];\n        r0[i]      = r0_global[i];\n        mu_targ[i] = mu_targ_global[i];\n        qp[i]      = qp_global[i];\n        gamc[i]    = gamc_global[i];\n        gamt[i]    = gamt_global[i];\n        sigc[i]    = sigc_global[i];\n        sigt[i]    = sigt_global[i];\n      }\n      #pragma omp barrier\n\n      for( int i = blockIdx_x * blocks + threadIdx_x;\n               i < n_local ; i += teams * blocks ) {\n\n        int n = nbond[i];\n        float4 coord1 = coord_merged[i];\n        float4 veloc1 = veloc[i];\n        r32 fxi = 0.f, fyi = 0.f, fzi = 0.f;\n\n        for( int p = 0; p < n; p++ ) {\n          int j = bonds[ i + p*padding ].x;\n          int type = bonds[ i + p*padding ].y;\n          float4 coord2 = coord_merged[j];\n          r32 delx = minimum_image( coord1.x - coord2.x, period.x );\n          r32 dely = minimum_image( coord1.y - coord2.y, period.y );\n          r32 delz = minimum_image( coord1.z - coord2.z, period.z );\n          float4 veloc2 = veloc[j];\n          r32 dvx = veloc1.x - veloc2.x;\n          r32 dvy = veloc1.y - veloc2.y;\n          r32 dvz = veloc1.z - veloc2.z;\n\n          r32 l0 = bond_r0[ i + p*padding ];\n          r32 ra = sqrtf(delx*delx + dely*dely + delz*delz);\n          r32 lmax = l0*r0[type];\n          r32 rr = 1.0f/r0[type];\n          r32 sr = (1.0f-rr)*(1.0f-rr);\n          r32 kph = powf(l0,qp[type])*temp[type]*(0.25f/sr-0.25f+rr);\n          \n\n          r32 mu = 0.433f*(   \n\n\t           temp[type]*(-0.25f/sr + 0.25f + \n             0.5f*rr/(sr*(1.0f-rr)))/(lmax*rr) +\n             kph*(qp[type]+1.0f)/powf(l0,qp[type]+1.0f));\n          r32 lambda = mu/mu_targ[type];\n          kph = kph/lambda;\n          rr = ra/lmax;\n          r32 rlogarg = powf(ra,qp[type]+1.0f);\n          r32 vv = (delx*dvx + dely*dvy + delz*dvz)/ra;\n\n          if (rr >= 0.99) rr = 0.99f;\n          if (rlogarg < 0.01) rlogarg = 0.01f;\n\n          float4 wrr;\n          r32 ww[3][3];\n\n          for (int tes=0; tes<3; tes++) {\n            for (int see=0; see<3; see++) {\n              int v1 = *((int*)&(veloc1.w));\n              int v2 = *((int*)&(veloc2.w));\n              ww[tes][see] = gaussian_TEA_fast<4>(v1 > v2, v1+tes, v2+see);\n            }\n          }\n\n          wrr.w = (ww[0][0]+ww[1][1]+ww[2][2])/3.0f;\n          wrr.x = (ww[0][0]-wrr.w)*delx + 0.5f*(ww[0][1]+ww[1][0])*dely + 0.5f*(ww[0][2]+ww[2][0])*delz;\n          wrr.y = 0.5f*(ww[1][0]+ww[0][1])*delx + (ww[1][1]-wrr.w)*dely + 0.5f*(ww[1][2]+ww[2][1])*delz;\n          wrr.z = 0.5f*(ww[2][0]+ww[0][2])*delx + 0.5f*(ww[2][1]+ww[1][2])*dely + (ww[2][2]-wrr.w)*delz;\n\n          r32 fforce = -temp[type]*(0.25f/(1.0f-rr)/(1.0f-rr)-0.25f+rr)/lambda/ra + \n                       kph/rlogarg + (sigc[type]*wrr.w - gamc[type]*vv)/ra;\n          r32 fxij = delx*fforce - gamt[type]*dvx + sigt[type]*wrr.x/ra;\n          r32 fyij = dely*fforce - gamt[type]*dvy + sigt[type]*wrr.y/ra;\n          r32 fzij = delz*fforce - gamt[type]*dvz + sigt[type]*wrr.z/ra;\n\n          fxi += fxij;\n          fyi += fyij;\n          fzi += fzij;\n        }\n        force_x[i] += fxi;\n        force_y[i] += fyi;\n        force_z[i] += fzi;\n      }\n    }\n  }\n}\n\ntemplate <typename T>\nT* resize (int n) {\n  return (T*) malloc (sizeof(T) * n);\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: ./%s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int repeat = atoi(argv[1]);\n\n  int i;\n\n  \n\n\n  float3 period = {0.5f, 0.5f, 0.5f};\n  int padding = 1;\n  int n = 1e6;  \n\n\n  float4 *coord_merged = resize<float4>(n+1);\n  float4 *veloc = resize<float4>(n+1);\n  int *nbond = resize<int>(n+1);\n\n  \n\n  int2 *bonds = resize<int2>(n+n+1);\n  r64 *bond_r0 = resize<r64>(n+n+1);\n\n  r64 *force_x = resize<r64>(n+1);\n  r64 *force_y = resize<r64>(n+1);\n  r64 *force_z = resize<r64>(n+1);\n\n  r32 *bond_l0 = resize<r32>(n+1);\n  r32 *temp = resize<r32>(n+1);\n  r32 *mu_targ = resize<r32>(n+1);\n  r32 *qp = resize<r32>(n+1);\n  r32 *gamc = resize<r32>(n+1);\n  r32 *gamt = resize<r32>(n+1);\n  r32 *sigc = resize<r32>(n+1);\n  r32 *sigt = resize<r32>(n+1);\n\n  std::mt19937 g (19937);\n  std::uniform_real_distribution<r64> dist_r64(0.1, 0.9);\n  std::uniform_real_distribution<r32> dist_r32(0.1, 0.9);\n  std::uniform_int_distribution<i32> dist_i32(0, n_type);\n\n  for (i = 0; i < n + n + 1; i++) {\n    bond_r0[i] = dist_r64(g) + 0.001;\n    \n\n    bonds[i] = { (i+1)%(n+1), \n                 dist_i32(g) };\n  }\n\n\n  for (i = 0; i < n + 1; i++) {\n    force_x[i] = force_y[i] = force_z[i] = 0.f;\n    nbond[i] = dist_i32(g);\n    coord_merged[i] = {dist_r32(g), dist_r32(g), dist_r32(g), 0};\n    r32 vx = dist_r32(g), vy = dist_r32(g), vz = dist_r32(g);\n    veloc[i] = {vx, vy, vz, sqrtf(vx*vx+vy*vy+vz*vz)};\n\n    bond_l0[i] = dist_r32(g);\n    gamt[i] = dist_r32(g);\n    gamc[i] = ((dist_i32(g) % 4) + 4) * gamt[i]; \n\n    temp[i] = dist_r32(g);\n    mu_targ[i] = dist_r32(g);\n    qp[i] = dist_r32(g);\n    sigc[i] = sqrt(2.0*temp[i]*(3.0*gamc[i]-gamt[i]));\n    sigt[i] = 2.0*sqrt(gamt[i]*temp[i]);\n  }\n\n#pragma omp target data \\\n  map (to: coord_merged[0:n+1], \\\n           veloc[0:n+1], \\\n           nbond[0:n+1], \\\n           bonds[0:n+n+1], \\\n           bond_r0[0:n+n+1], \\\n           bond_l0[0:n+1], \\\n           temp[0:n+1], \\\n           mu_targ[0:n+1], \\\n           qp[0:n+1], \\\n           gamc[0:n+1], \\\n           gamt[0:n+1], \\\n           sigc[0:n+1], \\\n           sigt[0:n+1]) \\\n  map(tofrom: force_x[0:n+1],\\\n              force_y[0:n+1],\\\n              force_z[0:n+1])\n  {\n    const int teams = (n + 127) / 128;\n    const int blocks = 128;\n\n    auto start = std::chrono::steady_clock::now();\n\n    \n\n    for (i = 0; i < repeat; i++) {\n      bond_wlcpowallvisc (\n        force_x,\n        force_y,\n        force_z,\n        coord_merged,\n        veloc,\n        nbond,\n        bonds,\n        bond_r0,\n        temp,\n        bond_l0,\n        mu_targ,\n        qp,\n        gamc,\n        gamt,\n        sigc,\n        sigt,\n        period,\n        padding,\n        n,\n        teams,\n        blocks);\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (us)\\n\", time * 1e-3f / repeat);\n  }\n\n  \n\n  for (i = 0; i < n+1; i++) {\n    bool r = (isnan(force_x[i]) || isnan(force_y[i]) || isnan(force_z[i]));\n    if (r) printf(\"There are NaN numbers at index %d\\n\", i);\n  }\n\n  double force_x_sum = 0, force_y_sum = 0, force_z_sum = 0;\n  for (i = 0; i < n+1; i++) {\n    force_x_sum += force_x[i];\n    force_y_sum += force_y[i];\n    force_z_sum += force_z[i];\n  }\n  \n\n  printf(\"checksum: forceX=%lf forceY=%lf forceZ=%lf\\n\",\n    force_x_sum/(n+1), force_y_sum/(n+1), force_z_sum/(n+1));\n  \n#ifdef DEBUG\n  for (i = 0; i < 16; i++) {\n    printf(\"%d %lf %lf %lf\\n\", i, force_x[i], force_y[i], force_z[i]);\n  }\n#endif\n\n  free(coord_merged);\n  free(veloc);\n  free(force_x);\n  free(force_y);\n  free(force_z);\n  free(nbond);\n  free(bonds);\n  free(bond_r0);\n  free(bond_l0);\n  free(temp);\n  free(mu_targ);\n  free(qp);\n  free(gamc);\n  free(gamt);\n  free(sigc);\n  free(sigt);\n  return 0;\n}\n"}}
{"kernel_name": "wlcpow", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <random>\n#include <chrono>\n#include \"utils.h\"\n\n#define n_type 32\n\nvoid bond_wlcpowallvisc(\n             r64* __restrict force_x,\n             r64* __restrict force_y,\n             r64* __restrict force_z,\n    const float4* __restrict coord_merged,\n    const float4* __restrict veloc,\n    const int*  __restrict nbond,\n    const int2* __restrict bonds,\n    const r64* __restrict bond_r0,\n    const r32* __restrict temp_global,\n    const r32* __restrict r0_global,\n    const r32* __restrict mu_targ_global,\n    const r32* __restrict qp_global,\n    const r32* __restrict gamc_global,\n    const r32* __restrict gamt_global,\n    const r32* __restrict sigc_global,\n    const r32* __restrict sigt_global,\n    const float3 period,\n    const int padding,\n    const int n_local,\n    const int teams,\n    const int blocks )\n{\n    {\n    r32 shared_data[(n_type + 1) * 8];  \n\n        {\n      int threadIdx_x = omp_get_thread_num();\n      int blockIdx_x = omp_get_team_num();\n\n      r32* temp    = &shared_data[0];\n      r32* r0      = &shared_data[1*(n_type+1)];\n      r32* mu_targ = &shared_data[2*(n_type+1)];\n      r32* qp      = &shared_data[3*(n_type+1)];\n      r32* gamc    = &shared_data[4*(n_type+1)];\n      r32* gamt    = &shared_data[5*(n_type+1)];\n      r32* sigc    = &shared_data[6*(n_type+1)];\n      r32* sigt    = &shared_data[7*(n_type+1)];\n\n      for ( int i = threadIdx_x; i < n_type + 1; i += blocks ) {\n        temp[i]    = temp_global[i];\n        r0[i]      = r0_global[i];\n        mu_targ[i] = mu_targ_global[i];\n        qp[i]      = qp_global[i];\n        gamc[i]    = gamc_global[i];\n        gamt[i]    = gamt_global[i];\n        sigc[i]    = sigc_global[i];\n        sigt[i]    = sigt_global[i];\n      }\n      \n      for( int i = blockIdx_x * blocks + threadIdx_x;\n               i < n_local ; i += teams * blocks ) {\n\n        int n = nbond[i];\n        float4 coord1 = coord_merged[i];\n        float4 veloc1 = veloc[i];\n        r32 fxi = 0.f, fyi = 0.f, fzi = 0.f;\n\n        for( int p = 0; p < n; p++ ) {\n          int j = bonds[ i + p*padding ].x;\n          int type = bonds[ i + p*padding ].y;\n          float4 coord2 = coord_merged[j];\n          r32 delx = minimum_image( coord1.x - coord2.x, period.x );\n          r32 dely = minimum_image( coord1.y - coord2.y, period.y );\n          r32 delz = minimum_image( coord1.z - coord2.z, period.z );\n          float4 veloc2 = veloc[j];\n          r32 dvx = veloc1.x - veloc2.x;\n          r32 dvy = veloc1.y - veloc2.y;\n          r32 dvz = veloc1.z - veloc2.z;\n\n          r32 l0 = bond_r0[ i + p*padding ];\n          r32 ra = sqrtf(delx*delx + dely*dely + delz*delz);\n          r32 lmax = l0*r0[type];\n          r32 rr = 1.0f/r0[type];\n          r32 sr = (1.0f-rr)*(1.0f-rr);\n          r32 kph = powf(l0,qp[type])*temp[type]*(0.25f/sr-0.25f+rr);\n          \n\n          r32 mu = 0.433f*(   \n\n\t           temp[type]*(-0.25f/sr + 0.25f + \n             0.5f*rr/(sr*(1.0f-rr)))/(lmax*rr) +\n             kph*(qp[type]+1.0f)/powf(l0,qp[type]+1.0f));\n          r32 lambda = mu/mu_targ[type];\n          kph = kph/lambda;\n          rr = ra/lmax;\n          r32 rlogarg = powf(ra,qp[type]+1.0f);\n          r32 vv = (delx*dvx + dely*dvy + delz*dvz)/ra;\n\n          if (rr >= 0.99) rr = 0.99f;\n          if (rlogarg < 0.01) rlogarg = 0.01f;\n\n          float4 wrr;\n          r32 ww[3][3];\n\n          for (int tes=0; tes<3; tes++) {\n            for (int see=0; see<3; see++) {\n              int v1 = *((int*)&(veloc1.w));\n              int v2 = *((int*)&(veloc2.w));\n              ww[tes][see] = gaussian_TEA_fast<4>(v1 > v2, v1+tes, v2+see);\n            }\n          }\n\n          wrr.w = (ww[0][0]+ww[1][1]+ww[2][2])/3.0f;\n          wrr.x = (ww[0][0]-wrr.w)*delx + 0.5f*(ww[0][1]+ww[1][0])*dely + 0.5f*(ww[0][2]+ww[2][0])*delz;\n          wrr.y = 0.5f*(ww[1][0]+ww[0][1])*delx + (ww[1][1]-wrr.w)*dely + 0.5f*(ww[1][2]+ww[2][1])*delz;\n          wrr.z = 0.5f*(ww[2][0]+ww[0][2])*delx + 0.5f*(ww[2][1]+ww[1][2])*dely + (ww[2][2]-wrr.w)*delz;\n\n          r32 fforce = -temp[type]*(0.25f/(1.0f-rr)/(1.0f-rr)-0.25f+rr)/lambda/ra + \n                       kph/rlogarg + (sigc[type]*wrr.w - gamc[type]*vv)/ra;\n          r32 fxij = delx*fforce - gamt[type]*dvx + sigt[type]*wrr.x/ra;\n          r32 fyij = dely*fforce - gamt[type]*dvy + sigt[type]*wrr.y/ra;\n          r32 fzij = delz*fforce - gamt[type]*dvz + sigt[type]*wrr.z/ra;\n\n          fxi += fxij;\n          fyi += fyij;\n          fzi += fzij;\n        }\n        force_x[i] += fxi;\n        force_y[i] += fyi;\n        force_z[i] += fzi;\n      }\n    }\n  }\n}\n\ntemplate <typename T>\nT* resize (int n) {\n  return (T*) malloc (sizeof(T) * n);\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: ./%s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int repeat = atoi(argv[1]);\n\n  int i;\n\n  \n\n\n  float3 period = {0.5f, 0.5f, 0.5f};\n  int padding = 1;\n  int n = 1e6;  \n\n\n  float4 *coord_merged = resize<float4>(n+1);\n  float4 *veloc = resize<float4>(n+1);\n  int *nbond = resize<int>(n+1);\n\n  \n\n  int2 *bonds = resize<int2>(n+n+1);\n  r64 *bond_r0 = resize<r64>(n+n+1);\n\n  r64 *force_x = resize<r64>(n+1);\n  r64 *force_y = resize<r64>(n+1);\n  r64 *force_z = resize<r64>(n+1);\n\n  r32 *bond_l0 = resize<r32>(n+1);\n  r32 *temp = resize<r32>(n+1);\n  r32 *mu_targ = resize<r32>(n+1);\n  r32 *qp = resize<r32>(n+1);\n  r32 *gamc = resize<r32>(n+1);\n  r32 *gamt = resize<r32>(n+1);\n  r32 *sigc = resize<r32>(n+1);\n  r32 *sigt = resize<r32>(n+1);\n\n  std::mt19937 g (19937);\n  std::uniform_real_distribution<r64> dist_r64(0.1, 0.9);\n  std::uniform_real_distribution<r32> dist_r32(0.1, 0.9);\n  std::uniform_int_distribution<i32> dist_i32(0, n_type);\n\n  for (i = 0; i < n + n + 1; i++) {\n    bond_r0[i] = dist_r64(g) + 0.001;\n    \n\n    bonds[i] = { (i+1)%(n+1), \n                 dist_i32(g) };\n  }\n\n\n  for (i = 0; i < n + 1; i++) {\n    force_x[i] = force_y[i] = force_z[i] = 0.f;\n    nbond[i] = dist_i32(g);\n    coord_merged[i] = {dist_r32(g), dist_r32(g), dist_r32(g), 0};\n    r32 vx = dist_r32(g), vy = dist_r32(g), vz = dist_r32(g);\n    veloc[i] = {vx, vy, vz, sqrtf(vx*vx+vy*vy+vz*vz)};\n\n    bond_l0[i] = dist_r32(g);\n    gamt[i] = dist_r32(g);\n    gamc[i] = ((dist_i32(g) % 4) + 4) * gamt[i]; \n\n    temp[i] = dist_r32(g);\n    mu_targ[i] = dist_r32(g);\n    qp[i] = dist_r32(g);\n    sigc[i] = sqrt(2.0*temp[i]*(3.0*gamc[i]-gamt[i]));\n    sigt[i] = 2.0*sqrt(gamt[i]*temp[i]);\n  }\n\n  {\n    const int teams = (n + 127) / 128;\n    const int blocks = 128;\n\n    auto start = std::chrono::steady_clock::now();\n\n    \n\n    for (i = 0; i < repeat; i++) {\n      bond_wlcpowallvisc (\n        force_x,\n        force_y,\n        force_z,\n        coord_merged,\n        veloc,\n        nbond,\n        bonds,\n        bond_r0,\n        temp,\n        bond_l0,\n        mu_targ,\n        qp,\n        gamc,\n        gamt,\n        sigc,\n        sigt,\n        period,\n        padding,\n        n,\n        teams,\n        blocks);\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (us)\\n\", time * 1e-3f / repeat);\n  }\n\n  \n\n  for (i = 0; i < n+1; i++) {\n    bool r = (isnan(force_x[i]) || isnan(force_y[i]) || isnan(force_z[i]));\n    if (r) printf(\"There are NaN numbers at index %d\\n\", i);\n  }\n\n  double force_x_sum = 0, force_y_sum = 0, force_z_sum = 0;\n  for (i = 0; i < n+1; i++) {\n    force_x_sum += force_x[i];\n    force_y_sum += force_y[i];\n    force_z_sum += force_z[i];\n  }\n  \n\n  printf(\"checksum: forceX=%lf forceY=%lf forceZ=%lf\\n\",\n    force_x_sum/(n+1), force_y_sum/(n+1), force_z_sum/(n+1));\n  \n#ifdef DEBUG\n  for (i = 0; i < 16; i++) {\n    printf(\"%d %lf %lf %lf\\n\", i, force_x[i], force_y[i], force_z[i]);\n  }\n#endif\n\n  free(coord_merged);\n  free(veloc);\n  free(force_x);\n  free(force_y);\n  free(force_z);\n  free(nbond);\n  free(bonds);\n  free(bond_r0);\n  free(bond_l0);\n  free(temp);\n  free(mu_targ);\n  free(qp);\n  free(gamc);\n  free(gamt);\n  free(sigc);\n  free(sigt);\n  return 0;\n}"}}
{"kernel_name": "wlcpow", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <random>\n#include <chrono>\n#include <sycl/sycl.hpp>\n#include \"utils.h\"\n\nvoid bond_wlcpowallvisc(\n    r64 *__restrict force_x, r64 *__restrict force_y,\n    r64 *__restrict force_z, const float4 *__restrict coord_merged,\n    const float4 *__restrict veloc, const int *__restrict nbond,\n    const int2 *__restrict bonds, const r64 *__restrict bond_r0,\n    const r32 *__restrict temp_global, const r32 *__restrict r0_global,\n    const r32 *__restrict mu_targ_global, const r32 *__restrict qp_global,\n    const r32 *__restrict gamc_global, const r32 *__restrict gamt_global,\n    const r32 *__restrict sigc_global, const r32 *__restrict sigt_global,\n    const float3 period, const int padding, const int n_type,\n    const int n_local, sycl::nd_item<1> &item, r32 *__restrict shared_data)\n{\n  r32* temp    = &shared_data[0];\n  r32* r0      = &shared_data[1*(n_type+1)];\n  r32* mu_targ = &shared_data[2*(n_type+1)];\n  r32* qp      = &shared_data[3*(n_type+1)];\n  r32* gamc    = &shared_data[4*(n_type+1)];\n  r32* gamt    = &shared_data[5*(n_type+1)];\n  r32* sigc    = &shared_data[6*(n_type+1)];\n  r32* sigt    = &shared_data[7*(n_type+1)];\n\n  int threadIdx_x = item.get_local_id(0);\n  int blockIdx_x = item.get_group(0);\n  int blockDim_x = item.get_local_range(0);\n  int gridDim_x = item.get_group_range(0);\n\n  for ( int i = threadIdx_x; i < n_type + 1; i += blockDim_x ) {\n    temp[i]    = temp_global[i];\n    r0[i]      = r0_global[i];\n    mu_targ[i] = mu_targ_global[i];\n    qp[i]      = qp_global[i];\n    gamc[i]    = gamc_global[i];\n    gamt[i]    = gamt_global[i];\n    sigc[i]    = sigc_global[i];\n    sigt[i]    = sigt_global[i];\n  }\n  item.barrier(sycl::access::fence_space::local_space);\n\n  for( int i = blockIdx_x * blockDim_x + threadIdx_x;\n           i < n_local ; i += gridDim_x * blockDim_x ) {\n    int n = nbond[i];\n    float4 coord1 = coord_merged[i];\n    float4 veloc1 = veloc[i];\n    r32 fxi = 0.f, fyi = 0.f, fzi = 0.f;\n\n    for( int p = 0; p < n; p++ ) {\n      int j = bonds[i + p * padding].x();\n      int type = bonds[i + p * padding].y();\n      float4 coord2 = coord_merged[j];\n      r32 delx = minimum_image(coord1.x() - coord2.x(), period.x());\n      r32 dely = minimum_image(coord1.y() - coord2.y(), period.y());\n      r32 delz = minimum_image(coord1.z() - coord2.z(), period.z());\n      float4 veloc2 = veloc[j];\n      r32 dvx = veloc1.x() - veloc2.x();\n      r32 dvy = veloc1.y() - veloc2.y();\n      r32 dvz = veloc1.z() - veloc2.z();\n\n      r32 l0 = bond_r0[ i + p*padding ];\n      r32 ra = sycl::sqrt(delx * delx + dely * dely + delz * delz);\n      r32 lmax = l0*r0[type];\n      r32 rr = 1.0f/r0[type];\n      r32 sr = (1.0f-rr)*(1.0f-rr);\n      r32 kph = sycl::pow<float>(l0, qp[type]) * temp[type] *\n                (0.25f / sr - 0.25f + rr);\n      \n\n      r32 mu = 0.433f * ( \n\n         temp[type] * (-0.25f / sr + 0.25f +\n         0.5f * rr / (sr * (1.0f - rr))) /\n         (lmax * rr) + kph * (qp[type] + 1.0f) /\n         sycl::pow<float>(l0, qp[type] + 1.0f));\n      r32 lambda = mu/mu_targ[type];\n      kph = kph/lambda;\n      rr = ra/lmax;\n      r32 rlogarg = sycl::pow<float>(ra, qp[type] + 1.0f);\n      r32 vv = (delx*dvx + dely*dvy + delz*dvz)/ra;\n\n      if (rr >= 0.99) rr = 0.99f;\n      if (rlogarg < 0.01) rlogarg = 0.01f;\n\n      float4 wrr;\n      r32 ww[3][3];\n\n      for (int tes=0; tes<3; tes++) {\n        for (int see=0; see<3; see++) {\n          int v1 = sycl::bit_cast<int>(veloc1.w());\n          int v2 = sycl::bit_cast<int>(veloc2.w());\n          ww[tes][see] = gaussian_TEA_fast<4>(v1 > v2, v1+tes, v2+see);\n        }\n      }\n\n      wrr.w() = (ww[0][0] + ww[1][1] + ww[2][2]) / 3.0f;\n      wrr.x() = (ww[0][0] - wrr.w()) * delx +\n                0.5f * (ww[0][1] + ww[1][0]) * dely +\n                0.5f * (ww[0][2] + ww[2][0]) * delz;\n      wrr.y() = 0.5f * (ww[1][0] + ww[0][1]) * delx +\n                (ww[1][1] - wrr.w()) * dely +\n                0.5f * (ww[1][2] + ww[2][1]) * delz;\n      wrr.z() = 0.5f * (ww[2][0] + ww[0][2]) * delx +\n                0.5f * (ww[2][1] + ww[1][2]) * dely +\n                (ww[2][2] - wrr.w()) * delz;\n\n      r32 fforce = -temp[type] * (0.25f / (1.0f - rr) / (1.0f - rr) - 0.25f + rr) /\n              lambda / ra + kph / rlogarg + (sigc[type] * wrr.w() - gamc[type] * vv) / ra;\n      r32 fxij = delx * fforce - gamt[type] * dvx + sigt[type] * wrr.x() / ra;\n      r32 fyij = dely * fforce - gamt[type] * dvy + sigt[type] * wrr.y() / ra;\n      r32 fzij = delz * fforce - gamt[type] * dvz + sigt[type] * wrr.z() / ra;\n\n      fxi += fxij;\n      fyi += fyij;\n      fzi += fzij;\n    }\n    force_x[i] += fxi;\n    force_y[i] += fyi;\n    force_z[i] += fzi;\n  }\n}\n\ntemplate <typename T>\nT* resize (int n) {\n  return (T*) malloc (sizeof(T) * n);\n}\n\ntemplate <typename T>\nT* grow (sycl::queue &q, int n) {\n  return sycl::malloc_device<T>(n, q);\n}\n\ntemplate <typename T>\nvoid upload(sycl::queue &q, T* d, T* h, int n) {\n  q.memcpy(d, h, sizeof(T) * n);\n}\n\ntemplate <typename T>\nvoid reset(sycl::queue &q, T* d, int n) {\n  q.memset(d, (T)0, sizeof(T) * n);\n}\n\ntemplate <typename T>\nvoid download(sycl::queue &q, T* h, T* d, int n) {\n  q.memcpy(h, d, sizeof(T) * n);\n}\n\nint main(int argc, char *argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: ./%s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int repeat = atoi(argv[1]);\n\n  int i;\n\n  \n\n\n  float3 period = {0.5f, 0.5f, 0.5f};\n  int padding = 1;\n  int n_type = 32;\n  int n = 1e6;  \n\n\n  float4 *coord_merged = resize<float4>(n + 1);\n  float4 *veloc = resize<float4>(n + 1);\n  int *nbond = resize<int>(n+1);\n\n  \n\n  int2 *bonds = resize<int2>(n + n + 1);\n  r64 *bond_r0 = resize<r64>(n+n+1);\n\n  r64 *force_x = resize<r64>(n+1);\n  r64 *force_y = resize<r64>(n+1);\n  r64 *force_z = resize<r64>(n+1);\n\n  r32 *bond_l0 = resize<r32>(n+1);\n  r32 *temp = resize<r32>(n+1);\n  r32 *mu_targ = resize<r32>(n+1);\n  r32 *qp = resize<r32>(n+1);\n  r32 *gamc = resize<r32>(n+1);\n  r32 *gamt = resize<r32>(n+1);\n  r32 *sigc = resize<r32>(n+1);\n  r32 *sigt = resize<r32>(n+1);\n\n  std::mt19937 g (19937);\n  std::uniform_real_distribution<r64> dist_r64(0.1, 0.9);\n  std::uniform_real_distribution<r32> dist_r32(0.1, 0.9);\n  std::uniform_int_distribution<i32> dist_i32(0, n_type);\n\n  for (i = 0; i < n + n + 1; i++) {\n    bond_r0[i] = dist_r64(g) + 0.001;\n    \n\n    bonds[i] = { (i+1)%(n+1),\n                 dist_i32(g) };\n  }\n\n  for (i = 0; i < n + 1; i++) {\n    nbond[i] = dist_i32(g);\n    coord_merged[i] = {dist_r32(g), dist_r32(g), dist_r32(g), 0};\n    r32 vx = dist_r32(g), vy = dist_r32(g), vz = dist_r32(g);\n    veloc[i] = {vx, vy, vz, sqrtf(vx*vx+vy*vy+vz*vz)};\n    bond_l0[i] = dist_r32(g);\n    gamt[i] = dist_r32(g);\n    gamc[i] = ((dist_i32(g) % 4) + 4) * gamt[i]; \n\n    temp[i] = dist_r32(g);\n    mu_targ[i] = dist_r32(g);\n    qp[i] = dist_r32(g);\n    sigc[i] = sqrt(2.0*temp[i]*(3.0*gamc[i]-gamt[i]));\n    sigt[i] = 2.0*sqrt(gamt[i]*temp[i]);\n  }\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  float4 *dev_coord_merged = grow<float4>(q, n + 1);\n  float4 *dev_veloc = grow<float4>(q, n + 1);\n  int *dev_nbond = grow<int>(q, n + 1);\n  int2 *dev_bonds = grow<int2>(q, n + n + 1);\n  r64 *dev_bond_r0 = grow<r64>(q, n + n + 1);\n\n  r64 *dev_force_x = grow<r64>(q, n + 1);\n  r64 *dev_force_y = grow<r64>(q, n + 1);\n  r64 *dev_force_z = grow<r64>(q, n + 1);\n\n  r32 *dev_bond_l0 = grow<r32>(q, n + 1);\n  r32 *dev_temp = grow<r32>(q, n + 1);\n  r32 *dev_mu_targ = grow<r32>(q, n + 1);\n  r32 *dev_qp = grow<r32>(q, n + 1);\n  r32 *dev_gamc = grow<r32>(q, n + 1);\n  r32 *dev_gamt = grow<r32>(q, n + 1);\n  r32 *dev_sigc = grow<r32>(q, n + 1);\n  r32 *dev_sigt = grow<r32>(q, n + 1);\n\n  reset (q, dev_force_x, n+1);\n  reset (q, dev_force_y, n+1);\n  reset (q, dev_force_z, n+1);\n  upload (q, dev_coord_merged, coord_merged, n+1);\n  upload (q, dev_veloc, veloc, n+1);\n  upload (q, dev_nbond, nbond, n+1);\n  upload (q, dev_bonds, bonds, n+n+1);\n  upload (q, dev_bond_r0, bond_r0, n+n+1);\n  upload (q, dev_temp, temp, n+1);\n  upload (q, dev_bond_l0, bond_l0, n+1);\n  upload (q, dev_mu_targ, mu_targ, n+1);\n  upload (q, dev_qp, qp, n+1);\n  upload (q, dev_gamc, gamc, n+1);\n  upload (q, dev_gamt, gamt, n+1);\n  upload (q, dev_sigc, sigc, n+1);\n  upload (q, dev_sigt, sigt, n+1);\n\n  sycl::range<1> gws ((n + 127) / 128 * 128);\n  sycl::range<1> lws (128);\n\n  const int sm_size = (n_type + 1) * 8;\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  \n\n  for (i = 0; i < repeat; i++) {\n    q.submit([&](sycl::handler &cgh) {\n      sycl::local_accessor<r32, 1> sm (sycl::range<1>(sm_size), cgh);\n      cgh.parallel_for(sycl::nd_range<1>(gws, lws), [=](sycl::nd_item<1> item) {\n        bond_wlcpowallvisc(dev_force_x, dev_force_y, dev_force_z,\n                           dev_coord_merged, dev_veloc, dev_nbond, dev_bonds,\n                           dev_bond_r0, dev_temp, dev_bond_l0, dev_mu_targ,\n                           dev_qp, dev_gamc, dev_gamt, dev_sigc, dev_sigt,\n                           period, padding, n_type, n, item,\n                           sm.get_pointer());\n      });\n    });\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", time * 1e-3f / repeat);\n\n  download (q, force_x, dev_force_x, n+1);\n  download (q, force_y, dev_force_y, n+1);\n  download (q, force_z, dev_force_z, n+1);\n\n  q.wait();\n\n  \n\n  for (i = 0; i < n+1; i++) {\n    bool r = (isnan(force_x[i]) || isnan(force_y[i]) || isnan(force_z[i]));\n    if (r) printf(\"There are NaN numbers at index %d\\n\", i);\n  }\n\n  double force_x_sum = 0, force_y_sum = 0, force_z_sum = 0;\n  for (i = 0; i < n+1; i++) {\n    force_x_sum += force_x[i];\n    force_y_sum += force_y[i];\n    force_z_sum += force_z[i];\n  }\n  \n\n  printf(\"checksum: forceX=%lf forceY=%lf forceZ=%lf\\n\",\n    force_x_sum/(n+1), force_y_sum/(n+1), force_z_sum/(n+1));\n\n#ifdef DEBUG\n  for (i = 0; i < 16; i++) {\n    printf(\"%d %lf %lf %lf\\n\", i, force_x[i], force_y[i], force_z[i]);\n  }\n#endif\n\n  free(coord_merged);\n  free(veloc);\n  free(force_x);\n  free(force_y);\n  free(force_z);\n  free(nbond);\n  free(bonds);\n  free(bond_r0);\n  free(bond_l0);\n  free(temp);\n  free(mu_targ);\n  free(qp);\n  free(gamc);\n  free(gamt);\n  free(sigc);\n  free(sigt);\n\n  sycl::free(dev_coord_merged, q);\n  sycl::free(dev_veloc, q);\n  sycl::free(dev_force_x, q);\n  sycl::free(dev_force_y, q);\n  sycl::free(dev_force_z, q);\n  sycl::free(dev_nbond, q);\n  sycl::free(dev_bonds, q);\n  sycl::free(dev_bond_r0, q);\n  sycl::free(dev_bond_l0, q);\n  sycl::free(dev_temp, q);\n  sycl::free(dev_mu_targ, q);\n  sycl::free(dev_qp, q);\n  sycl::free(dev_gamc, q);\n  sycl::free(dev_gamt, q);\n  sycl::free(dev_sigc, q);\n  sycl::free(dev_sigt, q);\n  return 0;\n}\n"}}
{"kernel_name": "wsm5", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <cuda.h>\n#include \"utils.h\"\n#include \"kernel.h\"\n\nint main(int argc, char* argv[])\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n  float *th, *pii, *q;\n  float *qc, *qi, *qr, *qs;\n  float *den, *p, *delz;\n  float *rain,*rainncv;\n  float *sr;\n  float *snow, *snowncv;\n\n  float delt = 10.f;\n  int ims = 0, ime = 59, jms = 0, jme = 45, kms = 0, kme = 2;\n  int ips = 0, ipe = 59, jps = 0, jpe = 45, kps = 0, kpe = 2;\n  int d3 = (ime-ims+1) * (jme-jms+1) * (kme-kms+1) ;\n  int d2 = (ime-ims+1) * (jme-jms+1) ;\n\n  int dips = 0 ; int dipe = (ipe-ips+1) ;\n  int djps = 0 ; int djpe = (jpe-jps+1) ;\n  int dkps = 0 ; int dkpe = (kpe-kps+1) ;\n\n  int remx = (ipe-ips+1) % XXX != 0 ? 1 : 0 ;\n  int remy = (jpe-jps+1) % YYY != 0 ? 1 : 0 ;\n\n  dim3 dimBlock( XXX , YYY ) ;\n  dim3 dimGrid ( (ipe-ips+1) / XXX + remx , (jpe-jps+1) / YYY + remy ) ;\n\n  float rain_sum = 0, snow_sum = 0;\n\n  long time = 0;\n  for (int i = 0; i < repeat; i++) {\n    \n\n    TODEV3(pii) ;\n    TODEV3(den) ;\n    TODEV3(p) ;\n    TODEV3(delz) ;\n\n    TODEV3(th) ;\n    TODEV3(q) ;\n    TODEV3(qc) ;\n    TODEV3(qi) ;\n    TODEV3(qr) ;\n    TODEV3(qs) ;\n    TODEV2(rain) ;\n    TODEV2(rainncv) ;\n    TODEV2(sr) ;\n    TODEV2(snow) ;\n    TODEV2(snowncv) ;\n\n    cudaDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    wsm <<< dimGrid, dimBlock >>> (\n      th_d, pii_d, q_d, qc_d, qi_d, qr_d, qs_d, den_d, p_d, delz_d,\n      rain_d, rainncv_d,\n      sr_d,\n      snow_d, snowncv_d,\n      delt,\n      dips+1 , (ipe-ips+1) , \n\n      djps+1 , (jpe-jps+1) , \n\n      dkps+1 , (kpe-kps+1),  \n\n      dips+1 , dipe ,        \n\n      djps+1 , djpe ,        \n\n      dkps+1 , dkpe,         \n\n      dips+1 , dipe ,        \n\n      djps+1 , djpe ,        \n\n      dkps+1 , dkpe) ;       \n\n\n    cudaDeviceSynchronize() ;\n    auto end = std::chrono::steady_clock::now();\n    time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n    FROMDEV2(rain) ;\n    FROMDEV2(snow) ;\n\n    rain_sum = snow_sum = 0;\n    for (int i = 0; i < d2; i++) {\n      rain_sum += rain[i];\n      snow_sum += snow[i];\n    }\n\n    FREE(pii) ;\n    FREE(den) ;\n    FREE(p) ;\n    FREE(delz) ;\n    FREE(th) ;\n    FREE(q) ;\n    FREE(qc) ;\n    FREE(qi) ;\n    FREE(qr) ;\n    FREE(qs) ;\n    FREE(rain) ;\n    FREE(rainncv) ;\n    FREE(sr) ;\n    FREE(snow) ;\n    FREE(snowncv) ;\n  }\n\n  printf(\"Average kernel execution time: %lf (ms)\\n\", (time * 1e-6) / repeat);\n  printf(\"Checksum: rain = %f snow = %f\\n\", rain_sum, snow_sum);\n\n  return(0) ;\n}\n"}}
{"kernel_name": "wsm5", "parallel_api": "hip", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n#include \"utils.h\"\n#include \"kernel.h\"\n\nint main(int argc, char* argv[])\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n  float *th, *pii, *q;\n  float *qc, *qi, *qr, *qs;\n  float *den, *p, *delz;\n  float *rain,*rainncv;\n  float *sr;\n  float *snow, *snowncv;\n\n  float delt = 10.f;\n  int ims = 0, ime = 59, jms = 0, jme = 45, kms = 0, kme = 2;\n  int ips = 0, ipe = 59, jps = 0, jpe = 45, kps = 0, kpe = 2;\n  int d3 = (ime-ims+1) * (jme-jms+1) * (kme-kms+1) ;\n  int d2 = (ime-ims+1) * (jme-jms+1) ;\n\n  int dips = 0 ; int dipe = (ipe-ips+1) ;\n  int djps = 0 ; int djpe = (jpe-jps+1) ;\n  int dkps = 0 ; int dkpe = (kpe-kps+1) ;\n\n  int remx = (ipe-ips+1) % XXX != 0 ? 1 : 0 ;\n  int remy = (jpe-jps+1) % YYY != 0 ? 1 : 0 ;\n\n  dim3 dimBlock( XXX , YYY ) ;\n  dim3 dimGrid ( (ipe-ips+1) / XXX + remx , (jpe-jps+1) / YYY + remy ) ;\n\n  float rain_sum = 0, snow_sum = 0;\n\n  long time = 0;\n  for (int i = 0; i < repeat; i++) {\n    \n\n    TODEV3(pii) ;\n    TODEV3(den) ;\n    TODEV3(p) ;\n    TODEV3(delz) ;\n\n    TODEV3(th) ;\n    TODEV3(q) ;\n    TODEV3(qc) ;\n    TODEV3(qi) ;\n    TODEV3(qr) ;\n    TODEV3(qs) ;\n    TODEV2(rain) ;\n    TODEV2(rainncv) ;\n    TODEV2(sr) ;\n    TODEV2(snow) ;\n    TODEV2(snowncv) ;\n\n    hipDeviceSynchronize();\n    auto start = std::chrono::steady_clock::now();\n\n    wsm <<< dimGrid, dimBlock >>> (\n      th_d, pii_d, q_d, qc_d, qi_d, qr_d, qs_d, den_d, p_d, delz_d,\n      rain_d, rainncv_d,\n      sr_d,\n      snow_d, snowncv_d,\n      delt,\n      dips+1 , (ipe-ips+1) , \n\n      djps+1 , (jpe-jps+1) , \n\n      dkps+1 , (kpe-kps+1),  \n\n      dips+1 , dipe ,        \n\n      djps+1 , djpe ,        \n\n      dkps+1 , dkpe,         \n\n      dips+1 , dipe ,        \n\n      djps+1 , djpe ,        \n\n      dkps+1 , dkpe) ;       \n\n\n    hipDeviceSynchronize() ;\n    auto end = std::chrono::steady_clock::now();\n    time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n    FROMDEV2(rain) ;\n    FROMDEV2(snow) ;\n\n    rain_sum = snow_sum = 0;\n    for (int i = 0; i < d2; i++) {\n      rain_sum += rain[i];\n      snow_sum += snow[i];\n    }\n\n    FREE(pii) ;\n    FREE(den) ;\n    FREE(p) ;\n    FREE(delz) ;\n    FREE(th) ;\n    FREE(q) ;\n    FREE(qc) ;\n    FREE(qi) ;\n    FREE(qr) ;\n    FREE(qs) ;\n    FREE(rain) ;\n    FREE(rainncv) ;\n    FREE(sr) ;\n    FREE(snow) ;\n    FREE(snowncv) ;\n  }\n\n  printf(\"Average kernel execution time: %lf (ms)\\n\", (time * 1e-6) / repeat);\n  printf(\"Checksum: rain = %f snow = %f\\n\", rain_sum, snow_sum);\n\n  return(0) ;\n}\n"}}
{"kernel_name": "wsm5", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <omp.h>\n#include \"utils.h\"\n#include \"kernel.h\"\n\nint main(int argc, char* argv[])\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n  float *th, *pii, *q;\n  float *qc, *qi, *qr, *qs;\n  float *den, *p, *delz;\n  float *rain,*rainncv;\n  float *sr;\n  float *snow, *snowncv;\n\n  float delt = 10.f;\n  int ims = 0, ime = 59, jms = 0, jme = 45, kms = 0, kme = 2;\n  int ips = 0, ipe = 59, jps = 0, jpe = 45, kps = 0, kpe = 2;\n  int d3 = (ime-ims+1) * (jme-jms+1) * (kme-kms+1) ;\n  int d2 = (ime-ims+1) * (jme-jms+1) ;\n\n  int dips = 0 ; int dipe = (ipe-ips+1) ;\n  int djps = 0 ; int djpe = (jpe-jps+1) ;\n  int dkps = 0 ; int dkpe = (kpe-kps+1) ;\n\n  float rain_sum = 0, snow_sum = 0;\n\n  long time = 0;\n  for (int i = 0; i < repeat; i++) {\n    ALLOC3(th) ;\n    ALLOC3(pii) ;\n    ALLOC3(q) ;\n    ALLOC3(qc) ;\n    ALLOC3(qi) ;\n    ALLOC3(qr) ;\n    ALLOC3(qs) ;\n    ALLOC3(den) ;\n    ALLOC3(p) ;\n    ALLOC3(delz) ;\n    ALLOC2(rain) ;\n    ALLOC2(rainncv) ;\n    ALLOC2(sr) ;\n    ALLOC2(snow) ;\n    ALLOC2(snowncv) ;\n\n    int remx = (ipe-ips+1) % XXX != 0 ? 1 : 0 ;\n    int remy = (jpe-jps+1) % YYY != 0 ? 1 : 0 ;\n\n    const int teamX = (ipe-ips+1) / XXX + remx;\n    const int teamY = (jpe-jps+1) / YYY + remy;\n\n    #pragma omp target data map(to: th[0:d3], \\\n                                    pii[0:d3], \\\n                                    q[0:d3], \\\n                                    qc[0:d3], \\\n                                    qi[0:d3], \\\n                                    qr[0:d3], \\\n                                    qs[0:d3], \\\n                                    den[0:d3], \\\n                                    p[0:d3], \\\n                                    delz[0:d3], \\\n                                    rainncv[0:d2], \\\n                                    snowncv[0:d2], \\\n                                    sr[0:d2]) \\\n                            map(tofrom: rain[0:d2],\\\n                                        snow[0:d2])\n    {\n      auto start = std::chrono::steady_clock::now();\n\n      wsm(th, pii, q, qc, qi, qr, qs, den, p, delz,\n        rain, rainncv,\n        sr,\n        snow, snowncv,\n        delt,\n        dips+1 , (ipe-ips+1) , \n\n        djps+1 , (jpe-jps+1) , \n\n        dkps+1 , (kpe-kps+1),  \n\n        dips+1 , dipe ,        \n\n        djps+1 , djpe ,        \n\n        dkps+1 , dkpe ,        \n\n        dips+1 , dipe ,        \n\n        djps+1 , djpe ,        \n\n        dkps+1 , dkpe ,        \n\n        teamX , teamY );\n\n      auto end = std::chrono::steady_clock::now();\n      time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    }\n\n    rain_sum = snow_sum = 0;\n    for (int i = 0; i < d2; i++) {\n      rain_sum += rain[i];\n      snow_sum += snow[i];\n    }\n\n    FREE(th) ;\n    FREE(pii) ;\n    FREE(q) ;\n    FREE(qc) ;\n    FREE(qi) ;\n    FREE(qr) ;\n    FREE(qs) ;\n    FREE(den) ;\n    FREE(p) ;\n    FREE(delz) ;\n    FREE(rain) ;\n    FREE(rainncv) ;\n    FREE(sr) ;\n    FREE(snow) ;\n    FREE(snowncv) ;\n  }\n\n  printf(\"Average kernel execution time: %lf (ms)\\n\", (time * 1e-6) / repeat);\n  printf(\"Checksum: rain = %f snow = %f\\n\", rain_sum, snow_sum);\n  return(0) ;\n}\n"}}
{"kernel_name": "wsm5", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include \"utils.h\"\n#include \"kernel.h\"\n\nint main(int argc, char* argv[])\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n  float *th, *pii, *q;\n  float *qc, *qi, *qr, *qs;\n  float *den, *p, *delz;\n  float *rain,*rainncv;\n  float *sr;\n  float *snow, *snowncv;\n\n  float delt = 10.f;\n  int ims = 0, ime = 59, jms = 0, jme = 45, kms = 0, kme = 2;\n  int ips = 0, ipe = 59, jps = 0, jpe = 45, kps = 0, kpe = 2;\n  int d3 = (ime-ims+1) * (jme-jms+1) * (kme-kms+1) ;\n  int d2 = (ime-ims+1) * (jme-jms+1) ;\n\n  int dips = 0 ; int dipe = (ipe-ips+1) ;\n  int djps = 0 ; int djpe = (jpe-jps+1) ;\n  int dkps = 0 ; int dkpe = (kpe-kps+1) ;\n\n  float rain_sum = 0, snow_sum = 0;\n\n  long time = 0;\n  for (int i = 0; i < repeat; i++) {\n    ALLOC3(th) ;\n    ALLOC3(pii) ;\n    ALLOC3(q) ;\n    ALLOC3(qc) ;\n    ALLOC3(qi) ;\n    ALLOC3(qr) ;\n    ALLOC3(qs) ;\n    ALLOC3(den) ;\n    ALLOC3(p) ;\n    ALLOC3(delz) ;\n    ALLOC2(rain) ;\n    ALLOC2(rainncv) ;\n    ALLOC2(sr) ;\n    ALLOC2(snow) ;\n    ALLOC2(snowncv) ;\n\n    int remx = (ipe-ips+1) % XXX != 0 ? 1 : 0 ;\n    int remy = (jpe-jps+1) % YYY != 0 ? 1 : 0 ;\n\n    const int teamX = (ipe-ips+1) / XXX + remx;\n    const int teamY = (jpe-jps+1) / YYY + remy;\n\n        {\n      auto start = std::chrono::steady_clock::now();\n\n      wsm(th, pii, q, qc, qi, qr, qs, den, p, delz,\n        rain, rainncv,\n        sr,\n        snow, snowncv,\n        delt,\n        dips+1 , (ipe-ips+1) , \n\n        djps+1 , (jpe-jps+1) , \n\n        dkps+1 , (kpe-kps+1),  \n\n        dips+1 , dipe ,        \n\n        djps+1 , djpe ,        \n\n        dkps+1 , dkpe ,        \n\n        dips+1 , dipe ,        \n\n        djps+1 , djpe ,        \n\n        dkps+1 , dkpe ,        \n\n        teamX , teamY );\n\n      auto end = std::chrono::steady_clock::now();\n      time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    }\n\n    rain_sum = snow_sum = 0;\n    for (int i = 0; i < d2; i++) {\n      rain_sum += rain[i];\n      snow_sum += snow[i];\n    }\n\n    FREE(th) ;\n    FREE(pii) ;\n    FREE(q) ;\n    FREE(qc) ;\n    FREE(qi) ;\n    FREE(qr) ;\n    FREE(qs) ;\n    FREE(den) ;\n    FREE(p) ;\n    FREE(delz) ;\n    FREE(rain) ;\n    FREE(rainncv) ;\n    FREE(sr) ;\n    FREE(snow) ;\n    FREE(snowncv) ;\n  }\n\n  printf(\"Average kernel execution time: %lf (ms)\\n\", (time * 1e-6) / repeat);\n  printf(\"Checksum: rain = %f snow = %f\\n\", rain_sum, snow_sum);\n  return(0) ;\n}"}}
{"kernel_name": "wsm5", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <sycl/sycl.hpp>\n#include \"utils.h\"\n#include \"kernel.h\"\n\nint main(int argc, char* argv[])\n{\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n  float *th, *pii, *q;\n  float *qc, *qi, *qr, *qs;\n  float *den, *p, *delz;\n  float *rain,*rainncv;\n  float *sr;\n  float *snow, *snowncv;\n\n  float delt = 10.f;\n  int ims = 0, ime = 59, jms = 0, jme = 45, kms = 0, kme = 2;\n  int ips = 0, ipe = 59, jps = 0, jpe = 45, kps = 0, kpe = 2;\n  int d3 = (ime-ims+1) * (jme-jms+1) * (kme-kms+1) ;\n  int d2 = (ime-ims+1) * (jme-jms+1) ;\n\n  int dips = 0 ; int dipe = (ipe-ips+1) ;\n  int djps = 0 ; int djpe = (jpe-jps+1) ;\n  int dkps = 0 ; int dkpe = (kpe-kps+1) ;\n\n#ifdef USE_GPU\n  sycl::queue Q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue Q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  float rain_sum = 0, snow_sum = 0;\n\n  long time = 0;\n  for (int i = 0; i < repeat; i++) {\n    TODEV3(th) ;\n    TODEV3(pii) ;\n    TODEV3(q) ;\n    TODEV3(qc) ;\n    TODEV3(qi) ;\n    TODEV3(qr) ;\n    TODEV3(qs) ;\n    TODEV3(den) ;\n    TODEV3(p) ;\n    TODEV3(delz) ;\n    TODEV2(rain) ;\n    TODEV2(rainncv) ;\n    TODEV2(sr) ;\n    TODEV2(snow) ;\n    TODEV2(snowncv) ;\n\n    int remx = (ipe-ips+1) % XXX != 0 ? 1 : 0 ;\n    int remy = (jpe-jps+1) % YYY != 0 ? 1 : 0 ;\n\n    sycl::range<2> lws ( YYY, XXX ) ;\n    sycl::range<2> gws ( YYY * ((jpe-jps+1) / YYY + remy),\n                         XXX * ((ipe-ips+1) / XXX + remx) );\n\n    Q.wait();\n    auto start = std::chrono::steady_clock::now();\n\n    Q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class weather>(\n        sycl::nd_range<2>(gws, lws), [=] (sycl::nd_item<2> item) {\n        wsm (item,\n             th_d,\n             pii_d,\n             q_d,\n             qc_d,\n             qi_d,\n             qr_d,\n             qs_d,\n             den_d,\n             p_d,\n             delz_d,\n             rain_d,\n             rainncv_d,\n             sr_d,\n             snow_d,\n             snowncv_d,\n             delt,\n             dips+1 , (ipe-ips+1) , \n\n             djps+1 , (jpe-jps+1) , \n\n             dkps+1 , (kpe-kps+1),  \n\n             dips+1 , dipe ,        \n\n             djps+1 , djpe ,        \n\n             dkps+1 , dkpe,         \n\n             dips+1 , dipe ,        \n\n             djps+1 , djpe ,        \n\n             dkps+1 , dkpe) ;       \n\n      });\n    });\n\n    Q.wait();\n    auto end = std::chrono::steady_clock::now();\n    time += std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n    FROMDEV2(rain) ;\n    FROMDEV2(snow) ;\n\n    rain_sum = snow_sum = 0;\n    for (int i = 0; i < d2; i++) {\n      rain_sum += rain[i];\n      snow_sum += snow[i];\n    }\n\n    FREE(th) ;\n    FREE(pii) ;\n    FREE(q) ;\n    FREE(qc) ;\n    FREE(qi) ;\n    FREE(qr) ;\n    FREE(qs) ;\n    FREE(den) ;\n    FREE(p) ;\n    FREE(delz) ;\n    FREE(rain) ;\n    FREE(rainncv) ;\n    FREE(sr) ;\n    FREE(snow) ;\n    FREE(snowncv) ;\n  }\n\n  printf(\"Average kernel execution time: %lf (ms)\\n\", (time * 1e-6) / repeat);\n  printf(\"Checksum: rain = %f snow = %f\\n\", rain_sum, snow_sum);\n  return(0) ;\n}\n"}}
