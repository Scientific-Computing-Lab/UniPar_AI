{"kernel_name": "log2", "parallel_api": "cuda", "code": {"main.cu": "#include <iostream>\n#include <fstream>\n#include <iomanip>\n#include <vector>\n#include <cmath>\n#include \"kernel.h\"\n\nint main(int argc, char* argv[]) {\n\n  if (argc != 2) {\n    std::cout << \"Usage: ./main <config filename>\\n\";\n    return 1;\n  }\n\n  std::ifstream message_file (argv[1]);\n\n  std::string placeholder;\n  message_file >> placeholder;\n  long ceilingVal;\n  message_file >> ceilingVal;\n\n  message_file >> placeholder;\n  int repeat;\n  message_file >> repeat;\n\n  message_file >> placeholder;\n  int precision_count;\n  message_file >> precision_count;\n\n  std::vector<int> precision(precision_count, 0);\n  message_file >> placeholder;\n\n  for (int i = 0; i < precision_count; ++i) {\n    message_file >> precision[i];\n  }\n\n  std::vector<float> inputs;\n\n  long i = 1;\n  int increment = 1;\n\n  while (i <= ceilingVal) {\n    inputs.push_back((float) i);\n    i += increment;\n  }\n\n  size_t inputs_size = inputs.size();\n\n  std::cout << \"Number of precision counts : \" \n            << precision_count << std::endl\n            << \" Number of inputs to evaluate for each precision: \"\n            << inputs_size << std::endl\n            << \" Number of runs for each precision : \" << repeat << std::endl;\n\n  std::vector<float> empty_vector(inputs_size, 0);\n\n#ifdef HOST\n  \n\n  std::vector<std::vector<float>> output_vals(precision_count, empty_vector);\n\n  for(int i = 0; i < precision_count; ++i) {\n    for(int k = 0; k < repeat; ++k) {\n      for(size_t j = 0; j < inputs_size; ++j) {\n        output_vals[i][j] = binary_log(inputs[j], precision[i]);\n      }\n    }\n  }\n#endif\n\n  \n\n  std::vector<float> d_output_vals(inputs_size * precision_count);\n\n  \n\n  log2_approx(inputs, d_output_vals, precision, \n              inputs.size(), precision_count, repeat);\n\n  \n\n  std::vector<float> ref_vals(inputs_size, 0);\n  for (size_t i = 0; i < inputs_size; ++i)\n    ref_vals[i] = log2f (inputs[i]);\n\n  \n\n#ifdef HOST\n  std::cout << \"-------------- SUMMARY (Host results):\" << \" --------------\" << std::endl<<std::endl;\n  for (int i = 0; i < precision_count; ++i){\n    std::cout << \"----- Iterative approximation with \" << precision[i] <<\" bits of precision -----\" << std::endl;\n    float s = 0;\n    for (size_t j = 0; j < inputs_size; ++j){\n      s += (output_vals[i][j] - ref_vals[j]) * (output_vals[i][j] - ref_vals[j]);\n    }\n    s /= inputs.size();\n    std::cout << \"RMSE : \" << sqrtf(s) << std::endl;\n  }\n#endif\n\n  std::cout << \"-------------- SUMMARY (Device results):\" << \" --------------\" << std::endl<<std::endl;\n  for (int i = 0; i < precision_count; ++i){\n    std::cout << \"----- Iterative approximation with \" << precision[i] <<\" bits of precision -----\" << std::endl;\n    float s = 0;\n    for (size_t j = 0; j < inputs_size; ++j){\n      s += (d_output_vals[i*inputs_size+j] - ref_vals[j]) * (d_output_vals[i*inputs_size+j] - ref_vals[j]);\n    }\n    s /= inputs.size();\n    std::cout << \"RMSE : \" << sqrtf(s) << std::endl;\n  }\n\n  return 0;\n}\n"}}
{"kernel_name": "log2", "parallel_api": "omp", "code": {"main.cpp": "#include <iostream>\n#include <fstream>\n#include <iomanip>\n#include <vector>\n#include <cmath>\n#include \"kernel.h\"\n\nint main(int argc, char* argv[]) {\n\n  if (argc != 2) {\n    std::cout << \"Usage: ./main <config filename>\\n\";\n    return 1;\n  }\n\n  std::ifstream message_file (argv[1]);\n\n  std::string placeholder;\n  message_file >> placeholder;\n  long ceilingVal;\n  message_file >> ceilingVal;\n\n  message_file >> placeholder;\n  int repeat;\n  message_file >> repeat;\n\n  message_file >> placeholder;\n  int precision_count;\n  message_file >> precision_count;\n\n  std::vector<int> precision(precision_count, 0);\n  message_file >> placeholder;\n\n  for (int i = 0; i < precision_count; ++i) {\n    message_file >> precision[i];\n  }\n\n  std::vector<float> inputs;\n\n  long i = 1;\n  int increment = 1;\n\n  while (i <= ceilingVal) {\n    inputs.push_back((float) i);\n    i += increment;\n  }\n\n  size_t inputs_size = inputs.size();\n\n  std::cout << \"Number of precision counts : \" \n            << precision_count << std::endl\n            << \" Number of inputs to evaluate for each precision: \"\n            << inputs_size << std::endl\n            << \" Number of runs for each precision : \" << repeat << std::endl;\n\n  std::vector<float> empty_vector(inputs_size, 0);\n\n#ifdef HOST\n  \n\n  std::vector<std::vector<float>> output_vals(precision_count, empty_vector);\n\n  for(int i = 0; i < precision_count; ++i) {\n    for(int k = 0; k < repeat; ++k) {\n      for(size_t j = 0; j < inputs_size; ++j) {\n        output_vals[i][j] = binary_log(inputs[j], precision[i]);\n      }\n    }\n  }\n#endif\n\n  \n\n  std::vector<float> d_output_vals(inputs_size * precision_count);\n\n  \n\n  log2_approx(inputs, d_output_vals, precision, \n              inputs.size(), precision_count, repeat);\n\n  \n\n  std::vector<float> ref_vals(inputs_size, 0);\n  for (size_t i = 0; i < inputs_size; ++i)\n    ref_vals[i] = log2f (inputs[i]);\n\n  \n\n#ifdef HOST\n  std::cout << \"-------------- SUMMARY (Host results):\" << \" --------------\" << std::endl<<std::endl;\n  for (int i = 0; i < precision_count; ++i){\n    std::cout << \"----- Iterative approximation with \" << precision[i] <<\" bits of precision -----\" << std::endl;\n    float s = 0;\n    for (size_t j = 0; j < inputs_size; ++j){\n      s += (output_vals[i][j] - ref_vals[j]) * (output_vals[i][j] - ref_vals[j]);\n    }\n    s /= inputs.size();\n    std::cout << \"RMSE : \" << sqrtf(s) << std::endl;\n  }\n#endif\n\n  std::cout << \"-------------- SUMMARY (Device results):\" << \" --------------\" << std::endl<<std::endl;\n  for (int i = 0; i < precision_count; ++i){\n    std::cout << \"----- Iterative approximation with \" << precision[i] <<\" bits of precision -----\" << std::endl;\n    float s = 0;\n    for (size_t j = 0; j < inputs_size; ++j){\n      s += (d_output_vals[i*inputs_size+j] - ref_vals[j]) * (d_output_vals[i*inputs_size+j] - ref_vals[j]);\n    }\n    s /= inputs.size();\n    std::cout << \"RMSE : \" << sqrtf(s) << std::endl;\n  }\n\n  return 0;\n}\n"}}
{"kernel_name": "log2", "parallel_api": "serial", "code": {"main.cpp": "#include <iostream>\n#include <fstream>\n#include <iomanip>\n#include <vector>\n#include <cmath>\n#include \"kernel.h\"\n\nint main(int argc, char* argv[]) {\n\n  if (argc != 2) {\n    std::cout << \"Usage: ./main <config filename>\\n\";\n    return 1;\n  }\n\n  std::ifstream message_file (argv[1]);\n\n  std::string placeholder;\n  message_file >> placeholder;\n  long ceilingVal;\n  message_file >> ceilingVal;\n\n  message_file >> placeholder;\n  int repeat;\n  message_file >> repeat;\n\n  message_file >> placeholder;\n  int precision_count;\n  message_file >> precision_count;\n\n  std::vector<int> precision(precision_count, 0);\n  message_file >> placeholder;\n\n  for (int i = 0; i < precision_count; ++i) {\n    message_file >> precision[i];\n  }\n\n  std::vector<float> inputs;\n\n  long i = 1;\n  int increment = 1;\n\n  while (i <= ceilingVal) {\n    inputs.push_back((float) i);\n    i += increment;\n  }\n\n  size_t inputs_size = inputs.size();\n\n  std::cout << \"Number of precision counts : \" \n            << precision_count << std::endl\n            << \" Number of inputs to evaluate for each precision: \"\n            << inputs_size << std::endl\n            << \" Number of runs for each precision : \" << repeat << std::endl;\n\n  std::vector<float> empty_vector(inputs_size, 0);\n\n#ifdef HOST\n  \n\n  std::vector<std::vector<float>> output_vals(precision_count, empty_vector);\n\n  for(int i = 0; i < precision_count; ++i) {\n    for(int k = 0; k < repeat; ++k) {\n      for(size_t j = 0; j < inputs_size; ++j) {\n        output_vals[i][j] = binary_log(inputs[j], precision[i]);\n      }\n    }\n  }\n#endif\n\n  \n\n  std::vector<float> d_output_vals(inputs_size * precision_count);\n\n  \n\n  log2_approx(inputs, d_output_vals, precision, \n              inputs.size(), precision_count, repeat);\n\n  \n\n  std::vector<float> ref_vals(inputs_size, 0);\n  for (size_t i = 0; i < inputs_size; ++i)\n    ref_vals[i] = log2f (inputs[i]);\n\n  \n\n#ifdef HOST\n  std::cout << \"-------------- SUMMARY (Host results):\" << \" --------------\" << std::endl<<std::endl;\n  for (int i = 0; i < precision_count; ++i){\n    std::cout << \"----- Iterative approximation with \" << precision[i] <<\" bits of precision -----\" << std::endl;\n    float s = 0;\n    for (size_t j = 0; j < inputs_size; ++j){\n      s += (output_vals[i][j] - ref_vals[j]) * (output_vals[i][j] - ref_vals[j]);\n    }\n    s /= inputs.size();\n    std::cout << \"RMSE : \" << sqrtf(s) << std::endl;\n  }\n#endif\n\n  std::cout << \"-------------- SUMMARY (Device results):\" << \" --------------\" << std::endl<<std::endl;\n  for (int i = 0; i < precision_count; ++i){\n    std::cout << \"----- Iterative approximation with \" << precision[i] <<\" bits of precision -----\" << std::endl;\n    float s = 0;\n    for (size_t j = 0; j < inputs_size; ++j){\n      s += (d_output_vals[i*inputs_size+j] - ref_vals[j]) * (d_output_vals[i*inputs_size+j] - ref_vals[j]);\n    }\n    s /= inputs.size();\n    std::cout << \"RMSE : \" << sqrtf(s) << std::endl;\n  }\n\n  return 0;\n}"}}
{"kernel_name": "ntt", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <cuda.h>\n\n#define  bidx  blockIdx.x\n#define  tidx  threadIdx.x\n\n#include \"modP.h\"\n\n__global__ void intt_3_64k_modcrt(\n        uint32 *__restrict__ dst,\n  const uint64 *__restrict__ src)\n{\n  __shared__ uint64 buffer[512];\n  register uint64 samples[8], s8[8];\n  register uint32 fmem, tmem, fbuf, tbuf;\n  fmem = (bidx<<9)|((tidx&0x3E)<<3)|(tidx&0x1);\n  tbuf = tidx<<3;\n  fbuf = ((tidx&0x38)<<3) | (tidx&0x7);\n  tmem = (bidx<<9)|((tidx&0x38)<<3) | (tidx&0x7);\n#pragma unroll\n  for (int i=0; i<8; i++)\n    samples[i] = src[fmem|(i<<1)];\n  ntt8(samples);\n\n#pragma unroll\n  for (int i=0; i<8; i++)\n    buffer[tbuf|i] = _ls_modP(samples[i], ((tidx&0x1)<<2)*i*3);\n  __syncthreads();\n\n#pragma unroll\n  for (int i=0; i<8; i++)\n    samples[i] = buffer[fbuf|(i<<3)];\n\n#pragma unroll\n  for (int i=0; i<4; i++) {\n    s8[2*i] = _add_modP(samples[2*i], samples[2*i+1]);\n    s8[2*i+1] = _sub_modP(samples[2*i], samples[2*i+1]);\n  }\n\n#pragma unroll\n  for (int i=0; i<8; i++) {\n    dst[(((tmem|(i<<3))&0xf)<<12)|((tmem|(i<<3))>>4)] =\n      (uint32)(_mul_modP(s8[i], 18446462594437939201UL, valP));\n  }\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  const int nttLen = 64 * 1024;\n  uint64 *ntt = (uint64*) malloc (nttLen*sizeof(uint64));\n  uint32 *res = (uint32*) malloc (nttLen*sizeof(uint32));\n\n  srand(123);\n  for (int i = 0; i < nttLen; i++) {\n    uint64 hi = rand();\n    uint64 lo = rand();\n    ntt[i] = (hi << 32) | lo;\n  }\n\n  uint64 *d_ntt;\n  uint32 *d_res;\n  cudaMalloc(&d_ntt, nttLen*sizeof(uint64));\n  cudaMalloc(&d_res, nttLen*sizeof(uint32));\n  cudaMemcpy(d_ntt, ntt, nttLen*sizeof(uint64), cudaMemcpyHostToDevice);\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++)\n    intt_3_64k_modcrt<<<nttLen/512, 64>>>(d_res, d_ntt);\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  cudaMemcpy(res, d_res, nttLen*sizeof(uint32), cudaMemcpyDeviceToHost);\n\n  uint64_t checksum = 0;\n  for (int i = 0; i < nttLen; i++)\n    checksum += res[i];\n  printf(\"Checksum: %lu\\n\", checksum);\n\n  cudaFree(d_ntt);\n  cudaFree(d_res);\n  free(ntt);\n  free(res);\n  return 0;\n}\n"}}
{"kernel_name": "ntt", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <omp.h>\n\n#define  bidx  omp_get_team_num()\n#define  tidx  omp_get_thread_num()\n\n#include \"modP.h\"\n\nvoid intt_3_64k_modcrt(\n  const uint32 numTeams,\n        uint32 *__restrict dst,\n  const uint64 *__restrict src)\n{\n  #pragma omp target teams num_teams(numTeams) thread_limit(64)\n  {\n    uint64 buffer[512];\n    #pragma omp parallel \n    {\n      register uint64 samples[8], s8[8];\n      register uint32 fmem, tmem, fbuf, tbuf;\n      fmem = (bidx<<9)|((tidx&0x3E)<<3)|(tidx&0x1);\n      tbuf = tidx<<3;\n      fbuf = ((tidx&0x38)<<3) | (tidx&0x7);\n      tmem = (bidx<<9)|((tidx&0x38)<<3) | (tidx&0x7);\n    #pragma unroll\n      for (int i=0; i<8; i++)\n        samples[i] = src[fmem|(i<<1)];\n      ntt8(samples);\n    \n    #pragma unroll\n      for (int i=0; i<8; i++)\n        buffer[tbuf|i] = _ls_modP(samples[i], ((tidx&0x1)<<2)*i*3);\n    #pragma omp barrier\n    \n    #pragma unroll\n      for (int i=0; i<8; i++)\n        samples[i] = buffer[fbuf|(i<<3)];\n    \n    #pragma unroll\n      for (int i=0; i<4; i++) {\n        s8[2*i] = _add_modP(samples[2*i], samples[2*i+1]);\n        s8[2*i+1] = _sub_modP(samples[2*i], samples[2*i+1]);\n      }\n    \n    #pragma unroll\n      for (int i=0; i<8; i++) {\n        dst[(((tmem|(i<<3))&0xf)<<12)|((tmem|(i<<3))>>4)] =\n          (uint32)(_mul_modP(s8[i], 18446462594437939201UL, valP));\n      }\n    }\n  }\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  const int nttLen = 64 * 1024;\n  uint64 *ntt = (uint64*) malloc (nttLen*sizeof(uint64));\n  uint32 *res = (uint32*) malloc (nttLen*sizeof(uint32));\n\n  srand(123);\n  for (int i = 0; i < nttLen; i++) {\n    uint64 hi = rand();\n    uint64 lo = rand();\n    ntt[i] = (hi << 32) | lo;\n  }\n\n  #pragma omp target data map (to: ntt[0:nttLen]) \\\n                          map (from: res[0:nttLen])\n  {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++)\n      intt_3_64k_modcrt(nttLen/512, res, ntt);\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n  }\n\n  uint64 checksum = 0;\n  for (int i = 0; i < nttLen; i++)\n    checksum += res[i];\n  printf(\"Checksum: %lu\\n\", checksum);\n\n  free(ntt);\n  free(res);\n  return 0;\n}\n"}}
{"kernel_name": "ntt", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n\n#define  bidx  omp_get_team_num()\n#define  tidx  omp_get_thread_num()\n\n#include \"modP.h\"\n\nvoid intt_3_64k_modcrt(\n  const uint32 numTeams,\n        uint32 *__restrict dst,\n  const uint64 *__restrict src)\n{\n    {\n    uint64 buffer[512];\n        {\n      register uint64 samples[8], s8[8];\n      register uint32 fmem, tmem, fbuf, tbuf;\n      fmem = (bidx<<9)|((tidx&0x3E)<<3)|(tidx&0x1);\n      tbuf = tidx<<3;\n      fbuf = ((tidx&0x38)<<3) | (tidx&0x7);\n      tmem = (bidx<<9)|((tidx&0x38)<<3) | (tidx&0x7);\n          for (int i=0; i<8; i++)\n        samples[i] = src[fmem|(i<<1)];\n      ntt8(samples);\n    \n          for (int i=0; i<8; i++)\n        buffer[tbuf|i] = _ls_modP(samples[i], ((tidx&0x1)<<2)*i*3);\n        \n          for (int i=0; i<8; i++)\n        samples[i] = buffer[fbuf|(i<<3)];\n    \n          for (int i=0; i<4; i++) {\n        s8[2*i] = _add_modP(samples[2*i], samples[2*i+1]);\n        s8[2*i+1] = _sub_modP(samples[2*i], samples[2*i+1]);\n      }\n    \n          for (int i=0; i<8; i++) {\n        dst[(((tmem|(i<<3))&0xf)<<12)|((tmem|(i<<3))>>4)] =\n          (uint32)(_mul_modP(s8[i], 18446462594437939201UL, valP));\n      }\n    }\n  }\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 2) {\n    printf(\"Usage: %s <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int repeat = atoi(argv[1]);\n\n  const int nttLen = 64 * 1024;\n  uint64 *ntt = (uint64*) malloc (nttLen*sizeof(uint64));\n  uint32 *res = (uint32*) malloc (nttLen*sizeof(uint32));\n\n  srand(123);\n  for (int i = 0; i < nttLen; i++) {\n    uint64 hi = rand();\n    uint64 lo = rand();\n    ntt[i] = (hi << 32) | lo;\n  }\n\n    {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++)\n      intt_3_64k_modcrt(nttLen/512, res, ntt);\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n  }\n\n  uint64 checksum = 0;\n  for (int i = 0; i < nttLen; i++)\n    checksum += res[i];\n  printf(\"Checksum: %lu\\n\", checksum);\n\n  free(ntt);\n  free(res);\n  return 0;\n}"}}
{"kernel_name": "interval", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n\n\n\n#include <stdio.h>\n#include <math.h>\n#include <iostream>\n#include <chrono>\n#include \"interval.h\"\n#include \"gpu_interval.h\"\n#include \"cpu_interval.h\"\n\nint main(int argc, char *argv[]) {\n  if (argc != 3) {\n    printf(\"Usage: %s <implementation choice> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int implementation_choice = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n\n  switch (implementation_choice) {\n    case 0:\n      printf(\"GPU implementation 1\\n\");\n      break;\n\n    case 1:\n      printf(\"GPU implementation 2\\n\");\n      break;\n\n    default:\n      printf(\"GPU implementation 1\\n\");\n  }\n\n  interval_gpu<T> *d_result;\n  int *d_nresults;\n  int *h_nresults = new int[THREADS];\n\n  cudaMalloc((void **)&d_result, THREADS * DEPTH_RESULT * sizeof(*d_result));\n  cudaMalloc((void **)&d_nresults, THREADS * sizeof(*d_nresults));\n\n  interval_gpu<T> i(0.01f, 4.0f);\n  std::cout << \"Searching for roots in [\" << i.lower() << \", \" << i.upper()\n            << \"]...\\n\";\n\n  auto start = std::chrono::steady_clock::now();\n\n  for (int it = 0; it < repeat; ++it) {\n    test_interval_newton<T><<<GRID_SIZE, BLOCK_SIZE>>>(\n      d_result, d_nresults, i, implementation_choice);\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n\n  I_CPU *h_result = new I_CPU[THREADS * DEPTH_RESULT];\n  cudaMemcpy(h_result, d_result, THREADS * DEPTH_RESULT * sizeof(*d_result),\n             cudaMemcpyDeviceToHost);\n\n  cudaMemcpy(h_nresults, d_nresults, THREADS * sizeof(*d_nresults),\n             cudaMemcpyDeviceToHost);\n\n  std::cout << \"Found \" << h_nresults[0]\n            << \" intervals that may contain the root(s)\\n\";\n  std::cout.precision(15);\n\n  for (int i = 0; i != h_nresults[0]; ++i) {\n    std::cout << \" i[\" << i << \"] =\"\n              << \" [\" << h_result[THREADS * i + 0].lower() << \", \"\n              << h_result[THREADS * i + 0].upper() << \"]\\n\";\n  }\n\n  std::cout << \"Number of equations solved: \" << THREADS << \"\\n\";\n  std::cout << \"Average execution time of test_interval_newton: \"\n            << (time * 1e-3f) / repeat << \" us\\n\";\n\n  cudaFree(d_result);\n  cudaFree(d_nresults);\n\n  \n\n  I_CPU i_cpu(0.01f, 4.0f);\n  I_CPU *h_result_cpu = new I_CPU[THREADS * DEPTH_RESULT];\n  int *h_nresults_cpu = new int[THREADS];\n  test_interval_newton_cpu<I_CPU>(h_result_cpu, h_nresults_cpu, i_cpu);\n\n  \n\n  bool bTestResult =\n      checkAgainstHost(h_nresults, h_nresults_cpu, h_result, h_result_cpu);\n  std::cout << (bTestResult ? \"PASS\" : \"FAIL\") << \"\\n\";\n\n  delete[] h_result_cpu;\n  delete[] h_nresults_cpu;\n  delete[] h_result;\n  delete[] h_nresults;\n\n  return 0;\n}\n"}}
{"kernel_name": "interval", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n\n\n\n#include <stdio.h>\n#include <iostream>\n#include <chrono>\n#include <cmath>\n#include <math.h>\n#include <omp.h>\n#include \"interval.h\"\n#include \"gpu_interval.h\"\n#include \"cpu_interval.h\"\n\nint main(int argc, char *argv[]) {\n  if (argc != 3) {\n    printf(\"Usage: %s <implementation choice> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int implementation_choice = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n\n  switch (implementation_choice) {\n    case 0:\n      printf(\"GPU implementation 1\\n\");\n      break;\n\n    case 1:\n      printf(\"GPU implementation 2\\n\");\n      break;\n\n    default:\n      printf(\"GPU implementation 1\\n\");\n  }\n\n  int *h_nresults = new int[THREADS];\n  interval_gpu<T> *buffer = new interval_gpu<T>[THREADS * DEPTH_RESULT];\n\n  interval_gpu<T> i(0.01f, 4.0f);\n  std::cout << \"Searching for roots in [\" << i.lower() << \", \" << i.upper()\n            << \"]...\\n\";\n\n  long time;\n\n  #pragma omp target data map (from: buffer[0:THREADS * DEPTH_RESULT], \\\n                                     h_nresults[0:THREADS]) \n  {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int it = 0; it < repeat; ++it) {\n      #pragma omp target teams distribute parallel for \\\n        num_teams(GRID_SIZE) num_threads(BLOCK_SIZE)\n      for (int thread_id = 0; thread_id < BLOCK_SIZE * GRID_SIZE; thread_id++) {\n        typedef interval_gpu<T> I;\n\n        \n\n        global_stack<I, DEPTH_RESULT, THREADS> result(buffer, thread_id);\n\n        switch (implementation_choice) {\n          case 0:\n            newton_interval_naive<T, THREADS>(result, i, thread_id);\n            break;\n\n          case 1:\n            newton_interval<T, THREADS>(result, i, thread_id);\n            break;\n\n          default:\n            newton_interval_naive<T, THREADS>(result, i, thread_id);\n        }\n\n        h_nresults[thread_id] = result.size();\n      }\n    }\n    auto end = std::chrono::steady_clock::now();\n    time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  }\n\n  I_CPU *h_result = (I_CPU*) buffer;\n\n  std::cout << \"Found \" << h_nresults[0]\n            << \" intervals that may contain the root(s)\\n\";\n  std::cout.precision(15);\n\n  for (int i = 0; i != h_nresults[0]; ++i) {\n    std::cout << \" i[\" << i << \"] =\"\n              << \" [\" << h_result[THREADS * i + 0].lower() << \", \"\n              << h_result[THREADS * i + 0].upper() << \"]\\n\";\n  }\n\n  std::cout << \"Number of equations solved: \" << THREADS << \"\\n\";\n  std::cout << \"Average execution time of test_interval_newton: \"\n            << (time * 1e-3f) / repeat << \" us\\n\";\n  \n\n  \n\n  I_CPU i_cpu(0.01f, 4.0f);\n  I_CPU *h_result_cpu = new I_CPU[THREADS * DEPTH_RESULT];\n  int *h_nresults_cpu = new int[THREADS];\n  test_interval_newton_cpu<I_CPU>(h_result_cpu, h_nresults_cpu, i_cpu);\n\n  \n\n  bool bTestResult =\n      checkAgainstHost(h_nresults, h_nresults_cpu, h_result, h_result_cpu);\n  std::cout << (bTestResult ? \"PASS\" : \"FAIL\") << \"\\n\";\n\n  delete[] h_result_cpu;\n  delete[] h_nresults_cpu;\n  delete[] h_result;\n  delete[] h_nresults;\n\n  return 0;\n}\n"}}
{"kernel_name": "interval", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n\n\n\n#include <stdio.h>\n#include <iostream>\n#include <chrono>\n#include <cmath>\n#include <math.h>\n#include \"interval.h\"\n#include \"gpu_interval.h\"\n#include \"cpu_interval.h\"\n\nint main(int argc, char *argv[]) {\n  if (argc != 3) {\n    printf(\"Usage: %s <implementation choice> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int implementation_choice = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n\n  switch (implementation_choice) {\n    case 0:\n      printf(\"GPU implementation 1\\n\");\n      break;\n\n    case 1:\n      printf(\"GPU implementation 2\\n\");\n      break;\n\n    default:\n      printf(\"GPU implementation 1\\n\");\n  }\n\n  int *h_nresults = new int[THREADS];\n  interval_gpu<T> *buffer = new interval_gpu<T>[THREADS * DEPTH_RESULT];\n\n  interval_gpu<T> i(0.01f, 4.0f);\n  std::cout << \"Searching for roots in [\" << i.lower() << \", \" << i.upper()\n            << \"]...\\n\";\n\n  long time;\n\n    {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int it = 0; it < repeat; ++it) {\n            for (int thread_id = 0; thread_id < BLOCK_SIZE * GRID_SIZE; thread_id++) {\n        typedef interval_gpu<T> I;\n\n        \n\n        global_stack<I, DEPTH_RESULT, THREADS> result(buffer, thread_id);\n\n        switch (implementation_choice) {\n          case 0:\n            newton_interval_naive<T, THREADS>(result, i, thread_id);\n            break;\n\n          case 1:\n            newton_interval<T, THREADS>(result, i, thread_id);\n            break;\n\n          default:\n            newton_interval_naive<T, THREADS>(result, i, thread_id);\n        }\n\n        h_nresults[thread_id] = result.size();\n      }\n    }\n    auto end = std::chrono::steady_clock::now();\n    time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  }\n\n  I_CPU *h_result = (I_CPU*) buffer;\n\n  std::cout << \"Found \" << h_nresults[0]\n            << \" intervals that may contain the root(s)\\n\";\n  std::cout.precision(15);\n\n  for (int i = 0; i != h_nresults[0]; ++i) {\n    std::cout << \" i[\" << i << \"] =\"\n              << \" [\" << h_result[THREADS * i + 0].lower() << \", \"\n              << h_result[THREADS * i + 0].upper() << \"]\\n\";\n  }\n\n  std::cout << \"Number of equations solved: \" << THREADS << \"\\n\";\n  std::cout << \"Average execution time of test_interval_newton: \"\n            << (time * 1e-3f) / repeat << \" us\\n\";\n  \n\n  \n\n  I_CPU i_cpu(0.01f, 4.0f);\n  I_CPU *h_result_cpu = new I_CPU[THREADS * DEPTH_RESULT];\n  int *h_nresults_cpu = new int[THREADS];\n  test_interval_newton_cpu<I_CPU>(h_result_cpu, h_nresults_cpu, i_cpu);\n\n  \n\n  bool bTestResult =\n      checkAgainstHost(h_nresults, h_nresults_cpu, h_result, h_result_cpu);\n  std::cout << (bTestResult ? \"PASS\" : \"FAIL\") << \"\\n\";\n\n  delete[] h_result_cpu;\n  delete[] h_nresults_cpu;\n  delete[] h_result;\n  delete[] h_nresults;\n\n  return 0;\n}"}}