{"kernel_name": "accuracy", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <random>\n#include <cuda.h>\n#include <cub/cub.cuh>\n#include \"reference.h\"\n\n#define GPU_NUM_THREADS 256\n\n__global__\nvoid accuracy_kernel(\n    const int N,\n    const int D,\n    const int top_k,\n    const float* Xdata,\n    const int* labelData,\n    int* accuracy)\n{\n  typedef cub::BlockReduce<int, GPU_NUM_THREADS> BlockReduce;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n  int count = 0;\n\n  for (int row = blockIdx.x; row < N; row += gridDim.x) {\n    const int label = labelData[row];\n    const float label_pred = Xdata[row * D + label];\n    int ngt = 0;\n    for (int col = threadIdx.x; col < D; col += blockDim.x) {\n      const float pred = Xdata[row * D + col];\n      if (pred > label_pred || (pred == label_pred && col <= label)) {\n        ++ngt;\n      }\n    }\n    ngt = BlockReduce(temp_storage).Sum(ngt);\n    if (ngt <= top_k) {\n      ++count;\n    }\n    __syncthreads();\n  }\n  if (threadIdx.x == 0) { \n    atomicAdd(accuracy, count);\n  }\n}\n \nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: %s <number of rows> <number of columns> <top K> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int nrows = atoi(argv[1]);\n  const int ndims = atoi(argv[2]);\n  const int top_k = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int data_size = nrows * ndims;\n\n  const int label_size_bytes = nrows * sizeof(int); \n  const size_t data_size_bytes = data_size * sizeof(float); \n\n  int *label = (int*) malloc (label_size_bytes);\n\n  srand(123);\n  for (int i = 0; i < nrows; i++)\n    label[i] = rand() % ndims; \n\n  float *data = (float*) malloc (data_size_bytes);\n\n  std::default_random_engine g (123);\n  std::uniform_real_distribution<float> distr (0.f, 1.f);\n  for (int i = 0; i < data_size; i++) {\n    data[i] = distr(g);\n  }\n\n  int count_ref = reference(nrows, ndims, top_k, data, label);\n\n  int *d_label;\n  cudaMalloc((void**)&d_label, label_size_bytes);\n  cudaMemcpy(d_label, label, label_size_bytes, cudaMemcpyHostToDevice);\n\n  float *d_data;\n  cudaMalloc((void**)&d_data, data_size_bytes);\n  cudaMemcpy(d_data, data, data_size_bytes, cudaMemcpyHostToDevice);\n\n  int *d_count;\n  cudaMalloc((void**)&d_count, sizeof(int));\n\n  cudaDeviceSynchronize();\n  dim3 block (GPU_NUM_THREADS);\n\n  for (int ngrid = nrows / 4; ngrid <= nrows; ngrid += nrows / 4) {\n\n    dim3 grid (ngrid);\n    printf(\"Grid size is %d\\n\", ngrid);\n\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      cudaMemset(d_count, 0, sizeof(int));\n      accuracy_kernel<<<grid, block>>>(nrows, ndims, top_k, d_data, d_label, d_count);\n    }\n\n    cudaDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average execution time of accuracy kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n    int count;\n    cudaMemcpy(&count, d_count, sizeof(int), cudaMemcpyDeviceToHost);\n    bool ok = (count == count_ref);\n    printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n    \n\n  }\n\n  cudaFree(d_label);\n  cudaFree(d_data);\n  cudaFree(d_count);\n\n  free(label);\n  free(data);\n\n  return 0;\n}\n"}}
{"kernel_name": "accuracy", "parallel_api": "hip", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <random>\n#include <hip/hip_runtime.h>\n#include <hipcub/hipcub.hpp>\n#include \"reference.h\"\n\n#define GPU_NUM_THREADS 256\n\n__global__\nvoid accuracy_kernel(\n    const int N,\n    const int D,\n    const int top_k,\n    const float* Xdata,\n    const int* labelData,\n    int* accuracy)\n{\n  typedef hipcub::BlockReduce<int, GPU_NUM_THREADS> BlockReduce;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n  int count = 0;\n\n  for (int row = blockIdx.x; row < N; row += gridDim.x) {\n    const int label = labelData[row];\n    const float label_pred = Xdata[row * D + label];\n    int ngt = 0;\n    for (int col = threadIdx.x; col < D; col += blockDim.x) {\n      const float pred = Xdata[row * D + col];\n      if (pred > label_pred || (pred == label_pred && col <= label)) {\n        ++ngt;\n      }\n    }\n    ngt = BlockReduce(temp_storage).Sum(ngt);\n    if (ngt <= top_k) {\n      ++count;\n    }\n    __syncthreads();\n  }\n  if (threadIdx.x == 0) { \n    atomicAdd(accuracy, count);\n  }\n}\n \nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: %s <number of rows> <number of columns> <top K> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int nrows = atoi(argv[1]);\n  const int ndims = atoi(argv[2]);\n  const int top_k = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int data_size = nrows * ndims;\n\n  const int label_size_bytes = nrows * sizeof(int); \n  const size_t data_size_bytes = data_size * sizeof(float); \n\n  int *label = (int*) malloc (label_size_bytes);\n\n  srand(123);\n  for (int i = 0; i < nrows; i++)\n    label[i] = rand() % ndims; \n\n  float *data = (float*) malloc (data_size_bytes);\n\n  std::default_random_engine g (123);\n  std::uniform_real_distribution<float> distr (0.f, 1.f);\n  for (int i = 0; i < data_size; i++) {\n    data[i] = distr(g);\n  }\n\n  int count_ref = reference(nrows, ndims, top_k, data, label);\n\n  int *d_label;\n  hipMalloc((void**)&d_label, label_size_bytes);\n  hipMemcpy(d_label, label, label_size_bytes, hipMemcpyHostToDevice);\n\n  float *d_data;\n  hipMalloc((void**)&d_data, data_size_bytes);\n  hipMemcpy(d_data, data, data_size_bytes, hipMemcpyHostToDevice);\n\n  int *d_count;\n  hipMalloc((void**)&d_count, sizeof(int));\n\n  hipDeviceSynchronize();\n  dim3 block (GPU_NUM_THREADS);\n\n  for (int ngrid = nrows / 4; ngrid <= nrows; ngrid += nrows / 4) {\n\n    dim3 grid (ngrid);\n    printf(\"Grid size is %d\\n\", ngrid);\n\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      hipMemset(d_count, 0, sizeof(int));\n      accuracy_kernel<<<grid, block>>>(nrows, ndims, top_k, d_data, d_label, d_count);\n    }\n\n    hipDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average execution time of accuracy kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n    int count;\n    hipMemcpy(&count, d_count, sizeof(int), hipMemcpyDeviceToHost);\n    bool ok = (count == count_ref);\n    printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n    \n\n  }\n\n  hipFree(d_label);\n  hipFree(d_data);\n  hipFree(d_count);\n\n  free(label);\n  free(data);\n\n  return 0;\n}\n"}}
{"kernel_name": "accuracy", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <random>\n#include <omp.h>\n#include \"reference.h\"\n\n#define NUM_THREADS 256\n\nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: %s <number of rows> <number of columns> <top K> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int nrows = atoi(argv[1]);\n  const int ndims = atoi(argv[2]);\n  const int top_k = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int data_size = nrows * ndims;\n\n  const int label_size_bytes = nrows * sizeof(int); \n  const size_t data_size_bytes = data_size * sizeof(float); \n\n  int *label = (int*) malloc (label_size_bytes);\n\n  srand(123);\n  for (int i = 0; i < nrows; i++)\n    label[i] = rand() % ndims; \n\n  float *data = (float*) malloc (data_size_bytes);\n\n  std::default_random_engine g (123);\n  std::uniform_real_distribution<float> distr (0.f, 1.f);\n  for (int i = 0; i < data_size; i++) {\n    data[i] = distr(g);\n  }\n\n  int count_ref = reference(nrows, ndims, top_k, data, label);\n\n  int count[1];\n\n  #pragma omp target data map(to: label[0:nrows], data[0:data_size]) \\\n                          map(alloc: count[0:1])\n  {\n    for (int ngrid = nrows / 4; ngrid <= nrows; ngrid += nrows / 4) {\n\n      printf(\"Grid size is %d\\n\", ngrid);\n\n      auto start = std::chrono::steady_clock::now();\n\n      for (int i = 0; i < repeat; i++) {\n        count[0] = 0;\n        #pragma omp target update to (count[0:1]) \n\n        #pragma omp target teams distribute num_teams(ngrid)\n        for (int row = 0; row < nrows; row++) {\n          const int label_data = label[row];\n          const float label_pred = data[row * ndims + label_data];\n          int ngt = 0;\n          #pragma omp parallel for reduction(+:ngt) num_threads(NUM_THREADS)\n          for (int col = 0; col < ndims; col++) {\n            const float pred = data[row * ndims + col];\n            if (pred > label_pred || (pred == label_pred && col <= label_data)) {\n              ++ngt;\n            }\n          }\n          if (ngt <= top_k) {\n            #pragma omp atomic update\n            ++count[0];\n          }\n        }\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      printf(\"Average execution time of accuracy kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n      #pragma omp target update from (count[0:1]) \n      bool ok = (count[0] == count_ref);\n      printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n      \n\n    }\n  }\n\n  free(label);\n  free(data);\n\n  return 0;\n}\n"}}
{"kernel_name": "accuracy", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <random>\n#include \"reference.h\"\n\n#define NUM_THREADS 256\n\nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: %s <number of rows> <number of columns> <top K> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int nrows = atoi(argv[1]);\n  const int ndims = atoi(argv[2]);\n  const int top_k = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int data_size = nrows * ndims;\n\n  const int label_size_bytes = nrows * sizeof(int); \n  const size_t data_size_bytes = data_size * sizeof(float); \n\n  int *label = (int*) malloc (label_size_bytes);\n\n  srand(123);\n  for (int i = 0; i < nrows; i++)\n    label[i] = rand() % ndims; \n\n  float *data = (float*) malloc (data_size_bytes);\n\n  std::default_random_engine g (123);\n  std::uniform_real_distribution<float> distr (0.f, 1.f);\n  for (int i = 0; i < data_size; i++) {\n    data[i] = distr(g);\n  }\n\n  int count_ref = reference(nrows, ndims, top_k, data, label);\n\n  int count[1];\n\n    {\n    for (int ngrid = nrows / 4; ngrid <= nrows; ngrid += nrows / 4) {\n\n      printf(\"Grid size is %d\\n\", ngrid);\n\n      auto start = std::chrono::steady_clock::now();\n\n      for (int i = 0; i < repeat; i++) {\n        count[0] = 0;\n        \n                for (int row = 0; row < nrows; row++) {\n          const int label_data = label[row];\n          const float label_pred = data[row * ndims + label_data];\n          int ngt = 0;\n                    for (int col = 0; col < ndims; col++) {\n            const float pred = data[row * ndims + col];\n            if (pred > label_pred || (pred == label_pred && col <= label_data)) {\n              ++ngt;\n            }\n          }\n          if (ngt <= top_k) {\n                        ++count[0];\n          }\n        }\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      printf(\"Average execution time of accuracy kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n            bool ok = (count[0] == count_ref);\n      printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n      \n\n    }\n  }\n\n  free(label);\n  free(data);\n\n  return 0;\n}"}}
{"kernel_name": "accuracy", "parallel_api": "sycl", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <random>\n#include <sycl/sycl.hpp>\n#include \"reference.h\"\n\n#define GPU_NUM_THREADS 256\n\nvoid accuracy_kernel(\n    sycl::nd_item<1> &item,\n    const int N,\n    const int D,\n    const int top_k,\n    const float* Xdata,\n    const int* labelData,\n    int* accuracy)\n{\n  int count = 0;\n\n  for (int row = item.get_group(0); row < N; row += item.get_group_range(0)) {\n    const int label = labelData[row];\n    const float label_pred = Xdata[row * D + label];\n    int ngt = 0;\n    for (int col = item.get_local_id(0); col < D; col += item.get_local_range(0)) {\n      const float pred = Xdata[row * D + col];\n      if (pred > label_pred || (pred == label_pred && col <= label)) {\n        ++ngt;\n      }\n    }\n    ngt = sycl::reduce_over_group(item.get_group(), ngt, std::plus<>());\n    if (ngt <= top_k) {\n      ++count;\n    }\n    item.barrier(sycl::access::fence_space::local_space);\n  }\n  if (item.get_local_id(0) == 0) { \n    auto ao = sycl::atomic_ref<int,\n                                sycl::memory_order::relaxed,\n                                sycl::memory_scope::device,\n                                sycl::access::address_space::global_space> (accuracy[0]);\n    ao.fetch_add(count);\n  }\n}\n \nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: %s <number of rows> <number of columns> <top K> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int nrows = atoi(argv[1]);\n  const int ndims = atoi(argv[2]);\n  const int top_k = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int data_size = nrows * ndims;\n\n  const int label_size_bytes = nrows * sizeof(int); \n  const size_t data_size_bytes = data_size * sizeof(float);\n\n  int *label = (int*) malloc (label_size_bytes);\n\n  srand(123);\n  for (int i = 0; i < nrows; i++)\n    label[i] = rand() % ndims; \n\n  float *data = (float*) malloc (data_size_bytes);\n\n  std::default_random_engine g (123);\n  std::uniform_real_distribution<float> distr (0.f, 1.f);\n  for (int i = 0; i < data_size; i++) {\n    data[i] = distr(g);\n  }\n\n  int count_ref = reference(nrows, ndims, top_k, data, label);\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  int *d_label = sycl::malloc_device<int>(nrows, q);\n  q.memcpy(d_label, label, label_size_bytes);\n\n  float *d_data = sycl::malloc_device<float>(data_size, q);\n  q.memcpy(d_data, data, data_size_bytes);\n\n  int *d_count = sycl::malloc_device<int>(1, q);\n\n  q.wait();\n  sycl::range<1> lws (GPU_NUM_THREADS);\n\n  for (int ngrid = nrows / 4; ngrid <= nrows; ngrid += nrows / 4) {\n\n    printf(\"Grid size is %d\\n\", ngrid);\n    sycl::range<1> gws (ngrid * GPU_NUM_THREADS);\n\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      q.memset(d_count, 0, sizeof(int));\n      q.submit([&] (sycl::handler &cgh) {\n        cgh.parallel_for<class accuracy>(\n          sycl::nd_range<1>(gws, lws), [=] (sycl::nd_item<1> item) {\n          accuracy_kernel(item, nrows, ndims, top_k, d_data, d_label, d_count);\n\t});\n      });\n    }\n\n    q.wait();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average execution time of accuracy kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n    int count;\n    q.memcpy(&count, d_count, sizeof(int)).wait();\n    bool ok = (count == count_ref);\n    printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n    \n\n  }\n\n  sycl::free(d_label, q);\n  sycl::free(d_data, q);\n  sycl::free(d_count, q);\n\n  free(label);\n  free(data);\n\n  return 0;\n}\n"}}
{"kernel_name": "gabor", "parallel_api": "cuda", "code": {"main.cu": "#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <cuda.h>\n#include \"reference.h\"\n\n__global__\nvoid gabor (\n  double *gabor_spatial,\n  const unsigned int height,\n  const unsigned int width,\n  const double center_y,\n  const double center_x,\n  const double ctheta,\n  const double stheta,\n  const double scale,\n  const double sx_2,\n  const double sy_2,\n  const double fx)\n{\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  double centered_x, centered_y, u, v;\n\n  if (x < width && y < height) {\n    centered_y = (double)y - center_y;\n    centered_x = (double)x - center_x;\n    u = ctheta * centered_x - stheta * centered_y;\n    v = ctheta * centered_y + stheta * centered_x;\n    gabor_spatial[y*width + x] = scale * exp(-0.5*(u*u/sx_2 + v*v/sy_2)) * cos(2.0*M_PI*fx*u);\n  }\n}\n\ndouble* generateGaborKernelDevice(\n  const int repeat,\n  const unsigned int height,\n  const unsigned int width,\n  const unsigned int par_T,\n  const double par_L,\n  const double theta)\n{\n  const double sx = (double)par_T / (2.0*sqrt(2.0*log(2.0)));\n  const double sy = par_L * sx;\n  const double sx_2 = sx*sx;\n  const double sy_2 = sy*sy;\n  const double fx = 1.0 / (double)par_T;\n  const double ctheta = cos(theta);\n  const double stheta = sin(theta);\n  const double center_y = (double)height / 2.0;\n  const double center_x = (double)width / 2.0;\n  const double scale = 1.0/(2.0*M_PI*sx*sy);\n\n  size_t image_size_bytes = height * width * sizeof(double);\n  double *h_gabor_spatial = (double*) malloc (image_size_bytes);\n\n  double *d_gabor_spatial;\n  cudaMalloc((void**)&d_gabor_spatial, image_size_bytes);\n\n  dim3 grids ((width+15)/16, (height+15)/16);\n  dim3 blocks (16, 16);\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    gabor<<<grids, blocks>>>(d_gabor_spatial,\n                             height,\n                             width, \n                             center_y,\n                             center_x,\n                             ctheta,\n                             stheta,\n                             scale,\n                             sx_2,\n                             sy_2,\n                             fx);\n  }\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  cudaMemcpy(h_gabor_spatial, d_gabor_spatial, image_size_bytes, cudaMemcpyDeviceToHost);\n  cudaFree(d_gabor_spatial);\n  return h_gabor_spatial;\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 4) {\n    printf(\"Usage: %s <height> <width> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int height = atoi(argv[1]);\n  const int width = atoi(argv[2]);\n  const int repeat = atoi(argv[3]);\n\n  const unsigned int par_T = 13;\n  const double par_L = 2.65;\n  const double theta = 45;\n\n  double *h_filter = generateGaborKernelHost(height, width, par_T, par_L, theta);\n  double *d_filter = generateGaborKernelDevice(repeat, height, width, par_T, par_L, theta);\n  \n  bool ok = true;\n  for (int i = 0; i < width * height; i++) {\n    if (fabs(h_filter[i] - d_filter[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n  free(h_filter);\n  free(d_filter);\n}\n"}}
{"kernel_name": "gabor", "parallel_api": "hip", "code": {"main.cu": "#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <hip/hip_runtime.h>\n#include \"reference.h\"\n\n__global__\nvoid gabor (\n  double *gabor_spatial,\n  const unsigned int height,\n  const unsigned int width,\n  const double center_y,\n  const double center_x,\n  const double ctheta,\n  const double stheta,\n  const double scale,\n  const double sx_2,\n  const double sy_2,\n  const double fx)\n{\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  double centered_x, centered_y, u, v;\n\n  if (x < width && y < height) {\n    centered_y = (double)y - center_y;\n    centered_x = (double)x - center_x;\n    u = ctheta * centered_x - stheta * centered_y;\n    v = ctheta * centered_y + stheta * centered_x;\n    gabor_spatial[y*width + x] = scale * exp(-0.5*(u*u/sx_2 + v*v/sy_2)) * cos(2.0*M_PI*fx*u);\n  }\n}\n\ndouble* generateGaborKernelDevice(\n  const int repeat,\n  const unsigned int height,\n  const unsigned int width,\n  const unsigned int par_T,\n  const double par_L,\n  const double theta)\n{\n  const double sx = (double)par_T / (2.0*sqrt(2.0*log(2.0)));\n  const double sy = par_L * sx;\n  const double sx_2 = sx*sx;\n  const double sy_2 = sy*sy;\n  const double fx = 1.0 / (double)par_T;\n  const double ctheta = cos(theta);\n  const double stheta = sin(theta);\n  const double center_y = (double)height / 2.0;\n  const double center_x = (double)width / 2.0;\n  const double scale = 1.0/(2.0*M_PI*sx*sy);\n\n  size_t image_size_bytes = height * width * sizeof(double);\n  double *h_gabor_spatial = (double*) malloc (image_size_bytes);\n\n  double *d_gabor_spatial;\n  hipMalloc((void**)&d_gabor_spatial, image_size_bytes);\n\n  dim3 grids ((width+15)/16, (height+15)/16);\n  dim3 blocks (16, 16);\n\n  hipDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    hipLaunchKernelGGL(gabor, grids, blocks, 0, 0, d_gabor_spatial,\n                             height,\n                             width, \n                             center_y,\n                             center_x,\n                             ctheta,\n                             stheta,\n                             scale,\n                             sx_2,\n                             sy_2,\n                             fx);\n  }\n\n  hipDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  hipMemcpy(h_gabor_spatial, d_gabor_spatial, image_size_bytes, hipMemcpyDeviceToHost);\n  hipFree(d_gabor_spatial);\n  return h_gabor_spatial;\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 4) {\n    printf(\"Usage: %s <height> <width> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int height = atoi(argv[1]);\n  const int width = atoi(argv[2]);\n  const int repeat = atoi(argv[3]);\n\n  const unsigned int par_T = 13;\n  const double par_L = 2.65;\n  const double theta = 45;\n\n  double *h_filter = generateGaborKernelHost(height, width, par_T, par_L, theta);\n  double *d_filter = generateGaborKernelDevice(repeat, height, width, par_T, par_L, theta);\n  \n  bool ok = true;\n  for (int i = 0; i < width * height; i++) {\n    if (fabs(h_filter[i] - d_filter[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n  free(h_filter);\n  free(d_filter);\n}\n"}}
{"kernel_name": "gabor", "parallel_api": "omp", "code": {"main.cpp": "#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <omp.h>\n#include \"reference.h\"\n\ndouble* generateGaborKernelDevice(\n  const int repeat,\n  const unsigned int height,\n  const unsigned int width,\n  const unsigned int par_T,\n  const double par_L,\n  const double theta)\n{\n  const double sx = (double)par_T / (2.0*sqrt(2.0*log(2.0)));\n  const double sy = par_L * sx;\n  const double sx_2 = sx*sx;\n  const double sy_2 = sy*sy;\n  const double fx = 1.0 / (double)par_T;\n  const double ctheta = cos(theta);\n  const double stheta = sin(theta);\n  const double center_y = (double)height / 2.0;\n  const double center_x = (double)width / 2.0;\n  const double scale = 1.0/(2.0*M_PI*sx*sy);\n\n  size_t image_size_bytes = height * width * sizeof(double);\n  double *gabor_spatial = (double*) malloc (image_size_bytes);\n\n  #pragma omp target data map (from: gabor_spatial[0:height * width])\n  {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      #pragma omp target teams distribute parallel for collapse(2) thread_limit(256)\n      for (int y = 0; y < height; y++) {\n        for (int x = 0; x < width; x++) {\n          double centered_y = (double)y - center_y;\n          double centered_x = (double)x - center_x;\n          double u = ctheta * centered_x - stheta * centered_y;\n          double v = ctheta * centered_y + stheta * centered_x;\n          gabor_spatial[y*width + x] = scale * exp(-0.5*(u*u/sx_2 + v*v/sy_2)) * cos(2.0*M_PI*fx*u);\n        }\n      }\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n  }\n\n  return gabor_spatial;\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 4) {\n    printf(\"Usage: %s <height> <width> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int height = atoi(argv[1]);\n  const int width = atoi(argv[2]);\n  const int repeat = atoi(argv[3]);\n\n  const unsigned int par_T = 13;\n  const double par_L = 2.65;\n  const double theta = 45;\n\n  double *h_filter = generateGaborKernelHost(height, width, par_T, par_L, theta);\n  double *d_filter = generateGaborKernelDevice(repeat, height, width, par_T, par_L, theta);\n  \n  bool ok = true;\n  for (int i = 0; i < width * height; i++) {\n    if (fabs(h_filter[i] - d_filter[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n  free(h_filter);\n  free(d_filter);\n}\n"}}
{"kernel_name": "gabor", "parallel_api": "serial", "code": {"main.cpp": "#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include \"reference.h\"\n\ndouble* generateGaborKernelDevice(\n  const int repeat,\n  const unsigned int height,\n  const unsigned int width,\n  const unsigned int par_T,\n  const double par_L,\n  const double theta)\n{\n  const double sx = (double)par_T / (2.0*sqrt(2.0*log(2.0)));\n  const double sy = par_L * sx;\n  const double sx_2 = sx*sx;\n  const double sy_2 = sy*sy;\n  const double fx = 1.0 / (double)par_T;\n  const double ctheta = cos(theta);\n  const double stheta = sin(theta);\n  const double center_y = (double)height / 2.0;\n  const double center_x = (double)width / 2.0;\n  const double scale = 1.0/(2.0*M_PI*sx*sy);\n\n  size_t image_size_bytes = height * width * sizeof(double);\n  double *gabor_spatial = (double*) malloc (image_size_bytes);\n\n    {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n            for (int y = 0; y < height; y++) {\n        for (int x = 0; x < width; x++) {\n          double centered_y = (double)y - center_y;\n          double centered_x = (double)x - center_x;\n          double u = ctheta * centered_x - stheta * centered_y;\n          double v = ctheta * centered_y + stheta * centered_x;\n          gabor_spatial[y*width + x] = scale * exp(-0.5*(u*u/sx_2 + v*v/sy_2)) * cos(2.0*M_PI*fx*u);\n        }\n      }\n    }\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n  }\n\n  return gabor_spatial;\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 4) {\n    printf(\"Usage: %s <height> <width> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int height = atoi(argv[1]);\n  const int width = atoi(argv[2]);\n  const int repeat = atoi(argv[3]);\n\n  const unsigned int par_T = 13;\n  const double par_L = 2.65;\n  const double theta = 45;\n\n  double *h_filter = generateGaborKernelHost(height, width, par_T, par_L, theta);\n  double *d_filter = generateGaborKernelDevice(repeat, height, width, par_T, par_L, theta);\n  \n  bool ok = true;\n  for (int i = 0; i < width * height; i++) {\n    if (fabs(h_filter[i] - d_filter[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n  free(h_filter);\n  free(d_filter);\n}"}}
{"kernel_name": "gabor", "parallel_api": "sycl", "code": {"main.cpp": "#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <sycl/sycl.hpp>\n#include \"reference.h\"\n\nvoid gabor (\n  sycl::nd_item<2> &item,\n  double *gabor_spatial,\n  const unsigned int height,\n  const unsigned int width,\n  const double center_y,\n  const double center_x,\n  const double ctheta,\n  const double stheta,\n  const double scale,\n  const double sx_2,\n  const double sy_2,\n  const double fx)\n{\n  int x = item.get_global_id(1);\n  int y = item.get_global_id(0);\n\n  double centered_x, centered_y, u, v;\n\n  if (x < width && y < height) {\n    centered_y = (double)y - center_y;\n    centered_x = (double)x - center_x;\n    u = ctheta * centered_x - stheta * centered_y;\n    v = ctheta * centered_y + stheta * centered_x;\n    gabor_spatial[y*width + x] = scale * sycl::exp(-0.5*(u*u/sx_2 + v*v/sy_2)) *\n                                 sycl::cos(2.0*M_PI*fx*u);\n  }\n}\n\ndouble* generateGaborKernelDevice(\n  const int repeat,\n  const unsigned int height,\n  const unsigned int width,\n  const unsigned int par_T,\n  const double par_L,\n  const double theta)\n{\n  const double sx = (double)par_T / (2.0*sqrt(2.0*log(2.0)));\n  const double sy = par_L * sx;\n  const double sx_2 = sx*sx;\n  const double sy_2 = sy*sy;\n  const double fx = 1.0 / (double)par_T;\n  const double ctheta = cos(theta);\n  const double stheta = sin(theta);\n  const double center_y = (double)height / 2.0;\n  const double center_x = (double)width / 2.0;\n  const double scale = 1.0/(2.0*M_PI*sx*sy);\n\n  size_t image_size_bytes = height * width * sizeof(double);\n  double *h_gabor_spatial = (double*) malloc (image_size_bytes);\n\n#ifdef USE_GPU\n  sycl::queue q(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue q(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  double *d_gabor_spatial = sycl::malloc_device<double>(height * width, q);\n  q.memcpy(d_gabor_spatial, h_gabor_spatial, image_size_bytes);\n\n  sycl::range<2> gws ((height+15)/16*16, (width+15)/16*16);\n  sycl::range<2> lws (16, 16);\n\n  q.wait();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) {\n    q.submit([&] (sycl::handler &cgh) {\n      cgh.parallel_for<class image_process>(\n         sycl::nd_range<2>(gws, lws), [=] (sycl::nd_item<2> item) {\n         gabor(item,\n               d_gabor_spatial,\n               height,\n               width, \n               center_y,\n               center_x,\n               ctheta,\n               stheta,\n               scale,\n               sx_2,\n               sy_2,\n               fx);\n      });\n    });\n  }\n\n  q.wait();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average kernel execution time: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  q.memcpy(h_gabor_spatial, d_gabor_spatial, image_size_bytes).wait();\n  sycl::free(d_gabor_spatial, q);\n\n  return h_gabor_spatial;\n}\n\nint main(int argc, char* argv[]) {\n  if (argc != 4) {\n    printf(\"Usage: %s <height> <width> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int height = atoi(argv[1]);\n  const int width = atoi(argv[2]);\n  const int repeat = atoi(argv[3]);\n\n  const unsigned int par_T = 13;\n  const double par_L = 2.65;\n  const double theta = 45;\n\n  double *h_filter = generateGaborKernelHost(height, width, par_T, par_L, theta);\n  double *d_filter = generateGaborKernelDevice(repeat, height, width, par_T, par_L, theta);\n  \n  bool ok = true;\n  for (int i = 0; i < width * height; i++) {\n    if (fabs(h_filter[i] - d_filter[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n  free(h_filter);\n  free(d_filter);\n}\n"}}
{"kernel_name": "permute", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include \"common.h\"\n\n\n\nvoid permuate_cpu(float *inp, float *q, float *k, float *v, int B, int T, int C, int NH) {\n  int i = 0;\n  for (int b = 0; b < B; b++) {\n    for (int n = 0; n < NH; n++) {\n      for (int t = 0; t < T; t++) {\n        for (int c = n * (C/NH); c < (n+1)*(C/NH); c++) {\n          q[i] = inp[b * T * 3 * C + t * 3 * C + c];\n          k[i] = inp[b * T * 3 * C + t * 3 * C + C + c];\n          v[i] = inp[b * T * 3 * C + t * 3 * C + 2*C + c];\n          i++;\n        }}}}\n}\n\n__global__ void permute_kernel(\n    float* __restrict__ q,\n    float* __restrict__ k,\n    float* __restrict__ v,\n    const float* inp,\n    int B, int T, int NH, int d) {\n  \n\n  \n\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  \n\n  int C = NH * d;\n\n  if (idx < B * C * T) {\n    int b = idx / (C * T);\n    int rest = idx % (C * T);\n    int nh_ = rest / (T * d);\n    rest = rest % (T * d);\n    int n = rest / d;\n    int d_ = rest % d;\n\n    int inp_idx = \\\n            (b * T * 3 * C)\n            +   (n * 3 * C)\n            +       (0 * C)\n            +          (nh_ * d)\n            +                d_;\n\n    q[idx] = inp[inp_idx];\n    k[idx] = inp[inp_idx + C];\n    v[idx] = inp[inp_idx + 2 * C];\n  }\n}\n\nvoid permute (float* out, const float* inp,\n    int B, int T, int C, int NH,\n    const int block_size) {\n  \n\n  int HS = C / NH; \n\n\n  \n\n  float *q, *k, *v;\n  q = out + 0 * B * T * C;\n  k = out + 1 * B * T * C;\n  v = out + 2 * B * T * C;\n  int total_threads = B * T * C;\n  int num_blocks = ceil_div(total_threads, block_size);\n  permute_kernel<<<num_blocks, block_size>>>(q, k, v, inp, B, T, NH, HS);\n  cudaDeviceSynchronize();\n}\n\nint main(int argc, char **argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <batch size> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int B = atoi(argv[1]);\n  const int repeat_times = atoi(argv[2]);\n\n  const int T = 1024;\n  const int C = 768;\n  const int NH = 12;\n\n  size_t S = (size_t)B * T * C;\n\n  \n\n  float* inp = make_random_float(S * 3);\n  float* out = make_random_float(S * 3);\n  float* q = make_random_float(S);\n  float* k = make_random_float(S);\n  float* v = make_random_float(S);\n\n  permuate_cpu(inp, q, k, v, B, T, C, NH);\n\n  \n\n  float *d_inp, *d_out;\n  cudaCheck(cudaMalloc(&d_inp, S * 3 * sizeof(float)));\n  cudaCheck(cudaMemcpy(d_inp, inp, S * 3 * sizeof(float), cudaMemcpyHostToDevice));\n  cudaCheck(cudaMalloc(&d_out, S * 3 * sizeof(float)));\n\n  int block_sizes[] = {32, 64, 128, 256, 512};\n\n  for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n    int block_size = block_sizes[j];\n    printf(\"Checking block size %d.\\n\", block_size);\n    permute (d_out, d_inp, B, T, C, NH, block_size);\n    validate_result(d_out, q, \"q\", S, 1e-6f);\n    validate_result(d_out+B*T*C, k, \"k\", S, 1e-6f);\n    validate_result(d_out+2*B*T*C, v, \"v\", S, 1e-6f);\n  }\n  printf(\"All results match. Starting benchmarks.\\n\\n\");\n\n  \n\n  for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n    int block_size = block_sizes[j];\n    float elapsed_time = benchmark_kernel(repeat_times, permute,\n        d_out, d_inp, B, T, C, NH, block_size);\n\n    printf(\"block_size %4d | time %f ms\\n\", block_size, elapsed_time);\n  }\n\n  \n\n  free(inp);\n  free(q);\n  free(k);\n  free(v);\n  free(out);\n  cudaCheck(cudaFree(d_inp));\n  cudaCheck(cudaFree(d_out));\n\n  return 0;\n}\n"}}
{"kernel_name": "permute", "parallel_api": "hip", "code": {"main.cu": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include \"common.h\"\n\n\n\nvoid permuate_cpu(float *inp, float *q, float *k, float *v, int B, int T, int C, int NH) {\n  int i = 0;\n  for (int b = 0; b < B; b++) {\n    for (int n = 0; n < NH; n++) {\n      for (int t = 0; t < T; t++) {\n        for (int c = n * (C/NH); c < (n+1)*(C/NH); c++) {\n          q[i] = inp[b * T * 3 * C + t * 3 * C + c];\n          k[i] = inp[b * T * 3 * C + t * 3 * C + C + c];\n          v[i] = inp[b * T * 3 * C + t * 3 * C + 2*C + c];\n          i++;\n        }}}}\n}\n\n__global__ void permute_kernel(\n    float* __restrict__ q,\n    float* __restrict__ k,\n    float* __restrict__ v,\n    const float* inp,\n    int B, int T, int NH, int d) {\n  \n\n  \n\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  \n\n  int C = NH * d;\n\n  if (idx < B * C * T) {\n    int b = idx / (C * T);\n    int rest = idx % (C * T);\n    int nh_ = rest / (T * d);\n    rest = rest % (T * d);\n    int n = rest / d;\n    int d_ = rest % d;\n\n    int inp_idx = \\\n            (b * T * 3 * C)\n            +   (n * 3 * C)\n            +       (0 * C)\n            +          (nh_ * d)\n            +                d_;\n\n    q[idx] = inp[inp_idx];\n    k[idx] = inp[inp_idx + C];\n    v[idx] = inp[inp_idx + 2 * C];\n  }\n}\n\nvoid permute (float* out, const float* inp,\n    int B, int T, int C, int NH,\n    const int block_size) {\n  \n\n  int HS = C / NH; \n\n\n  \n\n  float *q, *k, *v;\n  q = out + 0 * B * T * C;\n  k = out + 1 * B * T * C;\n  v = out + 2 * B * T * C;\n  int total_threads = B * T * C;\n  int num_blocks = ceil_div(total_threads, block_size);\n  permute_kernel<<<num_blocks, block_size>>>(q, k, v, inp, B, T, NH, HS);\n  hipDeviceSynchronize();\n}\n\nint main(int argc, char **argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <batch size> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int B = atoi(argv[1]);\n  const int repeat_times = atoi(argv[2]);\n\n  const int T = 1024;\n  const int C = 768;\n  const int NH = 12;\n\n  size_t S = (size_t)B * T * C;\n\n  \n\n  float* inp = make_random_float(S * 3);\n  float* out = make_random_float(S * 3);\n  float* q = make_random_float(S);\n  float* k = make_random_float(S);\n  float* v = make_random_float(S);\n\n  permuate_cpu(inp, q, k, v, B, T, C, NH);\n\n  \n\n  float *d_inp, *d_out;\n  hipCheck(hipMalloc(&d_inp, S * 3 * sizeof(float)));\n  hipCheck(hipMemcpy(d_inp, inp, S * 3 * sizeof(float), hipMemcpyHostToDevice));\n  hipCheck(hipMalloc(&d_out, S * 3 * sizeof(float)));\n\n  int block_sizes[] = {32, 64, 128, 256, 512};\n\n  for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n    int block_size = block_sizes[j];\n    printf(\"Checking block size %d.\\n\", block_size);\n    permute (d_out, d_inp, B, T, C, NH, block_size);\n    validate_result(d_out, q, \"q\", S, 1e-6f);\n    validate_result(d_out+B*T*C, k, \"k\", S, 1e-6f);\n    validate_result(d_out+2*B*T*C, v, \"v\", S, 1e-6f);\n  }\n  printf(\"All results match. Starting benchmarks.\\n\\n\");\n\n  \n\n  for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n    int block_size = block_sizes[j];\n    float elapsed_time = benchmark_kernel(repeat_times, permute,\n        d_out, d_inp, B, T, C, NH, block_size);\n\n    printf(\"block_size %4d | time %f ms\\n\", block_size, elapsed_time);\n  }\n\n  \n\n  free(inp);\n  free(q);\n  free(k);\n  free(v);\n  free(out);\n  hipCheck(hipFree(d_inp));\n  hipCheck(hipFree(d_out));\n\n  return 0;\n}\n"}}
{"kernel_name": "permute", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include \"common.h\"\n\n\n\nvoid permuate_cpu(float *inp, float *q, float *k, float *v, int B, int T, int C, int NH) {\n  int i = 0;\n  for (int b = 0; b < B; b++) {\n    for (int n = 0; n < NH; n++) {\n      for (int t = 0; t < T; t++) {\n        for (int c = n * (C/NH); c < (n+1)*(C/NH); c++) {\n          q[i] = inp[b * T * 3 * C + t * 3 * C + c];\n          k[i] = inp[b * T * 3 * C + t * 3 * C + C + c];\n          v[i] = inp[b * T * 3 * C + t * 3 * C + 2*C + c];\n          i++;\n        }}}}\n}\n\nvoid permute (float* out, const float* inp,\n    int B, int T, int C, int NH,\n    const int block_size) {\n  \n\n  int d = C / NH; \n\n\n  \n\n  float *q, *k, *v;\n  q = out + 0 * B * T * C;\n  k = out + 1 * B * T * C;\n  v = out + 2 * B * T * C;\n  int total_threads = B * T * C;\n  int num_blocks = ceil_div(total_threads, block_size);\n  \n\n  #pragma omp target teams distribute parallel for \\\n   num_teams(num_blocks) num_threads(block_size) \n  for (int idx = 0; idx < B * T * C; idx++) { \n    \n\n    int b = idx / (C * T);\n    int rest = idx % (C * T);\n    int nh_ = rest / (T * d);\n    rest = rest % (T * d);\n    int n = rest / d;\n    int d_ = rest % d;\n\n    int inp_idx = \\\n            (b * T * 3 * C)\n            +   (n * 3 * C)\n            +       (0 * C)\n            +          (nh_ * d)\n            +                d_;\n\n    q[idx] = inp[inp_idx];\n    k[idx] = inp[inp_idx + C];\n    v[idx] = inp[inp_idx + 2 * C];\n  }\n}\n\nint main(int argc, char **argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <batch size> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int B = atoi(argv[1]);\n  const int repeat_times = atoi(argv[2]);\n\n  const int T = 1024;\n  const int C = 768;\n  const int NH = 12;\n\n  size_t S = (size_t)B * T * C;\n\n  \n\n  float* inp = make_random_float(S * 3);\n  float* out = make_random_float(S * 3);\n  float* q = make_random_float(S);\n  float* k = make_random_float(S);\n  float* v = make_random_float(S);\n\n  permuate_cpu(inp, q, k, v, B, T, C, NH);\n\n  int block_sizes[] = {32, 64, 128, 256, 512};\n\n  #pragma omp target data map(to: inp[0:S*3]) map(alloc: out[0:S*3])\n  {\n    for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n      int block_size = block_sizes[j];\n      printf(\"Checking block size %d.\\n\", block_size);\n      permute (out, inp, B, T, C, NH, block_size);\n      validate_result(out, q, \"q\", S, 1e-6f);\n      validate_result(out+B*T*C, k, \"k\", S, 1e-6f);\n      validate_result(out+2*B*T*C, v, \"v\", S, 1e-6f);\n    }\n    printf(\"All results match. Starting benchmarks.\\n\\n\");\n\n    \n\n    for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n      int block_size = block_sizes[j];\n      float elapsed_time = benchmark_kernel(repeat_times, permute,\n          out, inp, B, T, C, NH, block_size);\n\n      printf(\"block_size %4d | time %f ms\\n\", block_size, elapsed_time);\n    }\n  }\n\n  \n\n  free(inp);\n  free(q);\n  free(k);\n  free(v);\n  free(out);\n\n  return 0;\n}\n"}}
{"kernel_name": "permute", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include \"common.h\"\n\n\n\nvoid permuate_cpu(float *inp, float *q, float *k, float *v, int B, int T, int C, int NH) {\n  int i = 0;\n  for (int b = 0; b < B; b++) {\n    for (int n = 0; n < NH; n++) {\n      for (int t = 0; t < T; t++) {\n        for (int c = n * (C/NH); c < (n+1)*(C/NH); c++) {\n          q[i] = inp[b * T * 3 * C + t * 3 * C + c];\n          k[i] = inp[b * T * 3 * C + t * 3 * C + C + c];\n          v[i] = inp[b * T * 3 * C + t * 3 * C + 2*C + c];\n          i++;\n        }}}}\n}\n\nvoid permute (float* out, const float* inp,\n    int B, int T, int C, int NH,\n    const int block_size) {\n  \n\n  int d = C / NH; \n\n\n  \n\n  float *q, *k, *v;\n  q = out + 0 * B * T * C;\n  k = out + 1 * B * T * C;\n  v = out + 2 * B * T * C;\n  int total_threads = B * T * C;\n  int num_blocks = ceil_div(total_threads, block_size);\n  \n\n    for (int idx = 0; idx < B * T * C; idx++) { \n    \n\n    int b = idx / (C * T);\n    int rest = idx % (C * T);\n    int nh_ = rest / (T * d);\n    rest = rest % (T * d);\n    int n = rest / d;\n    int d_ = rest % d;\n\n    int inp_idx = \\\n            (b * T * 3 * C)\n            +   (n * 3 * C)\n            +       (0 * C)\n            +          (nh_ * d)\n            +                d_;\n\n    q[idx] = inp[inp_idx];\n    k[idx] = inp[inp_idx + C];\n    v[idx] = inp[inp_idx + 2 * C];\n  }\n}\n\nint main(int argc, char **argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <batch size> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int B = atoi(argv[1]);\n  const int repeat_times = atoi(argv[2]);\n\n  const int T = 1024;\n  const int C = 768;\n  const int NH = 12;\n\n  size_t S = (size_t)B * T * C;\n\n  \n\n  float* inp = make_random_float(S * 3);\n  float* out = make_random_float(S * 3);\n  float* q = make_random_float(S);\n  float* k = make_random_float(S);\n  float* v = make_random_float(S);\n\n  permuate_cpu(inp, q, k, v, B, T, C, NH);\n\n  int block_sizes[] = {32, 64, 128, 256, 512};\n\n    {\n    for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n      int block_size = block_sizes[j];\n      printf(\"Checking block size %d.\\n\", block_size);\n      permute (out, inp, B, T, C, NH, block_size);\n      validate_result(out, q, \"q\", S, 1e-6f);\n      validate_result(out+B*T*C, k, \"k\", S, 1e-6f);\n      validate_result(out+2*B*T*C, v, \"v\", S, 1e-6f);\n    }\n    printf(\"All results match. Starting benchmarks.\\n\\n\");\n\n    \n\n    for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n      int block_size = block_sizes[j];\n      float elapsed_time = benchmark_kernel(repeat_times, permute,\n          out, inp, B, T, C, NH, block_size);\n\n      printf(\"block_size %4d | time %f ms\\n\", block_size, elapsed_time);\n    }\n  }\n\n  \n\n  free(inp);\n  free(q);\n  free(k);\n  free(v);\n  free(out);\n\n  return 0;\n}"}}
{"kernel_name": "permute", "parallel_api": "sycl", "code": {"main.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include \"common.hpp\"\n\n\n\nvoid permuate_cpu(float *inp, float *q, float *k, float *v, int B, int T, int C, int NH) {\n  int i = 0;\n  for (int b = 0; b < B; b++) {\n    for (int n = 0; n < NH; n++) {\n      for (int t = 0; t < T; t++) {\n        for (int c = n * (C/NH); c < (n+1)*(C/NH); c++) {\n          q[i] = inp[b * T * 3 * C + t * 3 * C + c];\n          k[i] = inp[b * T * 3 * C + t * 3 * C + C + c];\n          v[i] = inp[b * T * 3 * C + t * 3 * C + 2*C + c];\n          i++;\n        }}}}\n}\n\nvoid permute_kernel(\n    float* __restrict__ q,\n    float* __restrict__ k,\n    float* __restrict__ v,\n    const float* inp,\n    int B, int T, int NH, int d, const sycl::nd_item<1> &item) {\n  \n\n  \n\n  int idx = item.get_global_id(0);\n\n  \n\n  int C = NH * d;\n\n  if (idx < B * C * T) {\n    int b = idx / (C * T);\n    int rest = idx % (C * T);\n    int nh_ = rest / (T * d);\n    rest = rest % (T * d);\n    int n = rest / d;\n    int d_ = rest % d;\n\n    int inp_idx = \\\n            (b * T * 3 * C)\n            +   (n * 3 * C)\n            +       (0 * C)\n            +          (nh_ * d)\n            +                d_;\n\n    q[idx] = inp[inp_idx];\n    k[idx] = inp[inp_idx + C];\n    v[idx] = inp[inp_idx + 2 * C];\n  }\n}\n\nvoid permute (sycl::queue &que, float* out, const float* inp,\n    int B, int T, int C, int NH,\n    const int block_size) {\n  \n\n  int HS = C / NH; \n\n\n  \n\n  float *q, *k, *v;\n  q = out + 0 * B * T * C;\n  k = out + 1 * B * T * C;\n  v = out + 2 * B * T * C;\n  int total_threads = B * T * C;\n  int num_blocks = ceil_div(total_threads, block_size);\n  que.parallel_for(\n      sycl::nd_range<1>(sycl::range<1>(num_blocks * block_size),\n                        sycl::range<1>(block_size)),\n      [=](sycl::nd_item<1> item) {\n        permute_kernel(q, k, v, inp, B, T, NH, HS, item);\n  }).wait();\n}\n\nint main(int argc, char **argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <batch size> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int B = atoi(argv[1]);\n  const int repeat_times = atoi(argv[2]);\n\n  const int T = 1024;\n  const int C = 768;\n  const int NH = 12;\n\n  size_t S = (size_t)B * T * C;\n\n  \n\n  float* inp = make_random_float(S * 3);\n  float* out = make_random_float(S * 3);\n  float* q = make_random_float(S);\n  float* k = make_random_float(S);\n  float* v = make_random_float(S);\n\n  permuate_cpu(inp, q, k, v, B, T, C, NH);\n\n  \n\n#ifdef USE_GPU\n  sycl::queue que(sycl::gpu_selector_v, sycl::property::queue::in_order());\n#else\n  sycl::queue que(sycl::cpu_selector_v, sycl::property::queue::in_order());\n#endif\n\n  float *d_inp, *d_out;\n  d_inp = sycl::malloc_device<float>(S * 3, que);\n  que.memcpy(d_inp, inp, S * 3 * sizeof(float));\n  d_out = sycl::malloc_device<float>(S * 3, que);\n\n  int block_sizes[] = {32, 64, 128, 256, 512};\n\n  for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n    int block_size = block_sizes[j];\n    printf(\"Checking block size %d.\\n\", block_size);\n    permute (que, d_out, d_inp, B, T, C, NH, block_size);\n    validate_result(d_out, q, \"q\", S, 1e-6f);\n    validate_result(d_out+B*T*C, k, \"k\", S, 1e-6f);\n    validate_result(d_out+2*B*T*C, v, \"v\", S, 1e-6f);\n  }\n  printf(\"All results match. Starting benchmarks.\\n\\n\");\n\n  \n\n  for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n    int block_size = block_sizes[j];\n    float elapsed_time = benchmark_kernel(repeat_times, permute,\n        que, d_out, d_inp, B, T, C, NH, block_size);\n\n    printf(\"block_size %4d | time %f ms\\n\", block_size, elapsed_time);\n  }\n\n  \n\n  free(inp);\n  free(q);\n  free(k);\n  free(v);\n  free(out);\n  sycl::free(d_inp, que);\n  sycl::free(d_out, que);\n\n  return 0;\n}\n"}}
