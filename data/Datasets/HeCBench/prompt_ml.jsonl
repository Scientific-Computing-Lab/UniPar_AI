{"kernel_name": "accuracy", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <random>\n#include <cuda.h>\n#include <cub/cub.cuh>\n#include \"reference.h\"\n\n#define GPU_NUM_THREADS 256\n\n__global__\nvoid accuracy_kernel(\n    const int N,\n    const int D,\n    const int top_k,\n    const float* Xdata,\n    const int* labelData,\n    int* accuracy)\n{\n  typedef cub::BlockReduce<int, GPU_NUM_THREADS> BlockReduce;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n  int count = 0;\n\n  for (int row = blockIdx.x; row < N; row += gridDim.x) {\n    const int label = labelData[row];\n    const float label_pred = Xdata[row * D + label];\n    int ngt = 0;\n    for (int col = threadIdx.x; col < D; col += blockDim.x) {\n      const float pred = Xdata[row * D + col];\n      if (pred > label_pred || (pred == label_pred && col <= label)) {\n        ++ngt;\n      }\n    }\n    ngt = BlockReduce(temp_storage).Sum(ngt);\n    if (ngt <= top_k) {\n      ++count;\n    }\n    __syncthreads();\n  }\n  if (threadIdx.x == 0) { \n    atomicAdd(accuracy, count);\n  }\n}\n \nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: %s <number of rows> <number of columns> <top K> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int nrows = atoi(argv[1]);\n  const int ndims = atoi(argv[2]);\n  const int top_k = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int data_size = nrows * ndims;\n\n  const int label_size_bytes = nrows * sizeof(int); \n  const size_t data_size_bytes = data_size * sizeof(float); \n\n  int *label = (int*) malloc (label_size_bytes);\n\n  srand(123);\n  for (int i = 0; i < nrows; i++)\n    label[i] = rand() % ndims; \n\n  float *data = (float*) malloc (data_size_bytes);\n\n  std::default_random_engine g (123);\n  std::uniform_real_distribution<float> distr (0.f, 1.f);\n  for (int i = 0; i < data_size; i++) {\n    data[i] = distr(g);\n  }\n\n  int count_ref = reference(nrows, ndims, top_k, data, label);\n\n  int *d_label;\n  cudaMalloc((void**)&d_label, label_size_bytes);\n  cudaMemcpy(d_label, label, label_size_bytes, cudaMemcpyHostToDevice);\n\n  float *d_data;\n  cudaMalloc((void**)&d_data, data_size_bytes);\n  cudaMemcpy(d_data, data, data_size_bytes, cudaMemcpyHostToDevice);\n\n  int *d_count;\n  cudaMalloc((void**)&d_count, sizeof(int));\n\n  cudaDeviceSynchronize();\n  dim3 block (GPU_NUM_THREADS);\n\n  for (int ngrid = nrows / 4; ngrid <= nrows; ngrid += nrows / 4) {\n\n    dim3 grid (ngrid);\n    printf(\"Grid size is %d\\n\", ngrid);\n\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) {\n      cudaMemset(d_count, 0, sizeof(int));\n      accuracy_kernel<<<grid, block>>>(nrows, ndims, top_k, d_data, d_label, d_count);\n    }\n\n    cudaDeviceSynchronize();\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average execution time of accuracy kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n    int count;\n    cudaMemcpy(&count, d_count, sizeof(int), cudaMemcpyDeviceToHost);\n    bool ok = (count == count_ref);\n    printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n    \n\n  }\n\n  cudaFree(d_label);\n  cudaFree(d_data);\n  cudaFree(d_count);\n\n  free(label);\n  free(data);\n\n  return 0;\n}\n"}}
{"kernel_name": "accuracy", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <random>\n#include <omp.h>\n#include \"reference.h\"\n\n#define NUM_THREADS 256\n\nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: %s <number of rows> <number of columns> <top K> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int nrows = atoi(argv[1]);\n  const int ndims = atoi(argv[2]);\n  const int top_k = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int data_size = nrows * ndims;\n\n  const int label_size_bytes = nrows * sizeof(int); \n  const size_t data_size_bytes = data_size * sizeof(float); \n\n  int *label = (int*) malloc (label_size_bytes);\n\n  srand(123);\n  for (int i = 0; i < nrows; i++)\n    label[i] = rand() % ndims; \n\n  float *data = (float*) malloc (data_size_bytes);\n\n  std::default_random_engine g (123);\n  std::uniform_real_distribution<float> distr (0.f, 1.f);\n  for (int i = 0; i < data_size; i++) {\n    data[i] = distr(g);\n  }\n\n  int count_ref = reference(nrows, ndims, top_k, data, label);\n\n  int count[1];\n\n  #pragma omp target data map(to: label[0:nrows], data[0:data_size]) \\\n                          map(alloc: count[0:1])\n  {\n    for (int ngrid = nrows / 4; ngrid <= nrows; ngrid += nrows / 4) {\n\n      printf(\"Grid size is %d\\n\", ngrid);\n\n      auto start = std::chrono::steady_clock::now();\n\n      for (int i = 0; i < repeat; i++) {\n        count[0] = 0;\n        #pragma omp target update to (count[0:1]) \n\n        #pragma omp target teams distribute num_teams(ngrid)\n        for (int row = 0; row < nrows; row++) {\n          const int label_data = label[row];\n          const float label_pred = data[row * ndims + label_data];\n          int ngt = 0;\n          #pragma omp parallel for reduction(+:ngt) num_threads(NUM_THREADS)\n          for (int col = 0; col < ndims; col++) {\n            const float pred = data[row * ndims + col];\n            if (pred > label_pred || (pred == label_pred && col <= label_data)) {\n              ++ngt;\n            }\n          }\n          if (ngt <= top_k) {\n            #pragma omp atomic update\n            ++count[0];\n          }\n        }\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      printf(\"Average execution time of accuracy kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n      #pragma omp target update from (count[0:1]) \n      bool ok = (count[0] == count_ref);\n      printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n      \n\n    }\n  }\n\n  free(label);\n  free(data);\n\n  return 0;\n}\n"}}
{"kernel_name": "accuracy", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <chrono>\n#include <random>\n#include \"reference.h\"\n\n#define NUM_THREADS 256\n\nint main(int argc, char* argv[])\n{\n  if (argc != 5) {\n    printf(\"Usage: %s <number of rows> <number of columns> <top K> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int nrows = atoi(argv[1]);\n  const int ndims = atoi(argv[2]);\n  const int top_k = atoi(argv[3]);\n  const int repeat = atoi(argv[4]);\n\n  const int data_size = nrows * ndims;\n\n  const int label_size_bytes = nrows * sizeof(int); \n  const size_t data_size_bytes = data_size * sizeof(float); \n\n  int *label = (int*) malloc (label_size_bytes);\n\n  srand(123);\n  for (int i = 0; i < nrows; i++)\n    label[i] = rand() % ndims; \n\n  float *data = (float*) malloc (data_size_bytes);\n\n  std::default_random_engine g (123);\n  std::uniform_real_distribution<float> distr (0.f, 1.f);\n  for (int i = 0; i < data_size; i++) {\n    data[i] = distr(g);\n  }\n\n  int count_ref = reference(nrows, ndims, top_k, data, label);\n\n  int count[1];\n\n    {\n    for (int ngrid = nrows / 4; ngrid <= nrows; ngrid += nrows / 4) {\n\n      printf(\"Grid size is %d\\n\", ngrid);\n\n      auto start = std::chrono::steady_clock::now();\n\n      for (int i = 0; i < repeat; i++) {\n        count[0] = 0;\n        \n                for (int row = 0; row < nrows; row++) {\n          const int label_data = label[row];\n          const float label_pred = data[row * ndims + label_data];\n          int ngt = 0;\n                    for (int col = 0; col < ndims; col++) {\n            const float pred = data[row * ndims + col];\n            if (pred > label_pred || (pred == label_pred && col <= label_data)) {\n              ++ngt;\n            }\n          }\n          if (ngt <= top_k) {\n                        ++count[0];\n          }\n        }\n      }\n\n      auto end = std::chrono::steady_clock::now();\n      auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n      printf(\"Average execution time of accuracy kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n            bool ok = (count[0] == count_ref);\n      printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n      \n\n    }\n  }\n\n  free(label);\n  free(data);\n\n  return 0;\n}"}}
{"kernel_name": "permute", "parallel_api": "cuda", "code": {"main.cu": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include \"common.h\"\n\n\n\nvoid permuate_cpu(float *inp, float *q, float *k, float *v, int B, int T, int C, int NH) {\n  int i = 0;\n  for (int b = 0; b < B; b++) {\n    for (int n = 0; n < NH; n++) {\n      for (int t = 0; t < T; t++) {\n        for (int c = n * (C/NH); c < (n+1)*(C/NH); c++) {\n          q[i] = inp[b * T * 3 * C + t * 3 * C + c];\n          k[i] = inp[b * T * 3 * C + t * 3 * C + C + c];\n          v[i] = inp[b * T * 3 * C + t * 3 * C + 2*C + c];\n          i++;\n        }}}}\n}\n\n__global__ void permute_kernel(\n    float* __restrict__ q,\n    float* __restrict__ k,\n    float* __restrict__ v,\n    const float* inp,\n    int B, int T, int NH, int d) {\n  \n\n  \n\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  \n\n  int C = NH * d;\n\n  if (idx < B * C * T) {\n    int b = idx / (C * T);\n    int rest = idx % (C * T);\n    int nh_ = rest / (T * d);\n    rest = rest % (T * d);\n    int n = rest / d;\n    int d_ = rest % d;\n\n    int inp_idx = \\\n            (b * T * 3 * C)\n            +   (n * 3 * C)\n            +       (0 * C)\n            +          (nh_ * d)\n            +                d_;\n\n    q[idx] = inp[inp_idx];\n    k[idx] = inp[inp_idx + C];\n    v[idx] = inp[inp_idx + 2 * C];\n  }\n}\n\nvoid permute (float* out, const float* inp,\n    int B, int T, int C, int NH,\n    const int block_size) {\n  \n\n  int HS = C / NH; \n\n\n  \n\n  float *q, *k, *v;\n  q = out + 0 * B * T * C;\n  k = out + 1 * B * T * C;\n  v = out + 2 * B * T * C;\n  int total_threads = B * T * C;\n  int num_blocks = ceil_div(total_threads, block_size);\n  permute_kernel<<<num_blocks, block_size>>>(q, k, v, inp, B, T, NH, HS);\n  cudaDeviceSynchronize();\n}\n\nint main(int argc, char **argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <batch size> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int B = atoi(argv[1]);\n  const int repeat_times = atoi(argv[2]);\n\n  const int T = 1024;\n  const int C = 768;\n  const int NH = 12;\n\n  size_t S = (size_t)B * T * C;\n\n  \n\n  float* inp = make_random_float(S * 3);\n  float* out = make_random_float(S * 3);\n  float* q = make_random_float(S);\n  float* k = make_random_float(S);\n  float* v = make_random_float(S);\n\n  permuate_cpu(inp, q, k, v, B, T, C, NH);\n\n  \n\n  float *d_inp, *d_out;\n  cudaCheck(cudaMalloc(&d_inp, S * 3 * sizeof(float)));\n  cudaCheck(cudaMemcpy(d_inp, inp, S * 3 * sizeof(float), cudaMemcpyHostToDevice));\n  cudaCheck(cudaMalloc(&d_out, S * 3 * sizeof(float)));\n\n  int block_sizes[] = {32, 64, 128, 256, 512};\n\n  for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n    int block_size = block_sizes[j];\n    printf(\"Checking block size %d.\\n\", block_size);\n    permute (d_out, d_inp, B, T, C, NH, block_size);\n    validate_result(d_out, q, \"q\", S, 1e-6f);\n    validate_result(d_out+B*T*C, k, \"k\", S, 1e-6f);\n    validate_result(d_out+2*B*T*C, v, \"v\", S, 1e-6f);\n  }\n  printf(\"All results match. Starting benchmarks.\\n\\n\");\n\n  \n\n  for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n    int block_size = block_sizes[j];\n    float elapsed_time = benchmark_kernel(repeat_times, permute,\n        d_out, d_inp, B, T, C, NH, block_size);\n\n    printf(\"block_size %4d | time %f ms\\n\", block_size, elapsed_time);\n  }\n\n  \n\n  free(inp);\n  free(q);\n  free(k);\n  free(v);\n  free(out);\n  cudaCheck(cudaFree(d_inp));\n  cudaCheck(cudaFree(d_out));\n\n  return 0;\n}\n"}}
{"kernel_name": "permute", "parallel_api": "omp", "code": {"main.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include \"common.h\"\n\n\n\nvoid permuate_cpu(float *inp, float *q, float *k, float *v, int B, int T, int C, int NH) {\n  int i = 0;\n  for (int b = 0; b < B; b++) {\n    for (int n = 0; n < NH; n++) {\n      for (int t = 0; t < T; t++) {\n        for (int c = n * (C/NH); c < (n+1)*(C/NH); c++) {\n          q[i] = inp[b * T * 3 * C + t * 3 * C + c];\n          k[i] = inp[b * T * 3 * C + t * 3 * C + C + c];\n          v[i] = inp[b * T * 3 * C + t * 3 * C + 2*C + c];\n          i++;\n        }}}}\n}\n\nvoid permute (float* out, const float* inp,\n    int B, int T, int C, int NH,\n    const int block_size) {\n  \n\n  int d = C / NH; \n\n\n  \n\n  float *q, *k, *v;\n  q = out + 0 * B * T * C;\n  k = out + 1 * B * T * C;\n  v = out + 2 * B * T * C;\n  int total_threads = B * T * C;\n  int num_blocks = ceil_div(total_threads, block_size);\n  \n\n  #pragma omp target teams distribute parallel for \\\n   num_teams(num_blocks) num_threads(block_size) \n  for (int idx = 0; idx < B * T * C; idx++) { \n    \n\n    int b = idx / (C * T);\n    int rest = idx % (C * T);\n    int nh_ = rest / (T * d);\n    rest = rest % (T * d);\n    int n = rest / d;\n    int d_ = rest % d;\n\n    int inp_idx = \\\n            (b * T * 3 * C)\n            +   (n * 3 * C)\n            +       (0 * C)\n            +          (nh_ * d)\n            +                d_;\n\n    q[idx] = inp[inp_idx];\n    k[idx] = inp[inp_idx + C];\n    v[idx] = inp[inp_idx + 2 * C];\n  }\n}\n\nint main(int argc, char **argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <batch size> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int B = atoi(argv[1]);\n  const int repeat_times = atoi(argv[2]);\n\n  const int T = 1024;\n  const int C = 768;\n  const int NH = 12;\n\n  size_t S = (size_t)B * T * C;\n\n  \n\n  float* inp = make_random_float(S * 3);\n  float* out = make_random_float(S * 3);\n  float* q = make_random_float(S);\n  float* k = make_random_float(S);\n  float* v = make_random_float(S);\n\n  permuate_cpu(inp, q, k, v, B, T, C, NH);\n\n  int block_sizes[] = {32, 64, 128, 256, 512};\n\n  #pragma omp target data map(to: inp[0:S*3]) map(alloc: out[0:S*3])\n  {\n    for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n      int block_size = block_sizes[j];\n      printf(\"Checking block size %d.\\n\", block_size);\n      permute (out, inp, B, T, C, NH, block_size);\n      validate_result(out, q, \"q\", S, 1e-6f);\n      validate_result(out+B*T*C, k, \"k\", S, 1e-6f);\n      validate_result(out+2*B*T*C, v, \"v\", S, 1e-6f);\n    }\n    printf(\"All results match. Starting benchmarks.\\n\\n\");\n\n    \n\n    for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n      int block_size = block_sizes[j];\n      float elapsed_time = benchmark_kernel(repeat_times, permute,\n          out, inp, B, T, C, NH, block_size);\n\n      printf(\"block_size %4d | time %f ms\\n\", block_size, elapsed_time);\n    }\n  }\n\n  \n\n  free(inp);\n  free(q);\n  free(k);\n  free(v);\n  free(out);\n\n  return 0;\n}\n"}}
{"kernel_name": "permute", "parallel_api": "serial", "code": {"main.cpp": "\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include \"common.h\"\n\n\n\nvoid permuate_cpu(float *inp, float *q, float *k, float *v, int B, int T, int C, int NH) {\n  int i = 0;\n  for (int b = 0; b < B; b++) {\n    for (int n = 0; n < NH; n++) {\n      for (int t = 0; t < T; t++) {\n        for (int c = n * (C/NH); c < (n+1)*(C/NH); c++) {\n          q[i] = inp[b * T * 3 * C + t * 3 * C + c];\n          k[i] = inp[b * T * 3 * C + t * 3 * C + C + c];\n          v[i] = inp[b * T * 3 * C + t * 3 * C + 2*C + c];\n          i++;\n        }}}}\n}\n\nvoid permute (float* out, const float* inp,\n    int B, int T, int C, int NH,\n    const int block_size) {\n  \n\n  int d = C / NH; \n\n\n  \n\n  float *q, *k, *v;\n  q = out + 0 * B * T * C;\n  k = out + 1 * B * T * C;\n  v = out + 2 * B * T * C;\n  int total_threads = B * T * C;\n  int num_blocks = ceil_div(total_threads, block_size);\n  \n\n    for (int idx = 0; idx < B * T * C; idx++) { \n    \n\n    int b = idx / (C * T);\n    int rest = idx % (C * T);\n    int nh_ = rest / (T * d);\n    rest = rest % (T * d);\n    int n = rest / d;\n    int d_ = rest % d;\n\n    int inp_idx = \\\n            (b * T * 3 * C)\n            +   (n * 3 * C)\n            +       (0 * C)\n            +          (nh_ * d)\n            +                d_;\n\n    q[idx] = inp[inp_idx];\n    k[idx] = inp[inp_idx + C];\n    v[idx] = inp[inp_idx + 2 * C];\n  }\n}\n\nint main(int argc, char **argv) {\n  if (argc != 3) {\n    printf(\"Usage: %s <batch size> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n  const int B = atoi(argv[1]);\n  const int repeat_times = atoi(argv[2]);\n\n  const int T = 1024;\n  const int C = 768;\n  const int NH = 12;\n\n  size_t S = (size_t)B * T * C;\n\n  \n\n  float* inp = make_random_float(S * 3);\n  float* out = make_random_float(S * 3);\n  float* q = make_random_float(S);\n  float* k = make_random_float(S);\n  float* v = make_random_float(S);\n\n  permuate_cpu(inp, q, k, v, B, T, C, NH);\n\n  int block_sizes[] = {32, 64, 128, 256, 512};\n\n    {\n    for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n      int block_size = block_sizes[j];\n      printf(\"Checking block size %d.\\n\", block_size);\n      permute (out, inp, B, T, C, NH, block_size);\n      validate_result(out, q, \"q\", S, 1e-6f);\n      validate_result(out+B*T*C, k, \"k\", S, 1e-6f);\n      validate_result(out+2*B*T*C, v, \"v\", S, 1e-6f);\n    }\n    printf(\"All results match. Starting benchmarks.\\n\\n\");\n\n    \n\n    for (size_t j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n      int block_size = block_sizes[j];\n      float elapsed_time = benchmark_kernel(repeat_times, permute,\n          out, inp, B, T, C, NH, block_size);\n\n      printf(\"block_size %4d | time %f ms\\n\", block_size, elapsed_time);\n    }\n  }\n\n  \n\n  free(inp);\n  free(q);\n  free(k);\n  free(v);\n  free(out);\n\n  return 0;\n}"}}
{"kernel_name": "swish", "parallel_api": "cuda", "code": {"main.cu": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <random>\n#include <cuda.h>\n#include \"reference.h\"\n\n#define GPU_THREADS 256\n\n#define KERNEL_LOOP(index, range) \\\n   for (int index = blockIdx.x * blockDim.x + threadIdx.x;  \\\n            index < (range); index += blockDim.x * gridDim.x) \n\ntemplate <typename T>\n__global__\nvoid SwishKernel(const int N, const T* X, T* Y)\n{\n  KERNEL_LOOP(i, N) {\n    Y[i] = __ldg(X + i) / (T(1) + exp(-__ldg(X + i)));\n  }\n}\n\ntemplate <typename T>\n__global__\nvoid SwishGradientKernel(\n    const int N,\n    const T* X,\n    const T* Y,\n    const T* dY,\n          T* dX)\n{\n  KERNEL_LOOP(i, N) {\n    dX[i] = __ldg(dY + i) *\n            (__ldg(Y + i) + (T(1) - __ldg(Y + i)) / (T(1) + exp(-__ldg(X + i))));\n  }\n}\n\ntemplate<typename T>\nvoid eval_swish (const int N, const int repeat) {\n\n  size_t size_bytes = N * sizeof(T); \n\n  T *h_X  = (T*) malloc (size_bytes);\n  T *h_Y  = (T*) malloc (size_bytes);\n  T *h_dY = (T*) malloc (size_bytes);\n  T *h_dX = (T*) malloc (size_bytes);\n  T *r_Y  = (T*) malloc (size_bytes);\n  T *r_dX = (T*) malloc (size_bytes);\n\n  std::default_random_engine gen (123);\n  std::uniform_real_distribution<float> distr (-2.f, 2.f);\n  for (int i = 0; i < N; i++) {\n    h_X[i] = distr(gen);\n    h_dY[i] = distr(gen);\n  }\n\n  T *d_X, *d_Y, *d_dX, *d_dY;\n  cudaMalloc((void**)&d_X, size_bytes);\n  cudaMemcpy(d_X, h_X, size_bytes, cudaMemcpyHostToDevice);\n\n  cudaMalloc((void**)&d_Y, size_bytes);\n\n  cudaMalloc((void**)&d_dY, size_bytes);\n  cudaMemcpy(d_dY, h_dY, size_bytes, cudaMemcpyHostToDevice);\n\n  cudaMalloc((void**)&d_dX, size_bytes);\n\n  dim3 grid ((N + GPU_THREADS - 1) / GPU_THREADS);\n  dim3 block (GPU_THREADS);\n\n  cudaDeviceSynchronize();\n  auto start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) \n    SwishKernel <<<grid, block>>> (N, d_X, d_Y);\n\n  cudaDeviceSynchronize();\n  auto end = std::chrono::steady_clock::now();\n  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average execution time of Swish kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  start = std::chrono::steady_clock::now();\n\n  for (int i = 0; i < repeat; i++) \n    SwishGradientKernel <<<grid, block>>> (N, d_X, d_Y, d_dY, d_dX);\n\n  cudaDeviceSynchronize();\n  end = std::chrono::steady_clock::now();\n  time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n  printf(\"Average execution time of SwishGradient kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n  \n\n  cudaMemcpy(h_dX, d_dX, size_bytes, cudaMemcpyDeviceToHost); \n  cudaMemcpy(h_Y, d_Y, size_bytes, cudaMemcpyDeviceToHost); \n\n  reference (N, h_X, r_Y, r_dX, h_dY);\n\n  bool ok = true;\n  for (int i = 0; i < N; i++) {\n    if (fabs(h_dX[i] - r_dX[i]) > 1e-3 || fabs(h_Y[i] - r_Y[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  cudaFree(d_X);\n  cudaFree(d_Y);\n  cudaFree(d_dX);\n  cudaFree(d_dY);\n\n  free(h_X);\n  free(h_Y);\n  free(h_dX);\n  free(h_dY);\n  free(r_dX);\n  free(r_Y);\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <number of elements> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int N = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n\n  eval_swish<float>(N, repeat);\n\n  return 0;\n}\n"}}
{"kernel_name": "swish", "parallel_api": "omp", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <random>\n#include \"reference.h\"\n\n#define GPU_THREADS 256\n\ntemplate <typename T>\nvoid SwishKernel(const int N, const T* X, T* Y)\n{\n  #pragma omp target teams distribute parallel for num_threads(GPU_THREADS)\n  for (int i = 0; i < N; i++) {\n    Y[i] = X[i] / (T(1) + exp(-X[i]));\n  }\n}\n\ntemplate <typename T>\nvoid SwishGradientKernel(\n    const int N,\n    const T* X,\n    const T* Y,\n    const T* dY,\n          T* dX)\n{\n  #pragma omp target teams distribute parallel for num_threads(GPU_THREADS)\n  for (int i = 0; i < N; i++) {\n    dX[i] = dY[i] * (Y[i] + (T(1) - Y[i]) / (T(1) + exp(-X[i])));\n  }\n}\n\ntemplate<typename T>\nvoid eval_swish (const int N, const int repeat) {\n\n  size_t size_bytes = N * sizeof(T); \n\n  T *h_X  = (T*) malloc (size_bytes);\n  T *h_Y  = (T*) malloc (size_bytes);\n  T *h_dY = (T*) malloc (size_bytes);\n  T *h_dX = (T*) malloc (size_bytes);\n  T *r_Y  = (T*) malloc (size_bytes);\n  T *r_dX = (T*) malloc (size_bytes);\n\n  std::default_random_engine gen (123);\n  std::uniform_real_distribution<float> distr (-2.f, 2.f);\n  for (int i = 0; i < N; i++) {\n    h_X[i] = distr(gen);\n    h_dY[i] = distr(gen);\n  }\n\n  #pragma omp target data map(to: h_X[0:N], h_dY[0:N]) \\\n                          map(from: h_Y[0:N], h_dX[0:N]) \n  {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) \n      SwishKernel(N, h_X, h_Y);\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average execution time of Swish kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n    start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) \n      SwishGradientKernel(N, h_X, h_Y, h_dY, h_dX);\n\n    end = std::chrono::steady_clock::now();\n    time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average execution time of SwishGradient kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n  }\n\n  \n\n  reference (N, h_X, r_Y, r_dX, h_dY);\n\n  bool ok = true;\n  for (int i = 0; i < N; i++) {\n    if (fabs(h_dX[i] - r_dX[i]) > 1e-3 || fabs(h_Y[i] - r_Y[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  free(h_X);\n  free(h_Y);\n  free(h_dX);\n  free(h_dY);\n  free(r_dX);\n  free(r_Y);\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <number of elements> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int N = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n\n  eval_swish<float>(N, repeat);\n\n  return 0;\n}\n"}}
{"kernel_name": "swish", "parallel_api": "serial", "code": {"main.cpp": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <chrono>\n#include <random>\n#include \"reference.h\"\n\n#define GPU_THREADS 256\n\ntemplate <typename T>\nvoid SwishKernel(const int N, const T* X, T* Y)\n{\n    for (int i = 0; i < N; i++) {\n    Y[i] = X[i] / (T(1) + exp(-X[i]));\n  }\n}\n\ntemplate <typename T>\nvoid SwishGradientKernel(\n    const int N,\n    const T* X,\n    const T* Y,\n    const T* dY,\n          T* dX)\n{\n    for (int i = 0; i < N; i++) {\n    dX[i] = dY[i] * (Y[i] + (T(1) - Y[i]) / (T(1) + exp(-X[i])));\n  }\n}\n\ntemplate<typename T>\nvoid eval_swish (const int N, const int repeat) {\n\n  size_t size_bytes = N * sizeof(T); \n\n  T *h_X  = (T*) malloc (size_bytes);\n  T *h_Y  = (T*) malloc (size_bytes);\n  T *h_dY = (T*) malloc (size_bytes);\n  T *h_dX = (T*) malloc (size_bytes);\n  T *r_Y  = (T*) malloc (size_bytes);\n  T *r_dX = (T*) malloc (size_bytes);\n\n  std::default_random_engine gen (123);\n  std::uniform_real_distribution<float> distr (-2.f, 2.f);\n  for (int i = 0; i < N; i++) {\n    h_X[i] = distr(gen);\n    h_dY[i] = distr(gen);\n  }\n\n    {\n    auto start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) \n      SwishKernel(N, h_X, h_Y);\n\n    auto end = std::chrono::steady_clock::now();\n    auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average execution time of Swish kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n\n    start = std::chrono::steady_clock::now();\n\n    for (int i = 0; i < repeat; i++) \n      SwishGradientKernel(N, h_X, h_Y, h_dY, h_dX);\n\n    end = std::chrono::steady_clock::now();\n    time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n    printf(\"Average execution time of SwishGradient kernel: %f (us)\\n\", (time * 1e-3f) / repeat);\n  }\n\n  \n\n  reference (N, h_X, r_Y, r_dX, h_dY);\n\n  bool ok = true;\n  for (int i = 0; i < N; i++) {\n    if (fabs(h_dX[i] - r_dX[i]) > 1e-3 || fabs(h_Y[i] - r_Y[i]) > 1e-3) {\n      ok = false;\n      break;\n    }\n  }\n  printf(\"%s\\n\", ok ? \"PASS\" : \"FAIL\");\n\n  free(h_X);\n  free(h_Y);\n  free(h_dX);\n  free(h_dY);\n  free(r_dX);\n  free(r_Y);\n}\n\nint main(int argc, char* argv[])\n{\n  if (argc != 3) {\n    printf(\"Usage: %s <number of elements> <repeat>\\n\", argv[0]);\n    return 1;\n  }\n\n  const int N = atoi(argv[1]);\n  const int repeat = atoi(argv[2]);\n\n  eval_swish<float>(N, repeat);\n\n  return 0;\n}"}}
